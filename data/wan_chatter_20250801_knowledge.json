{
  "channel": "wan_chatter",
  "date_range": "2025-08-01 to 2025-09-01",
  "messages_processed": 40080,
  "chunks_processed": 101,
  "api_usage": {
    "input_tokens": 1258646,
    "output_tokens": 260563,
    "estimated_cost": 7.684383
  },
  "extracted_at": "2026-02-03T14:54:23.199415Z",
  "discoveries": [
    {
      "finding": "VACE 2.2 is functional",
      "details": "Successfully working with openpose as controlnet, though results can be strange with limited controlnet types",
      "from": "GOD_IS_A_LIE"
    },
    {
      "finding": "Wan 2.2 can handle inpainting well",
      "details": "Model is smart enough to solve inpainting puzzles at very high denoise (0.7-0.8), going from nonsense to near perfect results",
      "from": "Shawneau \ud83c\udf41 [CA]"
    },
    {
      "finding": "GGUF Q3 may outperform FP8 in some cases",
      "details": "User observed better quality with GGUF Q3 compared to FP8 version",
      "from": "hicho"
    },
    {
      "finding": "Denoise parameter affects T2V generation significantly",
      "details": "Changing denoise from default to 0.5, 0.7 produces dramatically different results in custom node implementations",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "WANVideo wrapper now skips T5 loading when prompt is cached",
      "details": "The text encoder won't assign weights if the prompt was cached, even if T5 is connected, improving efficiency",
      "from": "Kijai"
    },
    {
      "finding": "WAN 2.2 supports multi-scene generation without EchoShot",
      "details": "Can write multiple scenes separated by | in prompts, works natively with 2.2 without needing separate models",
      "from": "Yan"
    },
    {
      "finding": "Q4 quantized model produces good quality results",
      "details": "Tested Q4 model shows surprisingly good quality output",
      "from": "avataraim"
    },
    {
      "finding": "Context windows work well with WAN 2.2",
      "details": "Using context options with | separator produces smooth scene transitions in long videos",
      "from": "thaakeno"
    },
    {
      "finding": "T2V is more dynamic and realistic than I2V",
      "details": "T2V generates more varied and realistic motion compared to I2V which often starts slower",
      "from": "seitanism"
    },
    {
      "finding": "Radial attention is faster than sage but requires quality tweaking",
      "details": "RadialAttention provides speed improvements but needs careful settings adjustment to maintain quality",
      "from": "Kijai"
    },
    {
      "finding": "WAN 2.2 frame limit for I2V is 109 frames instead of 81",
      "details": "From 113 frames it begins to reverse/loop. Maximum is 109 frames giving +1.5 seconds of video",
      "from": "N0NSens"
    },
    {
      "finding": "Context options can generate 15 second videos successfully",
      "details": "15 sec, 3 context windows achieved, though first context change is quite noticeable",
      "from": "Cseti"
    },
    {
      "finding": "Different resolutions affect motion quality in I2V",
      "details": "480x720 has more motion, 576x1024 looks like slowmo. WAN 2.2 14B performs best at maximum resolution of 1024\u00d7576",
      "from": "shockgun"
    },
    {
      "finding": "Stride settings help with window blending in context options",
      "details": "Stride on high noise model helps window blending, no stride on low noise fixes it mostly. Successfully used 10 stride, 48 overlap",
      "from": "Kijai"
    },
    {
      "finding": "Models from Kijai repo avoid black output issue",
      "details": "Black output occurs when using wrapper with ComfyUI provided models, using models from Kijai repo fixes this",
      "from": "crinklypaper"
    },
    {
      "finding": "WAN 2.2 shows improved leaf/foliage noise compared to 2.1",
      "details": "Noise in leaves was common in 2.1 generations, not happening as much in 2.2",
      "from": "Juan Gea"
    },
    {
      "finding": "3-stage sampling works well for I2V",
      "details": "9 Steps: 3H no Lora, 3H LightX2V@3, 3L@1",
      "from": "BobbyD4AI"
    },
    {
      "finding": "CFG 3.5 technique improves adherence",
      "details": "Using CFG 3.5 on high pass improves prompt adherence",
      "from": "Juan Gea"
    },
    {
      "finding": "FP8 model has better adherence than quants",
      "details": "Better luck with adherence using the fp8 model instead of the quants, and also steps above 2 for the high",
      "from": "nacho.money"
    },
    {
      "finding": "5B model can do ultrawide generation",
      "details": "Wan 2.2 ultrawide test at 256x2048 resolution",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "finding": "Loop args only work on 1.3B model",
      "details": "Loop/long video strategy only ever worked on the 1.3B model, didn't blend well with anything else",
      "from": "Kijai"
    },
    {
      "finding": "Context windows work with 5B model",
      "details": "5B with context windows, passing the image to each window - interesting results",
      "from": "Kijai"
    },
    {
      "finding": "6 steps needed for low noise upscaling",
      "details": "You need 6 steps of low noise to clean a wan 2.2 high noise generation for an upscaling latent by 2",
      "from": "GOD_IS_A_LIE"
    },
    {
      "finding": "Different samplers needed for high/low noise",
      "details": "You need to use euler for the low noise sampler and another one for the high noise generation",
      "from": "GOD_IS_A_LIE"
    },
    {
      "finding": "BF16 weights may be better than FP16 for Wan 2.2",
      "details": "Testing shows BF16 has max error 0.003890 vs FP16 max error 0.748233 when converting. Original code runs at FP32.",
      "from": "Benjimon"
    },
    {
      "finding": "Token padding with numbers can improve motion control",
      "details": "Adding string '1234567898765432123456789' between prompt elements may help control motion better in I2V generations",
      "from": "Rainsmellsnice"
    },
    {
      "finding": "Context options work with piped prompts",
      "details": "Automatically spreads piped prompts evenly over frame count, per window basis",
      "from": "Kijai"
    },
    {
      "finding": "F16 accumulation makes massive performance difference",
      "details": "Significantly improves generation speed",
      "from": "AJO"
    },
    {
      "finding": "Manual prompt scheduling works well without prompt scheduler node",
      "details": "Breaking down shots into frame stamps and adding camera movements directly in positive prompt works well for camera control",
      "from": "Bleedy (Madham)"
    },
    {
      "finding": "First-Last-Frame works with Wan 2.2",
      "details": "Works fantastic even without positive prompt, AI knows what to do motion wise",
      "from": "Lodis"
    },
    {
      "finding": "Shift affects motion characteristics",
      "details": "High shift on high model cures slow motion",
      "from": "Simjedi"
    },
    {
      "finding": "Different shift values create style variations while retaining same actions",
      "details": "Shift 12 stable, shift 5 adds detail, shift 3 makes subjects older/crabby, shift 1 creates abominations",
      "from": "nacho.money"
    },
    {
      "finding": "Context windows work better with different settings for high/low noise models",
      "details": "Low noise side doesn't need as much overlap, making it faster. Staggering overlaps helps with blending",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 14B model works well for V2V upscaling from 480p to 1080p",
      "details": "Successfully upscaled video while fixing LED lightbar issues on cars",
      "from": "thaakeno"
    },
    {
      "finding": "High noise vs low noise models produce different results during upscaling",
      "details": "Low noise model shows better detail for people in background during upscaling",
      "from": "thaakeno"
    },
    {
      "finding": "Start step parameter controls how much video changes during V2V",
      "details": "Lower start step (like 4 out of 12) changes more content but can fix motion, higher values stay closer to input",
      "from": "thaakeno"
    },
    {
      "finding": "Wan models can generate camera operator shadows in video",
      "details": "Model adds realistic camera shadows as if hiring a camera operator in latent space",
      "from": "N0NSens"
    },
    {
      "finding": "Chinese prompts may work better for T2V but not I2V in Wan 2.1",
      "details": "Native Chinese speaker reported Chinese worked well for T2V but not I2V",
      "from": "JohnDopamine"
    },
    {
      "finding": "More steps on low noise pass improves T2I quality",
      "details": "30 steps on low noise and 20 on high noise results in better sharpness and quality",
      "from": "\ud83d\udc1d bumblebee \ud83d\udc1d"
    },
    {
      "finding": "Qwen 2.5 integrated for prompt extension with zero memory usage after completion",
      "details": "Added to wrapper for local prompt enhancement using original system prompts",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 I2V supports First Frame Last Frame (FLF) natively",
      "details": "Unlike 2.1, Wan 2.2 I2V can work with first and last frame inputs without needing separate models",
      "from": "Juampab12"
    },
    {
      "finding": "Wan 2.2 can be used for V2V upscaling",
      "details": "Using high start step (like step 9 out of 12) to minimize changes while upscaling",
      "from": "thaakeno"
    },
    {
      "finding": "Can combine Wan 2.2 high + 2.1 low for smooth refinement",
      "details": "Using high 2.2 + 2.1 with 2 steps denoise of 0.10 to smooth the high while keeping its look",
      "from": "hicho"
    },
    {
      "finding": "First frame + last frame conditioning works well with Wan 2.2 for 161 frames (10 seconds)",
      "details": "Works pretty well but is slow to generate",
      "from": "comfy"
    },
    {
      "finding": "Non-guiding frames are grey in Wan 2.2 - can manually feed grey and normal frames as start image",
      "details": "For 2.2 first frame only: [first frame] + [x grey frames], last frame: [x grey frames] + [last frame]",
      "from": "comfy"
    },
    {
      "finding": "Middle frame conditioning works with proper implementation",
      "details": "Kijai confirmed middle frame works, showing start to end to start transitions",
      "from": "Kijai"
    },
    {
      "finding": "Sage attention provides minimal quality difference but massive speed difference vs flash attention",
      "details": "Sage attention recommended over flash attention for speed",
      "from": "Kijai"
    },
    {
      "finding": "Context options with stride 8+ improves continuity for longer videos",
      "details": "Increasing stride to 8 or more improves continuity with context options",
      "from": "xwsswww"
    },
    {
      "finding": "VACE 2.2 compatibility is limited",
      "details": "Some modalities don't work at all, some work weakly. Nothing worth using over 2.1 VACE for high noise model",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 naturally supports first-last-frame morphing without special implementation",
      "details": "Model itself handles FLF without needing special fork or fancy implementation, works with both methods",
      "from": "Kijai"
    },
    {
      "finding": "Different prompts on high vs low noise passes - low noise pass ignores different prompts",
      "details": "Using completely different prompt on low noise pass didn't add or change anything",
      "from": "Kagi"
    },
    {
      "finding": "Video models can handle random training techniques that wouldn't work on image models",
      "details": "Video models just figure things out, can get away with techniques that probably wouldn't work in image models",
      "from": "Fill"
    },
    {
      "finding": "Wan 2.2 model architecture allows parallel video generation",
      "details": "20 videos at 512 bucketed resolution, 49 frames rendered in 6 minutes total",
      "from": "Fill"
    },
    {
      "finding": "bf16 precision produces better quality than fp16 and fp8",
      "details": "Visual comparison shows bf16 has most details in generated videos, though sometimes 'goes too much' on certain elements",
      "from": "Kijai"
    },
    {
      "finding": "Original Alibaba code converts model to bf16 not fp16",
      "details": "The original code uses bf16 and doesn't quantize the VAE at all, producing higher quality than ComfyUI",
      "from": "aikitoria"
    },
    {
      "finding": "Wan 2.2 original VAE is designed for higher resolution",
      "details": "The new VAE doubles compression on each axis, suggesting it's meant for 2560x1440 video generation which hasn't been released",
      "from": "aikitoria"
    },
    {
      "finding": "Mixed precision models preserve quality",
      "details": "Using fp32 for norms and fp16 for weights (99% fp16, 1% fp32) improves listening/adherence to prompts",
      "from": "Benjimon"
    },
    {
      "finding": "First-Frame-Last-Frame (FFLF) feature works well for morphing",
      "details": "FFLF allows morphing between start and end images, produces good transitions with some fog/dust in middle parts",
      "from": "MACRO"
    },
    {
      "finding": "FP8 vs BF16 quantization testing shows minimal differences in quality",
      "details": "FP8 sometimes performs better than BF16 in specific scenarios - BF16 character didn't eat ice cream while FP8 did, FP8 had leaves swinging in wind while BF16 had static pines",
      "from": "Kijai"
    },
    {
      "finding": "Batch size affects temporal stability",
      "details": "Higher batch size leads to more temporally stable video generation according to official repo",
      "from": "MiGrain"
    },
    {
      "finding": "Wan 2.2 is essentially 28B parameters total",
      "details": "The combined size of high and low noise models makes it massive",
      "from": "Draken"
    },
    {
      "finding": "Video models quantize very well",
      "details": "Models are larger than needed considering how well they compress with quantization",
      "from": "Draken"
    },
    {
      "finding": "Character LoRA trained in 9 minutes with good results",
      "details": "Low_noise_model T2V 14B, 24 source images at 512px resolution, training time 9min with min_t=0, max_t=1",
      "from": "Kenk"
    },
    {
      "finding": "5B model can upscale 14B output effectively",
      "details": "1280x768 output from 14B can be upscaled to 1090x1152 with 5B using 0.5 denoise, providing same starting frame and simple quality prompts",
      "from": "Juan Gea"
    },
    {
      "finding": "Maximum resolution achieved: 2560x1536 for 81 frames",
      "details": "Upscaling from 14B at 1600x960 using 5B model",
      "from": "Juan Gea"
    },
    {
      "finding": "Sigma value 0.875 intended for model switching",
      "details": "Wan team intended the high/low noise model switch at sigma 0.875, occurs at step 8/20 with shift=5.0, step 10/20 with shift=8.0",
      "from": "Ablejones"
    },
    {
      "finding": "Model doesn't like changing object states",
      "details": "Difficult to get model to show ice cream being stolen - it maintains woman has ice cream even when it should be gone",
      "from": "Kijai"
    },
    {
      "finding": "Prompt scheduling works with context options in wrapper",
      "details": "Can use format [1] Prompt 1, [2] Prompt 2 for extending videos - cuts frames uniformly and applies prompt to each segment",
      "from": "mamad8"
    },
    {
      "finding": "LoRA timestep scheduling added to wrapper",
      "details": "Only works when unmerged, can schedule per step (0 skips applying it altogether on that step), enables curves and dropoff control",
      "from": "Kijai"
    },
    {
      "finding": "Disabling LoRA on first step improves quality",
      "details": "Since lightx2v doesn't work properly with high noise model, do first step(s) with cfg instead without LoRA loaded",
      "from": "Kijai"
    },
    {
      "finding": "Wan understands multiple languages well",
      "details": "Sometimes even better than English - Finnish prompts worked with good understanding",
      "from": "Daflon"
    },
    {
      "finding": "Chinese prompts give better prompt adherence on Wan 2.2",
      "details": "Heard to give way better prompt adherence compared to English prompts",
      "from": "thaakeno"
    },
    {
      "finding": "Shift 1 produces better T2I results than default shift values",
      "details": "Using shift 1 for text-to-image generation gives perfectly sharp images, even though this doesn't respect the intended boundary between high/low noise models",
      "from": "aikitoria"
    },
    {
      "finding": "LightX LoRA helps add details in T2I",
      "details": "Using LightX at 0.5 strength on second pass with 20 steps each improves detail quality compared to no LoRA with 40 steps",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Prompting has much stronger impact in Wan 2.2 than 2.1",
      "details": "The prompt has a much stronger impact on output quality in Wan 2.2, with detailed Claude-generated prompts producing better results on first try than manual prompting",
      "from": "Alisson Pereira"
    },
    {
      "finding": "Shift value dramatically affects Wan 2.2 output quality",
      "details": "Shift 12 matches original Alibaba code timesteps but may cause too much movement/instability. Users found different optimal shift values work better in practice.",
      "from": "aikitoria"
    },
    {
      "finding": "T2V and I2V use opposite step ratios in official implementation",
      "details": "T2V uses 25 steps high noise, 15 steps low noise. I2V uses 15 steps high noise, 25 steps low noise. Makes sense because I2V already has image so needs less composition steps.",
      "from": "aikitoria"
    },
    {
      "finding": "LightX2V LoRA changes Wan 2.2 behavior significantly",
      "details": "Using LightX2V makes 2.2 output more like 2.1 with less realistic motion. Even one step without LightX gives vastly more 'real' looking motion.",
      "from": "Draken"
    },
    {
      "finding": "Beta scheduler works better than simple scheduler at shift 12",
      "details": "Beta scheduler has more gradual dropoff which helps when using LoRA for low noise section only",
      "from": "Ablejones"
    },
    {
      "finding": "Wan 2.2 5B last frame encoding works when done separately",
      "details": "Previously thought it didn't work at all, but encoding the last frame separately makes it functional",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 High Noise Expert provides significantly better motion than 5B model",
      "details": "Quality is fine on 5B but the motion doesn't come close to what the high noise can do",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.1 LoRAs work on Wan 2.2 Low Noise with minimal compatibility issues",
      "details": "You can use old loras on 2.2 low mostly just fine, but they fall short when character is wide-distance or heavy motion scenes",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 Lightning LoRAs had hardcoded alpha values causing issues",
      "details": "Alpha 8, rank 64 = 0.125 strength. Original LoRAs didn't include alpha and had it hardcoded in the code",
      "from": "Kijai"
    },
    {
      "finding": "Higher shift values needed for Wan due to linear scheduling",
      "details": "Wan is linear and needs a high shelf at start. Shift around 8-9 is good for short steps",
      "from": "Simjedi"
    },
    {
      "finding": "VACE strength affects generation behavior significantly",
      "details": "High strength like 1 tries to put your ref or prompt subject on the masked area. Lowering the strength gives it more freedom to give a more realistic form",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "finding": "New Wan 2.2 Lightning LoRAs work at 1.0 strength instead of 0.125",
      "details": "Kijai's fixed versions can be used at 1.0 strength making them easier to control",
      "from": "Kijai"
    },
    {
      "finding": "Lightning LoRAs work better on high noise side only",
      "details": "There's no real need to run the new lightning LoRA on the low noise side",
      "from": "Kijai"
    },
    {
      "finding": "Old lightx2v can still be used on low noise side for styling",
      "details": "You can use the old lightx2v on the low side if you prefer it, as it affects the final look and detail level",
      "from": "Kijai"
    },
    {
      "finding": "Single CFG 3.5 step without LoRA improves lighting",
      "details": "Better lighting results with: 1 step w/ cfg=3.5 (no lora), then remaining steps with cfg=1 and new LoRAs",
      "from": "IceAero"
    },
    {
      "finding": "3+1 sigma split works better than even splits",
      "details": "For 4 total steps, 3 high noise steps and 1 low noise step works better than 2+2",
      "from": "Kijai"
    },
    {
      "finding": "New LightX2V Lightning LoRAs for Wan 2.2 released with 4-step generation capability",
      "details": "Wan 2.2 Lightning LoRAs can generate videos in 4 steps vs previous higher step counts",
      "from": "gokuvonlange"
    },
    {
      "finding": "Old LightX2V 2.1 LoRA works better on high noise pass than new 2.2 Lightning LoRA",
      "details": "Mixing old 2.1 LoRA on high noise with new 2.2 Lightning on low noise produces better results",
      "from": "Doctor Shotgun"
    },
    {
      "finding": "New Lightning LoRAs do not include text embed layers",
      "details": "Lightning LoRA missing text embed layers which have significant effect on LightX2V performance",
      "from": "Kijai"
    },
    {
      "finding": "720p resolution works better than 480p for Lightning LoRAs",
      "details": "480p results in slow motion and weird artifacts, 720p produces faster motion",
      "from": "Kijai"
    },
    {
      "finding": "Lower LoRA strength values on first steps of generation produce less fake-looking results",
      "details": "Using 0.4 strength for high LightX and 0.6 for low LightX on early steps looks more natural",
      "from": "PATATAJEC"
    },
    {
      "finding": "Lightning LoRA strength scheduling with list of floats",
      "details": "Can now provide a list of float values for LoRA strength scheduling across steps in the wrapper",
      "from": "Kijai"
    },
    {
      "finding": "FastWan 5B model performance",
      "details": "FastWan 5B can generate 121 frames at 1280x704 in ~14 seconds sampling + ~20 seconds decode, total ~34 seconds",
      "from": "Kijai"
    },
    {
      "finding": "Lightning LoRA works better on Low Noise model",
      "details": "Lightning 2.2 LoRA for low noise model performs well, not much difference vs no LoRA at higher CFG/more steps",
      "from": "gokuvonlange"
    },
    {
      "finding": "GGUF model quality correlation",
      "details": "Higher GGUF model sizes (Q8 vs Q6_K) provide better prompt adherence and video quality",
      "from": "Kijai"
    },
    {
      "finding": "Using both lightx2v and lightning loras together can improve motion",
      "details": "When applied like the lightx2v lora, camera motion improves but character motion decreases. Using both loras could fix the motion altogether",
      "from": "Rainsmellsnice"
    },
    {
      "finding": "New lightning lora degrades quality and motion significantly",
      "details": "The new lora honestly really degrades the quality and motion, only use it on the low noise model if you want full motion",
      "from": "flo1331"
    },
    {
      "finding": "Chinese text prompts may work with Wan models",
      "details": "Model prompts in Chinese just fine, wouldn't say it's any better or worse than English. Text encoder being better in one language for specific words due to English ambiguity",
      "from": "TK_999"
    },
    {
      "finding": "T2V loras can work on I2V with some success",
      "details": "T2V loras like fastwan work on I2V, kind of weird what works with what and what doesn't",
      "from": "mdkb"
    },
    {
      "finding": "Torch compile reduces VRAM and speeds up generation with no quality impact",
      "details": "It has no quality impact, reduces VRAM used and speeds up generation, win-win. Hard part is installing Triton on Windows",
      "from": "Kijai"
    },
    {
      "finding": "LightX2V 2.1 loras work better than new Lightning loras",
      "details": "Multiple users report switching back to LightX2V after testing Lightning",
      "from": "screwfunk"
    },
    {
      "finding": "Mixing old and new LightX loras gives good motion results",
      "details": "New lora in high (1 strength) and old lora in low (1.5 strength), 7-8 steps total, both cfg 1",
      "from": "Ashtar"
    },
    {
      "finding": "High model encodes significant data into latents",
      "details": "Base colors and motion are already there after 3 steps on high noise side",
      "from": "Kijai"
    },
    {
      "finding": "2.1 loras work differently on high vs low models",
      "details": "2.1 loras need 3x strength on high model but work normally on low model, low model is basically just 2.1",
      "from": "Kijai"
    },
    {
      "finding": "Qwen Image VAE is finetune of Wan 2.1 VAE",
      "details": "New Qwen image model reuses wan's VAE encoder/latent space with new image decoder, significantly higher reconstruction quality",
      "from": "spacepxl"
    },
    {
      "finding": "Wan 2.2 FLF (First-Frame-Last-Frame) produces exceptional quality",
      "details": "Multiple users report Wan 2.2 FLF as superior to other FLF implementations, with excellent character consistency and environment generation",
      "from": "thaakeno"
    },
    {
      "finding": "Stacking specific LoRAs dramatically improves Wan 2.2 I2V quality",
      "details": "Using phantom fusionx + pusa + lightx2v LoRAs together provides much better I2V results than lightning LoRA alone",
      "from": "Ada"
    },
    {
      "finding": "Lightning LoRA falls apart on complex prompts",
      "details": "The new lightning LoRA works fine for simple prompts but fails on complex scenarios that base Wan 2.2 handles well",
      "from": "Kijai"
    },
    {
      "finding": "Using cfg with lightx2v LoRA enables 2.2-style motion",
      "details": "Combining lightx2v LoRA with cfg provides the characteristic Wan 2.2 motion quality, though it's slower at 6 steps",
      "from": "Kijai"
    },
    {
      "finding": "Scene cutting technique for subject preservation",
      "details": "Can input an image and use prompts like 'the scene abruptly hard cuts to' or 'the scene immediately changes to' to maintain subject resemblance without VACE or Phantom. Works with prompt structure: Original Subject Description + Scene Cut Prompt + New Scene Description",
      "from": "Jonathan"
    },
    {
      "finding": "Lightning LoRAs work better with proper timestep scheduling",
      "details": "Lightning 2.2 requires euler/simple with shift of 5.0 and flowmatch_distill scheduler matching their timesteps for 4 steps",
      "from": "Kijai"
    },
    {
      "finding": "dpmpp_2m sampler much faster on Wan 5B",
      "details": "dpmpp_2m is 2-4x faster than other samplers like euler on Wan 5B native, though output quality may be compromised",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Multiple VACE processing behavior",
      "details": "When using same reference image in both VACE embeds, it's calculated twice and can't be separated. Mask part is especially problematic when using multiple VACEs",
      "from": "Kijai"
    },
    {
      "finding": "WAN and Qwen-Image VAEs are virtually identical",
      "details": "Testing shows 99.98% compatibility with MSE of 0.000040 and PSNR of 43.95dB. They share same architecture components, hyperparameters, and latent statistics",
      "from": "fredbliss"
    },
    {
      "finding": "You can decode WAN videos with Qwen-Image VAE",
      "details": "Cross-compatibility works - can generate WAN video and decode with Qwen-Image VAE, producing interesting results",
      "from": "fredbliss"
    },
    {
      "finding": "Two reference images at different aspect ratios work well with WAN 2.2",
      "details": "Using 2 reference images with WAN 2.2 at different aspect ratios produces good results for scene transitions",
      "from": "Jonathan"
    },
    {
      "finding": "Scene transition prompts maintain character resemblance in I2V",
      "details": "Using prompts like 'the scene immediately and abruptly cuts to' with character descriptions maintains likeness better than other methods",
      "from": "Jonathan"
    },
    {
      "finding": "VAE tile size optimization significantly reduces decode time",
      "details": "Using tile sizes 512x384 instead of default 272x144 cuts VAE decoding time in half on 16GB VRAM",
      "from": "patientx"
    },
    {
      "finding": "Quantizing Qwen2.5-VL encoder has much larger impact than VAE differences",
      "details": "PSNR drops to 35.17dB when quantizing the encoder vs 43.95dB for VAE differences",
      "from": "fredbliss"
    },
    {
      "finding": "Qwen VAE doesn't work for video decoding",
      "details": "The qwen vae doesn't work for video output, though you can use it for encoding",
      "from": "aikitoria"
    },
    {
      "finding": "Qwen image has superior text rendering",
      "details": "Text in qwen image is insanely better - they dynamically created ppts to train off of",
      "from": "fredbliss"
    },
    {
      "finding": "Adding 'on a white background' improves reference consistency",
      "details": "Makes it properly use the reference consistently for I2V workflows",
      "from": "Jonathan"
    },
    {
      "finding": "Different LightX ranks for different workflows",
      "details": "T2I uses rank 64, I2V uses rank 32",
      "from": "N0NSens"
    },
    {
      "finding": "Wan 2.2 14B is native 16FPS",
      "details": "The 14B model generates at 16fps natively, while 5B can do 720p 24fps",
      "from": "Kijai"
    },
    {
      "finding": "5B model has different latent space requirements",
      "details": "Some resolutions won't work - 1280x720 won't work but 1280x704 will",
      "from": "Kijai"
    },
    {
      "finding": "WAN 2.2 5B model performs better with hands than Kling Master",
      "details": "Even though limited to 1280x720 locally on 4090, WAN 2.2 produces better hand generation than Kling Master at 1920x1080 5s generation",
      "from": "Persoon"
    },
    {
      "finding": "WAN 2.2 5B VAE is significantly heavier than 2.1",
      "details": "5B VAE is 4x heavier at least - 2.1 fp32 is 480mb vs 1.4GB for 2.2, with 8x8 vs 16x16 architecture",
      "from": "Kijai"
    },
    {
      "finding": "Lower shift values produce sharper images",
      "details": "Shift 0.5 produces much sharper results than shift 5.0, especially for image generation",
      "from": "Kijai"
    },
    {
      "finding": "QwenImage outputs can now be used directly in WAN wrapper",
      "details": "Added functionality to use QwenImage latent outputs directly without PNG write, includes scaling back for compatibility",
      "from": "Kijai"
    },
    {
      "finding": "Specific prompt structure maintains character consistency",
      "details": "Using detailed character descriptions with 'maintains exact appearance' and hard cut transitions helps retain similarity across scenes",
      "from": "Juampab12"
    },
    {
      "finding": "Kijai's custom prompt splitting method works better than EchoShot implementation",
      "details": "Using | character for prompt splitting works as well or better than EchoShot's method, and worked before EchoShot came out. EchoShot implementation often doesn't trigger properly",
      "from": "Kijai"
    },
    {
      "finding": "EchoShot may just be a finetune with better weights",
      "details": "Results suggest EchoShot works better with their specific weights rather than adding novel functionality",
      "from": "Kijai"
    },
    {
      "finding": "VACE 2.2 compatibility issues traced to patch_embedding differences",
      "details": "Big part of VACE not working on 2.2 is that the high noise patch_embedding is totally different from 2.1",
      "from": "Kijai"
    },
    {
      "finding": "Lowering VACE strength gives model room for coherence",
      "details": "More strength in first steps to fix composition and movement, lower strength in later steps allows model to build its own coherence",
      "from": "Nekodificador"
    },
    {
      "finding": "Reference image background affects results significantly",
      "details": "Using SDXL refined reference instead of original reference dramatically improved results",
      "from": "Nekodificador"
    },
    {
      "finding": "Using DisableNoise node instead of RandomNoise when continuing generation",
      "details": "When continuing a generation from saved latent, use DisableNoise node instead of RandomNoise to avoid adding unnecessary noise that won't be removed in remaining steps",
      "from": "Ablejones"
    },
    {
      "finding": "Merging VACE 2.1 with Wan 2.2 models for reference following",
      "details": "Can merge VACE 2.1 patch embedding and block 0 with 2.2 models to get reference following capability, though motion quality may be affected",
      "from": "Kijai"
    },
    {
      "finding": "Block 0 adjustment affects identity vs motion tradeoff",
      "details": "Adjusting block 0 can improve identity matching but reduces motion quality - need to balance the merge weights",
      "from": "Kijai"
    },
    {
      "finding": "Early blocks have strongest effect in model merging",
      "details": "Early transformer blocks typically control composition while later blocks handle style/refinement",
      "from": "Kijai"
    },
    {
      "finding": "2.2 14B can handle up to 109 frames for I2V",
      "details": "While trained on 81 frames, can generate up to 109 frames with I2V, though motion may loop back after training limit",
      "from": "N0NSens"
    },
    {
      "finding": "LightX2V LoRA from version 2.1 performs better on T2V than the 2.2 Lightning",
      "details": "Doesn't reduce prompt adherence as much. Used strength 2 for high noise model and 1 for low noise model with faithful element following",
      "from": "shuzhi"
    },
    {
      "finding": "Perfect cinematic resolution found for radial attention",
      "details": "1216x512 (21:9 ratio) with 350s render time provides good balance between quality/speed",
      "from": ": Not Really Human :."
    },
    {
      "finding": "FP8 handles block swap better than GGUFs for higher resolutions",
      "details": "Achieved 1600x900x81 frames on 3060 12GB where before was limited to 41 frames",
      "from": "mdkb"
    },
    {
      "finding": "VACE frames need to be black for proper conditioning",
      "details": "White or grey frames cause major issues. Black frames make huge, reliable difference",
      "from": "Piblarg"
    },
    {
      "finding": "Speed LoRAs have colossal effect on changing image style",
      "details": "LightX turns everything into anime. Lightning and LightX have significant style influence beyond just speed",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "finding": "Waver 12B model as good as Veo3",
      "details": "New 12B DiT model using flow matching, performs on same level as Veo3 on video arena, uses 32B Qwen2.5 text encoder",
      "from": "yi"
    },
    {
      "finding": "ComfyUI frontend update broke VHS previews",
      "details": "Latest ComfyUI frontend package caused preview issues that were fixed by updating VHS nodes",
      "from": "Kijai"
    },
    {
      "finding": "Add noise setting creates dreamy effects in FlF2V",
      "details": "Using add noise with higher shift values (8-10) in FlF2V creates dreamy morphing effects when using LoRAs",
      "from": "piscesbody"
    },
    {
      "finding": "LightX2V 2.1 outperforms 2.2 Lightning LoRA",
      "details": "Lightx2v (high noise strength set to 2) consistently outperforms the 2.2 lighting one",
      "from": "shuzhi"
    },
    {
      "finding": "Frame count affects style consistency",
      "details": "More frames move generation closer to prompt - frame 1 is comical, frame 5 is mixed, frame 9 moves to realism",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "finding": "VACE character swapping works with Wan 2.2",
      "details": "VACE module with Low noise fp8_e5m2 Wan t2v 14B model with image ref and SAM2 points editor swaps characters perfectly",
      "from": "mdkb"
    },
    {
      "finding": "New T2V 1.1 LoRA works better on I2V workflows than I2V LoRA",
      "details": "Multiple users report better results using the T2V 1.1 LoRA for I2V generation compared to the dedicated I2V LoRA",
      "from": "NebSH"
    },
    {
      "finding": "New Lightning LoRAs work with all CFG steps on HN model",
      "details": "Can use 4 HN steps at cfg=3.5 with the LoRA without ruining anything, unlike 2.1 where cfg=1 was needed",
      "from": "IceAero"
    },
    {
      "finding": "LoRA rank inspection available on HuggingFace",
      "details": "Can inspect LoRA ranks at https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Lightx2v with ffn layers up to rank 319",
      "from": "Kijai"
    },
    {
      "finding": "Lightning LoRAs refuse to do dark/night scenes",
      "details": "LoRAs force daylight scenes and break on dark prompts, likely due to dataset limitations",
      "from": "Kijai"
    },
    {
      "finding": "First step without LoRA enables dark scenes",
      "details": "Using a single pre-step without LoRA allows dark scene generation, then apply LoRA for remaining steps",
      "from": "IceAero"
    },
    {
      "finding": "Lightning 1.1 LoRA causes severe issues with motion and lighting",
      "details": "New lightning LoRA dramatically redraws everything between frames, causes overbrightness, destroys lighting capabilities, can't make dark scenes with dim lights",
      "from": "N0NSens"
    },
    {
      "finding": "MagCache now supports Wan 2.2 with 1.5-2x speedup",
      "details": "Official support added for Wan 2.2 with significant speed improvements",
      "from": "NebSH"
    },
    {
      "finding": "ATI team added motion transfer trajectories tool",
      "details": "New code for creating motion transfer trajectories from a video, works as a script based on co-tracker",
      "from": "Kijai"
    },
    {
      "finding": "VACE 2.1 works with Wan 2.2 including High Noise model",
      "details": "Works with specific settings and strengths, requires custom workflow",
      "from": "Ablejones"
    },
    {
      "finding": "Lightning I2V LoRA works well at different strength ratios for motion control",
      "details": "1/1 lora = slowish motion, 2/1 = fastish motion, 1.5/1 = goldilocks balanced motion",
      "from": "CaptHook"
    },
    {
      "finding": "Wan 2.2 14B can generate at 1536x768 resolution natively",
      "details": "Rendered in 11 minutes using Q5_K_M quantized version with Lightning I2V LoRA",
      "from": ": Not Really Human :"
    },
    {
      "finding": "High resolution generation possible with Wan 2.2",
      "details": "Successfully generated at 1600x960 resolution",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "finding": "Radial sampler works inconsistently",
      "details": "Sometimes works fine but other times produces strange artifacts depending on the seed",
      "from": "N0NSens"
    },
    {
      "finding": "Lightning 2.2 distilled for specific resolutions and frame counts",
      "details": "High_noise_model and low_noise_model distilled simultaneously for 81x720x1280, 81x1280x720, 81x480x832, 81x832x480 sizes",
      "from": "CaptHook"
    },
    {
      "finding": "Wan 2.2 Fun Control has inpainting functionality combined",
      "details": "The Fun Control code can call for mask latents, bringing it closer to VACE functionality",
      "from": "DawnII"
    },
    {
      "finding": "Wan 2.2 Fun Control model supports 52 channels input instead of standard 48",
      "details": "Kijai discovered the new Fun Control model has 52 channels input and couldn't identify what the extra 4 channels are for",
      "from": "Kijai"
    },
    {
      "finding": "Fun Control model supports inpainting in addition to control",
      "details": "The control model now also supports inpainting functionality, making it more versatile",
      "from": "Kijai"
    },
    {
      "finding": "Lightning LoRAs force bright, well-lit daytime style",
      "details": "The newest lightning LoRAs work well with 2.2 but force a really bright well-lit daytime style on everything, even forcing clothing colors lighter",
      "from": "Ablejones"
    },
    {
      "finding": "MultiTalk works well but lip sync quality degrades after 7-8 seconds",
      "details": "User getting good lip sync for 7-8 seconds in middle of 10sec generation, but quality degrades beyond that point",
      "from": "hiroP"
    },
    {
      "finding": "Fun Control can generate 257 frames straight",
      "details": "Kijai demonstrated generating 257 frames in one go, though quality goes down with longer generations",
      "from": "Kijai"
    },
    {
      "finding": "Control end_percent affects WAN 2.2 stability",
      "details": "When control end_percent is too low, it causes errors in Fun Control workflows",
      "from": "DawnII"
    },
    {
      "finding": "Frame count changes WAN 2.2 output",
      "details": "Using the same seed with different frame counts produces different outputs",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Default workflow settings may be incorrect",
      "details": "Reddit analysis suggests some default parameters were off, particularly related to noise scheduling",
      "from": "MysteryShack"
    },
    {
      "finding": "Control signals can extend beyond 81 frame limitation",
      "details": "Using control signals (VACE or Fun) allows generating more than the standard 81 frame limit",
      "from": "Kijai"
    },
    {
      "finding": "WAN automatically trims frames to 4n+1 format",
      "details": "The model automatically adjusts frame count to fit the required mathematical format",
      "from": "Nekodificador"
    },
    {
      "finding": "Wan 2.2 MoE architecture uses high and low noise models with specific transition boundary",
      "details": "Process starts with high noise model and switches to low noise at later denoising stages. Switch happens at modified_sigma boundary of 0.875",
      "from": "fredbliss"
    },
    {
      "finding": "Lightning LoRAs should only be applied to low noise model, not high noise",
      "details": "High noise model needs full processing power, lightning optimization works better on low noise refinement stage",
      "from": "fredbliss"
    },
    {
      "finding": "High noise model creates rough structure, low noise refines details",
      "details": "High noise stage outputs mostly formed structure with face and limbs outlined, low noise stage acts like vid2vid to add details and cleanup",
      "from": "MysteryShack"
    },
    {
      "finding": "Distilled LoRAs don't reach 50% signal-to-noise ratio at same timestep as full models",
      "details": "Sigma boundaries shift differently for distill models, making them look less formed at equivalent stages compared to full step models",
      "from": "MysteryShack"
    },
    {
      "finding": "Fun Control models can handle both control AND inpainting simultaneously",
      "details": "Input size difference reveals mask support, suggesting the model can do control and inpainting together",
      "from": "Kijai"
    },
    {
      "finding": "First step should never use any LoRA to avoid slow-motion issues",
      "details": "All distill loras should have weight of zero on first step to fix both lighting and slow-mo problems",
      "from": "MysteryShack"
    },
    {
      "finding": "Wan2.2 VACE can be used with single frame controlnet input",
      "details": "Using Kijai's WanVideo VACE Start to End Frame node - pass controlnet input to start image connector, output goes to native WanVaceToVideo control video input. Results in non-static video that follows prompt and can add reference image",
      "from": "Lodis"
    },
    {
      "finding": "FP8 + FP8 operations not supported in PyTorch",
      "details": "torch doesn't support fp8 weight + fp8 weight add or mul operations, only matrix multiplication is supported",
      "from": "Kijai"
    },
    {
      "finding": "Split threshold for high/low noise models",
      "details": "T2V threshold to switch is 0.875, I2V threshold is 0.9. Check sigma values in logs to determine proper split point",
      "from": "Kijai"
    },
    {
      "finding": "14 high / 6 low steps gives better results than 11 high / 9 low",
      "details": "Testing with euler sampler on LoRA training showed better ripples, face definition, and detail preservation with 14/6 split",
      "from": "crinklypaper"
    },
    {
      "finding": "Vace with Wan2.2 workaround using step 2 injection",
      "details": "Hacky vid2vid workflow that injects reference video as latents and starts denoising at step 2 on first sampler, as step 1 obliterates Vace guidance",
      "from": "ingi // SYSTMS"
    },
    {
      "finding": "Lambda function patching with compile() prevents VRAM release",
      "details": "Using lambda function to patch linear layer with torch.compile wouldn't let it release VRAM, but GGUF didn't have this issue as it patches the whole Linear layer instead of just forward",
      "from": "Kijai"
    },
    {
      "finding": "FP8 implementation fixed by changing patching method",
      "details": "Implemented fp8 same way as GGUF (patching whole Linear layer instead of lambda forward) and VRAM issue is gone",
      "from": "Kijai"
    },
    {
      "finding": "Sapiens can find 11 people maximum",
      "details": "Sapiens detected up to 11 people in a 192 frame video, took about 5 minutes on 4090",
      "from": "fredbliss"
    },
    {
      "finding": "TorchScript + AMP provides best performance for Sapiens",
      "details": "30-40% faster than original PyTorch, works with bfloat16/fp16 for memory savings, production-ready and portable",
      "from": "fredbliss"
    },
    {
      "finding": "BFloat16 models offer 50% memory reduction",
      "details": "Native BF16 models provide 50% memory reduction with <0.5% accuracy loss on Ampere+ GPUs",
      "from": "fredbliss"
    },
    {
      "finding": "Wan 2.2 supports split-screen prompts",
      "details": "Can generate split-screen videos with detailed prompts describing synchronized perspectives",
      "from": "\ud83e\udd99rishappi"
    },
    {
      "finding": "Meta's BF16 models don't work with PyTorch >2.0",
      "details": "The published bf16 models from Meta have compatibility issues with newer PyTorch versions",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Frame count affects slow motion in Wan 2.2",
      "details": "At 81 frames, videos show slow motion effect. Reducing to 77 frames eliminates slow motion. This is likely because 81 doesn't match Wan 2.2's 16-frame architecture (16 x 5 = 80)",
      "from": "xwsswww"
    },
    {
      "finding": "Bypassed LoRA nodes can cause workflow issues",
      "details": "Bypassed LoRA nodes can be saved in a bad state, causing 'Required input is missing: model' errors. Unbypassing and rebypassing the LoRA nodes fixes the issue",
      "from": "TK_999"
    },
    {
      "finding": "GGUF models show cleaner results than fp8 scaled",
      "details": "Comparison between GGUF 2.2 and ComfyUI scaled fp8 shows GGUF produces cleaner results. The difference is larger with LoRAs due to how they're applied",
      "from": "Draken"
    },
    {
      "finding": "Prompt content affects slow motion in Wan 2.2",
      "details": "If prompts don't have enough action for 81 frames, the model stretches it out creating slow motion. More action in prompts allows full speed at higher frame counts",
      "from": "Hashu"
    },
    {
      "finding": "GGUF version improves image quality and sharpness",
      "details": "GGUF version makes text in ground sharp and creates more realistic interiors compared to FP16, though it adds unwanted elements like cars",
      "from": "Drommer-Kille"
    },
    {
      "finding": "GGUF vs FP16 performance comparison",
      "details": "FP16: 107.66 seconds, GGUF: 97.05 seconds for same prompt",
      "from": "Drommer-Kille"
    },
    {
      "finding": "Wan 2.2 14B is 16fps, not 24fps",
      "details": "Only the 5B model runs at 24fps, 14B variant is still 16fps",
      "from": "Drommer-Kille"
    },
    {
      "finding": "V2V workflow for style refinement",
      "details": "Can feed video to wrapper samples input for V2V processing, acts like denoise control by adjusting low-high step cutoff point",
      "from": "Drommer-Kille"
    },
    {
      "finding": "Combining Vace 2.1 with Wan 2.2 for enhanced realism",
      "details": "Using Vace as first pass then Wan 2.2 as second pass adds significant realism that Vace alone cannot achieve",
      "from": "Drommer-Kille"
    },
    {
      "finding": "SeedVR2 VRAM usage scaling",
      "details": "480p->720p 81 frames took 760 seconds, 720p->1080p uses 98% VRAM on RTX 3090",
      "from": "Hevi"
    },
    {
      "finding": "Fun 2.2 control model can do temporal inpainting",
      "details": "The new Fun 2.2 control model appears capable of temporal inpainting functionality",
      "from": "Kijai"
    },
    {
      "finding": "Fun 2.2 control model lost reference input capability",
      "details": "The Fun 2.2 control model no longer works with reference inputs, unlike Fun 1.1 which still works with reference",
      "from": "Kijai"
    },
    {
      "finding": "Fun 2.2 control refuses to work beyond 81 frames",
      "details": "The model won't move at all if trying to generate more than 81 frames",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 has attention sinks that could enable extension",
      "details": "Similar to LLMs with BOS tokens, finding attention sinks in Wan could enable video extension capabilities",
      "from": "fredbliss"
    },
    {
      "finding": "ComfyUI progress bar interrupt now frees memory",
      "details": "Fixed the progress bar to catch interrupts in try/except block, now properly frees memory when generation is cancelled",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 low noise model can be used as an upscaler by swapping it for 2.1 model",
      "details": "Works in same workflow setups, can achieve 1600x900x81 frames on 3060 GPU",
      "from": "mdkb"
    },
    {
      "finding": "Skyreels LoRA can help break 121 frame loop in 2.2",
      "details": "When applied, allows old man to leave frame in 121 frame generation, addresses looping issue",
      "from": "Kijai"
    },
    {
      "finding": "5B controlnet support available",
      "details": "Traditional depth controlnet works with 5B model, generates in few seconds",
      "from": "Kijai"
    },
    {
      "finding": "FastWan 5B released",
      "details": "Available for few days, used for controlnet tests",
      "from": "Kijai"
    },
    {
      "finding": "2.2 I2V low noise behaves differently than 2.1 I2V when used standalone",
      "details": "Notable behavioral differences observed",
      "from": "Kijai"
    },
    {
      "finding": "Official Flash version claims 12x faster inference speed than Wan 2.1",
      "details": "Reasoning speed improvement, though translation may be unclear",
      "from": "scf"
    },
    {
      "finding": "Fun models handle composited control inputs",
      "details": "Fun models used to handle composited control inputs fine",
      "from": "Kijai"
    },
    {
      "finding": "Block swap has minimal speed impact on high-end systems",
      "details": "Transfer time ~0.1 seconds per block on high-end systems, but first block transfer can take 5+ seconds on some systems",
      "from": "Kijai"
    },
    {
      "finding": "WAN 2.2 I2V loops at 121 frames",
      "details": "2.2 I2V generally just loops at 121 frames",
      "from": "Kijai"
    },
    {
      "finding": "Skyreels LoRA allows 121 frames",
      "details": "Skyreels 720p is trained on 121 frames, allows extending WAN 2.2 to that length",
      "from": "Kijai"
    },
    {
      "finding": "Block swap profiling shows compute dominates transfer time",
      "details": "Most blocks show compute_time of 0.2-0.3s vs transfer_time of 0.0001-0.0002s",
      "from": "Kijai"
    },
    {
      "finding": "Prefetch option significantly improves block swap performance",
      "details": "Makes generation 1 minute faster (on 6 minute total) for 3090",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Lightning + LightX2V LoRA combination works well",
      "details": "Lightning LoRAs at strength 1.0 followed by older LightX2V LoRAs at 0.2-0.5 strength clears up imaging and acts like glue for multiple LoRAs",
      "from": "screwfunk"
    },
    {
      "finding": "2.2 works better at 1280x720 than 480p",
      "details": "Lightning LoRAs or wan2.2-fun had significantly less artifacts at 720p vs 480p",
      "from": "ArtOfficial"
    },
    {
      "finding": "WAN fp8 fits on 5090 without block swap",
      "details": "wan fp8 fits quite nicely on the 5090, no need for block swap",
      "from": "pagan"
    },
    {
      "finding": "Skyreels LoRA works with T2V when configured correctly",
      "details": "Wan2_1_Skyreels-v2-I2V-720P_LoRA works with T2V using high = [3.5,3.5,4,4] (float list) and low = 2.0",
      "from": "avataraim"
    },
    {
      "finding": "LoRA strength scheduling improves results",
      "details": "Can schedule LoRA strength from 2.0 to 1.0 over steps for better convergence",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Wan 2.2 14B naturally runs at 16fps",
      "details": "Both Wan 2.1 and 2.2 14B models output at 16fps natively",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Frame count limits for looping",
      "details": "Up to 109 frames there are no loops, after 121 frames looping can break",
      "from": "N0NSens"
    },
    {
      "finding": "VACE can handle very long sequences",
      "details": "With VACE on 2.1, generated 350 frames in one go, worked much better than context",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Wan 2.2 wrapper calculates denoise based on steps differently than native",
      "details": "Wrapper does denoise calculation based on steps unlike native - need to increase steps to get more denoising steps",
      "from": "DawnII"
    },
    {
      "finding": "Stand-In LoRA uses shared attention mechanism",
      "details": "Much cleaner than having extra latent in sequence all the way, uses shared attention for face consistency",
      "from": "Kijai"
    },
    {
      "finding": "Stand-In input image resolution is handled separately",
      "details": "Input image doesn't have to be same resolution as output, handled separately. In their code it's always 512x512",
      "from": "Kijai"
    },
    {
      "finding": "Stand-In is trained at 24FPS",
      "details": "The Stand-In model was trained at 24 frames per second",
      "from": "mamad8"
    },
    {
      "finding": "torch.compile caches for same input size only",
      "details": "Torch compile issue where it caches for the same input size only, causes varying performance",
      "from": "Kijai"
    },
    {
      "finding": "Stand-in LoRA works better with larger reference images",
      "details": "640x640 worked better than 512x512, and higher resolutions like 768x768 and 1024x1024 show improved results",
      "from": "Kijai"
    },
    {
      "finding": "Can use multiple reference images in a batch",
      "details": "Repeated same reference image 8x in a batch, works without errors and may improve results",
      "from": "Kijai"
    },
    {
      "finding": "Stand-in LoRA doesn't work well with Wan 2.2 high noise",
      "details": "Limited effect on 2.2 high noise setting, better suited for other configurations",
      "from": "Kijai"
    },
    {
      "finding": "Lightning i2v LoRA performs better than t2v version",
      "details": "i2v lightning just 'works' way more compared to t2v lightning which requires more settings adjustment",
      "from": "Draken"
    },
    {
      "finding": "res_2s sampler gives better results",
      "details": "Does double the steps in essence, provides nicer results due to higher order method with smaller truncation error",
      "from": "Draken"
    },
    {
      "finding": "Stand-in doesn't work well with Wan 2.2 LOW model but works very good with Wan 2.1",
      "details": "Confirmed through testing, compatibility issue specifically with 2.2 LOW",
      "from": "Hevi"
    },
    {
      "finding": "Skyreels v2 can push Wan to 121 frames",
      "details": "Using Skyreels v2 LoRA allows extending video length to 121 frames",
      "from": "NebSH"
    },
    {
      "finding": "Q8 quantization is very close to fp16 quality",
      "details": "Visual comparison shows minimal difference between Q8 and fp16, with Q8 being much more efficient",
      "from": "Kijai"
    },
    {
      "finding": "Native Wan implementation does automatic block swapping while wrapper doesn't",
      "details": "Native reports as partial load and manages VRAM automatically, wrapper needs manual block swap setting",
      "from": "Kijai"
    },
    {
      "finding": "Uni3c control helps stabilize context windows for long generations",
      "details": "Using Uni3c to lock camera makes context windows stable for extended video generation",
      "from": "Kijai"
    },
    {
      "finding": "FP8 e4m3fn models need to be replaced with e5m2 versions to avoid torch compile errors",
      "details": "FP8 e4m3fn not supported in certain architectures, need e5m2 instead",
      "from": "Hevi"
    },
    {
      "finding": "Wan 2.2 LN (Low Noise) model can be used standalone as an improved version of Wan 2.1",
      "details": "The Low Noise model from Wan 2.2 works by itself and is essentially a further trained version of Wan 2.1 with better quality",
      "from": "Ablejones"
    },
    {
      "finding": "You can extract a LoRA from Wan 2.2 LN using Wan 2.1 as base",
      "details": "Users can create a LoRA from the Wan 2.2 Low Noise model to apply its improvements to other Wan 2.1 models like Phantom",
      "from": "Ablejones"
    },
    {
      "finding": "Wan 2.2 LN shows massive upgrade from 2.1",
      "details": "Users report significant quality improvements when switching from Wan 2.1 to Wan 2.2 LN model",
      "from": "Josiah"
    },
    {
      "finding": "Fun Control has different patch embedding structure",
      "details": "Fun Control uses 52 input channels (16+16+16+4 for noise + image cond + control + mask) vs T2V's 16 channels and I2V's 36 channels",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 doesn't snap strongly to init images",
      "details": "The 2.2 low noise model recreates the first frame but then quickly diverges from the init image, which could be useful for context windows",
      "from": "Kijai"
    },
    {
      "finding": "VACE with guidance frames allows fewer steps without acceleration LoRAs",
      "details": "When using guidance frames via VACE, you can use as few as 8 steps without needing distillation LoRAs",
      "from": "pom"
    },
    {
      "finding": "Stand-In reference LoRA provides good likeness when properly configured",
      "details": "Users found that Stand-In can provide good likeness results, but proper masking and single-person prompts work better",
      "from": "Intellectus Prime"
    },
    {
      "finding": "FantasyPortrait working for face motion transfer",
      "details": "Face motion transfer adapter for Wan 2.1 14B is functional, though still work in progress",
      "from": "Kijai"
    },
    {
      "finding": "Fun 2.2 camera control expects normal I2V model for low noise sampler",
      "details": "Camera control workflow requires normal I2V model, not the Fun model for the low noise sampler",
      "from": "Kijai"
    },
    {
      "finding": "fp8 scaled Fun 2.2 control models don't work if ref_conv layer is in fp8",
      "details": "Reference functionality fails silently when this specific layer is in fp8 format",
      "from": "Kijai"
    },
    {
      "finding": "Stand-in doesn't work with VACE + MultiTalk combination",
      "details": "Testing showed it couldn't get good results when combining all three, though stand-in + VACE or stand-in + MultiTalk works fine individually",
      "from": "piscesbody"
    },
    {
      "finding": "MAGREF works well with context windows using reference latents",
      "details": "Can utilize reference image instead of first frame for context windows, works better than traditional I2V for this use case",
      "from": "Kijai"
    },
    {
      "finding": "Fantasy Portrait works surprisingly well with animals",
      "details": "Cat test showed good results despite being designed for human portraits",
      "from": "Kijai"
    },
    {
      "finding": "PyTorch 2.7 to 2.9 upgrade reduces RAM usage",
      "details": "WSL2 RAM usage dropped from 85gb to 65gb for 2 sampler wan22 workflows",
      "from": "pagan"
    },
    {
      "finding": "Wan 2.2 Fun InP models work, but Fun Control Camera model has issues with GGUF format",
      "details": "Fun INP GGUF works fine, but Fun Control Camera GGUF has gradient errors and tensor type mismatches",
      "from": "Daflon"
    },
    {
      "finding": "Stand-in LoRA works with VACE",
      "details": "Standard workflow from Kijai's repo works, though results can be hit or miss",
      "from": "ArtOfficial"
    },
    {
      "finding": "Phantom works with Wan 2.2",
      "details": "Shows good character consistency across different frame counts",
      "from": "Ablejones"
    },
    {
      "finding": "MultiTalk can work with Wan 2.2 using block swap",
      "details": "Main wan2.2 model can be block swapped, MultiTalk fits in 23G VRAM when combined",
      "from": "nacho.money"
    },
    {
      "finding": "Q8 GGUF models provide better quality",
      "details": "User reports better results with Q8 GGUF compared to regular models",
      "from": "xwsswww"
    },
    {
      "finding": "Wan 2.1 LoRAs work on Wan 2.2 but with differences",
      "details": "Results work but camera may move away, requires prompting adjustments like 'fixed shot, static camera'",
      "from": "Mngbg"
    },
    {
      "finding": "Q8 gguf seems to prompt faster than Q6",
      "details": "User observed Q8 quantization performing faster than Q6 in testing",
      "from": "xwsswww"
    },
    {
      "finding": "LightX2V rank 128 gives better prompt adherence than rank 256",
      "details": "Lower rank LoRA showing better prompt following than higher rank",
      "from": "xwsswww"
    },
    {
      "finding": "Rank 16 was the only one to correctly generate koala and bird scene",
      "details": "Specific scene generation working better with lowest rank tested",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "finding": "Higher rank LoRA is stronger but stronger isn't always better",
      "details": "Higher rank means closer to original model it's extracted from, but may be too strong for some use cases",
      "from": "Kijai"
    },
    {
      "finding": "Fun 2.2 trajectory control with image doesn't work with reference input but works with start image",
      "details": "Reference image input breaks trajectory control, start image input works properly",
      "from": "Kijai"
    },
    {
      "finding": "VACE 2.1 is better than 2.2 for VACE tasks",
      "details": "Older version of VACE performs better than newer one",
      "from": "Kijai"
    },
    {
      "finding": "Fun 2.2 seems lot better than 2.1 Fun",
      "details": "Newer Fun model shows significant improvement over previous version",
      "from": "Kijai"
    },
    {
      "finding": "FusionX is Wan 2.1 with 5-6 LoRAs saved, then single LoRA extracted",
      "details": "Popular model is actually a composite that was then re-extracted",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 I2V can do FLF (First-Last-Frame) morphing",
      "details": "Doesn't always work but it has the capability, needs enough frames and can't do too quick transitions",
      "from": "Kijai"
    },
    {
      "finding": "VRAM profiling shows different peaks between 1.3B and 14B models",
      "details": "In 1.3B the FFN is the peak, but in 14B it's rarely the peak. Uncompiled shows RoPE as mountains, compiled shows attention as peaks",
      "from": "Kijai"
    },
    {
      "finding": "Torch compile significantly reduces VRAM peaks",
      "details": "Evens out the peaks during sampling, making memory usage more consistent",
      "from": "Kijai"
    },
    {
      "finding": "Block swap reduces the constant memory use underneath the peaks",
      "details": "It affects the baseline memory usage, not the peak spikes",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 demonstrates world model capabilities",
      "details": "Remembers the location of things in generated videos",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "finding": "Zero RAM usage possible with Wan 2.1",
      "details": "Can run from start to finish with zero RAM usage using proper setup",
      "from": "Kijai"
    },
    {
      "finding": "Native ComfyUI has automatic offloading that many users don't know exists",
      "details": "It's always on and provides super granular weight swapping",
      "from": "Kosinkadink"
    },
    {
      "finding": "Reserve-vram flag dramatically improves performance",
      "details": "Using --reserve-vram 5 changed generation time from 'taking ages to do 1 step' to '90 secs with 10 steps'",
      "from": "Nekodificador"
    },
    {
      "finding": "CFG 1.0 doubles speed by eliminating negative prompt evaluation",
      "details": "When cfg is 1.0, negative prompt pass is disabled, meaning twice the speed since only positive prompt needs evaluation",
      "from": "Kosinkadink"
    },
    {
      "finding": "NAG allows negative conditioning at CFG 1",
      "details": "NAG node provides cheaper negative prompting while maintaining CFG 1 speed benefits",
      "from": "DawnII"
    },
    {
      "finding": "Sage attention 3 improves speed with blocks set to 40",
      "details": "User achieved better speed than ksampler workflow by setting blocks to 40 and using sage attention 3",
      "from": "cocktailprawn1212"
    },
    {
      "finding": "Wan 2.2 understands time-based prompting",
      "details": "Can specify '0.0 seconds- 0.3 seconds: write what u want here' to solve slow motion issues, 81 frames \u2248 0.5 seconds",
      "from": "xwsswww"
    },
    {
      "finding": "FantasyPortrait node achieves 40 it/s on CPU",
      "details": "AMD Ryzen 9 9900X achieves 40 it/s, initially thought to be GPU but actually runs on CPU",
      "from": "Kijai"
    },
    {
      "finding": "VACE + Phantom combined workflow working in 1.3B model",
      "details": "Successfully combined 1.3B VACE + Phantom (style from prompt) in a working workflow",
      "from": "Kijai"
    },
    {
      "finding": "Phantom model can be tricked to work with VACE despite compatibility issues",
      "details": "User managed to get phantom model working but encountered tensor size mismatch errors when trying to use VACE with it",
      "from": "mdkb"
    },
    {
      "finding": "Lightning loras may have censoring behavior at high strengths",
      "details": "Lightning I2V lora generates objects to block body parts at strength 0.7+, behavior stops at 0.65 strength. Does not affect lightx2v loras",
      "from": "MysteryShack"
    },
    {
      "finding": "VACE reference works more like FaceID V2 than simple IPAdapter",
      "details": "VACE reference requires high quality input and behaves more like FaceID V2 for identity preservation rather than a simple IPAdapter",
      "from": "Nekodificador"
    },
    {
      "finding": "More frames causes slower motion in generated videos",
      "details": "33 frames vs 66 frames causes motion speed to halve, regardless of prompt modifications",
      "from": "Drommer-Kille"
    },
    {
      "finding": "WanFM (Frame Morphing) sampling method",
      "details": "Samples from both directions and blends - forward and reverse pass with reversed RoPE frequencies on each step, doubles sampling cost but improves morphing between frames",
      "from": "Kijai"
    },
    {
      "finding": "ComfyUI frontend now passes data through bypassed/muted nodes",
      "details": "Recent ComfyUI update changed behavior - bypassed and muted nodes now pass data through, breaking workflows that relied on them blocking data",
      "from": "phazei"
    },
    {
      "finding": "Wan 2.2 naturally handles bidirectional sampling",
      "details": "Wan 2.2 already does similar bidirectional processing internally, making WanFM potentially redundant",
      "from": "Kijai"
    },
    {
      "finding": "Empty audio track with MultiTalk stops unwanted character talking",
      "details": "Using empty audio with MultiTalk can shut up characters from talking when not desired",
      "from": "Kijai"
    },
    {
      "finding": "Block swap uses disk space for memory",
      "details": "Block swap setting stores memory overflow on disk, space doesn't automatically clear after generation completes",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "PUSA sampling uses more VRAM regardless of weight format",
      "details": "Merging PUSA LoRA doesn't reduce VRAM usage because PUSA sampling itself is inherently more VRAM intensive",
      "from": "Kijai"
    },
    {
      "finding": "PUSA is actually a whole new model in LoRA format",
      "details": "Makes T2V able to do I2V and start/end frame functionality, trained with only 4000 videos and $500 as a cheap training method proof of concept",
      "from": "Kijai"
    },
    {
      "finding": "RoPE implementation was incorrect for ComfyUI",
      "details": "Had RoPE reverse wrong for comfy rope, causing low noise side to make jittery mess. Now uses default rope always with WanFM",
      "from": "Kijai"
    },
    {
      "finding": "WanFM method works with both 2.1 and 5B models",
      "details": "The WanFM sampling method that does forwards and backwards predictions per step works across model variants",
      "from": "Kijai"
    },
    {
      "finding": "Low noise model in Wan2.2 primarily handles lip-syncing",
      "details": "High noise looks like almost done LCM sampler, low noise turns that into crisp visuals and does most of the lipsyncing",
      "from": "MysteryShack"
    },
    {
      "finding": "LCM on high noise model provides better prompt adherence",
      "details": "LCM gives 50% noise look that the low noise model is looking for, finally achieving good prompt adherence",
      "from": "MysteryShack"
    },
    {
      "finding": "SDPA attention needed for InFrame with fp16 models",
      "details": "SageATTN was ruining likeness with wan t2v fp16, needed to use SDPA. SageATTN works fine on fp8 model",
      "from": "hablaba"
    },
    {
      "finding": "MultiTalk captures song emotions realistically",
      "details": "Extremely good at capturing emotions of songs and looks realistic compared to other methods, though quality degrades over longer generations",
      "from": "seitanism"
    },
    {
      "finding": "VACE single image inpainting requires exactly 5 frames - first frame with the change/mask, 4 completely empty/full gray",
      "details": "Only works with this specific setup, trying to mask all 5 frames breaks it completely",
      "from": "Nekodificador"
    },
    {
      "finding": "VACE single image only works with wrapper, not native",
      "details": "Native implementation has problems with single image VACE",
      "from": "Nekodificador"
    },
    {
      "finding": "Shape and size of mask have huge impact on VACE results",
      "details": "Much more impact than expected during inpainting tests",
      "from": "Nekodificador"
    },
    {
      "finding": "Qwen-image latents can be bridged directly to WAN 2.2 without VAE decode",
      "details": "Same latent format allows direct conversion from Qwen-image to WAN video",
      "from": "fredbliss"
    },
    {
      "finding": "VACE works better than Redux for inpainting",
      "details": "Confirmed through testing comparison",
      "from": "Nekodificador"
    },
    {
      "finding": "81 frames = 21 latents due to 4x VAE compression",
      "details": "Important for latent bridging calculations",
      "from": "Kijai"
    },
    {
      "finding": "Qwen image and Wan share the same VAE and latent space",
      "details": "Both models from Alibaba use same/similar VAE, enabling direct latent passing between models without decode",
      "from": "fredbliss"
    },
    {
      "finding": "Negative prompts cause skull-like shapes in early diffusion steps",
      "details": "Every generation with negative prompts shows skull shapes at first step due to how CFG works, removing negative prompt eliminates this",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "finding": "Fantasy Portrait can work with first/last frame inputs",
      "details": "Using extracted first and last frames from input video as start/end frames creates video-to-video workflow",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Video LoRA training needs both video and image datasets",
      "details": "For video LoRAs with outfit variations, adding image dataset alongside video dataset fixes outfit generation issues",
      "from": "Ryzen"
    },
    {
      "finding": "GGUF version of VACE_Phantom works with fp8 scaled i2v model but not with gguf",
      "details": "Phantom_vACE merged scaled model works with scaled f8 i2v, but didn't work with gguf because Kijai's WanVideo VACE select doesn't support gguf",
      "from": "xwsswww"
    },
    {
      "finding": "Q8 gguf gives better results than f8 scaled but f8 scaled is faster",
      "details": "Quality vs speed tradeoff between quantization formats",
      "from": "xwsswww"
    },
    {
      "finding": "MAGREF + context windows work well for long stable generations",
      "details": "MAGREF being a hybrid model combined with control signals enables stable long video generation up to 700 frames",
      "from": "Kijai"
    },
    {
      "finding": "Context windows work better in latent space",
      "details": "InfiniteTalk does sliding windows in latent space which is better than decoded space",
      "from": "Kijai"
    },
    {
      "finding": "MultiTalk improves lip sync in long generations",
      "details": "Adding MultiTalk to MAGREF + Fantasy Portrait workflow significantly improves lip synchronization",
      "from": "Kijai"
    },
    {
      "finding": "GGUF Q8 sometimes faster than FP8 in wrapper despite theoretical slowness",
      "details": "User reported GGUF Q8 generating 486s vs FP8 505s total time, despite inference being slower",
      "from": "Josiah"
    },
    {
      "finding": "MTV Crafter only works with 49 frames",
      "details": "Model is hardcoded to only work with 49 frames, not 81 frames as expected",
      "from": "Kijai"
    },
    {
      "finding": "Context window memory usage is context frames * width * height",
      "details": "Not total frames - need enough RAM to handle frame count after decode",
      "from": "Kijai"
    },
    {
      "finding": "InfiniteTalk uses slice-by-slice generation with noise overlap",
      "details": "Continues from last frame with overlap in noise, new model should not degrade over time",
      "from": "Kijai"
    },
    {
      "finding": "New 3D pose control system",
      "details": "Controlled by 3D pose, not 2D like other pose controls. Stick figure is just visualization",
      "from": "Kijai"
    },
    {
      "finding": "Scene cuts can be achieved through training with prompting",
      "details": "Training model on 5 second clips with cuts in the middle, using consistent shots so model learns consistency naturally",
      "from": "Ablejones"
    },
    {
      "finding": "MTV Crafter motion adapter works with MAGREF",
      "details": "Extracted motion adapter from MTV Crafter and added to MAGREF, it actually does something and shows promise",
      "from": "Kijai"
    },
    {
      "finding": "UniAnimate with MAGREF produces better results than MTV weights",
      "details": "Using pose pred as just RGB from same predictor MTV uses, but not using MTV weights, produces superior results",
      "from": "Kijai"
    },
    {
      "finding": "Multi-stage sampling improves Wan 2.2 motion quality",
      "details": "Model won't do proper motion without CFG, so do some steps with CFG and no lora, or lora at lower strength",
      "from": "Kijai"
    },
    {
      "finding": "Scene cuts work with both T2V and I2V using specific prompting",
      "details": "Can use prompts like '[scene 1] description [cut] [scene 2]' for cuts, works well with 2/3 scenes being optimal",
      "from": "NebSH"
    },
    {
      "finding": "Using input video directly as control video without depth works well",
      "details": "By mistake used straight input video as control instead of depth/other controls and got good results",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Fun Control 2.2 works similarly to VACE for style transfer",
      "details": "Can use ref image with input video for style transfer effects",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "InfiniteTalk can replace MultiTalk in existing workflows",
      "details": "Just swap the model in the multitalk slot and it works with improved results",
      "from": "NC17z"
    },
    {
      "finding": "InfiniteTalk allows much longer video generation",
      "details": "Went from 250 frames max with MultiTalk to 1000 frames with InfiniteTalk",
      "from": "NC17z"
    },
    {
      "finding": "Face landmarks from Fantasy Portrait work with Fun 2.2",
      "details": "Can use the same face landmark detection system for control",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Normal i2v workflow with start and end frames works as well as FLF2V",
      "details": "Regular i2v model performs similarly to the specialized first-last frame model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "InfiniteTalk can generate 1000 frames using context windows workflow",
      "details": "Just swap MultiTalk model for InfiniteTalk model in existing workflows",
      "from": "NC17z"
    },
    {
      "finding": "InfiniteTalk uses improved looping method",
      "details": "Continues from last frames with mixed latents, prevents quality degradation over time",
      "from": "Kijai"
    },
    {
      "finding": "InfiniteTalk has single and multi versions",
      "details": "Single version is 5GB vs MultiTalk's 2.7GB, trained separately for better task performance",
      "from": "Kijai"
    },
    {
      "finding": "Qwen Image Edit requires disabling sage attention",
      "details": "Only works if sage attention is disabled, causes black output otherwise",
      "from": "Lodis"
    },
    {
      "finding": "Cache-none flag improves frame capacity",
      "details": "Adding --cache-none to ComfyUI startup allows generating over 1000 frames",
      "from": "samhodge"
    },
    {
      "finding": "InfiniteTalk works with MagRef",
      "details": "Can combine InfiniteTalk with MagRef for identity preservation",
      "from": "Kijai"
    },
    {
      "finding": "ByteDance MegaTTS3 has god-tier vocoder",
      "details": "Uses semantic-like tokens despite claiming phoneme input, supports speech2speech",
      "from": "MysteryShack"
    },
    {
      "finding": "Audio scale parameter controls conditioning strength in InfiniteTalk",
      "details": "Audio scale 2.0 provides better results, multiplies conditioning in every case unlike audio_cfg_scale which only works with main CFG",
      "from": "Kijai"
    },
    {
      "finding": "MAGREF provides more freedom and stability",
      "details": "Gives more control and image doesn't degrade as much compared to normal I2V model, works better without clip embeds for stronger reference",
      "from": "Kijai"
    },
    {
      "finding": "InfiniteTalk acts as endless I2V with audio control",
      "details": "Model seems specifically made for this type of sampling, audio provides guidance unlike basic I2V",
      "from": "Kijai"
    },
    {
      "finding": "Model can process silence to force mouth closure",
      "details": "The model actually processes silence too, it can be used to force people to keep their mouths shut",
      "from": "Kijai"
    },
    {
      "finding": "FP8 quality degradation on 2.1 models with fast settings",
      "details": "FP8 fast has quality hit on 2.1 models, produces significantly worse quality than GGUF Q8",
      "from": "iShootGood"
    },
    {
      "finding": "Q8 is superior to fp8 in quality but not speed on 3090",
      "details": "Q8 GGUF provides better quality than fp8 on RTX 3090, especially with torch.compile, though fp8 is faster",
      "from": "[\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-]"
    },
    {
      "finding": "GGUF LoRAs work better unmerged",
      "details": "Most major thing to know about using GGUF is that Loras are always used unmerged and they work better, in both native and wrapper",
      "from": "Kijai"
    },
    {
      "finding": "InfiniteTalk adds 28 extra frames",
      "details": "InfiniteTalk strategy adds 28 frames to generation, which can cause sync issues",
      "from": "DawnII"
    },
    {
      "finding": "E5m2 is worse than e4m3fn quantization",
      "details": "e5m2 quantization produces worse quality than e4m3fn, and unscaled is terrible",
      "from": "Kijai"
    },
    {
      "finding": "GGUF models require main model to also be GGUF",
      "details": "If using GGUF InfiniteTalk model, the main model must also be GGUF format",
      "from": "Kijai"
    },
    {
      "finding": "Frame window size affects multitalk processing",
      "details": "Frame window size is like context window length, should be set to frame number you generate normally (81)",
      "from": "DawnII"
    },
    {
      "finding": "Wan 2.2 with InfiniteTalk V2V uses keyframing approach",
      "details": "Analysis of code shows it skips every 72 frames by default, so it's not proper V2V but keyframing with start frames every 72 frames",
      "from": "DawnII"
    },
    {
      "finding": "InfiniteTalk can work with video input through uni3c",
      "details": "The uni3c component uses the original video, so keyframe + uni3c + talk combination is possible",
      "from": "Kijai"
    },
    {
      "finding": "GGUF files can mix different quantization types",
      "details": "It's perfectly fine to mix GGUF qtypes in the same file",
      "from": "Kijai"
    },
    {
      "finding": "Uni3c node modified for automatic video slicing",
      "details": "Edited uni3c node so you don't have to put latents in it - when using multitalk sampling it uses input video and slices by audio indices",
      "from": "Kijai"
    },
    {
      "finding": "Block swap files are not saved on drive",
      "details": "Block swap data is kept in memory, not written to disk storage",
      "from": "Kijai"
    },
    {
      "finding": "Higher LoRA rank parameters have bigger effect",
      "details": "Rank 256 produces much more movement than rank 32, especially in extracted loras like lightx2v",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "finding": "I2V makes crazy T2V when there is no image",
      "details": "Wan 2.2 I2V generates interesting text-to-video results when no image is provided",
      "from": "hicho"
    },
    {
      "finding": "Using Qwen2.5 VL then feeding to T5 for better results",
      "details": "Process of using Qwen2.5 VL to describe image and reprompting to get base image correct before feeding to Wan2.2 I2V workflow gives crazy results",
      "from": "Josiah"
    },
    {
      "finding": "Qwen2.5-VL best for input prompt",
      "details": "Qwen2.5-VL will always be best for input prompt because that's what they used to train it",
      "from": "fredbliss"
    },
    {
      "finding": "9 frame motion overlap works fine and is faster",
      "details": "Motion frame setting of 9 works fine compared to 25 and is significantly faster (15:35 vs 20:33)",
      "from": "Kijai"
    },
    {
      "finding": "CFG 1 doesn't use negative prompts",
      "details": "When using CFG=1, the negative prompt is not being used",
      "from": "mamad8"
    },
    {
      "finding": "Lightning T2V 1.1 HIGH works better than LOW model in some cases",
      "details": "User accidentally used HIGH on LOW model and found it worked better, could be due to tuning",
      "from": "phazei"
    },
    {
      "finding": "Swap blocks setting may only take effect on new prompts",
      "details": "Changing from 35 to 20 swap blocks didn't show speed improvement until prompt was changed, then it went from 11 minutes to 7 minutes 30 seconds",
      "from": ". Not Really Human :."
    },
    {
      "finding": "MultTalk branch ksampler re-initializes timesteps and scheduler",
      "details": "This causes it to completely ignore start and end steps of the ksampler",
      "from": "MysteryShack"
    },
    {
      "finding": "Wan 2.2 fp32/fp16 mixed model has performance issues",
      "details": "Model stuck at 22GB VRAM usage, did 1 step after 8 minutes with only 50% GPU utilization instead of normal 78\u00b0C",
      "from": "seitanism"
    },
    {
      "finding": "DawnII achieved good V2V results using magref",
      "details": "Successfully performed video-to-video generation with magref model, creating quality results",
      "from": "DawnII"
    },
    {
      "finding": "InfiniteTalk has noise at beginning due to extra latent conditioning",
      "details": "The noise at the beginning of InfiniteTalk generations is a feature - it expects extra latents or conditioning that aren't processed in regular ksampler pass",
      "from": "MysteryShack"
    },
    {
      "finding": "Manual sigma scheduling works better than BetaSamplingScheduler",
      "details": "Using manual sigmas [1.0000, 0.9375, 0.8750, 0.4375, 0.0000] on T2V gives better results than trying to adjust BetaSamplingScheduler for 4 steps",
      "from": "phazei"
    },
    {
      "finding": "InfiniteTalk model cannot handle full noise, it never does in the loop",
      "details": "The first frame noise issue in InfiniteTalk is caused by the model not being able to handle full noise. To fix this, encode the init image and add it to noise.",
      "from": "Kijai"
    },
    {
      "finding": "Using the same initial frame with noise for all 81 frames can produce better T2V results",
      "details": "Randomly tried repeating the same initial frame with noise for all 81 frames and it produced significantly better video results instead of shitty output.",
      "from": "fredbliss"
    },
    {
      "finding": "Qwen image -> Wan video integration working via T2V",
      "details": "Successfully got Qwen-image to Wan video pipeline working through T2V method, with aligned results when using same prompt for both Qwen image and Wan.",
      "from": "fredbliss"
    },
    {
      "finding": "Qwen2.5-VL can be used for image captioning to generate better Wan prompts",
      "details": "Using structured data output from Qwen2.5-VL, then rewriting into short paragraph style that Wan likes improves results",
      "from": "fredbliss"
    },
    {
      "finding": "Qwen Image latents can be passed directly to Wan sampler without text prompts",
      "details": "Still captures concepts from the original image even without prompt, qwen image at 1024x1024 becomes 768x768 in wan",
      "from": "fredbliss"
    },
    {
      "finding": "Wan 2.2 I2V model behaves differently from 2.1",
      "details": "Doesn't strongly stick to the init image, which could be useful for long I2V generations",
      "from": "Kijai"
    },
    {
      "finding": "Light2XV LoRA from Wan 2.1 works better on Wan 2.2 than the new 1.1 speed LoRAs",
      "details": "Settings: 3 on high noise, 1 on low noise pass",
      "from": "NebSH"
    },
    {
      "finding": "Video looping technique improves long audio generation",
      "details": "Generate short 3 second video with Wan 2.2, put in loop, then use with long audio for better results",
      "from": "NebSH"
    },
    {
      "finding": "Wan model works at 729p resolution",
      "details": "Not limited to 480p, higher resolutions work fine",
      "from": "Kijai"
    },
    {
      "finding": "InfiniteTalk produces extra tail frames at the end",
      "details": "Model always produces constant input, extra second of frames is normal behavior",
      "from": "Kijai"
    },
    {
      "finding": "Start step 2 in vid2vid means first 2 steps are considered done",
      "details": "Noise is added to input video based on what noise should be on those steps, same principle as img2img",
      "from": "Kijai"
    },
    {
      "finding": "Audio CFG should not be close to 1",
      "details": "Audio scale being too high changes the face, at least on 2.2 low noise",
      "from": "MysteryShack"
    },
    {
      "finding": "GGUF VACE modules now supported",
      "details": "Can load GGUF VACE modules, don't have to be same Q-type",
      "from": "Kijai"
    },
    {
      "finding": "Context windows use overlapping frames",
      "details": "Each window is 81 frames with 9 overlap for smooth transitions",
      "from": "Kijai"
    },
    {
      "finding": "Differential diffusion for inpainting was in wrong place and never worked properly",
      "details": "Kijai discovered the differential diffusion implementation in wrapper inpainting was incorrectly positioned, causing it to not function as intended for a year",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 14B doesn't make photo prompts into actual photos unlike other models",
      "details": "When using default sample prompt 'photo of chess playing in park', Wan 2.2 14B is first model that doesn't make it a photo, while 2.1 and 2.2 5b do",
      "from": "Drommer-Kille"
    },
    {
      "finding": "InfiniteTalk requires finished denoised output for proper I2V chaining",
      "details": "You can't use noisy input to I2V model - it has to finish the previous window for the method to work properly",
      "from": "Kijai"
    },
    {
      "finding": "Normal map control in VACE bleeds into colors heavily",
      "details": "Normal map control causes significant color bleeding issues making it unusable, though lowering saturation prevents 90% of bleed",
      "from": "Blink"
    },
    {
      "finding": "Wan models feel like they're meant to be interpolated",
      "details": "Raw wan output feels like it's skipping every other frame, probably byproduct of cutting 30fps videos down to 16fps",
      "from": "ArtOfficial"
    },
    {
      "finding": "VACE inpaint mask decides which part of the video will be processed",
      "details": "Best combination is to fill the mask with corresponding ControlNet, leaving outside unprocessed. If you blur the mask, artifacts and ghosting appear",
      "from": "Nekodificador"
    },
    {
      "finding": "VACE reference is underrated if used well",
      "details": "Works well for inpainting existing subjects, less effective for adding new subjects where they don't exist",
      "from": "Nekodificador"
    },
    {
      "finding": "First-to-last frame technique for VACE",
      "details": "Input first frame and leave rest grey/gray - gives VACE inertia but model dreams the rest. Repeating all frames makes model get stuck",
      "from": "Nekodificador"
    },
    {
      "finding": "NAG is applied to positive conditioning, not negative",
      "details": "With cfg 1.0, normal negative isn't used. NAG affects the positive prompt",
      "from": "Kijai"
    },
    {
      "finding": "Mixed fp32/fp16 models show minimal difference",
      "details": "99% fp16 with fp32 bias adds only 6mb. Difference is honestly minimal and mostly random compared to pure fp16",
      "from": "Kijai"
    },
    {
      "finding": "WanVideoWrapper supports Qwen text enhancement",
      "details": "Qwen with system prompts to enhance your prompt, uses similar captioning as training data",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "MultiTalk uses static box masks, not frame-by-frame",
      "details": "Multitalk node needs audio for each speaker and mask for each speaker in a batch. Uses static masks, not precise mouth targeting",
      "from": "Kijai"
    },
    {
      "finding": "Extra latent addition improves WAN 2.2 I2V prompt adherence for jump cuts and scene changes",
      "details": "Adding an extra latent to I2V node fixes issues with background dissolve and scene changes that were previously resulting in blackness around characters",
      "from": "MysteryShack"
    },
    {
      "finding": "First latent as reference technique works differently than normal I2V",
      "details": "Replacing first latent with image itself works more like VACE, where normal I2V uses full noise for first latent and image in separate 16 channel input",
      "from": "Kijai"
    },
    {
      "finding": "Lightning LoRA is censored and affects character consistency",
      "details": "Using lightning i2v lora gives better prompt adherence but censors content and reduces character consistency",
      "from": "MysteryShack"
    },
    {
      "finding": "Inpainting differential diffusion code placement fixed",
      "details": "Differential diffusion code was moved to correct place, improving inpainting results significantly",
      "from": "Kijai"
    },
    {
      "finding": "Control splines in Blender work well for camera and object movement",
      "details": "Using grease pencil outlines, curves and spheres in Blender for control splines provides more information than just openpose",
      "from": "Blink"
    },
    {
      "finding": "Gradation in masks works better with differential diffusion than black/white masks",
      "details": "Grayscale masks provide better blending results when using differential diffusion",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "CineScale uses 2-stage sampling process with latent upscale",
      "details": "First sampler at 1920x1088 with RoPE [1.0, 20.0, 20.0], then upscale and second sampler at 2880x1632 with RoPE [1.0, 25.0, 25.0]",
      "from": "Kijai"
    },
    {
      "finding": "CineScale LoRA without NTK scale produces overblown results",
      "details": "The RoPE scaling is important for proper results, spatial base frequency is scaled while temporal stays at 1.0",
      "from": "Kijai"
    },
    {
      "finding": "Phantom model shows decreasing consistency below 121 frames",
      "details": "Perfect character consistency at 121 frames, clothing/face loses consistency at 81, introduces midget at 49 frames",
      "from": "mdkb"
    },
    {
      "finding": "FP8 vs FP16 shows minimal quality difference for prompt adherence",
      "details": "Comparisons showed hardly any quality difference between fp8 and fp16 scaled models",
      "from": "seitanism"
    },
    {
      "finding": "Wan 2.2 with latent upscale works poorly as it's too noisy",
      "details": "The original code finishes the gen on the first sampler, and bilinear works better than nearest exact",
      "from": "Kijai"
    },
    {
      "finding": "Cinescale uses 1.5x upscale workflow",
      "details": "They do full generation on first sampler (50 steps), then vid2vid on second starting from step 15 with latent upscale in between",
      "from": "Kijai"
    },
    {
      "finding": "RoPE scaling is useful even without cinescale",
      "details": "Messing with the rope scale seems to be useful even without cinescale",
      "from": "Kijai"
    },
    {
      "finding": "Three sampler combination fights against slowmo",
      "details": "1x high (no acceleration lora, cfg 3.5, 4 steps), 1x high (acceleration lora, cfg 1, 4 steps), 1x low (acceleration too, cfg 1, 4 steps)",
      "from": "Juan Gea"
    },
    {
      "finding": "Qwen VAE vs Wan VAE differences for single images",
      "details": "QwenImage VAE is smoother and more aesthetically pleasing but also blurrier compared to Wan VAE for single image generation",
      "from": "Kijai"
    },
    {
      "finding": "Qwen VAE produces better alignment in small patches compared to Wan VAE",
      "details": "Qwen VAE produces more natural blurring while Wan VAE creates effects similar to upscalers inventing detail where there's no info",
      "from": "Kijai"
    },
    {
      "finding": "MAGREF can adjust strength by padding images with white",
      "details": "Even full frame it's weaker when padded, and you can use different images for context windows beyond the start image",
      "from": "Kijai"
    },
    {
      "finding": "I2V models are incompatible with context windows by nature",
      "details": "I2V models being start image models makes them incompatible with context windows concept, only MAGREF allows this functionality",
      "from": "Kijai"
    },
    {
      "finding": "Skyreels LoRA works to get 121 frames and breaks looping in 2.2",
      "details": "Successfully broke the looping in 2.2 I2V when trying 121 frames",
      "from": "Kijai"
    },
    {
      "finding": "CineScale LoRA released for 4K video generation",
      "details": "New LoRA came out for upscaling video generation to 4K resolution",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 has ping pong effect after 81 frames",
      "details": "Video tries to go back to first frame after 81 frames, related to 16fps training",
      "from": "JohnDopamine"
    },
    {
      "finding": "CFG scheduling with LightX2V LoRA strength scheduling gives best results",
      "details": "Using 6-8 total steps split at middle, first step lora at 1.0, cfg around 3, then 2nd step cfg 2, lora 2, then lora 3, cfg 1.0",
      "from": "Kijai"
    },
    {
      "finding": "Prompt adherence is mostly from the high noise side",
      "details": "The high noise side is responsible for most prompt adherence in WAN 2.2",
      "from": "Kijai"
    },
    {
      "finding": "CausVid is better than other distill LoRAs for high noise",
      "details": "CausVid performs better than alternatives for high noise sampling in WAN 2.2",
      "from": "DawnII"
    },
    {
      "finding": "VAE decoding performance varies significantly by precision on AMD",
      "details": "fp16: 1:16, fp32: 3:20, bf16: 2:33 for VAE decoding",
      "from": "patientx"
    },
    {
      "finding": "MelBandRoFormer vocal separation model performs better than demucs in some cases",
      "details": "Can separate vocals and instruments from audio, processes 3min song in 33sec",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 Low I2V model produces superior quality with specific settings",
      "details": "Using euler/beta combination, 8 steps, old lightning lora at 1.0 on 720x720 images gives vastly superior quality and details, though artifacts occur on every other gen",
      "from": "gordo"
    },
    {
      "finding": "Speed LoRAs don't work with high noise model",
      "details": "LightX2V and other speedup loras don't work at all with high noise model unless trained specifically for it",
      "from": "MysteryShack"
    },
    {
      "finding": "Multi-sampler setup can make speed LoRAs work with Wan 2.2",
      "details": "3 sampler setup with 1 step full HN, or distribution like 3full/1lightning/3/1 with proper shift adjustments",
      "from": "Karo"
    },
    {
      "finding": "CFG Schedule improves prompt adherence",
      "details": "Wrapper with CFG Schedule reads prompts better and has more prompted look compared to native",
      "from": "Drommer-Kille"
    },
    {
      "finding": "VACE requires perfect masks without blur",
      "details": "VACE doesn't work with blurry masks, masks need to be perfect with no blur",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "finding": "Mix color by mask technique for VACE inpainting",
      "details": "When doing inpainting with VACE, feed the original video with white on the mask area besides providing the mask itself, using mix color by mask node to do this automatically",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "finding": "VAE cache memory leak in InfiniteTalk",
      "details": "VAE cache doesn't get cleared between windows in InfiniteTalk loop, causing 2-3GB more VRAM usage from 2nd window onwards",
      "from": "Kijai"
    },
    {
      "finding": "Lightning 1.1 High LoRA with WAN 2.2 Low gives better results",
      "details": "Using the wrong Lightning LoRA (1.1 High with 2.2 Low) in upscaling workflow gave better results - less saturated, more detailed",
      "from": "InsalaIlive"
    },
    {
      "finding": "CFG scheduling enables motion blur in WAN",
      "details": "Using CFG scheduling got WAN to finally do motion-blur, at least sort of",
      "from": "Drommer-Kille"
    },
    {
      "finding": "High noise WAN 2.2 model with CFG gives more motion",
      "details": "With the high noise 2.2 model, using CFG gives you more motion and more prompt adherence, not quality",
      "from": "Kijai"
    },
    {
      "finding": "4 or 5 steps minimum required for high noise model",
      "details": "Can't go below N number of steps on the high noise model - 4 or 5 steps is an epic fail, speedup LoRAs may not work properly with too few steps",
      "from": "MysteryShack"
    },
    {
      "finding": "Context windows don't work with I2V",
      "details": "Context windows only work with T2V, not I2V models",
      "from": "Kijai"
    },
    {
      "finding": "Context windows work better with limited movement",
      "details": "Works for content with pretty much no moving camera and repeating actions like dancing, as long as subject stays in frame",
      "from": "Draken"
    },
    {
      "finding": "2.2 works better with existing quantizations than 2.1",
      "details": "2.2 even works with fp8_fast while 2.1 quality dies with it",
      "from": "Kijai"
    },
    {
      "finding": "Block swapping is only faster if it prevents VRAM overflow",
      "details": "Block swapping is way faster than memory fallback, but only provides speed benefit when it saves you from going above 95% VRAM usage",
      "from": "Kijai"
    },
    {
      "finding": "New CFG handling method",
      "details": "Changed how cfg input is handled - now extends list by last value or cuts it to match steps instead of setting steps to match cfg",
      "from": "Kijai"
    },
    {
      "finding": "CFG scheduling hack for first step only",
      "details": "Setting end percent to 0.01 applies CFG float to only the initial step - not actual math, just a hack to ensure CFG 2 lands only on first step",
      "from": "DawnII"
    },
    {
      "finding": "Control latent strength 0 keeps generation static",
      "details": "Setting control latent str to 0 keeps the whole generation static",
      "from": "DawnII"
    },
    {
      "finding": "FantasyPortrait face detector improvement",
      "details": "Updated to use previous working detection for missing faces instead of failing or getting out of sync, prevents audio desync with Infinite/MultiTalk",
      "from": "Kijai"
    },
    {
      "finding": "VRAM optimization for Long I2V Multi/InfiniteTalk",
      "details": "Optimizations reduce VRAM use by 2-3GB at default res by clearing VAE cache between windows",
      "from": "Kijai"
    },
    {
      "finding": "Hidden helper node for mask operations",
      "details": "Has helper node for VACE mask purposes, originally for Fun InP but works for other uses by inverting the mask",
      "from": "Kijai"
    },
    {
      "finding": "Keyframe interpolation is basic VACE feature",
      "details": "VACE always had interpolation capabilities, it's a basic feature that many people miss",
      "from": "Kijai"
    },
    {
      "finding": "VACE requires accurate positioning of reference images to work properly",
      "details": "Position matters in the reference image setup - had to overlay reference image in Krita over original at exact position to get proper character replacement",
      "from": "mdkb"
    },
    {
      "finding": "White padding around VACE reference images affects output drastically",
      "details": "Adding 50px white padding changed jacket color to bright blue, removing padding fixed the issue",
      "from": "mdkb"
    },
    {
      "finding": "VACE reference images are cropped to same size as video",
      "details": "If you have a tall reference image with your subject filling the image, it will be cropped to just the person's waist",
      "from": "Ablejones"
    },
    {
      "finding": "Wan 2.2 Lightning LoRAs have bias towards bright lighting",
      "details": "Lightning 2.2 lora absolutely loves bright lights and results in more well-lit videos",
      "from": "Ablejones"
    },
    {
      "finding": "Model loading times are inconsistent in latest wrapper version",
      "details": "Generation times fluctuate between 15-18 sec/it with latest version vs steady 14 sec/it with older version from August 19th",
      "from": "patientx"
    },
    {
      "finding": "2.2 VAE is only for the 5B model, 14B still uses 2.1 VAE",
      "details": "Users were confused about which VAE to use with different model variants",
      "from": "DawnII"
    },
    {
      "finding": "VACE + standin works very well for a full one shot solution",
      "details": "Combination provides effective single-step workflow",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "MAGREF doesn't degrade video quality over time but has lower image fidelity",
      "details": "Appears blurry in comparison to other models but maintains consistency",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 low noise model works great for T2I generation",
      "details": "Unexpected application discovered for single image generation",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "finding": "Kijai converted wav2vec2 model to safetensors format with fp16 version",
      "details": "Single file loading like all models, fp16 version for slight RAM savings and faster loading",
      "from": "Kijai"
    },
    {
      "finding": "N0NSens got 2.2 + InfiniteTalk + color match working well",
      "details": "Uses simple color match node after generation, works better than VACE for color consistency",
      "from": "N0NSens"
    },
    {
      "finding": "Canny control needs to be inverted for VACE workflows",
      "details": "Use image invert node after canny for proper results",
      "from": "mdkb"
    },
    {
      "finding": "JalenBrunson chains multiple VACE encoders with different controlnets",
      "details": "1 vace encoder with dwpose, 1 with depth, 1 with ref image, adjusting strength for best combination",
      "from": "JalenBrunson"
    },
    {
      "finding": "Wan 2.2 S2V has pose control ability built-in",
      "details": "The model includes pose control functionality similar to UniAnimate",
      "from": "DawnII"
    },
    {
      "finding": "VRAM usage reduction with iGPU setup",
      "details": "Switching monitor to motherboard iGPU saves 4% VRAM on 5090 (from 7% to 3% baseline usage)",
      "from": "Drommer-Kille"
    },
    {
      "finding": "Wan 2.2 InfiniteTalk tends to move camera regardless of prompt",
      "details": "No matter what's in prompt, it tends to move the camera, with no visible glitches using context_opt",
      "from": "N0NSens"
    },
    {
      "finding": "WanFM doubles inference time with minimal quality improvement",
      "details": "Results are almost same as just using 2.2 without it, while taking twice as long",
      "from": "Kijai"
    },
    {
      "finding": "Wan S2V outputs are 16 fps",
      "details": "The video is saved at 16 fps in their code, outputs are 16 fps",
      "from": "ZeusZeus/Kijai"
    },
    {
      "finding": "Wan S2V uses interpolation from 50 fps input to 16 fps",
      "details": "Input fps is hardcoded to 50, sample rate is 16, involves interpolation between these rates",
      "from": "Kijai"
    },
    {
      "finding": "Wan S2V is heavier on VRAM than Wan 2.2 A14B but easier on RAM",
      "details": "The new S2V model requires more VRAM than the 14B model but uses less RAM",
      "from": "Kijai"
    },
    {
      "finding": "Wan S2V is actually 4 models combined",
      "details": "Consists of base model + pose model + audio model + framepack",
      "from": "Kijai"
    },
    {
      "finding": "Reference latent is used exactly like Phantom",
      "details": "The ref latent implementation follows the same pattern as Phantom model",
      "from": "Kijai"
    },
    {
      "finding": "CFG Schedule with normal steps and sampler works better than 3 samplers",
      "details": "Changing Create CFG Schedule Float List to normal steps makes wonders, keeps prompt adherence and is faster than using 3 samplers",
      "from": "Mu5hr00m_oO"
    },
    {
      "finding": "S2V requires specific resolution formula for successful generation",
      "details": "Always ensure (width\u00f716) \u00d7 (height\u00f716) \u00d7 frames \u00f7 30 is a whole number. Examples: 832x480 with 81 frames works, 320x320x121 doesn't, 480x480 with 121 frames works",
      "from": "patientx"
    },
    {
      "finding": "S2V appears to respond to music rhythm",
      "details": "Character's dance changes with the rhythm of the music when using audio input",
      "from": "slmonker"
    },
    {
      "finding": "CFG Skimming is CFG interpolation",
      "details": "CFG Skimming experiment allows 4 steps (2+2) Lightning generation with LoRA on HIGH & LOW",
      "from": "DawnII"
    },
    {
      "finding": "WAN 2.2 defaults to 16 fps output",
      "details": "The model generates at 16 fps by default, not 24 fps",
      "from": "DawnII"
    },
    {
      "finding": "CFG 1 reduces motion and prompt adherence in WAN 2.2",
      "details": "Setting CFG to 1 diminishes motion and prompt adherence, ideally need full CFG for high noise steps",
      "from": "MysteryShack"
    },
    {
      "finding": "Context windows don't increase VRAM usage",
      "details": "Generating 1000 frames uses same VRAM as 81 frames, same principle as framepack",
      "from": "Kijai"
    },
    {
      "finding": "WAN S2V uses framepack architecture",
      "details": "The S2V implementation is based on framepack technology",
      "from": "Kijai"
    },
    {
      "finding": "Native ComfyUI implementation can do infinite length video",
      "details": "Once properly implemented, the native version will support infinite length video generation",
      "from": "comfy"
    },
    {
      "finding": "bf16 performs better than other precisions for Wan",
      "details": "bf16 bit better as expected",
      "from": "Kijai"
    },
    {
      "finding": "Original code drops initial frames",
      "details": "even the original code drops some of the inital frames btw",
      "from": "Kijai"
    },
    {
      "finding": "S2V works better with simpler prompts",
      "details": "weirdly works better with less words in the prompt? huh just 'man talking' giving me better results than a detailed description",
      "from": "pewpewpew"
    },
    {
      "finding": "Audio needs to be zeroed for uncond in CFG",
      "details": "boosting lightx2v strength actually does improve it, and at least in the wrapper I had cfg implemented wrong, the audio needs to be zeroed for uncond",
      "from": "Kijai"
    },
    {
      "finding": "Custom linear sigmas work better for Lightning LoRA",
      "details": "for lightning lora and exactly 4 steps the key is to have 4 linear sigmas, normal schedulers tend to put the last sigma way too low. you would use custom sigmas with 1.0, 0.75, 0.5, 0.25 and shift 5",
      "from": "Kijai"
    },
    {
      "finding": "Resolution affects LoRA strength requirements",
      "details": "lower the res, less strong should the lightx2v be, and also shift should be lower. and at higher res it benefits a lot from stronger lightx2v or cfg",
      "from": "Kijai"
    },
    {
      "finding": "CLIP vision can be used to split image attention",
      "details": "It will split the image cross attention so that the start image affects first half of the video and end image the latter half....it's not super strong effect but somewhat useful if your start and end are very different images",
      "from": "Kijai"
    },
    {
      "finding": "Merged vs unmerged LoRAs behave very differently in wrapper",
      "details": "Unmerged LoRAs can completely break custom LoRAs that work fine in native, while merged LoRAs work similarly to native. Native always merges LoRAs, while GGUF is always unmerged",
      "from": "mamad8, Kijai, Draken"
    },
    {
      "finding": "LightX 2.1 LoRAs work with Wan 2.2",
      "details": "Users report LightX 2.1 LoRAs give good results with Wan 2.2, better than Lightning LoRAs",
      "from": "Lodis"
    },
    {
      "finding": "S2V pose control just works without additional setup",
      "details": "OpenPose control works with S2V model without needing complex configuration",
      "from": "Kijai"
    },
    {
      "finding": "Scheduler state reset issue between generations",
      "details": "Scheduler isn't properly reset between iterations, causing index out of bounds errors on second generation",
      "from": "Mu5hr00m_oO"
    },
    {
      "finding": "Context window frame count must not exceed input frames",
      "details": "Setting context window frame count higher than input frames causes errors",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "finding": "VACE doesn't support combined controls, Fun Control does",
      "details": "VACE embeds can be chained but don't support combined controls, while Fun Control supports combined inputs",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Multiple VACE encoders reduce skeleton artifacts",
      "details": "Using multiple VACE encoders instead of image blend of controlnets shows skeleton artifacts less frequently",
      "from": "JalenBrunson"
    },
    {
      "finding": "Lora merge explanation solves quality issues",
      "details": "Matching configurations between quantized and non-quantized versions, with proper lora merging giving perfect results on 6 steps using wrapper",
      "from": "mamad8"
    },
    {
      "finding": "VACE inpainting adds extra frames at beginning instead of end",
      "details": "When num_frames is larger than input control video, extra frames are added at beginning rather than end",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "finding": "Zero frames for motioner fixes start issue",
      "details": "Setting zero frames for the motioner gets rid of the start issue, though the code suggests it should set ref latent instead",
      "from": "Kijai"
    },
    {
      "finding": "S2V model outputs 16 fps while MultiTalk outputs 25 fps",
      "details": "S2V has stop motion/lagging issues compared to MultiTalk's smoother 25fps output",
      "from": "Kijai"
    },
    {
      "finding": "Face points only pose control reduces teeth artifacts",
      "details": "Using only facial pose points instead of full body pose reduces excessive teeth generation",
      "from": "Kijai"
    },
    {
      "finding": "Pose condition layer shouldn't be in fp8",
      "details": "Initial fp8 model had issues, pose cond layer needs to stay in higher precision while audio injection layers work fine in fp8",
      "from": "Kijai"
    },
    {
      "finding": "First step only with 0.5 multiplied pose condition works better",
      "details": "Using pose condition only on first step and multiplying by 0.5 reduces artifacts",
      "from": "Kijai"
    },
    {
      "finding": "Sapiens pose detection works better than DWPose for VACE",
      "details": "Sapiens with filtered side bones provides better pose control for VACE, avoiding headphone artifacts",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Context windows don't burn framepack implementation",
      "details": "Framepack method doesn't have burning issues but lipsync quality is reduced due to overlap",
      "from": "Kijai"
    },
    {
      "finding": "GGUF models offer better quality than fp8 quantization for Wan models",
      "details": "GGUF provides superior visual quality compared to fp8 quantized models, making it worth supporting despite complexity",
      "from": "Kijai"
    },
    {
      "finding": "S2V model can be used for infinite video generation without audio",
      "details": "S2V model has reference motion capability and can extend videos indefinitely without requiring audio input",
      "from": "comfy"
    },
    {
      "finding": "S2V model is hardcoded to 16fps output",
      "details": "The S2V model does interpolation to maintain 16fps output regardless of input audio characteristics",
      "from": "Kijai"
    },
    {
      "finding": "Speed LoRAs don't work well with S2V model for T2V",
      "details": "LightX and Lightning LoRAs lead to very blurry results with S2V model, and regular LoRAs need strength 2.0+ to show proper changes",
      "from": "flo1331"
    },
    {
      "finding": "wav2vec2 model outputs at 50fps but bucketing function expects 16fps",
      "details": "The wav2vec2 model outputs what's considered 50fps with 30 being the norm, but this model trained with 16 so their bucketing function expects that. The 16 is determined in the bucket function",
      "from": "Kijai"
    },
    {
      "finding": "S2V can interpret depth maps",
      "details": "S2V appears to have the ability to interpret depth maps in addition to other inputs",
      "from": "DawnII"
    },
    {
      "finding": "S2V model is closer to 2.1 than 2.2 high noise",
      "details": "S2V is much closer to 2.1 than the high noise model is for sure, but also further than 2.2 low noise",
      "from": "Kijai"
    },
    {
      "finding": "Infinite talk doesn't like complex prompts",
      "details": "User realized infinite talk doesn't work well with overly complex prompts",
      "from": "l\u0488u\u0488c\u0488i\u0488f\u0488e\u0488r\u0488"
    },
    {
      "finding": "uni3c extracts camera motion from reference videos",
      "details": "uni3c can look at what's happening in iPhone videos and apply camera movements (snap zooms, pans, orbits) to generations",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "finding": "Image encode extreme slowdown problem was resolved",
      "details": "User experienced extreme slowdown with image-to-video encode that got resolved after recent updates",
      "from": "cocktailprawn1212"
    },
    {
      "finding": "Memory issues fixed with merged LoRAs",
      "details": "Model loading bugs when using merged LoRAs were fixed, which would not leave memory for encode node to work properly",
      "from": "Kijai"
    },
    {
      "finding": "MAGREF is a full Wan I2V model, not a clip vision model",
      "details": "MAGREF is not a clip vision model but a complete Wan I2V model that can still use clip vision but it's not necessary",
      "from": "Kijai"
    },
    {
      "finding": "VACE encode memory usage is optimized",
      "details": "VACE encode peaked at 15GB VRAM for 85 frames at 1280x720 with 6 steps, taking 2:47 to complete",
      "from": "Kijai"
    },
    {
      "finding": "Framepack quality degrades over time",
      "details": "Framepack goes bad the longer it goes, need to use weak settings or find balance between motion and image quality",
      "from": "Kijai"
    },
    {
      "finding": "Context windows vs Framepack tradeoff",
      "details": "Context windows give better quality for long gens but lipsync isn't as good compared to Framepack",
      "from": "Kijai"
    },
    {
      "finding": "Beta scheduler differences between ComfyUI and diffusers",
      "details": "ComfyUI scheduler beta is completely different from diffusers beta - diffusers applies shift first then beta, while ComfyUI may do it the other way around",
      "from": "Kijai"
    },
    {
      "finding": "CFG guidance works differently with empty negative prompts vs no conditioning",
      "details": "CFG with empty negative prompt still runs unconditional passes and subtracts that result, it's not the same as subtracting zero",
      "from": "Ablejones"
    },
    {
      "finding": "Context windows work better for Wan than other extension methods",
      "details": "Zero degradation over time compared to other approaches, though increases inference time due to overlap areas",
      "from": "Kijai"
    },
    {
      "finding": "VACE can be used for scene transitions by setting strength to 0.0",
      "details": "Dropping VACE strength low enough can change scenes mid-clip while maintaining style/subject coherence",
      "from": "Ablejones"
    },
    {
      "finding": "Alpha channel information is preserved in different ways",
      "details": "VACE can animate subjects while keeping backgrounds even with transparent input because RGB data is preserved under alpha",
      "from": "hicho"
    },
    {
      "finding": "Wrapper automatically burns alpha backgrounds with framepack",
      "details": "When using rgba images, the wrapper with framepack burns the alpha into the background automatically",
      "from": "Kijai"
    },
    {
      "finding": "S2V pose control is more absolute than FantasyPortrait",
      "details": "S2V control works in pixel space absolutely, while FP does it relatively. S2V at full strength is strict but at lower strength and first step only it's less strict",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca- and Kijai"
    },
    {
      "finding": "Multiple control methods can be combined",
      "details": "Can use FantasyPortrait + S2V + InfiniteTalk + VACE all together",
      "from": "Kijai"
    },
    {
      "finding": "Character LoRAs help with FantasyPortrait character changes",
      "details": "Using character LoRAs helps prevent FP from changing the character too much at full strength",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Context windows work with prompt travel",
      "details": "Prompt travel functionality works with context windows and infinite talk loop",
      "from": "Kijai"
    },
    {
      "finding": "Padding embeddings with zero vectors works for torch compile",
      "details": "Fixed torch compile issues by padding the embedding with zero vectors instead of tokenized string padding",
      "from": "phazei"
    },
    {
      "finding": "Fun Control 5b can mimic Uni3c in 2.2",
      "details": "Can mimic uni3c camera motion in 2.2 model using 5b fun control by providing video with camera motion without controlnet",
      "from": "hicho"
    },
    {
      "finding": "InfiniteTalk can work with T2V for short generations",
      "details": "For short gens it just works, for long gens maybe with context windows. Doesn't work with the long gen loop",
      "from": "Kijai"
    },
    {
      "finding": "Regular Wan model can handle up to 109 frames",
      "details": "Regular model do 81. sky do 121. Regular can do up to 109 and has no issues with 105 frames in terms of following prompt",
      "from": "N0NSens"
    },
    {
      "finding": "InfiniteTalk setup for T2V requires specific configuration",
      "details": "T2V only put infinitetalk on the high sample - user found this was what they were doing wrong",
      "from": "NebSH"
    },
    {
      "finding": "Lightx2v LoRA works better with native S2V at specific settings",
      "details": "Try with the Lightx2v I2V lora, run 8 steps, at CFG 2.5 or 3.0, strength 1.0 on the lora",
      "from": "Ablejones"
    },
    {
      "finding": "Camera control can be done entirely through prompts without complex nodes",
      "details": "User discovered they don't need crazy nodes for camera control, everything can be done in the prompt",
      "from": "Akumetsu971"
    },
    {
      "finding": "Multiple characters can work with InfiniteTalk",
      "details": "Found method to have multi-character conversations with infinite",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "finding": "Context window panda T2V can be converted to I2V",
      "details": "Successfully converted by replacing empty embeds and T2V models with I2V models",
      "from": "Dever"
    }
  ],
  "troubleshooting": [
    {
      "problem": "Thaakenos workflow error with custom nodes",
      "solution": "Disable the VAE node that sends VAE everywhere",
      "from": "GOD_IS_A_LIE"
    },
    {
      "problem": "ComfyUI memory not releasing after model unload",
      "solution": "Use --cache-none flag for everything out of memory, --disable-smart-memory for VRAM clearing",
      "from": "mdkb"
    },
    {
      "problem": "ComfyUI Manager not installing nodes properly",
      "solution": "Manually git clone the missing node repositories into custom_nodes folder",
      "from": "Simjedi"
    },
    {
      "problem": "VAE getting stuck in crawl mode after several generations",
      "solution": "Full PC restart required, WSL2 recommended for easier instance restart",
      "from": "Ghost"
    },
    {
      "problem": "Memory issues with WAN 2.2 after updates",
      "solution": "Use --cache-none launch flag to prevent node caching and reduce RAM usage",
      "from": "Kijai"
    },
    {
      "problem": "VRAM fills up on second generation",
      "solution": "Enable 'offload_device' setting on first sampler to free VRAM",
      "from": "Kijai"
    },
    {
      "problem": "RadialAttention resolution errors",
      "solution": "Image size must be divisible by block size (128), use 1280x768 instead of 1280x704",
      "from": "NebSH"
    },
    {
      "problem": "Triton compilation errors with RadialAttention",
      "solution": "Install Triton from woct0rdho/triton-windows repository rather than self-compiling",
      "from": "Kijai"
    },
    {
      "problem": "WanVideoTextEncodeSingle meta tensor error",
      "solution": "Update to latest wrapper version - bug was fixed in single encode node",
      "from": "Kijai"
    },
    {
      "problem": "Preview not showing on sampler nodes",
      "solution": "Update ComfyUI and wrapper - may be related to ComfyUI preview code changes",
      "from": "au"
    },
    {
      "problem": "WanVideoSampler error: invalid literal for int() with base 10",
      "solution": "Change starting steps from 0,-1 to 1,10000. End step -1 is correct for latest version",
      "from": "Jas"
    },
    {
      "problem": "Frontend update broke wrapper with VHS nodes",
      "solution": "Need latest VHS nodes to work, refresh frontend",
      "from": "Kijai"
    },
    {
      "problem": "Context options causing 'Dimension out of range' error",
      "solution": "Update to latest version, Kijai fixed this issue recently",
      "from": "Kijai"
    },
    {
      "problem": "VRAM OOM on second sampler with context windows",
      "solution": "Don't connect context options to second sampler, or use less overlap on low noise side",
      "from": "Kijai"
    },
    {
      "problem": "Wrapper sampler previews broken",
      "solution": "Use latent2rgb instead of TAESD, though quality is worse",
      "from": "lostintranslation"
    },
    {
      "problem": "Black output when disabling lightx2v",
      "solution": "Check if using correct models from Kijai repo instead of ComfyUI provided models",
      "from": "crinklypaper"
    },
    {
      "problem": "Loop detected error when using workflows from Discord",
      "solution": "Copying the entire workflow and pasting it into a blank comfy session fixes it",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "problem": "Resolution tensor issues with 2.2 VAE",
      "solution": "Update ComfyUI - the error just means your comfyui is not updated",
      "from": "Kijai"
    },
    {
      "problem": "5Bi2v workflow not working",
      "solution": "If you disconnect samples from extra latents it works",
      "from": "Dita"
    },
    {
      "problem": "Silent OOM state after canceling jobs",
      "solution": "Nothing except the top bar free and node cache actually clears VRAM properly",
      "from": "StableVibrations"
    },
    {
      "problem": "CG use everywhere custom node conflict",
      "solution": "Install the nightly version of the cg use everywhere custom node",
      "from": "xwsswww"
    },
    {
      "problem": "NAG context error with split_cross_attn_ffn",
      "solution": "This happens when you prompt with | - NAG not added there yet",
      "from": "Kijai"
    },
    {
      "problem": "IndexError with dimension out of range",
      "solution": "Needs negative prompt when CFG is enabled - NAG prompt doesn't count",
      "from": "Kijai"
    },
    {
      "problem": "LoadLora with CLIP on Wan 2.2 tries to make loop in I2V generation",
      "solution": "Issue reported but no solution provided",
      "from": "1022987353859575808"
    },
    {
      "problem": "Error 'nag_context is not supported in split_cross_attn_ffn'",
      "solution": "No solution provided",
      "from": "Yae"
    },
    {
      "problem": "Sageattention related errors in workflows",
      "solution": "Turn off sageattention",
      "from": "Kijai"
    },
    {
      "problem": "Color shift in middle of video with sliding windows",
      "solution": "Known issue with sliding window approach",
      "from": "1296968408147169280"
    },
    {
      "problem": "RAM maxing out and system freezing",
      "solution": "Use --disable-smart-memory and --cache-none flags when booting ComfyUI, create large swap files on NVME SSD",
      "from": "mdkb"
    },
    {
      "problem": "Latest ComfyUI core update breaking things",
      "solution": "Issue identified as breaking hotreload hack",
      "from": "Kijai"
    },
    {
      "problem": "Wrapper restarting runs even with no changes",
      "solution": "Update ComfyUI core to latest version along with wrapper",
      "from": "Rau"
    },
    {
      "problem": "Context options lack continuity, video replays after 81 frames",
      "solution": "This is a known issue with I2V - context doesn't work well with I2V due to strong image conditioning",
      "from": "Kijai"
    },
    {
      "problem": "5B model LoRAs don't work with 14B model due to size mismatch",
      "solution": "Cannot merge different shaped weights - connect LoRAs to correct model size",
      "from": "Kijai"
    },
    {
      "problem": "CFG 3.5 requires negative prompt to work",
      "solution": "Must add negative text encode node, not just NAG conditioning",
      "from": "Kijai"
    },
    {
      "problem": "Burnt frames when generating at 24fps",
      "solution": "Generate at 16fps then interpolate to 24fps using RIFE, as 14B model not optimized for 24fps",
      "from": "thaakeno"
    },
    {
      "problem": "Washed out video output with wrapper",
      "solution": "Use high noise without lightx2v LoRAs at CFG 3.5, then low noise with lightx2v at CFG 1",
      "from": "BondoMan"
    },
    {
      "problem": "TAESD previews not working after update",
      "solution": "Need to enable specific settings in manager - taew2_1 should be in vae_approx",
      "from": "DawnII"
    },
    {
      "problem": "Mat1 and mat2 shapes cannot be multiplied error",
      "solution": "Wrong text encoder being used for native ComfyUI clip loader",
      "from": "Kijai"
    },
    {
      "problem": "Model loader not working with default setting",
      "solution": "Manually select fp8_e5m2 vs default in model loader",
      "from": "topmass"
    },
    {
      "problem": "Blurry outputs with working generation",
      "solution": "Switch back to 2.1 VAE, increase steps from 10 to 25",
      "from": "topmass"
    },
    {
      "problem": "WanVideoVAE38 object has no attribute 'z_dim' error",
      "solution": "Update the wrapper - this was fixed recently",
      "from": "Kijai"
    },
    {
      "problem": "VAE 2.2 only works with 5B model",
      "solution": "Use VAE 2.1 for 14B model, VAE 2.2 only compatible with 5B",
      "from": "Kijai"
    },
    {
      "problem": "System crashing when second model loads",
      "solution": "Set --cache-none in ComfyUI config if system RAM is filling up",
      "from": "zelgo_"
    },
    {
      "problem": "Input size error 'The size of tensor a (11400) must match the size of tensor b (11685)'",
      "solution": "Input must be divisible by 32 - change from 960x656 to compatible resolution",
      "from": "daking999"
    },
    {
      "problem": "Character LoRAs from 2.1 not triggering in 2.2",
      "solution": "No definitive solution provided, reported as ongoing issue",
      "from": "screwfunk"
    },
    {
      "problem": "TypeError: argument of type 'NoneType' is not iterable",
      "solution": "Check for bypassed nodes before set node, ensure prompt nodes are properly connected",
      "from": "Kijai"
    },
    {
      "problem": "Middle frame conditioning not working properly",
      "solution": "Dial up the lora strength on the high noise",
      "from": "Faux"
    },
    {
      "problem": "Issues with middle frame implementation",
      "solution": "Mask values need to be correct - frames you keep = 1, not 0 as in some implementations",
      "from": "Kijai"
    },
    {
      "problem": "SageAttention CUDA compatibility issues with 12.9",
      "solution": "Recompile Triton, use torch-2.9.0.dev20250802+cu129 with triton-3.4.0",
      "from": "Kijai"
    },
    {
      "problem": "Need to restart ComfyUI between runs",
      "solution": "Use built-in clear VRAM buttons or add clear VRAM to VAE decode node",
      "from": "Ryzen"
    },
    {
      "problem": "H100 slower performance with sage attention",
      "solution": "Use 'auto' mode on H100, not specific sage attention modes",
      "from": "Kijai"
    },
    {
      "problem": "Burned/overexposed renders in Wan 2.2",
      "solution": "Add 'low quality' to negative prompts",
      "from": "Ryzen"
    },
    {
      "problem": "Character LoRAs from 2.1 not working in 2.2 I2V",
      "solution": "No definitive solution found, still being investigated",
      "from": "screwfunk"
    },
    {
      "problem": "Lighting strobing with CFG 3.5 on high steps",
      "solution": "Enable zero star and TCFG to eliminate strobing while maintaining quality",
      "from": "phazei"
    },
    {
      "problem": "System memory usage regression in wrapper",
      "solution": "Identified commit 5406a72f62adf4a31a8a0a0e4923cc5288399652 as culprit via git bisect",
      "from": "Doctor Shotgun"
    },
    {
      "problem": "Camera motion keywords being ignored",
      "solution": "Need more high steps to 'fit it in', try full workflow or improve prompts",
      "from": "Rainsmellsnice"
    },
    {
      "problem": "Training LoRA without affecting face/facial bleeding",
      "solution": "Blur faces in training data and caption them as blurred to isolate from normal faces",
      "from": "Fill"
    },
    {
      "problem": "fp8 model causing upscaling issues with flashing/blipping textures",
      "solution": "Model converts back to bf16 during process, block swap settings may need adjustment",
      "from": "MiGrain"
    },
    {
      "problem": "Fog/dust appearing in middle of FFLF transitions",
      "solution": "Likely caused by using over 81 frames, reducing frame count improves quality",
      "from": "Kijai"
    },
    {
      "problem": "fp8 model hallucinating incorrect objects",
      "solution": "fp8 can change subject matter (making divers into seaweed), use higher precision for accuracy",
      "from": "hicho"
    },
    {
      "problem": "Official Alibaba code extremely slow (25 min on B200)",
      "solution": "Code quantizes from fp32 to bf16 every run, plus uses large model chunks causing slowdown",
      "from": "aikitoria"
    },
    {
      "problem": "Burnt/overexposed video output in WanWrapper",
      "solution": "Refresh browser after updating to see new start/end step widgets, reconnect split step node",
      "from": "nacho.money"
    },
    {
      "problem": "Missing start and end step widgets in sampler nodes",
      "solution": "Make sure to update properly, refresh browser, and reload workflow. Check for conflicting installations",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI Manager update issues",
      "solution": "Delete from custom_nodes and use git pull instead of manager installation",
      "from": "voxJT"
    },
    {
      "problem": "Multistep samplers causing burning with model switching",
      "solution": "Add single step of euler when switching to LN model to reset multistep sampling",
      "from": "Ablejones"
    },
    {
      "problem": "Multitalk won't sync with Wan 2.2",
      "solution": "Currently doesn't work properly with 2.2, works better with 2.1",
      "from": "Kijai"
    },
    {
      "problem": "Character LoRA affecting face when training body",
      "solution": "Crop body only and caption correctly that it's cropped showing specific parts",
      "from": "mamad8"
    },
    {
      "problem": "OOM at 3496 x 2096 resolution",
      "solution": "Actually was OOM despite different error message - reduce resolution",
      "from": "Juan Gea"
    },
    {
      "problem": "Core dump with torchcompile on 5090",
      "solution": "Issue with Triton or something similar, works on Linux with latest nightly and latest triton",
      "from": "Kijai"
    },
    {
      "problem": "Blurry outputs on larger than 512px with lightx2v",
      "solution": "Use higher LoRA strength (3.0+) and proper scheduler/sampler combinations",
      "from": "Kijai"
    },
    {
      "problem": "Snow effect when using lightx2v",
      "solution": "Fix by using it at higher strength",
      "from": "Kijai"
    },
    {
      "problem": "GGUF compatibility issues",
      "solution": "Fixed with latest update, should work with GGUF now",
      "from": "Kijai"
    },
    {
      "problem": "Torch compile errors with native",
      "solution": "Try compiling only transformer blocks instead of full model",
      "from": "Kijai"
    },
    {
      "problem": "WanVideoSampler cannot import 'Wan22' error",
      "solution": "Need to use Wan 2.2 model or rollback wrapper version when trying to use updated wrapper with Wan 2.1",
      "from": "ChristianAr(3090)"
    },
    {
      "problem": "T2I attempts producing poor quality",
      "solution": "Use shift 1 instead of higher shift values for single frame generation",
      "from": "aikitoria"
    },
    {
      "problem": "MMaudio producing weird output for laughter",
      "solution": "Use dead simple prompts like 'woman laugh' or 'the woman is laughing' instead of complex descriptions",
      "from": "TK_999"
    },
    {
      "problem": "Static video glitch output in first sampler",
      "solution": "Check if input image is connected properly to VAE encoder and ensure VAE is connected to multiple nodes as needed",
      "from": "TK_999"
    },
    {
      "problem": "Mat1 and mat2 error when using non-fp8 CLIP",
      "solution": "Use e5 fp8 weights for Ampere cards (3090s) as they have known compatibility issues",
      "from": "samhodge"
    },
    {
      "problem": "JSON prompts causing weird errors in Wan 2.2",
      "solution": "Normal prompts work fine, issue may be JSON formatting or prompt length specific to model",
      "from": "thaakeno"
    },
    {
      "problem": "LoRA scheduling doesn't work with merge_loras ON",
      "solution": "Use separate high and low LoRAs - high without merge, low with merge, or add 3rd sampler",
      "from": "Kijai"
    },
    {
      "problem": "CFG list sampling crashes at 50% with list index out of range error",
      "solution": "Issue occurs when using CFG list of values, crashes around 6th step of 12 steps",
      "from": "gokuvonlange"
    },
    {
      "problem": "Cannot import 'Wan22' from comfy.latent_formats error",
      "solution": "Need to update ComfyUI itself, not just the wrapper",
      "from": "Kijai"
    },
    {
      "problem": "Multitalk generating noise instead of proper output",
      "solution": "Disable merge lora option - the lightx merge lora causes the issue",
      "from": "Kijai"
    },
    {
      "problem": "Wan 2.2 Lightning LoRAs only producing noise",
      "solution": "Use strength 0.125 for both high and low noise LoRAs instead of default strength",
      "from": "Kijai"
    },
    {
      "problem": "Character consistency issues with VACE without LoRA",
      "solution": "Nearly impossible to get consistency without a character LoRA at this point",
      "from": "Nekodificador"
    },
    {
      "problem": "Original repo LoRAs don't work in native ComfyUI",
      "solution": "Use Kijai's fixed versions - original ones don't use ComfyUI standard key naming and are fp32 format issues",
      "from": "zelgo_/Kijai"
    },
    {
      "problem": "Black screen in T2I workflow with lightning LoRA",
      "solution": "Check model compatibility and LoRA strength settings",
      "from": "N0NSens"
    },
    {
      "problem": "New lightning LoRAs produce overly bright/overexposed results",
      "details": "Struggles with moody/cinematic tones, everything looks too bright and exposed like too much CFG",
      "from": "gokuvonlange/IceAero"
    },
    {
      "problem": "Static/slow motion videos with new Lightning LoRAs",
      "solution": "Use old LightX2V 2.1 LoRA on high noise pass, new Lightning on low noise pass",
      "from": "Doctor Shotgun"
    },
    {
      "problem": "New 2.2 Lightning high noise LoRA produces weird camera fade out effects",
      "solution": "Switch to old LightX2V 2.1 LoRA for high noise pass",
      "from": "Doctor Shotgun"
    },
    {
      "problem": "Overburned/oversaturated results with new Lightning LoRAs",
      "solution": "Use CFG scheduling or lower LoRA strengths",
      "from": "N0NSens"
    },
    {
      "problem": "First/Last frame encoding error with 5B model",
      "solution": "Update WanVideoWrapper node to latest version for 5B first/last frame support",
      "from": "Kijai"
    },
    {
      "problem": "WanVideo Empty Embeds error without image connection",
      "solution": "Connect an image to extra_latents in WanVideo Empty Embeds node, otherwise it refuses to work",
      "from": "Juan Gea"
    },
    {
      "problem": "LoRA merging error with scaled models",
      "solution": "Disable merge LoRAs toggle on LoRA loader or use non-scaled model. If memory issues occur, update Kijai nodes",
      "from": "Ashtar"
    },
    {
      "problem": "FLF workflow error",
      "solution": "Update ComfyUI - the error was fixed yesterday or day before",
      "from": "Kijai"
    },
    {
      "problem": "LoRA strength list bug",
      "solution": "Fixed bug where if first weight was 0, it would skip all LoRAs on that step",
      "from": "Kijai"
    },
    {
      "problem": "Split view generation with camera movement prompts",
      "solution": "The word 'cut' in prompts causes split screen effect",
      "from": "IceAero"
    },
    {
      "problem": "OOM error on native while wrapper runs fine",
      "solution": "Run comfy with --reserve-vram 2 or higher value. Native is more efficient offloading but automated estimation fails when GPU used for other tasks",
      "from": "Kijai"
    },
    {
      "problem": "Sigmas Split Value and HN init steps nodes causing errors",
      "solution": "Update RES4LYF by navigating to directory and typing 'git pull' since it's not registered in manager",
      "from": "Ablejones"
    },
    {
      "problem": "Overburned trash results with 5B model",
      "solution": "Try 4 steps euler and higher shift",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI making machines crash with Wan 2.2",
      "solution": "Use --disable-smart-memory argument and set up static swap file on SSD drive",
      "from": "mdkb"
    },
    {
      "problem": "torch._inductor.exc.InductorError: PY_SSIZE_T_CLEAN macro must be defined",
      "solution": "No solution provided",
      "from": "IceAero"
    },
    {
      "problem": "RuntimeError: mat1 and mat2 shapes cannot be multiplied (77x768 and 4096x5120)",
      "solution": "Use umt5xxl from comfy link or fp8 scaled version for native",
      "from": "hablaba"
    },
    {
      "problem": "Videos from high and low sampler not matching up",
      "solution": "No solution provided",
      "from": "The Dude"
    },
    {
      "problem": "White noise output with sliding context",
      "solution": "Issue occurs with I2V model, context windows work poorly with I2V anyway",
      "from": "Kijai"
    },
    {
      "problem": "New Lightning loras producing noise in wrapper",
      "solution": "Use strength 0.125 for original version or 1.0 for Kijai's converted version",
      "from": "Kijai"
    },
    {
      "problem": "Width/height must be divisible by 32 error",
      "solution": "Change resolution from 480x832 to 704x1280 or other values divisible by 32",
      "from": "Dita"
    },
    {
      "problem": "NameError: name 'Wan22' is not defined",
      "solution": "Update ComfyUI - Wan22 was added when Wan 2.2 was released",
      "from": "Kijai"
    },
    {
      "problem": "index 0 is out of bounds for dimension 0 with size 0",
      "solution": "Set start step to 0 instead of having same start and end step values",
      "from": "Kijai"
    },
    {
      "problem": "type NoneType doesn't define __round__ method in LoRA loader",
      "solution": "The strength parameter is being passed as None - ensure all LoRA strengths are set to numeric values",
      "from": "screwfunk"
    },
    {
      "problem": "bong_tangent scheduler not working well with low steps",
      "solution": "bong_tangent needs at least 20 steps, use beta or beta57 schedulers for higher step counts with video models instead",
      "from": "Ablejones"
    },
    {
      "problem": "Lightning LoRAs burning colors and not converging",
      "solution": "Try strength 0.2-0.35 and higher start steps. FastWan LoRA doesn't work well with I2V",
      "from": "3DBicio"
    },
    {
      "problem": "torchcompile fp8_e4m3fn hanging on 3090s",
      "solution": "It hangs due to one line in inductor, use e5 instead (though technically worse for Wan) or scaled e5 version",
      "from": "Kijai"
    },
    {
      "problem": "Wan 2.2 templates not showing in ComfyUI",
      "solution": "Manager update may show as updated but not actually update to latest version, try installing fresh ComfyUI",
      "from": "Blink"
    },
    {
      "problem": "SystemError: PY_SSIZE_T_CLEAN macro must be defined for '#' formats",
      "solution": "Restart ComfyUI to clear torch compile cache - caused by Anything Everywhere nodes auto-connecting compile args",
      "from": "IceAero"
    },
    {
      "problem": "Denoise below 0.5 causes 'index 0 is out of bounds' error on 14B model",
      "solution": "Use denoise values 0.5 and above, update ComfyUI and refresh browser",
      "from": "Juan Gea"
    },
    {
      "problem": "External masks from Photoshop/Krita/Blender don't work in ComfyUI",
      "solution": "Use 'Image to Mask' node to convert external masks, ensure mask is resized to same size as input manually",
      "from": "Kijai"
    },
    {
      "problem": "Qwen Image VAE causes burnout errors",
      "solution": "Set Qwen Image model to fp8_e5m2 precision instead of fp8_e4m3fn",
      "from": "fredbliss"
    },
    {
      "problem": "Hard drive full affecting swap and maxing out RAM",
      "solution": "Use 'Release node cache' button in ComfyUI Manager to free up space",
      "from": "Rainsmellsnice"
    },
    {
      "problem": "ComfyUI reloading models on each generation",
      "solution": "Models don't all fit in VRAM at once, causing reload. Use lower precision (VAE fp16, UMT5 on CPU) to save VRAM",
      "from": "aikitoria"
    },
    {
      "problem": "SageAttention patching message on every generation",
      "solution": "This is normal and doesn't take long - it just swaps which code is executed during steps",
      "from": "aikitoria"
    },
    {
      "problem": "LatentCompositeMasked error with video",
      "solution": "This node doesn't handle video latents - it's meant for images only",
      "from": "Kijai"
    },
    {
      "problem": "Flash_attn_2_cuda errors with torch 2.8",
      "solution": "Just uninstall flash attention - it's not actually used if not explicitly selected and SageAttention works fine",
      "from": "Kijai"
    },
    {
      "problem": "Slow motion videos from Wan 2.2",
      "solution": "Use CFG > 1 to fix slow motion issues",
      "from": "Kijai"
    },
    {
      "problem": "Blurry/foggy results with low contrast",
      "solution": "Increase CFG from 1.0 to 2.0, increase shift from 1.0 to 5.0, reduce LoRA strength to 0.7, stick with 81 frames",
      "from": "Kijai"
    },
    {
      "problem": "Context window crashes ComfyUI with SageAttention",
      "solution": "SageAttention uses Triton which does some compiling in scenarios, disable compile to get proper error messages",
      "from": "Kijai"
    },
    {
      "problem": "KSamplerAdvanced error with channel mismatch",
      "solution": "Reinstall ComfyUI - issue with weight size expecting 36 channels but getting 32",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "problem": "WAN 2.2 5B OOM on Mac M4 with 128GB RAM",
      "solution": "OOM occurs on VAE which is super heavy on fp32, 5B VAE is 4x heavier than 2.1",
      "from": "Kijai"
    },
    {
      "problem": "Depth + pose combined control issues",
      "solution": "Don't combine depth and pose for VACE, use separate VACE nodes for each control type",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "RuntimeError: required rank 4 tensor to use channels_last format with ComfyUI native Wan",
      "solution": "No solution provided in discussion",
      "from": "fredbliss"
    },
    {
      "problem": "Burned/overexposed results when using LightX2V LoRA with wrapper",
      "solution": "Try lowering LoRA strength or adjusting CFG settings. For I2V, shift=1 breaks everything, use shift=8",
      "from": "Juan Gea"
    },
    {
      "problem": "EchoShot implementation not triggering properly",
      "solution": "Use Kijai's | character prompt splitting method instead",
      "from": "Kijai"
    },
    {
      "problem": "Gorilla getting unwanted clothing/accessories in martial arts scenes",
      "solution": "Try specific outfit prompts or detailed anatomy descriptions like 'highly realistic silverback gorilla, natural anatomy, visible powerful muscles under thick fur'",
      "from": "Mancho"
    },
    {
      "problem": "Noisy output when using save/load latent workflow",
      "solution": "Use DisableNoise node instead of RandomNoise when continuing generation from saved latent",
      "from": "Ablejones"
    },
    {
      "problem": "RAM issues with save/load latent workflows",
      "solution": "Use --cache-none flag to automatically clear memory after each node",
      "from": "Kijai"
    },
    {
      "problem": "Poor results at higher resolutions like 1280x720",
      "solution": "Try higher CFG (5+) and more steps (20+) when not using Lightning LoRAs",
      "from": "N0NSens"
    },
    {
      "problem": "Model compilation fails after merging",
      "solution": "Save the merged model and reload it before compiling - don't compile directly after merging",
      "from": "Ablejones"
    },
    {
      "problem": "Reference image not working with HN model",
      "solution": "VACE blocks don't work on HN model without merging 2.1 patch embedding and block 0",
      "from": "Kijai"
    },
    {
      "problem": "Motion loops back or undoes in longer generations",
      "solution": "Stick to 81 frames or less for consistent motion, longer sequences may repeat/undo actions",
      "from": "Hoernchen"
    },
    {
      "problem": "Snow particles showing up in Wan 2.1 and 2.2 output videos",
      "solution": "Occurs when prompt is too short or shift too high, but no definitive solution found yet",
      "from": "JohnDopamine"
    },
    {
      "problem": "First preview step looks good but next step ruins details",
      "solution": "Try higher shift value if first step of high noise has discernable preview - indicates too big first step",
      "from": "Ablejones"
    },
    {
      "problem": "No preview being generated regardless of sampler",
      "solution": "Reboot ComfyUI, happens sometimes when swapping between workflows",
      "from": "DawnII"
    },
    {
      "problem": "FP8 version not running on A5000 card",
      "solution": "A5000 is compute 8.6, needs 8.9. 3000 series lacks native fp8 support, need at least 4000 series",
      "from": "Lodis"
    },
    {
      "problem": "Memory issues with 64GB RAM",
      "solution": "Use --disable-smart-memory in bat file and create large static swap file on SSD",
      "from": "mdkb"
    },
    {
      "problem": "ComfyUI previews not working after nightly update",
      "solution": "Update VHS (Video Helper Suite) nodes to latest version or rollback frontend to previous version",
      "from": "Kijai"
    },
    {
      "problem": "Out of memory with VAE decode even with tiling enabled",
      "solution": "Use fp16 or bf16 VAE instead of fp32 - fp32 provides minimal visual improvement but uses much more VRAM",
      "from": "Kijai"
    },
    {
      "problem": "Color drift in first/last frame generation",
      "solution": "Can be fixed with contrast/saturation/tint keyframes in DaVinci Resolve",
      "from": "CaptHook"
    },
    {
      "problem": "LoRA corruption error with Wan 2.2 Lightning",
      "solution": "Use fp8e5 checkpoint if compiling, or bypass sigma values list - error may indicate borderline OOM conditions",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "White noise results without LoRA",
      "solution": "Normal euler ends up with too low sigma on last step at 4 steps, won't remove all noise properly",
      "from": "Kijai"
    },
    {
      "problem": "New LoRAs don't play well with GGUF quants",
      "solution": "Use 6 steps instead of 4 for better results with GGUF",
      "from": "Josiah"
    },
    {
      "problem": "LoRA not reloading in workflow",
      "solution": "Replacing file directly caused workflow to not reload new LoRA, previously loaded LoRA was in memory",
      "from": "piscesbody"
    },
    {
      "problem": "Node taking super long to load",
      "solution": "Put the device to default solved the loading issue",
      "from": "AffenBrot"
    },
    {
      "problem": "Camera gets jerky with low steps using CFG",
      "solution": "Issue absent with cfg=1, or use more steps at higher sigmas to smooth out",
      "from": "IceAero"
    },
    {
      "problem": "Lightning LoRAs cause overbrightness and destroy contrast",
      "solution": "Add a ksampler for first step cfg 3.5 without any lora to render brightness like vanilla wan2.2",
      "from": "pagan"
    },
    {
      "problem": "Small window size in MultiTalk not working properly",
      "solution": "Don't reduce window size, there's overlap and small windows can't create many new frames per iteration",
      "from": "Kijai"
    },
    {
      "problem": "Image quality degradation in MultiTalk",
      "solution": "Problem is about amount of windows, not video length - too many windows result in poor quality",
      "from": "Kijai"
    },
    {
      "problem": "First frame flash when going over 81 frames",
      "solution": "Lowering to 85 frames removes first frame flash",
      "from": "MysteryShack"
    },
    {
      "problem": "NAG with WanVideo TextEncode Cached causing black output",
      "solution": "Error occurs specifically when using cached node with apply NAG",
      "from": "patientx"
    },
    {
      "problem": "Cached embeds causing issues",
      "solution": "Flush cache and use two separate text encode nodes (one for positive, one for negative) with NAG enabled",
      "from": "patientx"
    },
    {
      "problem": "Latent composite masked node doesn't work for videos",
      "solution": "Node only works for images, not video latents - confirmed by Kijai",
      "from": "xwsswww"
    },
    {
      "problem": "Tensor mismatch errors",
      "solution": "Usually happens when resolution isn't divisible by 16",
      "from": "WorldX"
    },
    {
      "problem": "New I2V LoRA producing all slow motion results",
      "solution": "Adjust LoRA strength ratios - higher values for faster motion, lower for slower",
      "from": "WorldX"
    },
    {
      "problem": "Fun Control model not loading due to unsupported 52 channels",
      "solution": "Update WanVideo wrapper to latest version to support the new channel count",
      "from": "Kijai"
    },
    {
      "problem": "fun_ref_image not working in Fun Control",
      "solution": "Feature appears to have some effect but doesn't work properly - first frame works as alternative",
      "from": "Kijai"
    },
    {
      "problem": "Native ComfyUI throwing CPU/GPU tensor errors with low VRAM",
      "solution": "Launch ComfyUI with --reserve-vram 2 command line argument to reserve extra VRAM for proper offloading",
      "from": "Kijai"
    },
    {
      "problem": "System RAM OOM with long video generations",
      "solution": "Use save latent node to save to disk before decode, then load and decode in separate session, or use --cache-none flag",
      "from": "Kijai"
    },
    {
      "problem": "MultiTalk lip sync degrading after 7-8 seconds",
      "solution": "Try raising audio scale parameter and use better vocal separation tools like Spleeter",
      "from": "Kijai"
    },
    {
      "problem": "WAN 2.2 Fun crashing with OOM before second ksampler",
      "solution": "Use T5 node that fully removes from RAM, try --cache-none, keep block swap counts consistent (use 30 instead of mixing 20/30)",
      "from": "Kijai"
    },
    {
      "problem": "MultiTalk lip sync going off at 9 seconds",
      "solution": "Use vocal only track, increase audio_scale and audio_cfg_scale, check chunk_overlap settings in AudioSeparation node",
      "from": "hiroP"
    },
    {
      "problem": "MultiTalk not working in video-to-video",
      "solution": "Use fusionx model instead of base WAN 2.1, set audio_scale to 3, ensure audio is loud and clear",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "problem": "T5 error in wrapper but not native",
      "solution": "Update to minimum required versions",
      "from": "Kijai"
    },
    {
      "problem": "Fun Control tensor size error (56 instead of 52)",
      "solution": "Fixed by removing unused clip vision connection and correcting channel concatenation",
      "from": "Kijai"
    },
    {
      "problem": "Sapiens pose video not working",
      "solution": "Remove the input video from the workflow",
      "from": "fredbliss"
    },
    {
      "problem": "Tensor mismatch error with Fun workflow",
      "solution": "Image size must be divisible by 16, resize accordingly (e.g., 1280x542 to 1280x528)",
      "from": "HeadOfOliver"
    },
    {
      "problem": "WanVideoModelLoader error 'cannot access local variable model_type'",
      "solution": "Delete and reinstall custom node, make sure using nightly version not latest",
      "from": "Rainsmellsnice"
    },
    {
      "problem": "Camera and cameraman appearing in reflective objects",
      "solution": "Remove 'camera' from prompt, use negatives, refer to official Wan prompt guide for camera movement descriptions",
      "from": "Dan"
    },
    {
      "problem": "Fun Control reference input not working",
      "solution": "Model needs matching start image to control first frame, reference image input may be defunct",
      "from": "Kijai"
    },
    {
      "problem": "GGUF model tensor error when encoding without control input",
      "solution": "Control embed channel is expected - ensure proper control input connection",
      "from": "Cubey/DawnII"
    },
    {
      "problem": "FP8 scaled with torch compile VRAM leak",
      "details": "When changing LoRAs or porq strength, next run uses LoRA size x3 more VRAM without block swap",
      "solution": "Kijai working on memory fixes but recommends not updating if current setup works",
      "from": "Kijai"
    },
    {
      "problem": "GGUF loading OOM at 60GB during WanVideo VACE Encode",
      "solution": "Check node connections, particularly DWPose connections",
      "from": "AmirKerr"
    },
    {
      "problem": "VideoX fun and easyAnimate node packs conflict",
      "solution": "Kijai clarified his workflows don't use those deprecated node packs",
      "from": "AR/Kijai"
    },
    {
      "problem": "RuntimeError: Sizes of tensors must match except in dimension 0",
      "solution": "Input dimension mismatch - make sure inputs are same and divisible by 16",
      "from": "Kijai"
    },
    {
      "problem": "UniAnimate doesn't work with GGUF unmerged",
      "solution": "Needs merging - easiest to make a merge",
      "from": "Kijai"
    },
    {
      "problem": "Color issues at end of generation with WanVideo Sampler",
      "solution": "Need to add noise to samples on first sampler and connect split steps",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "Validation error with fp8_e4m3fn_scaled quantization",
      "solution": "Update WanVideoWrapper nodes and refresh browser, or check for multiple fork installations",
      "from": "Kijai"
    },
    {
      "problem": "Mysterious camera shake with 2.2 I2V lightning LoRAs",
      "solution": "Lower the strength to 1 or less, using 3x3 steps",
      "from": "XWAVE"
    },
    {
      "problem": "Grainy output in WanVideoWrapper",
      "solution": "Use WanVideo Decode node instead of VAE Decode node when using the wrapper",
      "from": "Hashu"
    },
    {
      "problem": "Multitalk workflow error with two samplers",
      "solution": "Can't use multitalk image embed node with two samplers because it's a loop from last frame. Use context window approach for long multitalk instead",
      "from": "Kijai"
    },
    {
      "problem": "VRAM leakage causing slowdowns after few generations",
      "solution": "Using WSL2 with wsl --shutdown to restart WSL instead of full PC reboot helps with memory management",
      "from": "pagan"
    },
    {
      "problem": "Color/lighting changes in frame2frame i2v segments",
      "solution": "Issue occurs when using same image for end frame and first frame of next segment - lighting changes from start to end of each segment",
      "from": "PTMarks"
    },
    {
      "problem": "Context windows error in MultiTalk",
      "solution": "torch._dynamo.exc.TorchRuntimeError with shape mismatch in encoder_kv.view",
      "from": "NebSH"
    },
    {
      "problem": "Wan wrapper v1.2.7 device error",
      "solution": "Downgrade to v1.2.6 - v1.2.7 has 'Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!' error",
      "from": "army"
    },
    {
      "problem": "Preview sampler node running at 4fps with overlapped frames",
      "solution": "Use tiny VAE model (taew2_1.safetensors) in models/vae_approx folder, update video helper suite",
      "from": "Kijai"
    },
    {
      "problem": "Xformers compatibility issue on 5090",
      "solution": "Uninstall xformers - it's not needed for anything and causes tensor device errors",
      "from": "Kijai"
    },
    {
      "problem": "Intermittent OOM issues when cancelling Wan 2.2 gen",
      "solution": "Connect the model to offload node, requires full ComfyUI restart to recover",
      "from": "Hoernchen"
    },
    {
      "problem": "VACE workflow producing mangled output after ComfyUI update",
      "solution": "Update Kijai's nodes and ComfyUI, disconnect unused Samples input from WanVideo sampler, enable add_noise if using samples input with denoise 1.0",
      "from": "Kijai"
    },
    {
      "problem": "Florence2Run error 'Cache only has 0 layers'",
      "solution": "Update nodes manually by deleting folder and reinstalling, ComfyUI manager has issues with updates",
      "from": "Kijai"
    },
    {
      "problem": "SageAttention installation issues",
      "solution": "Install Triton from https://github.com/woct0rdho/triton-windows and SageAttention wheel from releases, use pip install with python.exe in portable's python_embeded folder",
      "from": "Kijai"
    },
    {
      "problem": "Quick flash effect at beginning of T2V generations",
      "solution": "Issue mentioned but solution not provided in this chunk",
      "from": "DaxRedding"
    },
    {
      "problem": "Fun workflow reference image only flashing for 1 frame",
      "solution": "Issue mentioned but solution not provided in this chunk",
      "from": "D'Squarius Green, Jr."
    },
    {
      "problem": "Inpainting only works with WanVideo Vace encode node",
      "solution": "Use Vace encode node instead of latent mask or WanVideo Encode node",
      "from": "xwsswww"
    },
    {
      "problem": "VHS custom node causing issues",
      "solution": "Update VHS custom node to fix problems",
      "from": "Lodis"
    },
    {
      "problem": "VHS node update caused lower quality preview",
      "solution": "Check preview quality settings after update",
      "from": "crinklypaper"
    },
    {
      "problem": "torch._inductor.exc.InductorError with CompiledKernel",
      "solution": "Check if torch compile/triton is installed properly or disconnect torch compile args",
      "from": "kendrick"
    },
    {
      "problem": "Training character LoRA on images causes slow motion output",
      "solution": "Try 77 frames or add videos to training dataset, lower LoRA strength, or mix images with videos",
      "from": "Juampab12"
    },
    {
      "problem": "Video2Video output too close to original",
      "solution": "Adjust denoise strength, but lower denoise may reduce motion",
      "from": "DaxRedding"
    },
    {
      "problem": "Lightning LoRA for 2.2 has style bias issues",
      "solution": "Avoid for dark scenes, causes oversaturation and brightness bias",
      "from": "Kijai"
    },
    {
      "problem": "MultiTalk can't use 1280x720, only works at lower res like 832x480",
      "solution": "Use lower resolution for MultiTalk generation",
      "from": "NebSH"
    },
    {
      "problem": "Block swap causing significant slowdown",
      "solution": "Use prefetch option to improve transfer speeds",
      "from": "Kijai"
    },
    {
      "problem": "Text encoder incompatibility error",
      "solution": "Use original T5 model, not scaled version, or use Cached Text Encoder node or Text Embed Bridge node",
      "from": "Kijai"
    },
    {
      "problem": "FP8e4nv not supported error on 3090",
      "solution": "Use fp8e5m2 models for torch compile on 3090, not fp8e4",
      "from": "Josiah"
    },
    {
      "problem": "Out of memory with prefetch",
      "solution": "Prefetch uses more VRAM (about single block amount), disable if causing OOM",
      "from": "Kijai"
    },
    {
      "problem": "WanTimeTextImageEmbedding.forward() got an unexpected keyword argument 'timestep_seq_len'",
      "solution": "Update diffusers with pip install -U git+https://github.com/huggingface/diffusers.git",
      "from": "Kijai"
    },
    {
      "problem": "LoRAs not loading with mismatched models",
      "solution": "ComfyUI silently ignores incompatible LoRAs - check console for warnings",
      "from": "Kijai"
    },
    {
      "problem": "Image resolution mismatch in I2V",
      "solution": "Use KJ Resize v2 node after input image to match generation size, and use correct Wan 2.1 VAE",
      "from": "The Shadow (NYC)"
    },
    {
      "problem": "Radial attention block size error",
      "solution": "Image size must be divisible by block size (128), use closest valid sizes like 640x896",
      "from": "el marzocco"
    },
    {
      "problem": "VACE Encode incorrectly connected to Control Embeds",
      "solution": "VACE is T2V so shouldn't use image encode at all, VACE embeds connects directly to image_embeds on sampler",
      "from": "DawnII"
    },
    {
      "problem": "Only getting 1 step when setting 2 steps with 0.3 denoise in wrapper",
      "solution": "Increase to 4 steps to get 2 actual steps - wrapper calculates denoise based on steps differently",
      "from": "DawnII"
    },
    {
      "problem": "Background people faces constantly mutating even with high steps",
      "solution": "Problem is insufficient pixel resolution, not background position. Need 1152p or higher, or crop/upscale faces to 1024x1024 then stitch back",
      "from": "Juan Gea"
    },
    {
      "problem": "MultiTalk denoises everything at 0.7, lower values don't move lips",
      "solution": "Try differential diffusion with mask for inpainting, give more strength to audio for expressiveness",
      "from": "Juan Gea"
    },
    {
      "problem": "SeedVR2 OOM error with 32GB VRAM",
      "solution": "Use GGUF models with offloading, try Q6 or Q8 with 32GB",
      "from": "AmirKerr"
    },
    {
      "problem": "Incomplete MultiTalk node update",
      "solution": "Update nodes again, push was incomplete by accident",
      "from": "Kijai"
    },
    {
      "problem": "AssertionError: All tensors must have the same dtype when using Stand-in LoRA",
      "solution": "Remove other LoRAs like MovieGenn or MPS, or disable SageAttention",
      "from": "Hashu"
    },
    {
      "problem": "Stand-in LoRA causing cryptic SageAttention errors",
      "solution": "Disable SageAttention to resolve the issue",
      "from": "pagan"
    },
    {
      "problem": "Stand-in workflow outputting full white",
      "solution": "Need to apply a mask in the image using the built-in editor or use rembg node for cropping",
      "from": "pagan"
    },
    {
      "problem": "Wrong VAE error with Wan 2.2 5B",
      "solution": "5B model requires the 2.2 VAE, not the standard one used for 14B",
      "from": "Kijai"
    },
    {
      "problem": "Memory issues when queueing multiple generations",
      "solution": "Use cached text encode node instead of memory cleaner nodes",
      "from": ".: Not Really Human :."
    },
    {
      "problem": "Stand-in LoRA not working with GGUF models",
      "solution": "Latest update fixed GGUF compatibility with Stand-in, allows using SageAttention without errors",
      "from": "garbus"
    },
    {
      "problem": "torch._inductor.exc.InductorError with fp8e4nv not supported",
      "solution": "Either bypass torch compile node or download e5m2 model instead of e4m3fn",
      "from": "Hevi"
    },
    {
      "problem": "OOM on 720p 121 I2V with wrapper vs native using same VRAM",
      "solution": "Native does automatic block swapping, wrapper needs manual block swap setting",
      "from": "Kijai"
    },
    {
      "problem": "AssertionError: All tensors must have the same dtype with sage attention",
      "solution": "Change sageatt to sdpa or apply patch",
      "from": "shockgun"
    },
    {
      "problem": "WanVideoSampler tensor size mismatch error",
      "solution": "Sounds like input size mismatch, check resolution compatibility",
      "from": "Kijai"
    },
    {
      "problem": "Context windows causing ghosting and unstable generations",
      "solution": "Use uni3c to lock camera, works especially well with multitalk embeds",
      "from": "Kijai"
    },
    {
      "problem": "Can't import SageAttention error",
      "solution": "Disable sage_attention in the Diffusion Model Loader KJ if you don't have sageattention installed",
      "from": "Ablejones"
    },
    {
      "problem": "fp8_e4m3fn doesn't work with torch compile on GPUs prior to 4000 series",
      "solution": "Use fp8_e5m2 instead for older GPUs",
      "from": "Kijai"
    },
    {
      "problem": "RAM not clearing after execution in ComfyUI",
      "solution": "Only restarting ComfyUI helps, clearing node cache doesn't fully clear RAM",
      "from": "iShootGood"
    },
    {
      "problem": "Set latent mask causes red tinting in unmasked areas",
      "solution": "This is a known issue with diff diff and model split in the wrapper",
      "from": "Kijai"
    },
    {
      "problem": "Temporary VRAM usage peak after updating",
      "solution": "After updating, first run may need to recompile causing temporary VRAM spike, should only affect first run with sage",
      "from": "Kijai"
    },
    {
      "problem": "Runtime error with shape mismatch when using last frame instead of first frame",
      "solution": "Enable a specific option in the workflow settings",
      "from": "Kijai"
    },
    {
      "problem": "Control_start_percent error on nightly WanVideoWrapper",
      "solution": "Downgrade from nightly to latest version or wait for fix",
      "from": "ronnykhalil"
    },
    {
      "problem": "Indentation error causing control_start_percent issue",
      "solution": "Fixed with code update",
      "from": "Kijai"
    },
    {
      "problem": "Width 840 not supported causing broadcast dimension error",
      "solution": "Use 832 instead, follow divisible by 16 rule",
      "from": "Kijai"
    },
    {
      "problem": "AssertionError about tensor dtype mismatch with sage attention",
      "solution": "Switch to SDPA in model loader or try without torch compile",
      "from": "Daflon"
    },
    {
      "problem": "Flash attention errors after torch updates",
      "solution": "Uninstall flash attention entirely - it's not necessary",
      "from": "Kijai"
    },
    {
      "problem": "Stand-In likeness degraded after wrapper update",
      "solution": "Issue with kv_cache implementation affecting performance vs quality",
      "from": "Kijai"
    },
    {
      "problem": "Official FUN workflow OOMs on RTX 5090 with Canny",
      "solution": "Add resize node before Canny preprocessing - the workflow was missing this essential step",
      "from": "Drommer-Kille"
    },
    {
      "problem": "Stand-in workflow produces black output with RuntimeWarning",
      "solution": "Merge LoRAs to true or use fp16 models instead of fp8",
      "from": "patientx"
    },
    {
      "problem": "Gaming USB keyboard doesn't activate early enough for BIOS access",
      "solution": "Use Windows 'advanced startup' option to restart to BIOS",
      "from": "Kijai"
    },
    {
      "problem": "MediaPipe face mesh node fails to parse",
      "solution": "Use alternative face detection methods like DWPose or Florence2, or install missing requirements: pip install onnx onnxruntime-gpu",
      "from": "Kijai"
    },
    {
      "problem": "Fun 2.2 reference not working",
      "solution": "Issue was fp8 scaling of ref_conv layer - use unscaled models or fp16 versions",
      "from": "Kijai"
    },
    {
      "problem": "Fantasy Portrait node errors on initial setup",
      "solution": "Install missing requirements: pip install opencv-python onnx onnxruntime-gpu scipy tqdm accelerate",
      "from": "852\u8a71 (hakoniwa)"
    },
    {
      "problem": "All WanVideoWrapper nodes fail to load due to FantasyPortrait dependencies",
      "solution": "Install onnxruntime dependency, Kijai updated code to catch exceptions",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "problem": "Wan 2.2 Fun Control Camera GGUF has gradient errors and tensor type mismatches",
      "solution": "Update WanVideoWrapper to latest version, Kijai added fixes for gradient issues",
      "from": "Daflon"
    },
    {
      "problem": "High noise sampler looking blown out without lightning/light2x",
      "solution": "Try plugging same lora into both high and low noise, possibly turn up strength in HIGH",
      "from": "anever"
    },
    {
      "problem": "Stand-in not using reference image properly",
      "solution": "Prompt also contributes a lot, need to avoid conflicting prompts like 'white glasses' when not wanted",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "problem": "Wan 2.2 Lightning LoRAs destroying motion quality",
      "solution": "Try 1.0 strength for both high and low instead of 3.0/1.0, or stick with LightXV 2.1 LoRAs",
      "from": "Lodis"
    },
    {
      "problem": "Canny control producing bad output with canny lines",
      "solution": "Use inverted canny (black lines, white background) and reduce strength from 1.5 to 1.0",
      "from": "xwsswww"
    },
    {
      "problem": "Context windows causing snapping to start frame",
      "solution": "Stride helps but may not give good results with context",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Canny map not working properly",
      "solution": "Inverting the canny map fixed the problem",
      "from": "Lodis"
    },
    {
      "problem": "I2V resize resizing from 720 to 704",
      "solution": "Must be divisible by 32 for proper functioning",
      "from": "mdkb"
    },
    {
      "problem": "Fantasy Portrait IndexError: list index out of range",
      "solution": "Probably frame count mismatch or face detection failure in some frame",
      "from": "Kijai"
    },
    {
      "problem": "FantasyPortrait face detection issues",
      "solution": "Use video with trackable face, check for frame mismatch issues",
      "from": "Josiah"
    },
    {
      "problem": "CompilationError with fp8e4nv not supported",
      "solution": "Use e5m2 quantization instead of e4m3fn when using torch.compile on RTX 3090",
      "from": "Kijai"
    },
    {
      "problem": "Memory allocation crashes with large RAM usage",
      "solution": "Remove all block offloading to solve RAM-related crashes",
      "from": "Obsolete"
    },
    {
      "problem": "VACE inpainting showing annoying lines in output",
      "solution": "Don't blur the mask, only blur the final composition. First frame with no mask causes VACE to register black lines as desired content",
      "from": "Kijai"
    },
    {
      "problem": "Missing onnx installation causing node failures",
      "solution": "Install onnx and onnxruntime-gpu: pip install -U onnx onnxruntime-gpu",
      "from": "Kijai"
    },
    {
      "problem": "I2V model channel mismatch (32 vs 36 channels)",
      "solution": "Must use WanImageToVideo node which adds required mask, can't use I2V model without it",
      "from": "Kijai"
    },
    {
      "problem": "Image was RGBA causing workflow failure",
      "solution": "Add an image to RGB node to convert RGBA to RGB",
      "from": "ManglerFTW"
    },
    {
      "problem": "OOM during model loading",
      "solution": "Use offload_device and don't launch comfy with --high-vram",
      "from": "Kijai"
    },
    {
      "problem": "Block swap causing ComfyUI crash",
      "solution": "Use lower block swap values, try 20 instead of 40",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "WanVideo nodes not showing up",
      "solution": "Delete all WanVideoWrapper folders, do fresh git pull and reinstall. Manager doesn't work well for this node",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "problem": "Fuse_method validation error with value '6'",
      "solution": "Change fuse_method from 6 to 'pyramid' or 'linear'",
      "from": "Drommer-Kille"
    },
    {
      "problem": "Extremely slow generation with WanVideoSampler",
      "solution": "Use block swap (try setting blocks_to_swap to 20) to prevent VRAM saturation",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI freezing with native nodes on 4090",
      "solution": "Increase --reserve-vram value (try 5 instead of 2)",
      "from": "Kosinkadink"
    },
    {
      "problem": "Wan 2.2 taking forever on first step due to shared GPU memory usage",
      "solution": "Use --reserve-vram 5 flag to prevent VRAM spillover to slow shared memory",
      "from": "Kosinkadink"
    },
    {
      "problem": "FantasyPortrait nodes error 'module onnxruntime has no attribute InferenceSession'",
      "solution": "pip uninstall onnxruntime onnxruntime-gpu, then pip install onnxruntime-gpu==1.22.0",
      "from": "A.I.Warper"
    },
    {
      "problem": "FantasyPortrait running extremely slow (1.75it/s)",
      "solution": "Remove conflicting onnxruntime package, keep only onnx and onnxruntime-gpu",
      "from": "Kijai"
    },
    {
      "problem": "Wan 2.2 first frame OK but degrading after with denoise 0.60",
      "solution": "Not explicitly solved but identified as common issue",
      "from": "DeeX"
    },
    {
      "problem": "Windows VRAM estimation causes slow swap to RAM",
      "solution": "Windows driver does super slow swap instead of OOM - reserve-vram flag prevents this",
      "from": "Kosinkadink"
    },
    {
      "problem": "RuntimeError: The size of tensor a (48360) must match the size of tensor b (53040) at non-singleton dimension 1",
      "solution": "Ensure all dimensions and frame counts are the same between VACE input vs model input",
      "from": "Kijai"
    },
    {
      "problem": "'WanVideoVAE' object has no attribute 'get' error",
      "solution": "Re-create the VAE node and re-connect it, or use muting instead of bypassing nodes",
      "from": "Kijai"
    },
    {
      "problem": "Ghosting issues with Fun Control models",
      "solution": "Increase high noise pass steps (try 4+4 instead of 2) and ensure correct lora strengths",
      "from": "Kijai"
    },
    {
      "problem": "Context options with Wan 2.2 not smooth between slots",
      "solution": "Flow between context slots restarts from first frame, unlike AnimateDiff which was smoother - no permanent solution provided",
      "from": "xwsswww"
    },
    {
      "problem": "Switch nodes breaking with bypassed nodes overnight",
      "solution": "Use lazy switches instead of bypass/muting, or switch to different switch node types that work with current ComfyUI",
      "from": "Nekodificador"
    },
    {
      "problem": "Unwanted character talking/yapping in generations",
      "solution": "Use empty audio track with MultiTalk, or negative prompts with 'character is talking' when using CFG",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI queue stops working and loader nodes won't accept input",
      "solution": "Disable multithread option in color match node as potential fix",
      "from": "Kijai"
    },
    {
      "problem": "No input found for flattened id error with GetNode",
      "solution": "Error occurs when GetNode is bypassed in new ComfyUI - frontend change now passes data through bypassed nodes",
      "from": "phazei"
    },
    {
      "problem": "Block swap disk space doesn't clear after generation",
      "solution": "Need to restart PC to clear disk swap buildup, or use old ComfyUI menu instead of new menu",
      "from": "xwsswww"
    },
    {
      "problem": "Flash effect when using full latents for video continuation",
      "solution": "Use I2V model instead of T2V for latent continuation, as I2V can handle full latents better",
      "from": "Kijai"
    },
    {
      "problem": "GGUF conversion requires llama.cpp compilation",
      "solution": "Use nodes that auto-build llama.cpp or install full CUDA toolkit with nvcc",
      "from": "Kijai"
    },
    {
      "problem": "PUSA LoRA hitting max VRAM on ksampler while i2v 2.2 works fine",
      "solution": "PUSA sampling inherently uses more VRAM regardless of LoRA format - this is expected behavior",
      "from": "Kijai"
    },
    {
      "problem": "FantasyPortrait nodes not available due to import error",
      "solution": "Install onnx and onnxruntime-gpu: pip install onnx onnxruntime-gpu",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI memory management issues with model offloading",
      "solution": "Use --disable-smart-memory flag, create 32GB static swap file, trade everything to RAM possible",
      "from": "mdkb"
    },
    {
      "problem": "Fun-Control reference image not working with fp8_scaled model",
      "solution": "Use fixed versions of models from Kijai's HF repo - initial models had mistake in ref_conv layer for fp8",
      "from": "Kijai"
    },
    {
      "problem": "Saved latent file not appearing in LoadLatent dropdown",
      "solution": "Move the .latent file to the /input directory for ComfyUI to recognize it",
      "from": "gokuvonlange"
    },
    {
      "problem": "Model loading order causing OOM with dual samplers",
      "solution": "Set second model loader load_device to offload_device instead of main_device",
      "from": "phazei"
    },
    {
      "problem": "UnboundLocalError: local variable 'bidirectional_sampling' referenced before assignment",
      "solution": "Fixed in latest commit on main branch",
      "from": "Kijai"
    },
    {
      "problem": "VACE single image failing with error about latent dimensions",
      "solution": "Use 5 frames: first with change/mask, 4 empty gray frames",
      "from": "Nekodificador"
    },
    {
      "problem": "Qwen-image to WAN bridge creating only noise",
      "solution": "Connect to image conditioning input, not samples input, and ensure correct temporal dimensions (21 latents for 81 frames)",
      "from": "fredbliss"
    },
    {
      "problem": "Context windows causing drift and inconsistency",
      "solution": "Use 48 overlap, but still has limitations with I2V models that need start images",
      "from": "Kijai"
    },
    {
      "problem": "WAN 2.2 outputs appearing sped up compared to 2.1",
      "solution": "Consider saving at 12fps then doubling with RIFE instead of 16fps",
      "from": "Fawks"
    },
    {
      "problem": "WanVideoContextOptions error: fuse_method '6' not in list",
      "solution": "Use 'linear' or 'pyramid' as fuse_method values instead of numeric values",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "problem": "Double LightX LoRA causing exaggerated movements",
      "solution": "Remove duplicate LightX LoRA connections",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Video LoRA only focused on one outfit despite captions",
      "solution": "Add image dataset with outfit variations alongside video dataset",
      "from": "Ryzen"
    },
    {
      "problem": "Bidirectional sampling OOMs on RTX 3090",
      "solution": "Issue identified and PR submitted to fix",
      "from": "Kosinkadink"
    },
    {
      "problem": "Fantasy Portrait arms don't move",
      "solution": "Use end frame where arms are positioned down",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Tensor size mismatch errors with context windows",
      "solution": "Check that frame count is divisible by 16 and use valid frame counts like 57, 61 instead of 60",
      "from": "Kijai"
    },
    {
      "problem": "12-pin connector melting during training",
      "solution": "System shutdown required, cable melted from center but connector side was okay",
      "from": "Ryzen"
    },
    {
      "problem": "ONNX dependency missing for FantasyPortrait",
      "solution": "Install onnx and onnxruntime-gpu: pip install onnx onnxruntime-gpu",
      "from": "Kijai"
    },
    {
      "problem": "Context window drift with fantasy portrait",
      "solution": "Use more overlap (32 instead of 16) and 81 frame context size",
      "from": "Kijai"
    },
    {
      "problem": "Jiggling motion with Lightning 4-steps",
      "solution": "Try CFG=2 on HIGH and CFG=1 on LOW, adjust shift value and CFG to find right motion",
      "from": "Ashtar"
    },
    {
      "problem": "MMAudio expecting 25fps but Wan generates 16fps",
      "solution": "Use 3x interpolation then drop every other frame to get close to 25fps",
      "from": "Kijai"
    },
    {
      "problem": "WanModel.__init__() got unexpected keyword argument 'use_motion_attn'",
      "solution": "Fixed in repository update",
      "from": "Kijai"
    },
    {
      "problem": "torch.compile errors with GGUF on second generation",
      "solution": "Claude suggested fix worked - input tensors on cuda:0, weight tensors after dequantization on cpu",
      "from": "patientx"
    },
    {
      "problem": "Rounding difference errors in video generation",
      "solution": "Set divisible by 16 or 32, ensure frame count and resolution match across inputs",
      "from": "Kijai"
    },
    {
      "problem": "OutOfMemoryError with context windows",
      "solution": "Increase block swap amount, reduce resolution to test, use save latent node for long videos",
      "from": "Kijai"
    },
    {
      "problem": "Face not detected in FantasyPortrait",
      "solution": "Input resolution needs to be higher or more direct face view",
      "from": "Kijai"
    },
    {
      "problem": "Block swap with use_non_blocking fails with torch.compile",
      "solution": "Turn off use_non_blocking when using torch.compile",
      "from": "scf"
    },
    {
      "problem": "Input shape mismatch error in Wan video wrapper",
      "solution": "Check image resolutions and/or frame count compatibility",
      "from": "Kijai"
    },
    {
      "problem": "Tensor size error with native context",
      "solution": "Sizes of tensors must match except in dimension 1. Expected size 21 but got size 42",
      "from": "Draken"
    },
    {
      "problem": "Queue control feature not working properly",
      "solution": "Clicking queue control runs full queue instead of selective execution",
      "from": "Ablejones"
    },
    {
      "problem": "Audio stops after 5s in Multi-talk workflow despite 16s video",
      "solution": "Issue with audio passthrough in Multi-talk-long workflow",
      "from": "BobbyD4AI"
    },
    {
      "problem": "Looping effect with skyreels lora on 2.2",
      "solution": "Increase LoRA strength to 1.8/1.5 (high/low) to break the looping",
      "from": "NebSH"
    },
    {
      "problem": "MultiTalk and InfiniteTalk mouth not moving",
      "solution": "Set multitalk fps to 30 and audio cfg to 2",
      "from": "Charlie"
    },
    {
      "problem": "Talking models not working properly",
      "solution": "Set add noise to samples to true and drop denoise down to 0.7",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "CUDA cache issues with fragmentation",
      "solution": "Cache management is unpredictable - fragments and grows due to ecosystem interactions",
      "from": "Clownshark Batwing"
    },
    {
      "problem": "FP32/FP16 mixed model running very slowly",
      "solution": "Model should use 30gb dedicated and 40gb total VRAM with 40 blocks swapped for proper performance",
      "from": "seitanism"
    },
    {
      "problem": "Mismatched landmarks between ref image and driving video",
      "solution": "Replace first frame of driving video with ref image to align landmarks, then use VHS nodes to merge frames",
      "from": "mamad8"
    },
    {
      "problem": "Infinite embed errors when using InfiniteTalk",
      "solution": "Cannot have both KJ's wrapper and MeiGen-AI's fork installed simultaneously",
      "from": "Kijai"
    },
    {
      "problem": "WanVideoSampler tensor size mismatch error",
      "solution": "Disable all FantasyPortrait related nodes",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "problem": "VACE embedding causes core dumps and disk filling",
      "solution": "Use MagRef instead for identity preservation",
      "from": "mdkb"
    },
    {
      "problem": "Qwen Image Edit black output",
      "solution": "Disable sage attention in settings",
      "from": "Lodis"
    },
    {
      "problem": "Can't see InfiniteTalk option in node",
      "solution": "Remove MeiGen-AI fork and use only KJ's wrapper",
      "from": "Kijai"
    },
    {
      "problem": "Scaled model compatibility",
      "solution": "Both main and extra models must be _scaled versions or neither",
      "from": "Kijai"
    },
    {
      "problem": "Noise output when using fp8_scaled models with merged LoRAs",
      "solution": "Disable merge_loras or use unmerged LoRAs - merging not working with _scaled multitalk models",
      "from": "Kijai"
    },
    {
      "problem": "Audio/video sync issues in InfiniteTalk",
      "solution": "Use input fps to feed output fps, keep 25fps for embeds and use exact frame calculation from audio length",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Error with clip_embeds NoneType",
      "solution": "Loop expects clip embeds to always be there, can work around by setting strengths to 0 or providing clip embeds",
      "from": "Kijai"
    },
    {
      "problem": "Can't use Image to Video MultiTalk with context options",
      "solution": "Use normal I2V node instead with context - they are not compatible conceptually",
      "from": "Kijai"
    },
    {
      "problem": "Latent previews not working",
      "solution": "Need to update VHS nodes and refresh browser fully after update",
      "from": "Kijai"
    },
    {
      "problem": "KeyError 'samples' in WanVideoWrapper",
      "solution": "Don't use multitalk image to vid node with regular samplers - it makes the first sampler loop and decode in the loop, so it never returns latents",
      "from": "Kijai"
    },
    {
      "problem": "Black generation when changing LoRA with scaled model",
      "solution": "Need to restart when changing LoRAs with scaled models",
      "from": "ArtOfficial"
    },
    {
      "problem": "Out of memory with InfiniteTalk + FantasyPortrait",
      "solution": "Use blockswap (try 20-40), reduce resolution, or use context windows instead of multitalk node",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Audio sync issues with InfiniteTalk",
      "solution": "Don't use multitalk node for 81 frames and under, use regular I2V node instead",
      "from": "Kijai"
    },
    {
      "problem": "Scaled fp8 model error",
      "solution": "Set quantization to '_scaled' when using scaled fp8 models",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Extra video frames beyond audio length",
      "solution": "For short clips under 81 frames, don't use the multitalk node at all",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI nodes out of date for InfiniteTalk",
      "solution": "Don't update through manager, it can't keep up with Kijai - use git pull directly from folder or GitHub Desktop",
      "from": "DawnII"
    },
    {
      "problem": "Error with torch.compile on RTX 3090",
      "solution": "Need 4000 series GPU or higher to use torch.compile with fp8_e4m3fn weights, use e5m2 or GGUF Q8 instead on 3090",
      "from": "Kijai"
    },
    {
      "problem": "numpy remapping warning",
      "solution": "Install numpy==1.26.4 with 'python -m pip install numpy==1.26.4' - manager is forcing numpy version under 2.0",
      "from": "shockgun"
    },
    {
      "problem": "ComfyUI refresh nodes after UI change",
      "solution": "Use 'R' shortcut key to refresh nodes without restarting",
      "from": "Kijai"
    },
    {
      "problem": "Bad T2I quality results",
      "solution": "Check CFG settings - user had CFG set to wrong value, also avoid using CFG with no negative prompt",
      "from": "Dream Making"
    },
    {
      "problem": "InfiniteTalk fails at specific frame lengths",
      "solution": "Try doing more frames - fails at certain lengths due to overlap issues",
      "from": "Kijai"
    },
    {
      "problem": "First frame noise issues in videos",
      "solution": "Use multitalk image to video encode node and change mode to 'infinitetalk', get rid of context windows, or add empty buffer frames to start",
      "from": "ArtOfficial, Kijai"
    },
    {
      "problem": "Tensor creation error with negative dimension when using context",
      "solution": "Getting error 'Trying to create tensor with negative dimension -10: [16, -10, 60, 104]' on high noise when plugging context",
      "from": "NebSH"
    },
    {
      "problem": "Motion reverses or does weird stuff",
      "solution": "Try context options with different context lengths for each, consider doing only one step on high noise since it seems to break in high noise but low noise is fine",
      "from": "Tango Adorbo"
    },
    {
      "problem": "LoRA only generating one type of outfit",
      "solution": "Lower LoRA strength to 0.8, set CFG to 3 instead of 1, use NAG to add specific outfit keywords to negative when using LoRA",
      "from": "Ryzen, mamad8"
    },
    {
      "problem": "OutOfMemoryError after updating WanWrapper",
      "solution": "Allocation error is about VRAM not RAM, check batch size settings",
      "from": "shockgun, Kijai"
    },
    {
      "problem": "Wan Video NAG does not work with gguf clip encoder",
      "solution": "User got it working but didn't specify the exact solution",
      "from": "xwsswww"
    },
    {
      "problem": "TypeError: sageattn() got an unexpected keyword argument 'tensor_layout'",
      "solution": "Need to have the Sage Attention lib installed and working",
      "from": ". Not Really Human :."
    },
    {
      "problem": "V2V with InfiniteTalk requires exact frame matching",
      "solution": "Input sample length must equal frame_window_size, may need to split input video into 81 frame chunks",
      "from": "MysteryShack"
    },
    {
      "problem": "Wan 2.2 low noise upscale causing OOM",
      "solution": "Use image upscaling instead of latent space",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Static appearing at start of each section in endless workflow with InfiniteTalk",
      "solution": "Need to adjust settings and create custom node to remove static",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "ComfyUI preview broke after update",
      "solution": "Update VHS",
      "from": "Lodis"
    },
    {
      "problem": "Node connection issues in workflow",
      "solution": "Clear cache and reboot",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "problem": "MMaudio only generating 3 seconds of audio for 5-second video",
      "solution": "Need to be at 25 fps for audio to match video duration. MMaudio is initially set to 8 seconds but requires 25 fps sync",
      "from": ".: Not Really Human :."
    },
    {
      "problem": "Latent input connection issue after update",
      "solution": "Refresh browser after update - the latent input to that node is optional in current version",
      "from": "Kijai"
    },
    {
      "problem": "High swap usage in wrapper",
      "solution": "Use --cache-none or --disable-smart-memory flags",
      "from": "MysteryShack and mdkb"
    },
    {
      "problem": "InfiniteTalk producing first frame noise when not using loop",
      "solution": "Update nodes and set 'encode init image' option when NOT using the Multitalk image to video loop sampling. The code detects when InfiniteTalk is loaded and injects encoded image to first latent on every step.",
      "from": "Kijai"
    },
    {
      "problem": "MAGREF_Wan2.1_I2V_14B-Q6_K.gguf producing all noise output with InfiniteTalk",
      "solution": "Don't mix gguf with other model types. Use gguf InfiniteTalk model with gguf Wan models, or safetensors with safetensors. LoRAs can be mixed with either type.",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI custom node conflicts causing issues",
      "solution": "If multiple node packs have identical node names, only one is used. Remove conflicting forks and ensure you're running the most recent code version. Use git pull and check you're on the correct branch.",
      "from": "Kijai"
    },
    {
      "problem": "Wan 2.2 FLF second input going grey",
      "solution": "Need at least 81 frames for Wan 2.2 FLF to work properly. Second input also goes grey when going 180+ frames - must stay within bounds.",
      "from": "cyber jock"
    },
    {
      "problem": "Cannot copy out of meta tensor; no data! issues with gguf Q8",
      "solution": "Use pytorch version: 2.8.0+cu129",
      "from": "yukass"
    },
    {
      "problem": "RuntimeError: Trying to create tensor with negative dimension",
      "solution": "Update WanVideoWrapper nodes",
      "from": "DawnII"
    },
    {
      "problem": "Fantasy Portrait Face Detector extremely slow on GPU",
      "solution": "Uninstall and reinstall onnxruntime-gpu",
      "from": "theUnlikely"
    },
    {
      "problem": "Control signal only works with Fun-Control model error",
      "solution": "Update Kijai nodes - support was added today",
      "from": "Kijai"
    },
    {
      "problem": "Extra tail frames being generated",
      "solution": "Audio frames should be (n*total frames - n*motion frames)",
      "from": "DawnII"
    },
    {
      "problem": "Can't mix GGUF with non-GGUF models",
      "solution": "Use matching format models - GGUF InfiniteTalk needs GGUF main Wan model",
      "from": "Kijai"
    },
    {
      "problem": "Preview Actual Frames doesn't work",
      "solution": "Nodes not up to date, need latest push from Kijai",
      "from": "Kijai"
    },
    {
      "problem": "Extra 81 tail frames appearing at end",
      "solution": "This is normal behavior, can cut off the extra frames",
      "from": "Kijai"
    },
    {
      "problem": "MultiTalkWav2VecEmbeds error 'audio' vs 'audio_1'",
      "solution": "Need to load multitalk weights and connect multitalk loader to WAN model",
      "from": "samhodge"
    },
    {
      "problem": "Video degrading quality",
      "solution": "Update to nightly version of WanVideoWrapper and set mode to InfiniteTalk",
      "from": "sawlike"
    },
    {
      "problem": "Can't find mode option in InfiniteTalk node",
      "solution": "Need to manually update with 'git pull' from ComfyUI-WanVideoWrapper folder, nightly is behind latest",
      "from": "Josiah"
    },
    {
      "problem": "Character inconsistency in vid2vid",
      "solution": "Later start step with less denoise means less change from original, need controlnet or crop/paste for better consistency",
      "from": "DawnII"
    },
    {
      "problem": "Getting noise from low noise sampler in Wan 2.2",
      "solution": "Need denoised output from high noise stage - samples doesn't return same dict in infinitetalk mode, just returns video entry instead of samples and denoised samples",
      "from": "MysteryShack"
    },
    {
      "problem": "VACE and Phantom won't work together",
      "solution": "Both work perfectly alone but fail when combined - possible bug when VACE model is plugged into combo of nodes with Phantom model",
      "from": "mdkb"
    },
    {
      "problem": "Ghost effect at borders with blurred masks in inpainting",
      "solution": "Binary masks work much better than blurred masks for this type of inpainting",
      "from": "Nekodificador"
    },
    {
      "problem": "InfiniteTalk locked at 1000 frames",
      "solution": "Check max frames setting in the workflow - there's a parameter that can be adjusted",
      "from": "JohnDopamine"
    },
    {
      "problem": "Quality degradation in I2V with Ksampler low",
      "solution": "Use native 2 ksamplers: first stage high 2 steps without lora, second low stage 2 steps with lora",
      "from": "Not Really Human"
    },
    {
      "problem": "VACE doesn't handle blurred masks properly",
      "solution": "Use binary masks instead of blurred ones for better results",
      "from": "Kijai"
    },
    {
      "problem": "InfiniteTalk became very slow after wrapper update",
      "solution": "Check audio_cfg value - it does 3 passes. Roll back wrapper or debug configuration",
      "from": "seitanism"
    },
    {
      "problem": "Getting noise with 3x ksampler trick",
      "solution": "Check shift node connection and ksampler step settings - ensure consistent step counts",
      "from": "Ghost"
    },
    {
      "problem": "Multi VACE broke after recent update",
      "solution": "Revert to commit 6d51934ae816e9c87fb9b6183fb644d7fd564943 which works",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Fantasy Portrait fails when no face detected",
      "solution": "Face detection skips frames without faces, causing len(frame_list)=0. Check face detection in input frames",
      "from": "fredbliss"
    },
    {
      "problem": "VACE acting weird after update",
      "solution": "Fixed by Kijai - was a bug when using multiple VACEs",
      "from": "chrisd0073"
    },
    {
      "problem": "Runtime error with mat1 and mat2 shapes in WAN bridge",
      "solution": "Was passing input image latent into samples instead of correct input",
      "from": "fredbliss"
    },
    {
      "problem": "Windows Explorer crashes when selecting video files from WanVideoSampler",
      "solution": "Remove metadata with ffmpeg or disable metadata saving in VHS node - issue caused by large workflow metadata (214KB)",
      "from": "Ashtar"
    },
    {
      "problem": "Error 'cannot access local variable noise_mask' in infinitetalk",
      "solution": "Fixed by Kijai - was testing without mask and forgot to handle that case",
      "from": "Intellectus Prime"
    },
    {
      "problem": "WanVideoEncode pbar error",
      "solution": "Force update of WanVideoWrapper nodes",
      "from": "Juan Gea"
    },
    {
      "problem": "5B model generating slow motion despite settings",
      "solution": "No specific solution provided, acknowledged as ongoing issue",
      "from": "Juan Gea"
    },
    {
      "problem": "Latent noise mask not working for vid2vid",
      "solution": "Need to set mask for both high and low noise sides, and original image required for 2nd sampler",
      "from": "Kijai"
    },
    {
      "problem": "Masks not working from Blender renders",
      "solution": "Ensure masks are white on black, convert to mask in ComfyUI. Source doesn't matter if format is correct",
      "from": "Kijai"
    },
    {
      "problem": "GGUF InfiniteTalk model not appearing in MultiTalk Model Loader",
      "solution": "Update WanVideoWrapper nodes or check for conflicting installations overwriting nodes",
      "from": "Kijai"
    },
    {
      "problem": "Differential diffusion works poorly with low steps",
      "solution": "Thresholds should be calculated from total steps rather than actual sampler steps",
      "from": "Kijai"
    },
    {
      "problem": "UniAnimate with GGUF models causes errors",
      "solution": "UniAnimate won't work with GGUF models, use non-GGUF versions",
      "from": "Kijai"
    },
    {
      "problem": "Blurred video output",
      "solution": "Check scheduler, CFG settings, and step count. Use steps 4, cfg 1, shift 1 for distill models",
      "from": "samhodge"
    },
    {
      "problem": "Latent upscale not working with nearest exact",
      "solution": "Use bilinear instead and apply cinescale lora on both high and low, but works better on low",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI shutting down when using prompt extender",
      "solution": "Change base precision from fp16 to bf16 in the qwen loader",
      "from": "seitanism"
    },
    {
      "problem": "Getting OOM with text encoders",
      "solution": "Issue was torch compiler - disabling it reduces VRAM use quite a bit",
      "from": "Kenk"
    },
    {
      "problem": "Repeat latent batch not working for static camera",
      "solution": "Repeat the image first, then encode - don't repeat the latents directly",
      "from": "DawnII"
    },
    {
      "problem": "T2V with 5B turbo getting confetti and distortion",
      "solution": "Use A14B model instead of E5, and use dpm++_sde sampler instead of flowmatch_distill for T2V",
      "from": "Kijai"
    },
    {
      "problem": "CUDA out of memory errors with VACE Start End Frame",
      "solution": "Disable non-blocking on block swap setting - CUDA out of memory = RAM error, not VRAM",
      "from": "Kijai"
    },
    {
      "problem": "Film grain node OOM errors on 800+ frame videos",
      "solution": "Node has limitations for processing very long videos, no current workaround for that length",
      "from": "Kenk"
    },
    {
      "problem": "Multitalk error 'cannot reshape tensor of 0 elements'",
      "solution": "Error means empty input or zero batch size, check audio loading and don't use Multitalk long I2V node with audio under 5 seconds",
      "from": "Kijai"
    },
    {
      "problem": "Bidirectional sampling OOM errors even on RTX 5090",
      "solution": "Known issue, no current solution provided",
      "from": "Roman_S"
    },
    {
      "problem": "Context windows producing size of tensor errors in native version",
      "solution": "Very difficult to run without errors, context windows not intended to work on 2.2 model yet",
      "from": "Mngbg"
    },
    {
      "problem": "First frames overexposure in videos longer than 5 seconds",
      "solution": "Use skyreels LoRA, though the fix node mentioned may not actually solve this issue",
      "from": "Kijai"
    },
    {
      "problem": "Control signal only works with Fun-Control model error",
      "solution": "Update nodes or check for conflicting WanVideoWrapper versions in custom nodes folder",
      "from": "Kijai"
    },
    {
      "problem": "Hard crashes after low noise model finishes",
      "solution": "Use Cached text encoder node to save 10GB RAM - swap regular text encoder for cached version",
      "from": "Kijai"
    },
    {
      "problem": "No audio in InfiniteTalk output",
      "solution": "Cannot have bypass on audio processing nodes - remove bypass",
      "from": "Kijai"
    },
    {
      "problem": "Torch compile using more VRAM on first run",
      "solution": "Use more block swap to get past initial VRAM spike, issue seems related to torch 2.8 update",
      "from": "Kijai"
    },
    {
      "problem": "Video breaks halfway in short clips",
      "solution": "Don't use multitalk node for clips under 7s, use normal I2V node instead",
      "from": "Kijai"
    },
    {
      "problem": "Strobe flashing in i2v generations",
      "solution": "Use more compute/original parameters, issue occurs when not using enough compute",
      "from": "aikitoria"
    },
    {
      "problem": "LoRAs not working with CFG Schedule in wrapper",
      "solution": "Bump lora strength to 2.0 to make them work with CFG Schedule",
      "from": "Drommer-Kille"
    },
    {
      "problem": "Color degradation with Wan 2.2",
      "solution": "Issue acknowledged but no specific solution provided",
      "from": "Kenk"
    },
    {
      "problem": "InfiniteTalk cutting audio short",
      "solution": "No solution provided, issue acknowledged",
      "from": "amli"
    },
    {
      "problem": "InfiniteTalk OOM on 12GB VRAM",
      "solution": "Memory issue caused by VAE cache not clearing between windows - fix in development",
      "from": "Kijai"
    },
    {
      "problem": "UniAnimate crash with resolution mismatch",
      "solution": "Need enough pose images to match the total frames including overlap - looping method needs more frames than window size due to overlap",
      "from": "Kijai"
    },
    {
      "problem": "VACE quality issues at 1280x720",
      "solution": "Check mask quality (must be perfect, no blur) and consider using PUSA LoRA for color preservation",
      "from": "dg1860"
    },
    {
      "problem": "FP8 fast mode won't work with unmerged LoRAs",
      "solution": "Use the merge_loras switch in the lora select node",
      "from": "Kijai"
    },
    {
      "problem": "Image resizing in wrapper",
      "solution": "Images not divisible by 16 get automatically resized (e.g. 512x910 becomes 512x896)",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "problem": "First 1-3 frames with InfiniteTalk look corrupted",
      "solution": "Try using additional nodes and connecting to first sampler cfg input",
      "from": "Kenk"
    },
    {
      "problem": "VRAM hitting 100% causing infinite generation times",
      "solution": "Keep VRAM below 95% on Windows by increasing block swap amount",
      "from": "Kijai"
    },
    {
      "problem": "Colors taking a hit when using InfiniteTalk",
      "solution": "Hook InfiniteTalk only into high sampler, not both high and low",
      "from": "Draken"
    },
    {
      "problem": "Fantasy face detector node failing",
      "solution": "Needs a video input, not a single image. Some frames may be skipped if no faces detected",
      "from": "Kijai"
    },
    {
      "problem": "Double application of shift values",
      "solution": "Set sampler shift to 1.0 to avoid double applying when using shift on both dummy object and sampler",
      "from": "Kijai"
    },
    {
      "problem": "InfiniteTalk V2V workflow broken after WanNode update",
      "solution": "Fixed in update",
      "from": "Yae/Kijai"
    },
    {
      "problem": "Openpose appearing in video outputs",
      "solution": "Condition separately, decrease VACE strength, try seed changes",
      "from": "Piblarg"
    },
    {
      "problem": "Mixing control signals when combining pose/depth/lineart",
      "solution": "Use WanVacePhantomDualV2 for separate control inputs instead of combining into single control image",
      "from": "Ablejones"
    },
    {
      "problem": "InfiniteTalk losing coherence at end with extensions",
      "solution": "Fix first latent noise by injecting the image as encoded latent to index 0 using extra_latents",
      "from": "Kijai"
    },
    {
      "problem": "FantasyPortrait overdoing teeth generation",
      "solution": "Tone down FantasyPortrait settings",
      "from": "Kijai"
    },
    {
      "problem": "Video not starting with input image in InfiniteTalk",
      "solution": "Mentioned as Problem 2 by user, no solution provided in chat",
      "from": "Gill Bastar"
    },
    {
      "problem": "Artifacts at end of InfiniteTalk videos",
      "solution": "Might be related to LoRAs being used, no specific solution provided",
      "from": "Gill Bastar"
    },
    {
      "problem": "Slow ImageToVideo encoding with BigASP SDXL images",
      "solution": "Resize image before the node - source model makes zero difference to encoding",
      "from": "Kijai"
    },
    {
      "problem": "VACE keeps generating cowboys/Brokeback Mountain theme instead of intended character",
      "solution": "Position the reference image accurately - overlay it in image editor at exact position where character should appear in final video",
      "from": "mdkb"
    },
    {
      "problem": "InfiniteTalk video artifacts get progressively worse",
      "solution": "Set mode to 'infinitetalk' in the node and update wanvideowrapper to nightly",
      "from": "brock0ut11"
    },
    {
      "problem": "First 4-5 frames appear latent/pixelated in InfiniteTalk",
      "solution": "Use the proper InfiniteTalk workflow from wrapper - set first latent to be the start image as InfiniteTalk model expects",
      "from": "Kijai"
    },
    {
      "problem": "Fun InP model channel mismatch error",
      "solution": "Cannot use control with InP model - must use Fun-Control model for control inputs. InP = 36 channels, Fun-Control = 52 channels",
      "from": "Kijai"
    },
    {
      "problem": "Temporal mask casting error in image encoding",
      "solution": "Fixed in latest update - was related to fp32 VAE optimization",
      "from": "Kijai"
    },
    {
      "problem": "Fun-Control reference image won't work with fp8_scaled model",
      "solution": "Fixed in latest version of the model, requires redownload",
      "from": "scf"
    },
    {
      "problem": "OOM error with 96GB VRAM and 128GB RAM when loading fp16 model",
      "solution": "Need to add paging file/swap file even with large RAM amounts",
      "from": "Ablejones"
    },
    {
      "problem": "Cannot access local variable 'h_len' error when using standin",
      "solution": "Update to latest version with fix",
      "from": "Kijai"
    },
    {
      "problem": "Infinite Talk color degradation in I2V mode",
      "solution": "Use MAGREF model to prevent degradation, though it's blurrier",
      "from": "seitanism"
    },
    {
      "problem": "Multitalk doesn't work with regular models",
      "solution": "Update nodes and use specified models",
      "from": "Gill Bastar"
    },
    {
      "problem": "Mmaudio truncation with 16fps Wan output",
      "solution": "Mmaudio expects 24 or 25 fps input",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI-WanVideoWrapper tensor size mismatch error in WanVideo Sampler",
      "solution": "Kijai fixed variable definition error, update the wrapper",
      "from": "Kijai"
    },
    {
      "problem": "VAE decode errors with second samplers in FFLF workflows",
      "solution": "Update wrapper - was error in variable definition affecting 2.2 first frame last frame workflows",
      "from": "Kijai"
    },
    {
      "problem": "Get/set nodes causing errors when bypassing",
      "solution": "Known ComfyUI issue since recent frontend updates",
      "from": "Kijai"
    },
    {
      "problem": "Ghosting/blurry generations with default Wan 2.2 i2v workflow",
      "solution": "Try changing resolution to match source aspect ratio (e.g. 432x768), check high/low lightx LoRAs are correct",
      "from": "garbus"
    },
    {
      "problem": "Characters keep talking despite prompting for silence",
      "solution": "Try multitalk node with 5 seconds of silence in audio, or prompts like 'characters do not talk to each other' at end",
      "from": "mdkb"
    },
    {
      "problem": "2.2 + infinite context causing OOM",
      "solution": "Won't work with any 2 sampler setup, check max frames not set as frame window size",
      "from": "Kijai"
    },
    {
      "problem": "Non-blocking block swap stops workflow on 3060",
      "solution": "Disable non-blocking religiously on 3060",
      "from": "mdkb"
    },
    {
      "problem": "Video pixelation with InfiniteTalk when using Tea Cache",
      "solution": "Tea Cache doesn't work with LoRA accelerators, disable it",
      "from": ".: Not Really Human :."
    },
    {
      "problem": "Color degradation with 2.2 + multitalk",
      "solution": "Use color match node after generation",
      "from": "N0NSens"
    },
    {
      "problem": "Getting picture instead of video with InfiniteTalk",
      "solution": "Make sure WanVideo Long I2V Multi/InfiniteTalk node is included - it's necessary for InfiniteTalk to work",
      "from": "Mancho"
    },
    {
      "problem": "Control input missing error (52 vs 36 channels)",
      "solution": "Control model is loaded but input is normal I2V input missing the control part of 16 channels",
      "from": "Kijai"
    },
    {
      "problem": "HotReloadHack broken after ComfyUI update",
      "solution": "Switch to Kijai's fork: https://github.com/kijai/ComfyUI-HotReloadHack",
      "from": "Kijai"
    },
    {
      "problem": "VRAM OOM with WanFM on A100 80GB",
      "solution": "Cancel bidirectional_sampling to make it work",
      "from": "Yan"
    },
    {
      "problem": "Differential diffusion affecting areas outside mask",
      "solution": "Should do normal pixel space composition after sampling since latent masking always affects whole image through VAE",
      "from": "Kijai"
    },
    {
      "problem": "Wan 2.2 Turbo em52 not working well with e5 model",
      "solution": "Try the fp16 model instead",
      "from": "Kijai"
    },
    {
      "problem": "WanVideoWrapper error on second run - 'cannot copy out of meta tensor; no data!'",
      "solution": "Roll back to previous version, issue appears to be with latest version",
      "from": "Kenk"
    },
    {
      "problem": "Blurry unusable results with Wan 2.2 I2V default workflow",
      "solution": "Try 6 or 8 steps instead of 4, update PyTorch to 2.7+, use shift of 8 for both samplers",
      "from": "JohnDopamine/screwfunk"
    },
    {
      "problem": "Beta sigma conversion applied twice",
      "solution": "Don't use both beta sigmas and beta version of scheduler together, use normal dpm++_sde",
      "from": "Kijai"
    },
    {
      "problem": "Tensor dimension error with negative values",
      "solution": "Check for batch mismatch in workflow",
      "from": "DawnII"
    },
    {
      "problem": "Block swap error on second generation with T2V models",
      "solution": "Fixed in latest update - model loading issue with merged LoRAs resolved",
      "from": "Kijai"
    },
    {
      "problem": "IndexError: index 3 is out of bounds for dimension 0 with size 3",
      "solution": "Wrong wav2vec2 model - need to use correct model from HuggingFace",
      "from": "Kijai"
    },
    {
      "problem": "einops.EinopsError with S2V resolutions",
      "solution": "Use resolution formula: (width\u00f716) \u00d7 (height\u00f716) \u00d7 frames \u00f7 30 must be whole number",
      "from": "patientx"
    },
    {
      "problem": "Error running sage attention: Input tensors must be on cuda",
      "solution": "Don't use --use-sage-attention flag at startup, select it in WanVideo model loader instead",
      "from": "patientx"
    },
    {
      "problem": "Missing AudioEncoderLoader node",
      "solution": "Need to update to ComfyUI nightly build and git pull",
      "from": "Kijai"
    },
    {
      "problem": "WanVideoSampler stride error with GGUF models",
      "solution": "Use Kijai's VACE models from HuggingFace instead of VACE+Phantom model - the combined model doesn't work well with wrapper or isn't compatible with WAN 2.2",
      "from": "xwsswww"
    },
    {
      "problem": "Memory spikes and instability in latest WanVideoWrapper",
      "solution": "Older version of WanVideoWrapper is more stable, latest dev changes cause VRAM spikes",
      "from": "Kenk"
    },
    {
      "problem": "InfiniteTalk creates extra noise frames at end",
      "solution": "Pad audio with 2 seconds of silence that can be cut off later",
      "from": "ArtOfficial"
    },
    {
      "problem": "ComfyUI native S2V resolution errors",
      "solution": "Use the bf16 model specifically designed for native implementation with lightx2v",
      "from": "patientx"
    },
    {
      "problem": "Context options causing VRAM OOM on low sampler",
      "solution": "Connect context options to low sampler, can use less overlap (like 16) on low side while high noise may need up to 48",
      "from": "Kijai"
    },
    {
      "problem": "Context windows cause overblown start in videos",
      "solution": "so the overblown start is probably something to be expected",
      "from": "Kijai"
    },
    {
      "problem": "Generic issue with Uni3C offloading",
      "solution": "this should be fixed now, it was generic issue with Uni3C offloading introduced in the refactor. first run would work but any 2nd run would fail because it was accidentally clearing it's weights too along with the model",
      "from": "Kijai"
    },
    {
      "problem": "Memory issues with WanWrapper vs native nodes",
      "solution": "probably because the offloading is automatic in native and in the wrapper it's manual with the block swap",
      "from": "Kijai"
    },
    {
      "problem": "E5 model not working",
      "solution": "Try using gguf instead",
      "from": "Slavrix"
    },
    {
      "problem": "fp8_e5m2_fast mode issues",
      "solution": "you need 4000+ series GPU to use the _fast mode. fp16 fast thing should work on 3090",
      "from": "Kijai"
    },
    {
      "problem": "VAE loader automatically selecting wrong VAE",
      "solution": "stupid vae loader automatically selecting ae.safetensors each time",
      "from": "patientx"
    },
    {
      "problem": "S2V video gets noised when decoding",
      "solution": "enabling the second sampler seems to have 'fixed'",
      "from": "Kenk"
    },
    {
      "problem": "Index out of bounds error on second generation",
      "solution": "Force node reset between runs using cache-clearing nodes or restart workflow",
      "from": "GDuque, \u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Custom LoRAs not working in wrapper",
      "solution": "Enable merge option in wrapper, as unmerged LoRAs behave differently than native",
      "from": "mamad8, Kijai"
    },
    {
      "problem": "S2V nodes missing in ComfyUI",
      "solution": "Switch to nightly branch of ComfyUI and s2v branch of WanVideoWrapper",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "Cannot copy out of meta tensor error with InfiniteTalk",
      "solution": "Disable offloading when using block swap, as offloading isn't needed",
      "from": "Kijai"
    },
    {
      "problem": "Trying to create tensor with negative dimension error",
      "solution": "Check audio length and frame count compatibility, ensure proper audio preprocessing",
      "from": "Slavrix"
    },
    {
      "problem": "Color degradation with InfiniteTalk beyond 1 minute",
      "solution": "Try disabling LightX LoRAs or adjusting their strength",
      "from": "boorayjenkins"
    },
    {
      "problem": "Slow model loading on better GPU",
      "solution": "Issue may be related to non-native filesystem (NTFS/exFAT on Linux) causing FUSE performance problems",
      "from": "MysteryShack"
    },
    {
      "problem": "VACE Encode OOM with 48GB VRAM on 119 frames 1280x720",
      "solution": "Issue reported, likely related to recent updates",
      "from": "AmirKerr"
    },
    {
      "problem": "Rife-VFI making 7 second video from 4 second original",
      "solution": "Probably didn't change fps settings",
      "from": "Mu5hr00m_oO"
    },
    {
      "problem": "DWPose adding excessive teeth in generations",
      "solution": "Use face points only or multiply pose condition by 0.5",
      "from": "Kijai"
    },
    {
      "problem": "S2V branch installation issues",
      "solution": "Use git fetch --all, git switch s2v, then git switch main to go back. Use git pull only updates current branch",
      "from": "Kijai"
    },
    {
      "problem": "KeyError: 'frame_packer.proj.weight' in S2V",
      "solution": "Model loading issue with framepack implementation",
      "from": "DawnII"
    },
    {
      "problem": "Color shift issues with VACE",
      "solution": "Color Match helps but doesn't solve extreme cases, not a full relight solution",
      "from": "Dream Making"
    },
    {
      "problem": "GGUF models had memory offloading issues causing high VRAM usage",
      "solution": "Fixed bug where offloading didn't work with GGUF models due to int8 weights not supporting meta device",
      "from": "Kijai"
    },
    {
      "problem": "Chunked rope error in WanVideo model causing RuntimeError",
      "solution": "Disable compile for now or use e5m2_scaled quantization instead",
      "from": "Kijai"
    },
    {
      "problem": "High and Low noise models were connected incorrectly causing mosaic/confetti artifacts",
      "solution": "Ensure High noise model goes to second sampler, Low noise model goes to first sampler",
      "from": "DawnII"
    },
    {
      "problem": "Control signals only work with Fun-Control model, not 1.3B model",
      "solution": "Use Fun-Control model specifically when using control inputs",
      "from": "N0NSens"
    },
    {
      "problem": "Missing start_step value in sampler causing conversion error",
      "solution": "Right-click on ksampler and pick 'fix node' to reset moved values from updates",
      "from": "JohnDopamine"
    },
    {
      "problem": "S2V model generates wrong number of frames and audio sync issues",
      "solution": "Check frame count settings and use proper audio loading nodes, model pads with empty if more frames than audio",
      "from": "Kijai"
    },
    {
      "problem": "Error when control lora wasn't last in the list",
      "solution": "Fixed by ensuring control lora is the last lora in the list",
      "from": "Kijai"
    },
    {
      "problem": "S2V color mismatch in first few frames",
      "solution": "Comfy posted a fixed workflow for that issue",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Git branch switching issues with WanVideoWrapper",
      "solution": "Reclone the wrapper repository to fix git issues",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Memory accumulation in Framepack loop",
      "solution": "Fixed memory use by putting cache clear in correct place - was accumulating endlessly leading to 7GB wasted",
      "from": "Kijai"
    },
    {
      "problem": "Inpainting distortion in wrapper",
      "solution": "Disable tiled VAE on the WanVideo VACE encode node to fix center pixel shifting",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "problem": "S2V generates too much music",
      "solution": "Use negative prompt to control unwanted audio generation, similar to how mmaudio had tendency to add speech",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "OOM errors and extreme slowdown after updating",
      "solution": "Set model loader to offload_device instead of main device, and enable tiling on video encoder",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "problem": "Pure noise in I2V infinite talk workflow",
      "solution": "Connect the VAE - this was missing",
      "from": "Kijai"
    },
    {
      "problem": "InfinityTalk workflow showing KeyError about cross_attn weights",
      "solution": "Roll back a few commits (git checkout bc22008) - caused by block prefetch feature change",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "problem": "ComfyUI crashing when generating 500+ frames",
      "solution": "Change main device to offload_device in model loader settings",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "problem": "Artifacts and overlap in infinite talk outputs",
      "solution": "Remove AccVid LoRA - it was causing the artifacts",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Resolution error with T2V",
      "solution": "Bump resolution from 384x640 to 392x648, and make sure using T2V model not I2V model",
      "from": "jeffcookio"
    },
    {
      "problem": "Uni3C not working with VACE",
      "solution": "Uni3C only works with I2V, not with VACE workflows",
      "from": "DawnII"
    },
    {
      "problem": "Camera movement in VACE when unwanted",
      "solution": "Use spline editor to lock motion by keeping spline static on background, or use FUN control",
      "from": "DawnII"
    },
    {
      "problem": "Wan 2.2 adding random body parts frequently",
      "solution": "Remove negative prompts, add 'human' and 'body' to negatives if needed",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "problem": "Trash output with control LoRAs after update",
      "solution": "Reduce denoise value when switching from old to new WanVideoSampler node",
      "from": "N0NSens"
    },
    {
      "problem": "VACE color changes between different scenes",
      "solution": "Color matching can help slightly but gets worse with very different scenes",
      "from": "yukass"
    },
    {
      "problem": "InfiniteTalk cutting video length shorter than audio",
      "solution": "Unknown - user reported 1:09 audio only generating 45 seconds",
      "from": "amli"
    },
    {
      "problem": "Wan 2.2 I2V producing poor quality results in native",
      "solution": "Use wrapper version instead, or ensure no shortcuts, no LoRAs on high noise model, and use full CFG",
      "from": "N0NSens and MysteryShack"
    },
    {
      "problem": "VRAM issues with upscaling workflow on RTX 5090",
      "solution": "Try Q8 quantized models instead of FP16, or use quantization on both nodes",
      "from": "Gill Bastar"
    },
    {
      "problem": "Context windows work poorly with I2V",
      "solution": "I2V works poorly with context windows, MAGREF can work but technique cannot work with regular I2V",
      "from": "Kijai"
    },
    {
      "problem": "First-to-last frame generation coming out noisy",
      "solution": "Connect first sampler output to second sampler properly",
      "from": "Abx"
    },
    {
      "problem": "Disk space not clearing after block swap",
      "solution": "Need to do hard restart to clear disk space",
      "from": "xwsswww"
    },
    {
      "problem": "RAM usage issues resolved",
      "solution": "New pytorch update helps with RAM usage - setting memory allocator to 0 makes it give memory back to OS more aggressively",
      "from": "comfy and Ablejones"
    },
    {
      "problem": "Error 'int' object has no attribute 'clone' in WanVideoEncode",
      "solution": "Something is wrong with the image input into your WanVideoEncode node, probably the one with the ref video. Double check to make sure that the image input is getting a real image batch",
      "from": "Ablejones"
    },
    {
      "problem": "Shape error with T2V 2.2 + InfiniteTalk",
      "solution": "Error during model prediction: shape '[21, 32, 2, 40, 128]' is invalid for input of size 13434880",
      "from": "NebSH"
    },
    {
      "problem": "VHS nodes progress bar keeps going after workflow completion",
      "solution": "Bug where while workflow is done based by console output, the VHS nodes progress bar keeps going for a lot longer time before you get the output visible. Fixed by updating front end version",
      "from": "Kijai"
    },
    {
      "problem": "Beyond 45 frames getting blurred smoke results",
      "solution": "Max 81 is stable if you dont use any extension methods. Beyond that may not have enough memory for longer frames",
      "from": "Mu5hr00m_oO"
    },
    {
      "problem": "Recent OOM issues with wrapper",
      "solution": "Need to increase blockswaps from 20 to 26 for 480x720 resolution",
      "from": ".: Not Really Human :."
    },
    {
      "problem": "Sudden OOM on second generation filling 96GB VRAM",
      "solution": "Issue persists even with older commits, suggesting recent changes causing memory leaks",
      "from": "HeadOfOliver"
    }
  ],
  "comparisons": [
    {
      "comparison": "SDXL vs Flux vs Wan",
      "verdict": "SDXL still top for artistic creation, especially with IPAdapterV2 for style control. Flux useful for editing/cleaning SDXL pictures but disliked overall. Wan great for generation but impossible to control light colors",
      "from": "GOD_IS_A_LIE"
    },
    {
      "comparison": "Wan 2.2 vs Veo 3",
      "verdict": "If Wan 2.2 could do 24fps and longer generations consistently, it would be better than Veo 3. Veo has better content control and timestamping, Wan has better camera control",
      "from": "Ruairi Robinson"
    },
    {
      "comparison": "WAN 2.2 vs closed models",
      "verdict": "2.2 has best quality but +15 min runs still not worth it compared to closed models",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "T2V vs I2V workflow preference",
      "verdict": "T2V more dynamic/realistic, I2V requires extensive seed hunting but gives more control",
      "from": "seitanism"
    },
    {
      "comparison": "RadialAttention vs SageAttention",
      "verdict": "Radial is faster but requires quality tweaking, sage more stable",
      "from": "Kijai"
    },
    {
      "comparison": "SageAttention3 quality",
      "verdict": "Early access shows terrible quality, can be mitigated by using on certain steps/blocks only",
      "from": "Kijai"
    },
    {
      "comparison": "WAN 2.2 vs VEO3 I2V quality",
      "verdict": "Very similar quality, both 1280x720x24fps. WAN 2.2 is on correct path to match VEO3. VEO3 better at prompt adherence, WAN better at avoiding leaf noise",
      "from": "Juan Gea"
    },
    {
      "comparison": "Native vs Kijai wrapper performance",
      "verdict": "Wrapper much faster - 400 seconds vs 20+ minutes for same 1536x768x81 frames on RTX 5090",
      "from": "IceAero"
    },
    {
      "comparison": "WAN 2.1 vs 2.2 for LoRAs",
      "verdict": "Can use 2.1 LoRAs on 2.2 14B for NSFW. I2V works great with LoRAs, T2V not as much",
      "from": "crinklypaper"
    },
    {
      "comparison": "DDIM vs DPMPP_SDE samplers",
      "verdict": "DDIM runtime 12m vs DPMPP_SDE 21m - DDIM is twice as fast",
      "from": "The Shadow (NYC)"
    },
    {
      "comparison": "5B vs 14B motion quality",
      "verdict": "14B motion is far superior to 5B, though 5B image quality is great",
      "from": "Kijai"
    },
    {
      "comparison": "Q3 GGUF vs FP8 results",
      "verdict": "Q3 was giving better results than fp8, possibly due to lora not being active on fp8",
      "from": "hicho"
    },
    {
      "comparison": "5090 vs 4090 performance",
      "verdict": "5090 is ~30-40% faster than 4090, about 3x faster than 4080",
      "from": "Kijai"
    },
    {
      "comparison": "Native vs Wrapper with LightX performance",
      "verdict": "Native: 30 mins for 5 seconds. Wrapper with LightX: 2-3 mins for 5 seconds",
      "from": "AJO"
    },
    {
      "comparison": "Wan 2.2 vs FusionX quality",
      "verdict": "Wan 2.2 shows better prompt adherence and camera movement",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "LightX vs no LoRA on Wan 2.2",
      "verdict": "No LoRA shows better hair details and galloping movement, but LightX is much faster (146s vs 431s)",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "I2V distill vs other approaches",
      "verdict": "I2V distill described as 'the best'",
      "from": "hicho"
    },
    {
      "comparison": "0-2-10 vs 5B upscale",
      "verdict": "0-2-10 has better detail than 5B upscale",
      "from": "shockgun"
    },
    {
      "comparison": "VACE 2.1 vs 2.2 compatibility",
      "verdict": "VACE works fine on low noise model, terrible on high noise model",
      "from": "Kijai"
    },
    {
      "comparison": "FP8/FP16 vs GGUF quantized models",
      "verdict": "Only FP8 or FP16 models work well, Q models don't work good",
      "from": "avataraim"
    },
    {
      "comparison": "Pyramid vs Linear fuse method for context windows",
      "verdict": "Pyramid almost always better - smoother blend, less ghosting artifacts",
      "from": "thaakeno"
    },
    {
      "comparison": "14B model vs 5B model for upscaling",
      "verdict": "14B better for upscaling with LoRAs, 5B purpose may be just basic upscaling",
      "from": "thaakeno"
    },
    {
      "comparison": "High overlap (48) vs lower overlap (16) for context windows",
      "verdict": "48 overlap takes much longer, 16 overlap with 4 stride is good balance",
      "from": "thaakeno"
    },
    {
      "comparison": "Different CFG values for prompt adherence",
      "verdict": "Sometimes lower CFG (1.5) works when higher CFG (3.5) fails",
      "from": "Juampab12"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 for FLF support",
      "verdict": "2.2 supports FLF natively while 2.1 does not",
      "from": "Juampab12"
    },
    {
      "comparison": "SeedVR2 quality vs expectations",
      "verdict": "Results look oversharpened and don't match original samples closely",
      "from": "gokuvonlange"
    },
    {
      "comparison": "Fun models vs VACE",
      "verdict": "Can mostly forget about Fun models now that VACE exists, though there are fringe use cases",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 physics",
      "verdict": "Physics are much better with 2.2 14B",
      "from": "Fill"
    },
    {
      "comparison": "Native vs Wrapper performance",
      "verdict": "Native prevents RAM maxing during encoder caching, wrapper sometimes maxes RAM",
      "from": "Fill"
    },
    {
      "comparison": "Sage vs Flash attention",
      "verdict": "Minimal quality difference, massive speed advantage for Sage",
      "from": "Kijai"
    },
    {
      "comparison": "5B vs 14B models",
      "verdict": "5B is faster but generally pretty garbage quality compared to 14B",
      "from": "Screeb"
    },
    {
      "comparison": "Topaz vs other upscalers",
      "verdict": "Topaz is better for upscaling if video has enough detail",
      "from": "Dan"
    },
    {
      "comparison": "3090 to 4090 performance",
      "verdict": "4090 is roughly double the speed of 3090",
      "from": "screwfunk"
    },
    {
      "comparison": "2.2 vs 2.1 artifacts",
      "verdict": "2.2 eliminates the 'wanny motion' and strange artifacts present in 2.1",
      "from": "Draken"
    },
    {
      "comparison": "Precision quality ranking",
      "verdict": "fp32 > bf16 > fp16 > fp8 E5 \u2248 fp8 E4, with bf16 showing most details in practice",
      "from": "Juan Gea"
    },
    {
      "comparison": "ComfyUI scaled models vs GGUF models",
      "verdict": "Official ComfyUI scaled models have very bad quality compared to bullerwins GGUF models",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "comparison": "Original Alibaba code vs ComfyUI",
      "verdict": "Original code produces less noise artifacts and higher quality, but much slower",
      "from": "aikitoria"
    },
    {
      "comparison": "FlashAttention vs SageAttention",
      "verdict": "Quality difference exists between flash and sage, but not between flash and sdpa",
      "from": "Kijai"
    },
    {
      "comparison": "FP8 vs BF16 quantization",
      "verdict": "Differences are minimal, sometimes FP8 performs better in specific motion scenarios",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 with lightx",
      "verdict": "2.2 with lightx slightly better than 2.1, worth upgrading",
      "from": "Karo"
    },
    {
      "comparison": "Text vs JSON prompt formatting",
      "verdict": "Text-based prompts perform much better than JSON format, though JSON is mostly understood",
      "from": "mamad8"
    },
    {
      "comparison": "bf16 vs fp8 for ice cream generation",
      "verdict": "bf16 keeps ice cream consistent, better lighting, doesn't burn out easily, looks more like real animal vs guy in costume",
      "from": "HeadOfOliver"
    },
    {
      "comparison": "h1111 vs wangp for speed",
      "verdict": "wangp is faster than h1111",
      "from": "hicho"
    },
    {
      "comparison": "2.1 + VACE vs Wan 2.2 FLF2V",
      "verdict": "2.1 + VACE much better than Wan 2.2 FLF2V",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 vs Flux Krea for detailed face generation",
      "verdict": "Wan 2.2 shows superior detail quality, especially for close-up faces",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "I2V with good image model vs T2V",
      "verdict": "I2V with a really good image model input is probably better than any T2V could ever be",
      "from": "Draken"
    },
    {
      "comparison": "Wan 2.2 motion vs 2.1",
      "verdict": "2.2 feels like 30fps downsampled to 16fps instead of raw 16fps, much more realistic motion",
      "from": "Draken"
    },
    {
      "comparison": "With vs without LightX2V LoRA",
      "verdict": "Without LightX gives vastly more realistic motion, with LightX makes output more like 2.1",
      "from": "Draken"
    },
    {
      "comparison": "Beta vs simple scheduler at shift 12",
      "verdict": "Beta scheduler produces better results due to more gradual dropoff",
      "from": "Ablejones"
    },
    {
      "comparison": "Wan 2.2 5B vs 14B High Noise motion quality",
      "verdict": "5B has good image quality but motion doesn't come close to high noise capabilities",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 VACE requirements",
      "verdict": "2.2 needs better masks and more accurate prompts than 2.1. Used to work with controlnet + mask + short prompt, now needs more accurate prompting",
      "from": "JalenBrunson"
    },
    {
      "comparison": "5B vs 14B popularity and LoRA availability",
      "verdict": "14B more popular having part of old LoRA etc working for it. No LoRAs seen for 5B since release",
      "from": "QANICS\ud83d\udd50"
    },
    {
      "comparison": "New vs Old LightX LoRAs",
      "verdict": "Old LightX looks better for some use cases, new is faster because it works on both passes",
      "from": "VRGameDevGirl84"
    },
    {
      "comparison": "Official repo LoRA vs Kijai fixed LoRA",
      "verdict": "Kijai's version works better at proper strength, gets hands right compared to original",
      "from": "RRR"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 dynamic range",
      "verdict": "2.2 Lightning has significantly reduced dynamic range compared to 2.1",
      "from": "wange1002"
    },
    {
      "comparison": "New Lightning 2.2 vs Old LightX2V 2.1 LoRAs",
      "verdict": "Old 2.1 LoRA has better prompt adherence and motion on high noise pass",
      "from": "Doctor Shotgun"
    },
    {
      "comparison": "14B Lightning vs 5B model quality",
      "verdict": "Both produce similar results, 5B model appears to handle prompts differently",
      "from": "TK_999"
    },
    {
      "comparison": "Lightning 4 steps vs LightX2V 3.0/1.0 strength",
      "verdict": "LightX2V still produces better overall quality",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 vs FusionX",
      "verdict": "Results comparable for similar prompts",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "Lightning LoRA vs original LightX2V",
      "verdict": "Original LightX2V v1 performs better than new Lightning LoRA, especially for complex camera movements",
      "from": "Luis Clement"
    },
    {
      "comparison": "CFG 1 vs CFG 5 with Lightning LoRA",
      "verdict": "CFG=1 is 30% faster but details lacking compared to CFG=5",
      "from": "The Shadow (NYC)"
    },
    {
      "comparison": "2 steps CFG 1 vs 10 steps CFG 3.5",
      "verdict": "2 steps on CFG 1 definitely better than 10 steps on CFG 3.5",
      "from": "gokuvonlange"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 motion quality",
      "verdict": "Wan 2.2 has more realistic movement, timing of each move is less visible, better micro motions/subsecond motion",
      "from": "loopen44"
    },
    {
      "comparison": "New lightning lora vs old lightx2v lora",
      "verdict": "Old one is better for I2V, new one works but degrades motion and quality",
      "from": "N0NSens"
    },
    {
      "comparison": "Wan 2.2 vs Wan 2.1",
      "verdict": "2.2 has more knowledge, increased training dataset, does stuff 2.1 doesn't. Better prompt adherence and mostly uncensored",
      "from": "Lodis"
    },
    {
      "comparison": "Wan vs LTXV",
      "verdict": "LTXV is fast but generations are terrible compared to Wan",
      "from": "Lodis"
    },
    {
      "comparison": "5B model visual look",
      "verdict": "Reminds of HY mixed with Wan, has that cellophane look like LTX",
      "from": "Draken"
    },
    {
      "comparison": "5B vs 14B models",
      "verdict": "5B is fast but doesn't have the knowledge that 14b has, reminds me of 2.1. Anything other than a human gets turned into a human",
      "from": "Rainsmellsnice"
    },
    {
      "comparison": "LightX2V vs Lightning loras",
      "verdict": "LightX2V gives better, more predictable cinematic results despite taking more steps",
      "from": "screwfunk"
    },
    {
      "comparison": "FastWan vs regular models",
      "verdict": "FastWan is faster and much worse quality, not worth it",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 Plus (website) vs open models",
      "verdict": "Wan 2.2 Plus on their site is infinitely superior and does 1080p without grid stepping artifacts",
      "from": "aikitoria"
    },
    {
      "comparison": "Wan 2.2 FLF vs other FLF models",
      "verdict": "Wan 2.2 FLF is superior - better than Kling and other closed source models for FLF morphing",
      "from": "thaakeno"
    },
    {
      "comparison": "Lightning LoRA vs lightx2v at 3.0 strength",
      "verdict": "Lightning LoRA not much different from using lightx2v at 3.0 strength",
      "from": "Kijai"
    },
    {
      "comparison": "T2V vs I2V motion quality",
      "verdict": "T2V can have more motion than I2V, T2V works fine with 2.2 but I2V needs specific LoRA stacking",
      "from": "Ada"
    },
    {
      "comparison": "Q8 vs fp8 quantization",
      "verdict": "Q8 is better quality but slower than fp8",
      "from": "Kijai"
    },
    {
      "comparison": "Lightning 2.2 vs original Lightning",
      "verdict": "Lightning 2.2 is not nearly as good as the previous distill LoRA",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 5B vs 1.3B quality",
      "verdict": "5B has nearly 4x parameters but is worse in many ways compared to 1.3B, possibly due to VAE overwhelming the model",
      "from": "Draken"
    },
    {
      "comparison": "WAN VAE vs Qwen-Image VAE",
      "verdict": "Virtually identical - 99.98% compatibility with negligible visual differences",
      "from": "fredbliss"
    },
    {
      "comparison": "WAN 2.2 I2V vs WAN 2.1 I2V at 1080p",
      "verdict": "2.2 works much better, 2.1 died into noise at borders",
      "from": "aikitoria"
    },
    {
      "comparison": "bf16 vs GGUF Q8 and fp8_scaled quality",
      "verdict": "bf16 is overkill unless going for absolute maximum quality, Q8 and fp8_scaled are good enough",
      "from": "Kijai"
    },
    {
      "comparison": "Old LightX2V LoRAs vs new lightning LoRAs",
      "verdict": "Old LightX2V LoRAs work better on NH with CFG3 for 3-5 steps without being too slow",
      "from": "IceAero"
    },
    {
      "comparison": "Qwen image vs Wan 2.2 T2I",
      "verdict": "Qwen image has superior prompt following and text rendering",
      "from": "aikitoria"
    },
    {
      "comparison": "Old vs new LightX LoRAs",
      "verdict": "Results vary by seed - sometimes old performs better, sometimes new. New LoRAs can mess up outputs for some users",
      "from": "piscesbody"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 First-Last-Frame",
      "verdict": "Wan 2.2 FLF shows improvement over 2.1 wrapper implementation",
      "from": "VRGameDevGirl84"
    },
    {
      "comparison": "Lightning vs LightX2V LoRAs",
      "verdict": "Many users prefer old LightX2V for both high and low, new Lightning can make results worse",
      "from": "screwfunk"
    },
    {
      "comparison": "WAN 2.2 vs Kling Master hands",
      "verdict": "WAN 2.2 does much better job with hands even at lower resolution",
      "from": "Persoon"
    },
    {
      "comparison": "5B vs 2.1 VAE memory usage",
      "verdict": "5B VAE is 4x heavier (1.4GB vs 480mb) with better quality",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Kijai's prompt splitting vs EchoShot",
      "verdict": "Kijai's method works better and more reliably, EchoShot often fails to trigger",
      "from": "Kijai"
    },
    {
      "comparison": "2.2 speed LoRAs vs old LightX2V",
      "verdict": "New 2.2 speed LoRAs are not better or even as good as the old LightX2V",
      "from": "Kijai"
    },
    {
      "comparison": "Shift 8 vs Shift 1 with different schedulers",
      "verdict": "With DMP++_SDE scheduler, results look almost identical despite opposite curves. With UniPC, shift 1 and 8 also look nearly identical",
      "from": "Nekodificador"
    },
    {
      "comparison": "20 frame vs 60 frame VACE generations",
      "verdict": "Wan2.1 VACE degrades heavily in quality for style transfer when going from 20 to 60 frames",
      "from": "Poppi"
    },
    {
      "comparison": "2.2 vs 2.1 quality",
      "verdict": "2.2 14B is better at basically everything except VACE according to tests",
      "from": "Josiah"
    },
    {
      "comparison": "5B vs 14B models",
      "verdict": "5B requires more effort to get good results, 14B produces better quality with minimal effort",
      "from": "DawnII"
    },
    {
      "comparison": "Lightning LoRA vs full steps",
      "verdict": "Can work without Lightning LoRA but need CFG 5+ and 20+ steps, making Lightning faster overall",
      "from": "N0NSens"
    },
    {
      "comparison": "GIMM vs RIFE for frame interpolation",
      "verdict": "GIMM slightly better than RIFE but slower. Both don't match Topaz quality",
      "from": "Kijai"
    },
    {
      "comparison": "FastWan vs LightXv2 for I2V",
      "verdict": "FastWan not better or faster than LightXv2 for I2V, makes results overly high contrast",
      "from": "mdkb"
    },
    {
      "comparison": "FP8 vs GGUF memory usage",
      "verdict": "GGUF saves more RAM (stays 32-40G vs fp8_scaled hitting 63-64G), but FP8 faster if you have memory",
      "from": ": Not Really Human :."
    },
    {
      "comparison": "e5m2 vs e4m3 FP8 formats",
      "verdict": "e4m3 is 0.5% better for Wan, but only 40xx+ cards support compiling e4m3. e5m2 needed for 30xx cards",
      "from": "phazei"
    },
    {
      "comparison": "LightX2V 2.1 vs 2.2 Lightning",
      "verdict": "2.1 version consistently outperforms 2.2 Lightning LoRA",
      "from": "shuzhi"
    },
    {
      "comparison": "fp16 vs bf16 vs fp32 VAE decode",
      "verdict": "Difference between fp16/bf16 and fp32 is tiny and not visible to eye, fp32 not worth the VRAM cost",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 with/without LoRAs for I2V",
      "verdict": "Massive difference - no LoRA produces poor results, combining lightx2v and 2.2 lightning gives much better output",
      "from": "FL13"
    },
    {
      "comparison": "New Lightning LoRA vs old LightX2V",
      "verdict": "Old LightX2V can do darker scenes, new Lightning forces brightness but has better prompt adherence",
      "from": "IceAero"
    },
    {
      "comparison": "T2V 1.1 vs T2V 1.0 LoRA",
      "verdict": "1.1 is stronger and follows prompts better but sometimes fails on complex tasks",
      "from": "Kijai"
    },
    {
      "comparison": "6 steps vs 4 steps with new Lightning",
      "verdict": "6 steps feels better, 4 steps needs custom sigmas or scheduler that doesn't drop off",
      "from": "Kijai"
    },
    {
      "comparison": "WAN 2.2 Lightning vs LightX2V 2.1",
      "verdict": "Some users prefer sticking with 2.1, results vary by use case",
      "from": "NebSH"
    },
    {
      "comparison": "Lightning 1.1 vs old Lightning LoRAs",
      "verdict": "Old LightX2V preferred - new ones only slightly better for movement but destroy lighting capabilities",
      "from": "Critorio"
    },
    {
      "comparison": "Lightning 2.2 vs LightX2V 2.1",
      "verdict": "Older version has more motion, lightning kills motion even more",
      "from": "Daflon"
    },
    {
      "comparison": "fp16 vs fp8 models",
      "verdict": "Difference is noticeable but not huge, fp16 requires more offloading but better quality",
      "from": "Kijai"
    },
    {
      "comparison": "bf16 vs fp16",
      "verdict": "bf16 can have quite significant difference, closer to fp32 than fp16",
      "from": "Kijai"
    },
    {
      "comparison": "Lightning 2.2 vs LightX2V",
      "verdict": "LightX2V generally produces better quality results, Lightning appears more 'cursed' than Wan 2.1 lightx2v",
      "from": "MysteryShack"
    },
    {
      "comparison": "Lightning I2V vs original 2.1 LoRA",
      "verdict": "Original 2.1 LoRA produces better motion and prompt adherence for some users",
      "from": "WorldX"
    },
    {
      "comparison": "LoraLoaderModelOnly vs WanVideo Lora Select Multi",
      "verdict": "Both work well with minimal differences",
      "from": "The Dude"
    },
    {
      "comparison": "Different Lightning versions",
      "verdict": "Lightning 1.1 seems worse than the old 2.1 x2v version",
      "from": "MysteryShack"
    },
    {
      "comparison": "Native ComfyUI vs WanVideo Wrapper performance",
      "verdict": "Native consistently 2 times faster than wrapper for same workflow",
      "from": "pagan"
    },
    {
      "comparison": "fp8_fast vs fp16 performance",
      "verdict": "fp8_fast maybe 30% faster than fp16 and only ~10% faster than fp16_fast, but with quality hit",
      "from": "Kijai"
    },
    {
      "comparison": "Lightning LoRAs on 2.2 vs 2.1",
      "verdict": "Loading lightning feels like loading a 2.1 wan lora - more strength makes it look more like 2.1",
      "from": "Draken"
    },
    {
      "comparison": "Native vs Wrapper speed",
      "verdict": "Native is 2x faster (5s vs 10s on 5090)",
      "from": "pagan"
    },
    {
      "comparison": "WAN 2.2 distilled vs normal steps",
      "verdict": "4 steps CFG 1 distilled looks 200x better than 20 steps CFG 3.5",
      "from": "Draken"
    },
    {
      "comparison": "FP8 vs FP16 quality",
      "verdict": "FP8 scaled changes output quality, was very bad in 2.1, seems better in 2.2 but still affects results",
      "from": "Kijai"
    },
    {
      "comparison": "I2V lightning vs T2V lightning",
      "verdict": "I2V lightning gives really similar results to T2V lightning at boosted amounts, could be T2V lightning boosted by x2",
      "from": "Draken"
    },
    {
      "comparison": "Fun Control vs VACE control",
      "verdict": "Fun has always been more lenient but not quite as good as UniAnimate",
      "from": "DawnII"
    },
    {
      "comparison": "2.2 Fun vs base model quality",
      "verdict": "Quality is probably not as good as base model, same was true for 2.1 fun models",
      "from": "DawnII"
    },
    {
      "comparison": "bf16 vs fp16_fast",
      "verdict": "bf16 not faster than fp16_fast, but they only released bf16 weights. With fp8 scaled it doesn't matter",
      "from": "Kijai"
    },
    {
      "comparison": "Wan2.2 lightning LoRAs vs LightX LoRAs",
      "verdict": "Kijai's lightning LoRAs not well trained and have style bias, recommends sticking with LightX or using a mix",
      "from": "Kijai"
    },
    {
      "comparison": "Fun models vs original models",
      "verdict": "Fun models consistently worse quality than originals across all releases - lack compute and dataset to match original",
      "from": "aikitoria"
    },
    {
      "comparison": "Veo3 vs other video generators",
      "verdict": "Veo3 with audio still king but much more expensive. Seedance rates best in user voting. Performance varies by use case",
      "from": "Draken"
    },
    {
      "comparison": "Sapiens vs OpenPose/DWPose",
      "verdict": "Sapiens is better but more complex, much better results for pose detection",
      "from": "fredbliss"
    },
    {
      "comparison": "WAN 2.2 PRO vs Open Source",
      "verdict": "Local/OS version preferred - PRO looks like sepia and lens flare slapped in post process",
      "from": "DawnII"
    },
    {
      "comparison": "Fun 2.2 high + base low+unianimate vs other combinations",
      "verdict": "Mixing fun high and base low+unianimate gives higher fidelity",
      "from": "DawnII"
    },
    {
      "comparison": "GGUF vs fp8 scaled Wan 2.2",
      "verdict": "GGUF produces cleaner results, especially with LoRAs due to different application methods",
      "from": "Draken"
    },
    {
      "comparison": "Qwen Lightning vs Wan Lightning LoRAs",
      "verdict": "Qwen Lightning doesn't ruin style like Wan version, looks almost exactly the same as full Qwen image",
      "from": "Kijai"
    },
    {
      "comparison": "PUSA vs VACE for extension",
      "verdict": "PUSA has no color shifting like VACE, but VACE has advantages with control/ref. Killer combination would be using both together",
      "from": "Hashu"
    },
    {
      "comparison": "Wan 2.2 5B vs 14B performance",
      "verdict": "5B: 22 minutes for 1080x1920, significantly faster than 14B's 1 hour for comparable results",
      "from": "QuintForms"
    },
    {
      "comparison": "RIFE vs GIMM interpolation",
      "verdict": "RIFE better for sword action, GIMM warps background more but creative interpolators become noisy when confused",
      "from": "Kijai"
    },
    {
      "comparison": "Lightning LoRA vs LightX2V LoRA",
      "verdict": "New lightning LoRA gives more natural walking, LightX2V causes slipping feet and unwanted behaviors when used with Wan 2.2",
      "from": "Alpha-Neo"
    },
    {
      "comparison": "SeedVR2 GGUF vs regular versions",
      "verdict": "GGUF versions produce blurry and weird results, regular versions work better",
      "from": "Gill Bastar"
    },
    {
      "comparison": "Topaz vs ComfyUI upscaling",
      "verdict": "ComfyUI with 1.5x upscaler + RIFE interpolation claimed to be better than Topaz for speed, quality comparison inconclusive",
      "from": "Alpha-Neo"
    },
    {
      "comparison": "Wan 2.2 vs Grok Imagine",
      "verdict": "Wan 2.2 is miles better than Grok Imagine, which is described as 'a joke' and '100x better'",
      "from": "thaakeno"
    },
    {
      "comparison": "Fun low noise vs base Wan 2.2 low noise",
      "verdict": "Base Wan 2.2 has much better detail on low noise, Fun low doesn't finish properly",
      "from": "DawnII"
    },
    {
      "comparison": "DPM vs Euler sampler speed",
      "verdict": "DPM is twice as slow as Euler per step",
      "from": "Karo"
    },
    {
      "comparison": "SeedVR2 preference",
      "verdict": "Still prefer SeedVR2 the most despite infinite VRAM requirements",
      "from": "Karo"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 Lightning LoRA quality",
      "verdict": "2.1 lightx2v is amazing, 2.2 Lightning has something very wrong - too much style bias",
      "from": "Kijai"
    },
    {
      "comparison": "5B vs 14B model capabilities",
      "verdict": "5B can do 1080p but 14B can't, 5B gives solid video in 40 seconds on 3090",
      "from": "QuintForms"
    },
    {
      "comparison": "Skyreels vs base Wan",
      "verdict": "Skyreels offers better human motion and portraits, 24fps vs 16fps",
      "from": "DawnII"
    },
    {
      "comparison": "FastWan vs LightX2V for speed",
      "verdict": "FastWan may be better than lightx2v for some cases",
      "from": "Kiwv"
    },
    {
      "comparison": "cfg 1.0 vs higher cfg",
      "verdict": "cfg other than 1.0 is slower because model runs twice per step",
      "from": "Kijai"
    },
    {
      "comparison": "Block swap speed impact varies by system",
      "verdict": "Minimal impact on high-end systems (~0.1s per block) but significant impact on lower-end systems",
      "from": "Kijai"
    },
    {
      "comparison": "2.1 + 2.2 merges vs pure 2.2",
      "verdict": "These merges are more 2.1 than 2.2, wouldn't even call them 2.2",
      "from": "Kijai"
    },
    {
      "comparison": "GGUF vs fp8 on 24GB VRAM",
      "verdict": "No real reason to use GGUF other than Q8 on 24GB VRAM/64GB RAM, fp8_scaled preferred for speed",
      "from": "Kijai"
    },
    {
      "comparison": "FP8 vs GGUF inference speed",
      "verdict": "FP8 has faster inference than GGUF",
      "from": "Juampab12"
    },
    {
      "comparison": "480p vs 720p I2V models for coherence",
      "verdict": "480p i2vs (including Sky Reels) were better with coherence than 720p models",
      "from": "JohnDopamine"
    },
    {
      "comparison": "GGUF Q6 vs FP8",
      "verdict": "FP8 would be faster than Q6 and better quality even with slight offloading",
      "from": "Kijai"
    },
    {
      "comparison": "GGUF vs standard models",
      "verdict": "GGUF about 20% slower due to dequant operation, but allows larger models to fit in memory",
      "from": "Kijai"
    },
    {
      "comparison": "T2V vs I2V quality with Lightning LoRA",
      "verdict": "The difference of quality between T2V and I2V both having light lora is miles apart",
      "from": "Hevi"
    },
    {
      "comparison": "Fun Control vs VACE",
      "verdict": "Fun control superior for getting correct control but inferior in quality. Underrated once VACE came out",
      "from": "Hashu"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 for V2V",
      "verdict": "Still use Wan 2.1 V2V workflow because 2.2 doesn't compare as of yet",
      "from": "Josiah"
    },
    {
      "comparison": "GGUF vs native ComfyUI for 4070ti super",
      "verdict": "Native ComfyUI workflow faster than GGUF, but needs 64GB RAM",
      "from": "pewpewpew"
    },
    {
      "comparison": "Stand-In vs Phantom",
      "verdict": "Similar to Phantom but just 300MB LoRA instead of full model",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 for emotions",
      "verdict": "Wan 2.1 emotions were always uncanny and something was off, 2.2 provides better facial expressions",
      "from": "gokuvonlange"
    },
    {
      "comparison": "Native vs Wrapper for Wan 2.2",
      "verdict": "Wrapper runs much better with low VRAM/RAM, saves 3GB VRAM and several GB RAM, faster generations",
      "from": "Jonathan"
    },
    {
      "comparison": "Lightning vs LightX2V LoRAs",
      "verdict": "Lightning pushes things into stylized results more often than LightX2V",
      "from": "garbus"
    },
    {
      "comparison": "Q8 vs fp16 quality",
      "verdict": "Q8 is very close to fp16 with much better efficiency, differences are minimal",
      "from": "Kijai"
    },
    {
      "comparison": "Multitalk vs FantasyTalk",
      "verdict": "Multitalk is better, FantasyTalk came out before multitalk and was quickly forgotten",
      "from": "DawnII"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 for stable subjects",
      "verdict": "If subject stays in frame without dynamic movement, 2.2 doesn't offer much over 2.1",
      "from": "Kijai"
    },
    {
      "comparison": "fp8 scaled vs Q8",
      "verdict": "fp8 scaled is preferred because it's very close quality and much faster",
      "from": "Kijai"
    },
    {
      "comparison": "LightX2V vs CausVid v2",
      "verdict": "LightX2V is superior to CausVid v2 in most cases, CausVid can be forgotten about except for edge cases",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 HN vs LN for motion",
      "verdict": "High Noise model better for motion, Low Noise better for controls and detail refinement",
      "from": "Ablejones"
    },
    {
      "comparison": "I2V vs T2V for face likeness",
      "verdict": "Much easier time keeping likeness with i2v in 2.2 than 2.1, but distills will skew results",
      "from": "DawnII"
    },
    {
      "comparison": "StandIn vs VACE reference for likeness",
      "verdict": "VACE reference works better than StandIn for identity preservation",
      "from": "hablaba"
    },
    {
      "comparison": "Multitalk with Wan 2.1 t2v VACE vs i2v with Phantom for speed",
      "verdict": "Wan 2.1 t2v with VACE much faster - 10 mins vs 40 mins for similar quality",
      "from": "mdkb"
    },
    {
      "comparison": "Lightning vs LightX2V LoRAs",
      "verdict": "Lightning is for Wan 2.2, LightX2V is for Wan 2.1. Lightning is faster but may have lower quality",
      "from": "Kagi"
    },
    {
      "comparison": "New Wan22-Lightning LoRAs vs LightX2V",
      "verdict": "New Lightning LoRAs are fast but not as good quality as LightX2V",
      "from": "xwsswww"
    },
    {
      "comparison": "Fun 2.2 vs VACE 2.1 for v2v",
      "verdict": "Fun 2.2 works better than VACE 2.1, first v2v that properly places objects like skateboard trucks",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "MAGREF vs Phantom",
      "verdict": "MAGREF is better for character subject consistency, works as reference-to-video instead of start-frame-to-video",
      "from": "gokuvonlange"
    },
    {
      "comparison": "fp8 vs GGUF Q8",
      "verdict": "Q8 has better quality but fp8 with fast mode is 30-40% faster on 4090/5090, and allows LoRA merging",
      "from": "Kijai"
    },
    {
      "comparison": "720p vs 1080p generation time",
      "verdict": "720p takes 85s, 1080p takes 15 minutes - 4x resolution but 10x longer time due to RAM spillover",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "Stand-in MultiTalk vs regular MultiTalk",
      "verdict": "Not quite as good lipsync quality as regular MultiTalk",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 character rotation",
      "verdict": "2.1 causes body parts to mix during rotation like nightmare, 2.2 fixes this with normal rotation",
      "from": "Lodis"
    },
    {
      "comparison": "Wan 2.2 Lightning LoRAs vs LightXV 2.1 LoRAs",
      "verdict": "2.2 Lightning faster with 4 steps but destroys motion quality, 2.1 LightXV better motion",
      "from": "Lodis"
    },
    {
      "comparison": "Stand-in vs Phantom for subject consistency",
      "verdict": "Phantom is better overall but Stand-in has benefit of being LoRA for other workflows",
      "from": "Juampab12"
    },
    {
      "comparison": "Phantom vs MagRef capabilities",
      "verdict": "Phantom can do pixel perfect character replacement including clothing, MagRef better for I2V likeness",
      "from": "Piblarg"
    },
    {
      "comparison": "MultiTalk compatibility",
      "verdict": "Works well with Wan 2.1, not as well with 2.2, would need model release from devs for 2.2",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.1 14B vs 2.2 speed",
      "verdict": "14B 2.1 is much faster than 2.2",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "VACE 2.1 vs 2.2",
      "verdict": "VACE 2.1 is better even than 2.2",
      "from": "Kijai"
    },
    {
      "comparison": "Fun 2.1 vs Fun 2.2",
      "verdict": "Fun 2.2 seems lot better than 2.1 Fun",
      "from": "Kijai"
    },
    {
      "comparison": "Reference image vs start image in trajectory control",
      "verdict": "Start image works, reference image doesn't work for trajectory control",
      "from": "Kijai"
    },
    {
      "comparison": "Different LightX2V ranks",
      "verdict": "Rank 128 gives better prompt adherence than rank 256, rank 16 worked best for specific scenes",
      "from": "xwsswww"
    },
    {
      "comparison": "Q6 vs Q8 gguf quantization",
      "verdict": "Q8 seems to prompt faster than Q6",
      "from": "xwsswww"
    },
    {
      "comparison": "Using distill model directly vs LoRAs",
      "verdict": "Running original distill model directly is stronger than using extracted LoRAs",
      "from": "Kijai"
    },
    {
      "comparison": "Lightning LoRA vs LightX2V for Wan 2.2",
      "verdict": "The team says Lightning is worse than using the old LightX2V",
      "from": "Juan Gea"
    },
    {
      "comparison": "FP8 vs BF16 text encoder",
      "verdict": "BF16 is better, not much reason to not use it since text encoder is unloaded anyway",
      "from": "Kijai"
    },
    {
      "comparison": "Compiled vs uncompiled memory usage",
      "verdict": "Compiled evens out VRAM peaks significantly, uncompiled shows mountain-like RoPE peaks",
      "from": "Kijai"
    },
    {
      "comparison": "Wrapper vs Native block swapping",
      "verdict": "Native ComfyUI has super granular automatic offloading that's superior to manual block swap",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 vs Kling speed",
      "verdict": "Wan 2.2: 10s video in 196s, faster than Kling",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "Wan 2.2 Fun-InP vs base model",
      "verdict": "Fun-InP has smooth transitions but loses creativity, feels more like interpolation. Base model better except for transition smoothness",
      "from": "dir2050"
    },
    {
      "comparison": "Phantom vs MagRef for likeness",
      "verdict": "Sometimes MagRef nails it better (five guys holding beers), but recent tests show Phantom better for likeness",
      "from": "mdkb"
    },
    {
      "comparison": "Lightning lora vs lightx2v for NSFW content",
      "verdict": "Lightning lora censors and generates blocking objects, lightx2v maintains capability",
      "from": "MysteryShack"
    },
    {
      "comparison": "Lightning lora vs base generation quality",
      "verdict": "Lightning ruins generations with more static scenes and altered coloring",
      "from": "crinklypaper"
    },
    {
      "comparison": "Context options Wan 2.2 vs AnimateDiff",
      "verdict": "AnimateDiff was much smoother for context transitions, Wan 2.2 restarts from first frame",
      "from": "xwsswww"
    },
    {
      "comparison": "WanFM vs normal Wan 2.2 FFLF",
      "verdict": "WanFM won in one test case but benefit unclear since 2.2 already does similar processing internally",
      "from": "Kijai"
    },
    {
      "comparison": "fp16 on-the-fly quantization vs pre-quantized models",
      "verdict": "fp16 with on-the-fly quant has fewer issues overall, downside is larger storage space",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Latent extraction vs decode/encode for video extension",
      "solution": "Latent method should be better quality but doesn't work due to first frame encoding differences",
      "from": "Kijai"
    },
    {
      "comparison": "WanWrapper vs Native workflows on RTX 3060",
      "verdict": "WanWrapper ksampler runs much slower compared to native workflow",
      "from": "Abx"
    },
    {
      "comparison": "MultiTalk vs expected sonic-level quality",
      "verdict": "MultiTalk still kinda sucks compared to expected quality levels",
      "from": "MysteryShack"
    },
    {
      "comparison": "Phantom vs MagRef for character consistency",
      "verdict": "Phantom is t2v ref image consistent and pretty good, MagRef is i2v equivalent but never quite tops Phantom",
      "from": "mdkb"
    },
    {
      "comparison": "FantasyPortrait vs MultiTalk for lip-sync",
      "verdict": "FantasyPortrait not great at lipsync but good at transferring head pose and emotion, MultiTalk better for lipsync",
      "from": "Kijai"
    },
    {
      "comparison": "VACE vs Redux for inpainting",
      "verdict": "VACE works better than Redux",
      "from": "Nekodificador"
    },
    {
      "comparison": "WAN 2.2 vs 2.1 speed perception",
      "verdict": "2.2 outputs often seem sped up compared to 2.1 at same fps",
      "from": "Fawks"
    },
    {
      "comparison": "T2V + Phantom vs I2V for dynamics",
      "verdict": "Much more dynamic with Phantom vs I2V, but lipsync not as good",
      "from": "Kijai"
    },
    {
      "comparison": "Wrapper vs Native for VACE single image",
      "verdict": "Only works with wrapper, native has problems",
      "from": "Nekodificador"
    },
    {
      "comparison": "GGUF vs FP8 scaled versions",
      "verdict": "FP8 scaled is faster, GGUF is smaller for VRAM but slower due to decompression overhead",
      "from": ". Not Really Human ."
    },
    {
      "comparison": "RTX 5090 vs RTX 6000 Pro",
      "verdict": "Pro 6000 much faster when models fit in VRAM (300% speed boost), 5090 only 20% slower when using block swap properly",
      "from": "WorldX"
    },
    {
      "comparison": "FP32 vs FP16 vs BF16 text encoders",
      "verdict": "Mixed weights FP32&FP16 best for intricate details, BF16 close behind",
      "from": "Benjimon"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 for Fantasy Portrait",
      "verdict": "User prefers 2.1 over 2.2 for Fantasy Portrait",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "Mixed weights model vs other variants",
      "verdict": "Best results achieved with mixed weights model from maybleMyers",
      "from": "Benjimon"
    },
    {
      "comparison": "fp8 e5m2 vs fp8 e4m3fn for 30xx cards",
      "verdict": "fp8 e5m2 works with 30xx cards, fp8 e4m3fn doesn't work with 30xx cards, need 40xx or above",
      "from": "mdkb"
    },
    {
      "comparison": "MAGREF vs standard I2V models",
      "verdict": "MAGREF is reference model not first frame model, works much better with context windows",
      "from": "Kijai"
    },
    {
      "comparison": "GGUF Q8 vs FP8 speed",
      "verdict": "FP8 faster even without fp8 matmul, but GGUF sometimes appears faster in wrapper due to memory issues",
      "from": "Kijai"
    },
    {
      "comparison": "GGUF Q8 vs FP8 quality",
      "verdict": "Q8 way better quality than plain fp8, slightly better than fp8 scaled",
      "from": "Kijai"
    },
    {
      "comparison": "GGUF Q8 vs FP8 VRAM usage",
      "verdict": "Q8 uses more VRAM than fp8 - T2V ~14GB fp8 vs 15.5GB Q8",
      "from": "Kijai"
    },
    {
      "comparison": "LightX 1.1 vs other versions for I2V",
      "verdict": "Better hands with lighting 1.1 on specific LoRA",
      "from": "Kenk"
    },
    {
      "comparison": "UniAnimate vs MTV Crafter weights",
      "verdict": "UniAnimate is just better, especially when used with MAGREF",
      "from": "Kijai"
    },
    {
      "comparison": "FantasyPortrait + Multitalk vs other lipsync",
      "verdict": "Best lipsync combination currently, brings quality from '1920s silent movie period to 1960s bad acting period'",
      "from": "mdkb"
    },
    {
      "comparison": "Dev branch vs main branch performance",
      "verdict": "Dev branch showing slower performance for some AMD/ZLUDA users, opposite of expected improvements",
      "from": "patientx"
    },
    {
      "comparison": "Depth control vs straight input video",
      "verdict": "Straight input video as control can work just as well as depth preprocessing",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "MultiTalk vs InfiniteTalk",
      "verdict": "InfiniteTalk allows much longer videos (1000 vs 250 frames) and better results",
      "from": "NC17z"
    },
    {
      "comparison": "Chinese AI models vs American companies",
      "verdict": "China is winning with free open source video models while American companies lag behind",
      "from": "Lodis"
    },
    {
      "comparison": "Wan 2.1 vs AnimateDiff",
      "verdict": "Some users get very bad results with Wan 2.1 and prefer going back to AnimateDiff",
      "from": "xwsswww"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk degradation",
      "verdict": "InfiniteTalk doesn't degrade over long generations, MultiTalk becomes mushy mess after ~15 seconds",
      "from": "Kijai"
    },
    {
      "comparison": "Qwen Image Edit vs Kontext",
      "verdict": "Initial tests show Qwen makes things plastic looking, needs experimentation",
      "from": "Lodis"
    },
    {
      "comparison": "F5-TTS language support",
      "verdict": "Only handles 2 languages well, French doesn't work, has catastrophic forgetting issue",
      "from": "MysteryShack"
    },
    {
      "comparison": "MagRef vs Phantom for identity",
      "verdict": "MagRef works better for identity preservation than Phantom with VACE",
      "from": "mdkb"
    },
    {
      "comparison": "Uni3C vs v2v with denoise",
      "verdict": "Uni3C less controlled than v2v with 0.65 denoise, ~25% success rate",
      "from": "Dream Making"
    },
    {
      "comparison": "DMP SDE vs Res Multistep schedulers",
      "verdict": "DMP SDE shows less noise in previews, Res Multistep creates very noisy previews but good final output",
      "from": "Kijai"
    },
    {
      "comparison": "FP8 vs GGUF Q8 quality",
      "verdict": "FP8 produces significantly worse quality than GGUF Q8, no speed difference on 3090",
      "from": "iShootGood"
    },
    {
      "comparison": "MAGREF with vs without clip embeds",
      "verdict": "Without clip embeds provides stronger reference, with clip embeds more stable",
      "from": "Kijai"
    },
    {
      "comparison": "Q8 vs fp8 on RTX 3090",
      "verdict": "Q8 is better choice for 3090, especially with compile - better quality than e5m2",
      "from": "Kijai"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk",
      "verdict": "MultiTalk has more even transitions when using same context windows and overlap",
      "from": "DawnII"
    },
    {
      "comparison": "e5m2 vs e4m3fn quantization",
      "verdict": "e4m3fn is better quality, e5m2 not worth the quality loss except for 3000 series with torch.compile",
      "from": "Kijai"
    },
    {
      "comparison": "FantasyTalk vs MultiTalk vs InfiniteTalk",
      "verdict": "FantasyTalk was redundant when MultiTalk released, InfiniteTalk is like MultiTalk but better. FantasyPortrait is still relevant as it's not audio driven",
      "from": "Kijai"
    },
    {
      "comparison": "With vs without LightX2V LoRA",
      "verdict": "Without LightX2V it's very slow but much better quality - 1min 30sec clip takes about 5 hours",
      "from": "seitanism"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk for V2V",
      "verdict": "InfiniteTalk seems better, MultiTalk couldn't do the same V2V results",
      "from": "Draken"
    },
    {
      "comparison": "Motion frame 9 vs 25",
      "verdict": "9 works fine and is faster (15:35 vs 20:33 execution time), 9 is default for new model while 25 was default for multitalk",
      "from": "Kijai"
    },
    {
      "comparison": "ComfyUI vs full 9GB InfiniteTalk",
      "verdict": "Using ComfyUI version changes face drastically compared to full version",
      "from": "MysteryShack"
    },
    {
      "comparison": "Single vs Multi talk models",
      "verdict": "Single changes faces, Multi doesn't change faces as much",
      "from": "MysteryShack"
    },
    {
      "comparison": "With vs without LightX2V LoRA",
      "verdict": "With LightX2V LoRA you get very static faces, without LoRA and more steps more movement happens",
      "from": "seitanism"
    },
    {
      "comparison": "High CFG vs Low CFG on Wan 2.2",
      "verdict": "Lower CFG produces less dynamic but cleaner results, high CFG causes wonkiness",
      "from": "NC17z"
    },
    {
      "comparison": "Original vs upscaled video quality",
      "verdict": "Upscaling tidies up artifacts but removes expression and emotion, making it look bland and plastic",
      "from": "seitanism"
    },
    {
      "comparison": "Fantasy Portrait vs Fantasy Talk",
      "verdict": "Fantasy Portrait captures more emotion than Fantasy Talk",
      "from": "ManglerFTW"
    },
    {
      "comparison": "Image upscaling vs latent upscaling",
      "verdict": "Image upscaling works better according to Kijai",
      "from": "daking999"
    },
    {
      "comparison": "Qwen Edit vs Kontext",
      "verdict": "Qwen has better prompt adherence but worse image quality. Qwen easily loses details that Kontext preserves",
      "from": "blake37 and Drommer-Kille"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk generation speed",
      "verdict": "InfiniteTalk takes significantly longer to generate than MultiTalk",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "comparison": "FusionX performance with InfiniteTalk",
      "verdict": "FusionX is not faster in any way with InfiniteTalk",
      "from": "NC17z"
    },
    {
      "comparison": "MultiTalk vs InfiniteTalk",
      "verdict": "MultiTalk doesn't do much for animating anything other than head, mouth and some body movements. InfiniteTalk does better job moving camera and other things, but produces more questionable output.",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk for long videos",
      "verdict": "InfiniteTalk is amazing for stability even after 1 minute with no degradation, never worked as well with other methods for 1-3 minute generations. Unclear if this is mainly because of MagRef or InfiniteTalk or both together.",
      "from": "seitanism"
    },
    {
      "comparison": "Light2XV vs new 1.1 speed LoRAs",
      "verdict": "Light2XV from Wan 2.1 performs better on Wan 2.2",
      "from": "NebSH"
    },
    {
      "comparison": "GGUF Q8 vs FP8 scaled",
      "verdict": "Q8 better quality, FP8 has benefit of fp8 matmuls on supporting hardware",
      "from": "Kijai"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk for long videos",
      "verdict": "InfiniteTalk keeps quality better even with super long videos, more stable",
      "from": "Juan Gea"
    },
    {
      "comparison": "Wan 2.1 vs 2.2",
      "verdict": "2.2 is better, faster, allows more control with less effort overall, more prompt work though",
      "from": "Josiah"
    },
    {
      "comparison": "GGUF scaled_fast vs Q8 quant mode",
      "verdict": "scaled_fast is a lot faster than Q8, otherwise really close, GGUF never allows merging LoRAs",
      "from": "Kijai"
    },
    {
      "comparison": "Start step 5/7 vs 2/4",
      "verdict": "Higher steps (5/7) cause stutter repeat mode, 2/4 steps work better, needs high enough sigmas",
      "from": "Kijai"
    },
    {
      "comparison": "VACE alone vs VACE + other methods",
      "verdict": "VACE alone can be very effective for inpainting existing subjects, better than expected",
      "from": "Dream Making"
    },
    {
      "comparison": "Fake VACE 2.2 models vs WAN 2.2 T2V with 2.1 VACE",
      "verdict": "Fake VACE 2.2 models work even worse than using WAN 2.2 T2V models with 2.1 VACE module",
      "from": "seitanism"
    },
    {
      "comparison": "InfiniteTalk extension vs VACE extension",
      "verdict": "VACE extension is smoother than InfiniteTalk, but InfiniteTalk preserves given frames better",
      "from": "DawnII"
    },
    {
      "comparison": "T2V vs I2V for pull-focus and camera pans",
      "verdict": "T2V often better than I2V because proper pull-focus is hard with starting image and pan to right person impossible with I2V",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "Control splines vs OpenPose",
      "verdict": "Control splines provide more information - openpose limited to pose only while splines handle camera movement and other objects",
      "from": "Blink"
    },
    {
      "comparison": "Infinitetalk on different models",
      "verdict": "Can't use infinitetalk on high model effectively, gets good prompt adherence but quality suffers, need to vid2vid with wan2.1",
      "from": "MysteryShack"
    },
    {
      "comparison": "Wan VACE vs Qwen Edit",
      "verdict": "Visual comparison shared showing different results",
      "from": "N0NSens"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 for complex shots",
      "verdict": "2.2 can handle shots that 2.1 typically fails at",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "AnimateDiff vs Wan frame counts",
      "verdict": "AnimateDiff was 16 frames, Wan variants use 81-121 frames causing longer render times",
      "from": "xwsswww"
    },
    {
      "comparison": "LTXV vs Wan quality",
      "verdict": "LTXV with lora was very good, much higher temporal compression than Wan, about 100x faster but Wan still has better overall quality",
      "from": "NebSH"
    },
    {
      "comparison": "Full Turbo model vs adaptive rank lora vs rank 64 lora",
      "verdict": "No huge differences between them, adaptive pretty close to full model, rank 64 is fine too",
      "from": "Kijai"
    },
    {
      "comparison": "GGUF Q8 vs fp8 speed",
      "verdict": "GGUF Q8 is almost as fast as fp8 and better quality, fp8 is way faster on 4000+ series than Q8",
      "from": "Kijai"
    },
    {
      "comparison": "Native VAE vs Wan VAE decode speed",
      "verdict": "Native VAE is slightly faster",
      "from": "Kijai"
    },
    {
      "comparison": "Fun InP vs I2V FLF",
      "verdict": "I2V FLF is better even with no prompt, Fun InP is like a simple transition",
      "from": "hicho"
    },
    {
      "comparison": "Wan VAE vs Qwen VAE for 4K generation",
      "verdict": "Qwen VAE produces more natural results with better small patch alignment",
      "from": "Instability01"
    },
    {
      "comparison": "Context windows T2V vs I2V extension",
      "verdict": "Better to extend I2V from last frame than using context windows - context is slower and lower quality",
      "from": "Kenk"
    },
    {
      "comparison": "20 steps vs LightX + 6 steps quality",
      "verdict": "Quality with 20 steps is still worse than with LightX and 6 steps, adds noise and artifacts",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk speed",
      "verdict": "InfiniteTalk is not slower than MultiTalk, MultiTalk is mostly redundant now",
      "from": "Kijai"
    },
    {
      "comparison": "Lightning LoRA vs LightX2V",
      "verdict": "Lightning LoRA makes everything brighter and more saturated, prefer LightX2V",
      "from": "Kijai"
    },
    {
      "comparison": "Full Turbo model vs normal model + LoRA",
      "verdict": "LoRA is fine, full model is slightly stronger",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 for lip sync",
      "verdict": "2.1 works better for multitalk/infinitetalk, 2.2 recreates mouth well but has color issues",
      "from": "Kenk"
    },
    {
      "comparison": "MelBandRoFormer vs demucs",
      "verdict": "MelBandRoFormer performs better in some cases for vocal separation",
      "from": "Kijai"
    },
    {
      "comparison": "Wrapper vs Native with LoRAs",
      "verdict": "Wrapper+CFG reads prompts better, Native preserves LoRA look better",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "WAN 2.1 I2V vs Runway",
      "verdict": "Getting better results with WAN 2.1 I2V than Runway for music video creation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "WAN 2.2 i2v Low noise vs MagRef with Uni3c",
      "verdict": "WAN 2.2 i2v Low noise model gave better results with Uni3c than MagRef, though not perfect",
      "from": "mdkb"
    },
    {
      "comparison": "Phantom model quality at different resolutions",
      "verdict": "Phantom makes great quality at 832x480, but quality degrades when using MagRef with Uni3c at 960x540",
      "from": "mdkb"
    },
    {
      "comparison": "Context windows vs other extension methods",
      "verdict": "Context is better than any released tech for extending wan, but requires extra compute time",
      "from": "Draken"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk",
      "verdict": "InfiniteTalk model 2x larger and 1.5x slower, but miles better with its own sampling loop",
      "from": "N0NSens"
    },
    {
      "comparison": "Lightning models vs LightX2V",
      "verdict": "Lightning lora is fast but with limited movements and prompt adherence compared to lightx2v for I2V",
      "from": "xwsswww"
    },
    {
      "comparison": "Nunchaku vs Speed LoRAs",
      "verdict": "Nunchaku 2-3x faster while distill lora is 12x faster, but nunchaku takes less VRAM",
      "from": "Kijai"
    },
    {
      "comparison": "Lightning vs LightX2V distillation models",
      "verdict": "Using Lightning on high noise, LightX2V on low noise, both at 0.8 weight",
      "from": "CJ"
    },
    {
      "comparison": "VACE vs Fun models",
      "verdict": "VACE is still better than Fun models, even old VACE GGUF 8 model compared to Fun",
      "from": "Gill Bastar"
    },
    {
      "comparison": "LoRA loader vs Power LoRA stacker",
      "verdict": "No difference in output or loading times, just aesthetic preference. Core nodes preferred for sharing workflows",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 Lightning LoRAs",
      "verdict": "2.2 Lightning LoRAs make quality look like 2.1 and aren't as good as LightX2V LoRA in 2.1",
      "from": "piscesbody"
    },
    {
      "comparison": "MagRef + InfiniteTalk vs other methods",
      "verdict": "Works 100 times better than anything else for keeping likeness and quality with basically no degradation",
      "from": "seitanism"
    },
    {
      "comparison": "Context windows vs InfiniteTalk",
      "verdict": "Context windows don't work well with I2V and are much slower, InfiniteTalk is better",
      "from": "Kijai"
    },
    {
      "comparison": "Dev version vs main version performance",
      "verdict": "Dev version saves up to 30GB RAM with 2.2 workflows but may have slightly slower loading times",
      "from": "Kijai"
    },
    {
      "comparison": "Using both high and low samplers vs only low sampler",
      "verdict": "Using only low sampler looks better, avoids elongated proportions from high sampler",
      "from": "Kenk"
    },
    {
      "comparison": "2.2 Lightning vs FusionX LoRA quality",
      "verdict": "Wan 2.1 FusionX LoRA was much better quality than lightning",
      "from": "BecauseReasons"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk for mimicking",
      "verdict": "InfiniteTalk makes better mimic but has color issues",
      "from": "N0NSens"
    },
    {
      "comparison": "3060 vs higher end cards for AI video",
      "verdict": "3060 is best value card, can do a lot for <$400 and replaceable without concern",
      "from": "mdkb"
    },
    {
      "comparison": "Wan 2.2 vs other models for gender role reversal",
      "verdict": "No model can do woman lifting man properly, all models fine with man lifting woman",
      "from": "seitanism"
    },
    {
      "comparison": "Wan 2.2 S2V vs other models",
      "verdict": "Single model unlike mixture of experts, only has 16 input channels (T2V), demo videos only look as good as wan 2.1 multitalk",
      "from": "MysteryShack"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk for sync",
      "verdict": "2.2 InfiniteTalk sync is 'kinda meh' compared to MultiTalk",
      "from": "N0NSens"
    },
    {
      "comparison": "InfiniteTalk vs Wan S2V",
      "verdict": "InfiniteTalk appears better for lip sync quality",
      "from": "DawnII"
    },
    {
      "comparison": "S2V vs InfiniteTalk",
      "verdict": "InfiniteTalk is definitely better quality, S2V only positive is image doesn't degrade as fast",
      "from": "ArtOfficial"
    },
    {
      "comparison": "S2V vs MultiTalk",
      "verdict": "Comparison shared showing S2V vs MultiTalk results at same small resolution",
      "from": "patientx"
    },
    {
      "comparison": "InfiniteTalk vs S2V",
      "verdict": "InfiniteTalk is superior for lip sync, S2V has weird mouth movement and feels off at 16fps",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "MultiTalk vs InfiniteTalk",
      "verdict": "MultiTalk preferred when InfiniteTalk makes weird noise at head of shot with certain models",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "comparison": "480p GGUF vs aniwan vs aniwan + Pusa",
      "verdict": "Comparison of InfiniteTalk dynamic tests showing different model performance",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "comparison": "MultiTalk/InfiniteTalk vs S2V with Wan 2.2",
      "verdict": "after my latest tests - I dont see any reason to use multi/infinite with 2.2. You got worse motion with worse quality and longer gen. So it doesn't worth it",
      "from": "N0NSens"
    },
    {
      "comparison": "Veo 3 I2V vs other models",
      "verdict": "I tried veo3 I2V, it is crap in terms of keeping resemblence. t2v is awesome, but not i2v",
      "from": "Hevi"
    },
    {
      "comparison": "InfiniteTalk vs S2V quality",
      "verdict": "so far infinitetalk really looks better",
      "from": "seitanism"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 capabilities",
      "verdict": "i switched to 2.2 the first day when i realized it is way more capable than 2.1",
      "from": "Lodis"
    },
    {
      "comparison": "S2V vs InfiniteTalk",
      "verdict": "S2V has better teeth handling, framepack instead of context windows, better lipsync, twice as fast inference, but quality degrades over time and less natural movement",
      "from": "Kijai, piscesbody, Impactframes."
    },
    {
      "comparison": "LightX 2.1 vs Lightning LoRAs",
      "verdict": "LightX 2.1 LoRAs give better results than Lightning LoRAs when used with Wan 2.2",
      "from": "Lodis"
    },
    {
      "comparison": "Wrapper vs Native quality differences",
      "verdict": "Native produces much better T2I results than wrapper even with same settings. For video generation, differences are less pronounced",
      "from": "hablaba"
    },
    {
      "comparison": "Framepack vs Context Windows",
      "verdict": "Framepack allows infinite generation but has quality degradation over time. Context window overlap is not good for lipsync",
      "from": "Kijai"
    },
    {
      "comparison": "HunyuanVideo-Foley vs MMAudio",
      "verdict": "Better than MMAudio but MMAudio is still considered king. Not worse than Eleven Labs",
      "from": "Draken, \u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-, Juampab12"
    },
    {
      "comparison": "S2V vs MultiTalk lip sync quality",
      "verdict": "MultiTalk has smoother motion at 25fps, S2V has stop motion issues at 16fps",
      "from": "hicho"
    },
    {
      "comparison": "Sapiens vs DWPose for pose control",
      "verdict": "Sapiens with filtered bones works better, avoids headphone artifacts",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "HunyuanVideo-Foley vs MMAudio",
      "verdict": "Doesn't seem much better than MMAudio, not worth implementing",
      "from": "DawnII"
    },
    {
      "comparison": "Framepack vs context windows for S2V",
      "verdict": "Framepack has no burning but worse lipsync due to overlap, context windows have burning but better lipsync",
      "from": "Kijai"
    },
    {
      "comparison": "S2V vs Infinite Talk/MultiTalk for lip sync",
      "verdict": "S2V is pretty stiff compared to MAGREF + Multi or InfiniteTalk for lip sync quality",
      "from": "Kijai"
    },
    {
      "comparison": "GGUF vs fp8 quantization quality",
      "verdict": "GGUF offers better quality over fp8, making it worth the complexity to support",
      "from": "Kijai"
    },
    {
      "comparison": "S2V model T2V performance vs other models",
      "verdict": "S2V disappointing for T2V, opposed to motions like walking, video extensions don't follow prompts",
      "from": "flo1331"
    },
    {
      "comparison": "Infinite Talk vs S2V",
      "verdict": "User finds S2V is trash compared to Infinite Talk for their use case",
      "from": "asd"
    },
    {
      "comparison": "HunyuanVideo-Foley vs mmaudio",
      "verdict": "Prompt following is better but not enough to make it usable, mmaudio still preferred",
      "from": "DawnII"
    },
    {
      "comparison": "MAGREF vs other I2V models",
      "verdict": "MAGREF 14B fp8 scaled gives best results on RTX3090 setup with Infinite Talk",
      "from": "NC17z"
    },
    {
      "comparison": "2.2 T2V vs 2.1",
      "verdict": "2.2 T2V is almost identical to 2.1, but I2V is not - 2.2 I2V low noise doesn't stick to start image",
      "from": "Kijai"
    },
    {
      "comparison": "MAGREF vs regular I2V",
      "verdict": "MAGREF works like phantom using image as reference, does better with context windows than normal I2V for longer videos",
      "from": "blake37"
    },
    {
      "comparison": "Context windows vs Framepack",
      "verdict": "Context windows give better quality for long generations but lipsync isn't as good",
      "from": "Kijai"
    },
    {
      "comparison": "FP8 vs GGUF on RTX 3060",
      "verdict": "FP8 usually works better in wrapper, GGUF in native, but depends on workflow",
      "from": "mdkb"
    },
    {
      "comparison": "MultiTalk vs InfiniteTalk",
      "verdict": "InfiniteTalk produces better mouth movements but can make weird noise frames at head of shot",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "comparison": "Context windows vs other extension methods",
      "verdict": "Context windows have zero degradation but higher inference cost",
      "from": "Kijai"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk",
      "verdict": "MultiTalk preferred for jumping less with uni3c controller and no noise on first frames",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "comparison": "Phantom with vs without negative prompts",
      "verdict": "Usually better without any negative prompt or minimal negatives",
      "from": "Ablejones"
    },
    {
      "comparison": "QwenEdit vs nano banana",
      "verdict": "Nano banana is on another level, QwenEdit with LoRAs might get close",
      "from": "Nekodificador"
    },
    {
      "comparison": "MAGREF vs regular I2V",
      "verdict": "MAGREF is slower but works better for longer contexts, though harder to go back from Wan 2.2 quality",
      "from": "Charlie and blake37"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 with InfiniteTalk",
      "verdict": "2.1 works better with infinitetalk but doesn't keep face likeness well in I2V, 2.2 keeps face likeness very well but doesn't follow infinitetalk as well",
      "from": "MysteryShack"
    },
    {
      "comparison": "Wan 2.2 I2V vs Veo3",
      "verdict": "Wan 2.2 I2V beats Veo3 with proper setup",
      "from": "MysteryShack"
    },
    {
      "comparison": "FP8 vs FP16 quality",
      "verdict": "Weight precision plays smaller role than expected, FP8_scaled produces very similar results to FP16",
      "from": "Kijai"
    },
    {
      "comparison": "InfiniteTalk vs MultiTalk with context",
      "verdict": "InfiniteTalk works better with context, MultiTalk has much more movement that struggles with context window",
      "from": "N0NSens"
    },
    {
      "comparison": "CineScale vs UltraWan",
      "verdict": "CineScale is way better than UltraWan for upscaling",
      "from": "DawnII"
    },
    {
      "comparison": "Regular context window vs InfiniteTalk",
      "verdict": "Regular context window works much worse than InfiniteTalk. InfiniteTalk is way faster than using context windows",
      "from": "N0NSens/Kijai"
    },
    {
      "comparison": "InfiniteTalk vs S2V",
      "verdict": "Most people would say InfiniteTalk still. S2V has its benefits but not really direct upgrade",
      "from": "Kijai"
    },
    {
      "comparison": "Native S2V vs KJ's workflow",
      "verdict": "S2V is mehh. Native implementation lets you use native tools",
      "from": "Charlie/Ablejones"
    },
    {
      "comparison": "LightX2V vs Lightning LoRAs",
      "verdict": "BecauseReasons prefers Lightning for cleaner results, Juampab12 prefers LightX for better motion",
      "from": "BecauseReasons, Juampab12"
    },
    {
      "comparison": "InfiniteTalk vs S2V",
      "verdict": "InfiniteTalk is better than S2V for lip-sync quality",
      "from": "piscesbody"
    },
    {
      "comparison": "FLF model vs I2V/VACE FLF",
      "verdict": "FLF model sucks, better to use I2V or VACE FLF",
      "from": "hicho"
    }
  ],
  "tips": [
    {
      "tip": "Use different LightX2V strengths for high/low noise models",
      "context": "Strength 3 on high noise, keep at 1 for low noise",
      "from": "gokuvonlange"
    },
    {
      "tip": "First few steps of high noise model better without LightX",
      "context": "General consensus for better quality",
      "from": "TK_999"
    },
    {
      "tip": "Use Euler sampler for second pass low noise",
      "context": "Better results with different samplers for each pass",
      "from": "GOD_IS_A_LIE"
    },
    {
      "tip": "VACE works as a 2-step refiner",
      "context": "First pass with 2.1 almost done, VACE refines in 2 steps",
      "from": "GOD_IS_A_LIE"
    },
    {
      "tip": "Use quantile adaptive lightxv2 lora for T2V",
      "context": "Works best out of all speed loras for text-to-video generation",
      "from": "gokuvonlange"
    },
    {
      "tip": "Generate image first to preview before video processing",
      "context": "More efficient workflow to check if you like the scene before starting heavy processing",
      "from": "aikitoria"
    },
    {
      "tip": "Use multiple input frames to preserve motion in VACE",
      "context": "When chaining I2V sections, using multiple input frames maintains better motion continuity",
      "from": "seitanism"
    },
    {
      "tip": "Adjust VACE strength between 1-1.5 for background stability",
      "context": "Strength of 1 can cause weird background changes, increase to 1.5 to alleviate but watch for oversaturation",
      "from": "seitanism"
    },
    {
      "tip": "Drop last 9 frames when splicing VACE videos",
      "context": "For smooth transitions when chaining videos, drop the last 9 frames of each clip except the final one",
      "from": "seitanism"
    },
    {
      "tip": "Use Github Desktop for custom node management",
      "context": "Easier to test PRs, roll back versions, and handle updates",
      "from": "Kijai"
    },
    {
      "tip": "Connect context options only to high noise sampler for better results",
      "context": "Uses more VRAM but avoids low noise sampler correcting window changes",
      "from": "Cseti"
    },
    {
      "tip": "Keep high noise model without LoRAs for better prompt adherence",
      "context": "When wanting better prompt following, avoid lightx2v on high model and use higher CFG",
      "from": "gokuvonlange"
    },
    {
      "tip": "Use different overlap values to prevent seam overlap",
      "context": "Can use different overlap so seams don't overlap in context windows",
      "from": "Kijai"
    },
    {
      "tip": "Generate at lower resolution then upscale",
      "context": "Generate at 640x352 then scale to 1280x704 for better performance",
      "from": "nacho.money"
    },
    {
      "tip": "Separate prompts with | for multi-scene context windows",
      "context": "Only works with wrapper encode node for changing prompts between context windows",
      "from": "Kijai"
    },
    {
      "tip": "Use specific camera terminology for better results",
      "context": "'Dolly in/out', 'tracking shot', 'orbiting', 'crane/vertical lift' work better than generic camera terms",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "Use T2V lightx2v Lora for 14B text to video, not I2V",
      "context": "Prevents pixel/stipple artifacts in motion blur",
      "from": "nacho.money"
    },
    {
      "tip": "Use quantile on low pass to speed up generation",
      "context": "When using CFG 3.5 technique",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "Avoid mixing 2.1 loras with 2.2 for camera control",
      "context": "Camera control is problematic, stay away from mixing versions",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "Run T5 on CPU if maxing out VRAM",
      "context": "Helps with VRAM limitations during generation",
      "from": "hicho"
    },
    {
      "tip": "Use sage + fp16 for 20% speed boost",
      "context": "Performance optimization",
      "from": "Jonathan"
    },
    {
      "tip": "Set keep alive=0 and use clean vram node with Ollama",
      "context": "When using OllamaGenerateAdvance node to avoid problems",
      "from": "Simjedi"
    },
    {
      "tip": "Use CFG 3-3.5 on high pass without LoRAs for better quality",
      "context": "When not using speed LoRAs on high noise model",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "LightX LoRA works better on low pass only",
      "context": "Use LightX on low pass, then CFG on high pass for good motion and prompt adherence",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "T2V LoRAs work better even for I2V",
      "context": "General recommendation for LoRA selection",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "Use at least first high noise step with CFG > 1",
      "context": "Most beneficial single improvement for prompt following and motion",
      "from": "Ablejones"
    },
    {
      "tip": "Beta or beta57 schedulers better than bong_tangent",
      "context": "For two-stage model sampling, bong_tangent too symmetric",
      "from": "Ablejones"
    },
    {
      "tip": "Use res_2m sampler for now over res_2s",
      "context": "Despite res_2s being preferred, res_2m recommended due to model swapping discrepancies",
      "from": "Ablejones"
    },
    {
      "tip": "Use different boundary values based on step count",
      "context": "For 40 steps use boundary 15, for 20 steps use boundary 8",
      "from": "aikitoria"
    },
    {
      "tip": "Negative prompt ignored at CFG=1",
      "context": "Use NAG for negative prompts at CFG 1 instead",
      "from": "garbus"
    },
    {
      "tip": "Video2Video denoise strength guidelines",
      "context": "<0.3 to refine details, ~0.6 to edit objects, >0.8 for drastic changes. Don't change low noise denoise, only adjust high noise",
      "from": "Ant"
    },
    {
      "tip": "Use block swap at 20 to manage VRAM during upscaling",
      "context": "When VRAM chokes during high-res processing",
      "from": "thaakeno"
    },
    {
      "tip": "Generate at lower resolution first, then upscale",
      "context": "Model doesn't do good motion at high res, also faster",
      "from": "Kijai"
    },
    {
      "tip": "Use 'camera man shadow' in negative prompt to avoid unwanted shadows",
      "context": "When model generates camera operator shadows",
      "from": "thaakeno"
    },
    {
      "tip": "Interpolate to 25fps for MMaudio compatibility",
      "context": "When using MMaudio with 16fps Wan output",
      "from": "thaakeno"
    },
    {
      "tip": "Try different CFG values if one doesn't work",
      "context": "When having trouble with prompt adherence",
      "from": "Juampab12"
    },
    {
      "tip": "Use higher denoise steps on low noise for more detail",
      "context": "Better than upscaling for adding details",
      "from": "shockgun"
    },
    {
      "tip": "Remove unused clip embeds to save RAM",
      "context": "For 2.2 models, clip embeds are only used in 2.1 I2V",
      "from": "Kijai"
    },
    {
      "tip": "Use more steps and start later for less change in V2V",
      "context": "When doing V2V upscaling to preserve original content",
      "from": "Kijai"
    },
    {
      "tip": "Don't use SDE/LCM samplers for less change",
      "context": "When doing V2V operations where you want to preserve the original",
      "from": "Kijai"
    },
    {
      "tip": "Lower shift should be less change",
      "context": "Parameter adjustment for V2V operations",
      "from": "Kijai"
    },
    {
      "tip": "Do interpolation after upscaling, not before",
      "context": "Interpolation is lighter process and should be the last step",
      "from": "gokuvonlange"
    },
    {
      "tip": "Generate at 768 instead of 720 and crop after",
      "context": "To avoid artifacts when rendering non-standard resolutions",
      "from": "Juan Gea"
    },
    {
      "tip": "Remove lightx2v lora on high noise, use CFG=5 on all timesteps of high noise",
      "context": "For T2I and T2V generations",
      "from": "mamad8"
    },
    {
      "tip": "Use 3+5 steps configuration",
      "context": "Seems to end on proper sigma the best",
      "from": "DawnII"
    },
    {
      "tip": "Use euler + beta for T2V/I2V, res_multistep + beta57 for T2I",
      "context": "Better results than default settings",
      "from": "mamad8"
    },
    {
      "tip": "Use native implementation over wrapper for better results with specific configurations",
      "context": "Never had good results with wrapper using certain configs",
      "from": "mamad8"
    },
    {
      "tip": "Use long detailed prompts with Wan",
      "context": "Wan likes long ass prompts, especially for transitions",
      "from": "Juampab12"
    },
    {
      "tip": "Prompt for specific effects to avoid slideshow results",
      "context": "If you don't prompt for any effect with different images, it makes slideshow",
      "from": "Kijai"
    },
    {
      "tip": "Use commas for powerful prompting control",
      "context": "Can describe sequence of events and specific moments between commas",
      "from": "Cubey"
    },
    {
      "tip": "Pull video back into ComfyUI to recover workflow settings",
      "context": "When you forgot the settings used for a successful generation",
      "from": "screwfunk"
    },
    {
      "tip": "Test with 4 steps first before increasing detail",
      "context": "Run quick test to verify movement is correct before committing to longer generations",
      "from": "shockgun"
    },
    {
      "tip": "Don't try to be a drop-in replacement",
      "context": "Being drop-in replacement would prevent significant model improvements",
      "from": "aikitoria"
    },
    {
      "tip": "Always backup LoRAs you like",
      "context": "Due to platform takedowns and availability issues",
      "from": "Karo"
    },
    {
      "tip": "Use cfg on high noise side for proper motion with lightx2v",
      "context": "High side needs cfg for motion, can use lightx2v carefully or skip it on high noise",
      "from": "Kijai"
    },
    {
      "tip": "Keep video generation to 81 frames or less",
      "context": "Above 81 frames causes significant quality degradation",
      "from": "\u2727\u0e05\u0e42\u0e51\u2180\u11ba\u2180 \u0e51\u0e43\u0e05\u2727PookieNumnums"
    },
    {
      "tip": "Use closeups for better action detail",
      "context": "More pixels per object allows model to understand and reproduce actions better",
      "from": "Juan Gea"
    },
    {
      "tip": "Start with proper models and lightx2v LoRAs at strength 2",
      "context": "Quick start recommendation for wan 2.2",
      "from": "Draken"
    },
    {
      "tip": "Use explicit action descriptions for complex behaviors",
      "context": "Instead of 'stealing ice cream' use 'taking the ice cream into its paws. The panda holds the ice cream away from the woman and starts eating it'",
      "from": "garbus"
    },
    {
      "tip": "Keep CFG at 3.5 for first pass in multi-sampler workflows",
      "context": "In workflows with multiple ksamplers, CFG 1 in second and third samplers doesn't affect negative prompt effectiveness",
      "from": "3DBicio"
    },
    {
      "tip": "Use different seeds for high and low noise samplers",
      "context": "When using multitalk, set high sampler to specific seed and low sampler to seed 0",
      "from": "nacho.money"
    },
    {
      "tip": "Avoid pumping distill steps too high",
      "context": "Like most distilled models, too many steps causes overcooking",
      "from": "Draken"
    },
    {
      "tip": "Reset multistep sampling when switching models",
      "context": "Add euler step when switching to prevent artifacts from multistep samplers using previous model's latents",
      "from": "Ablejones"
    },
    {
      "tip": "Use high quality prompt prefix",
      "context": "Add 'high-quality photorealistic video, dawn time, soft lighting, top lighting, balanced composition' at beginning improves quality infinitely",
      "from": "Alisson Pereira"
    },
    {
      "tip": "Use LoRA scheduling for motion control",
      "context": "For motion based loras you want it to dropoff, use timestep scheduling instead of multiple samplers",
      "from": "Juampab12"
    },
    {
      "tip": "Don't merge LoRAs with scaled fp8 models",
      "context": "In wrapper they work best if you don't merge the loras into scaled models",
      "from": "Kijai"
    },
    {
      "tip": "Adding last frame helps model significantly",
      "context": "For undertrained models, adding the last frame helps the model sooo much",
      "from": "mamad8"
    },
    {
      "tip": "Use Claude for prompt generation",
      "context": "When manual prompting fails multiple times, Claude-generated detailed prompts often work on first try",
      "from": "Juampab12"
    },
    {
      "tip": "Don't use shift for single frame generation",
      "context": "When doing 1 frame, higher shift does not increase quality",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use dedicated endpoints for different model types",
      "context": "Don't switch between I2V and T2V on same endpoint - make separate endpoints so Runpod can optimize flashboot",
      "from": "gokuvonlange"
    },
    {
      "tip": "Use different step configurations for T2V vs I2V",
      "context": "T2V needs more high noise steps (25/15), I2V needs more low noise steps (15/25)",
      "from": "aikitoria"
    },
    {
      "tip": "Don't use compressed schedule on high noise steps",
      "context": "Must perform all HN steps normally, otherwise output will be poor",
      "from": "Ablejones"
    },
    {
      "tip": "Safe approach is no LightX on high noise, LightX on low noise only",
      "context": "Low noise model responds well to LightX, high noise should be left normal",
      "from": "Ablejones"
    },
    {
      "tip": "Use lower rank LoRAs to reduce memory impact",
      "context": "32 rank should be fine instead of 64, and it's included in block swap calculations",
      "from": "Kijai"
    },
    {
      "tip": "Keep prompts short when using VACE to give more room for the rest",
      "context": "When using VACE control system",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Prompt for literally what's inside the mask, not how it's interacting with the outside",
      "context": "When doing VACE inpainting to avoid unwanted transformations",
      "from": "Nekodificador"
    },
    {
      "tip": "For LoRA training on Wan 2.2: motion/big concepts = high noise, details/person/style = low noise",
      "context": "When deciding which expert to train for dual LoRA approach",
      "from": "Juampab12"
    },
    {
      "tip": "You can get 80% results by training only one LoRA depending on the task",
      "context": "If lazy about dual LoRA training for 2.2",
      "from": "Juampab12"
    },
    {
      "tip": "Use different LoRA strengths for high vs low noise",
      "context": "Testing shows 2.25 strength high, 1.75 strength low works better than 1.0 on both",
      "from": "Juampab12"
    },
    {
      "tip": "Start with one non-LoRA step for better tone setting",
      "context": "1 step with cfg=3.5 and no LoRA, then remaining steps with cfg=1 and LoRAs for better lighting",
      "from": "IceAero"
    },
    {
      "tip": "Use minimum 3+3 steps for good results",
      "context": "2+2 steps is not enough, 3+3 is fine minimum, 4+4 is much better",
      "from": "Kijai/Juampab12"
    },
    {
      "tip": "Use mixed LoRA approach for best results",
      "context": "Old LightX2V 2.1 on high noise, new Lightning 2.2 on low noise",
      "from": "Doctor Shotgun"
    },
    {
      "tip": "Add AccVid LoRA for more accurate results",
      "context": "When using Lightning LoRAs, adding AccVid improves output quality",
      "from": "piscesbody"
    },
    {
      "tip": "Use lower strength values on initial generation steps",
      "context": "0.4-0.6 strength on first steps looks more natural than 1.0",
      "from": "PATATAJEC"
    },
    {
      "tip": "Try CFG 3.5 on first step, then CFG 1.0 for remaining steps",
      "context": "When using Lightning LoRAs for better control",
      "from": "IceAero"
    },
    {
      "tip": "Use block swapping for higher resolutions",
      "context": "30 blocks out with 5B model allows 3072 resolution on 24GB VRAM",
      "from": "Juan Gea"
    },
    {
      "tip": "Don't use Lightning LoRA on High Noise model",
      "context": "Lightning LoRA for high noise model isn't great, degrades output quality",
      "from": "gokuvonlange"
    },
    {
      "tip": "Use lower strength with Lightning LoRA",
      "context": "Lightning LoRA works better at lower strengths like 0.5 instead of 1.0",
      "from": "various users"
    },
    {
      "tip": "Frame rate affects generation feel",
      "context": "Sometimes generation works better at 24fps vs 16fps for timing, though it doesn't affect actual inference",
      "from": "Juan Gea"
    },
    {
      "tip": "Don't use any lora on high noise model",
      "context": "It destroys motion and prompt following. Use 10 steps without lora for high model, then 10 steps with lora on low model",
      "from": "Ada"
    },
    {
      "tip": "Use iterative prompting method",
      "context": "Each attempt trying to fix one specific thing by prompting more carefully based on what doesn't work on previous attempt",
      "from": "Ablejones"
    },
    {
      "tip": "Start with no lora or wan2.1 loras first",
      "context": "Getting it to understand the prompt on first step or two is key before applying new loras",
      "from": "Ablejones"
    },
    {
      "tip": "Use detailed, fluff text in prompts",
      "context": "The more instructions you write the better it works, it really likes fluff text because dataset captions are probably long text",
      "from": "aikitoria"
    },
    {
      "tip": "6 steps minimum for good quality",
      "context": "Less than 6 seems to produce errors or bad gens, 6 steps is consistently always good as minimum",
      "from": "Lodis"
    },
    {
      "tip": "Use different step distributions",
      "context": "Give more steps to low than high, e.g. 4 high + 6 low for 10 total steps",
      "from": "screwfunk"
    },
    {
      "tip": "Check denoised output to see lora effects",
      "context": "Look at preview or decode denoised output to compare with/without lora",
      "from": "Kijai"
    },
    {
      "tip": "Be careful not to ruin motion with high lora strength",
      "context": "When boosting lora strength on high model",
      "from": "Kijai"
    },
    {
      "tip": "Avoid Lightning loras for I2V",
      "context": "Should be avoided for I2V workflows",
      "from": "Kijai"
    },
    {
      "tip": "Character loras work better on low model only",
      "context": "Taking character loras off high and only applying to low gave decent results since they can stifle movement",
      "from": "screwfunk"
    },
    {
      "tip": "Use blockswapping for upscaling longer videos",
      "context": "When VRAM is limited, use blockswapping at 20 blocks to enable upscaling",
      "from": "thaakeno"
    },
    {
      "tip": "Keep prompts simple for better character consistency",
      "context": "Complex multi-part prompts can hurt character consistency across scenes",
      "from": "kendrick"
    },
    {
      "tip": "Drag good output back into ComfyUI to recover settings",
      "context": "When workflow gets broken, use a previous good generation to restore all settings",
      "from": "screwfunk"
    },
    {
      "tip": "More steps help with consistency",
      "context": "Higher step counts improve temporal consistency in generations",
      "from": "Kijai"
    },
    {
      "tip": "Use higher start steps to avoid style killing",
      "context": "When using LoRAs or style preservation",
      "from": "thaakeno"
    },
    {
      "tip": "Balance VACE strength or schedule for different timesteps",
      "context": "Using 2 full strength VACEs isn't generally good",
      "from": "Kijai"
    },
    {
      "tip": "Use lightx2v 2.1 at higher strength on high noise model",
      "context": "Better than Lightning 2.2, low noise is basically 2.1",
      "from": "Kijai"
    },
    {
      "tip": "Add 'The subjects appearance/hairstyle remains the same' to prompts",
      "context": "When using scene cutting technique for better subject preservation",
      "from": "Jonathan"
    },
    {
      "tip": "Use exact same prompt for upscaling to avoid changes",
      "context": "When upscaling with WAN 2.2, keep step count and prompt identical to original generation",
      "from": "Gill Bastar"
    },
    {
      "tip": "Use CFG for best motion quality",
      "context": "For WAN 2.2, CFG produces better motion than lightning LoRAs",
      "from": "Kijai"
    },
    {
      "tip": "CFG 1 with 3.0 strength LightX2V works better for resemblance",
      "context": "Character resemblance stays better with CFG 1 + LightX2V rather than higher CFG values",
      "from": "FancyJustice"
    },
    {
      "tip": "Reference subjects at beginning of prompt for scene transitions",
      "context": "Start prompt with subject description, then use 'the scene cuts to' for maintaining resemblance",
      "from": "Jonathan"
    },
    {
      "tip": "Use Qwen2.5-VL 72B for WAN prompt optimization",
      "context": "Same family of models makes it ideal for optimizing prompts for WAN generation",
      "from": "fredbliss"
    },
    {
      "tip": "Use different schedulers for high and low samplers",
      "context": "Can improve results when using multiple samplers",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "tip": "Start simple with workflows",
      "context": "When having issues, stick to simpler workflows rather than complex multi-node setups",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "tip": "Focus on getting motion right first, then upscale",
      "context": "Even at low res, get the motion you want and upscale that specific result",
      "from": "Fill"
    },
    {
      "tip": "Match or mix seeds between HP and LP samplers",
      "context": "Experiment with different seed strategies for high pass and low pass",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "Use Beta scheduler instead of Beta57 with Euler",
      "context": "Beta scheduler works much better than Beta57 for certain styles",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "Use lightning LoRA at 0.7 strength with low CFG for high noise",
      "context": "When using CFG + LoRA mix for 5 second generations",
      "from": "Kijai"
    },
    {
      "tip": "Blur depth maps to reduce control strength",
      "context": "Alternative to lowering strength parameter for depth control",
      "from": "Draken"
    },
    {
      "tip": "Use separate VACE nodes for each control type",
      "context": "When using multiple controls like depth and pose",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "tip": "Prompt 'in a middle of' gesture/jump to make dynamic first frames",
      "context": "For creating more dynamic starting frames",
      "from": "Blink"
    },
    {
      "tip": "Use | character to separate prompts for scene changes",
      "context": "Alternative to EchoShot that works more reliably",
      "from": "Kijai"
    },
    {
      "tip": "Use float schedules for VACE strength and CFG",
      "context": "More control over generation process",
      "from": "Nekodificador"
    },
    {
      "tip": "For T2I, shift must be 1",
      "context": "When using certain workflows to avoid issues",
      "from": "N0NSens"
    },
    {
      "tip": "Use double reference with white background",
      "context": "Can help improve reference following in VACE",
      "from": "Nekodificador"
    },
    {
      "tip": "Refresh browser to show generation preview in SamplerCustomAdvanced",
      "context": "Preview sometimes doesn't show up until next generation",
      "from": "Kijai"
    },
    {
      "tip": "Use detailed prompting with frame-by-frame descriptions",
      "context": "For complex camera movements and character actions",
      "from": "Ablejones"
    },
    {
      "tip": "Describe lots of action in prompts",
      "context": "Don't just use simple prompts like 'girl exercising' - need detailed action descriptions",
      "from": "Cubey"
    },
    {
      "tip": "Use CFG 1 for T2V, higher CFG for I2V motion",
      "context": "T2V works well at CFG 1, I2V needs higher CFG for better motion but creates tradeoff with likeness",
      "from": "Josiah"
    },
    {
      "tip": "Higher CFG equals more motion",
      "context": "When you need more movement in generations",
      "from": "Josiah"
    },
    {
      "tip": "Don't use bong_tangent with low step counts",
      "context": "Bong tangent scheduler needs 40+ steps to work properly, especially with HN+LN models",
      "from": "Ablejones"
    },
    {
      "tip": "Use non-SDE samplers for less variation step-to-step",
      "context": "When experiencing unwanted changes between generation steps",
      "from": "Ablejones"
    },
    {
      "tip": "Don't use speed LoRAs when testing style LoRAs",
      "context": "Speed LoRAs have too much style influence and change the model's output significantly",
      "from": "crinklypaper"
    },
    {
      "tip": "For V2V upscaling, prompt is somewhat irrelevant",
      "context": "Content already exists, model can figure out minor fixes. Basic scene-setting prompt works as well as detailed ones",
      "from": "mdkb"
    },
    {
      "tip": "Use LightX 2.1 LoRAs as they work fantastic and don't affect motion",
      "context": "For Wan 2.2 generation",
      "from": "Lodis"
    },
    {
      "tip": "Enable Memory Context Restore in BIOS for faster boots with high RAM",
      "context": "When using 128GB+ RAM configurations",
      "from": "phazei"
    },
    {
      "tip": "Use VACE control strength 1:1 instead of 3 and 1.5",
      "context": "Produces better results with VACE module",
      "from": "Lodis"
    },
    {
      "tip": "Don't convert between fp8 formats",
      "context": "Converting e4 to e5 loses quality on both sides, avoid e4m2 conversion",
      "from": "Kijai"
    },
    {
      "tip": "WAN is trained on 81 frames",
      "context": "Going off this length won't work as well, model expects 81 frame videos",
      "from": "Draken"
    },
    {
      "tip": "Add noise setting only affects results when LoRAs are used",
      "context": "In FlF2V workflows, the add noise setting has no effect without LoRAs but creates interesting effects with them",
      "from": "piscesbody"
    },
    {
      "tip": "Use strength 0.9 instead of 1.0 for better results",
      "context": "With new Lightning LoRAs for less artifacts",
      "from": "Kijai"
    },
    {
      "tip": "Describe initial scene properly for I2V with new Lightning",
      "context": "Prompt matters a lot with 3+3 steps, need basic scene description like 'man in black suit in brightly lit room'",
      "from": "Hoernchen"
    },
    {
      "tip": "Use 'Hard cut transition' prompting for scene changes",
      "context": "For teleporting characters to new scenes, describe characters first, then hard cut transition, then scene",
      "from": "Juampab12"
    },
    {
      "tip": "Use character identity preservation prompts",
      "context": "Add phrases like 'maintaining same facial features', 'keeping same identity', or 'exact same person'",
      "from": "FancyJustice"
    },
    {
      "tip": "Use gray (127,127,127) for first frame in certain workflows",
      "context": "When working with specific generation setups",
      "from": "artemonary"
    },
    {
      "tip": "Lower LoRA strength to avoid issues",
      "context": "Try 0.75 or 0.5 strength when experiencing problems",
      "from": "\u30dc\u30b0\u30c0\u30f3\u304a\u3058\u3055\u3093"
    },
    {
      "tip": "For camera prompting, structure prompt differently",
      "context": "Use format like 'Sunny, Outdoor, [action]. The camera [movement], [lens type]' instead of starting with camera movement",
      "from": "HeadOfOliver"
    },
    {
      "tip": "Character likeness is better when faces are cropped closer",
      "context": "Doesn't do well with likeness when characters are far from camera",
      "from": "FancyJustice"
    },
    {
      "tip": "Keep original image stationary until scene changes",
      "context": "Whatever the character looks like right before scene change gets transferred",
      "from": "FancyJustice"
    },
    {
      "tip": "Use prompts like 'and after a brief flash' to improve prompt following",
      "context": "When Wan isn't following prompts properly, narrative language can help",
      "from": "Hoernchen"
    },
    {
      "tip": "Merge LoRA setting saves VRAM and speeds up subsequent runs",
      "context": "Slower first time but faster for next runs with same LoRA",
      "from": "pagan"
    },
    {
      "tip": "Use quantized models for high resolution generation",
      "context": "Q5_K_M quantized version helps with VRAM usage for large resolutions",
      "from": ": Not Really Human :"
    },
    {
      "tip": "Consider stepped CFG for complex LoRA setups",
      "context": "2cfg step 1 lora 0, 1cfg lora 1 step 2,3 on 6 step 3/3",
      "from": "CaptHook"
    },
    {
      "tip": "Use Kijai's defaults for best results",
      "context": "User spent 12 hours testing and found Kijai's defaults consistently produced best results",
      "from": "CaptHook"
    },
    {
      "tip": "Use Lightning LoRAs at strength 1 only",
      "context": "Higher strengths make output look more like 2.1 instead of 2.2",
      "from": "Ablejones"
    },
    {
      "tip": "Use VACE with lower strength for image guidance without fixed first frame",
      "context": "When you want image as guide but not as starting frame for I2V",
      "from": "Ablejones"
    },
    {
      "tip": "Use audio scale 3 and audio cfg 3 with vid2vid",
      "context": "Works well with 0.65 denoise for video-to-video workflows",
      "from": "samhodge"
    },
    {
      "tip": "Separate vocals properly for better MultiTalk results",
      "context": "Use Spleeter or better vocal separation tools instead of basic separation",
      "from": "Kijai"
    },
    {
      "tip": "Use First-Frame-Last-Frame technique with VLM automation",
      "context": "For batch processing multiple images automatically without manual intervention",
      "from": "R."
    },
    {
      "tip": "Eye the optimal split point from previews",
      "context": "Instead of calculating complex optimal schedules, visually determine when to switch between high/low noise models",
      "from": "Kijai"
    },
    {
      "tip": "Higher audio_cfg above 1 is slower but more accurate",
      "context": "For MultiTalk lip sync, values above 1 sample multiple times for better quality",
      "from": "MilesCorban"
    },
    {
      "tip": "Use denoise < 1.0 for video-to-video MultiTalk",
      "context": "Enables proper v2v functionality in MultiTalk workflows",
      "from": "Kijai"
    },
    {
      "tip": "Use qwen2.5-vl generated prompts for better results",
      "context": "Wan was captioned with qwen2.5-vl, so prompts generated from it work better with lower shift values",
      "from": "fredbliss"
    },
    {
      "tip": "Higher shift = let model run against training curves",
      "context": "Use higher shift when you want autopilot mode, lower shift for more control with solid prompting",
      "from": "fredbliss"
    },
    {
      "tip": "Set LoRA strength as float list for different steps",
      "context": "Even if high noise is 5-6 steps out of 8 total, you need 8 values in the list",
      "from": "MysteryShack"
    },
    {
      "tip": "Use start step directly instead of denoise percentage",
      "context": "Clearer than denoise percentage, especially for low step counts",
      "from": "Kijai"
    },
    {
      "tip": "Don't want slow motion? First step better not have ANY kind of lora",
      "context": "Universal rule for all distill loras to avoid motion artifacts",
      "from": "MysteryShack"
    },
    {
      "tip": "Use cached Qwen node to save memory",
      "context": "Node deletes models from memory after running and can cache prompts to disk for reuse",
      "from": "Kijai"
    },
    {
      "tip": "Merge LoRAs when using FP8 scaled + fast mode",
      "context": "Unmerged LoRAs not possible with FP8 as LoRA application requires upcasting",
      "from": "Kijai"
    },
    {
      "tip": "Check sigma values in logs to determine high/low split",
      "context": "Sigmas appear in logs when workflow runs, count steps from comma positions to determine proper split point",
      "from": "phazei"
    },
    {
      "tip": "Use Joy Caption for better prompting",
      "context": "Extra options work really well for video generation prompting",
      "from": "screwfunk"
    },
    {
      "tip": "Don't use radial attention on early steps",
      "context": "It's disastrous especially on early steps of the high noise model, ruins video coherence",
      "from": "Kijai"
    },
    {
      "tip": "Use confidence threshold of 0.5 for Sapiens keypoints",
      "context": "Remove anything under 0.5 confidence to avoid bad detection points",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Remove triangle and clavicule from VACE pose input",
      "context": "When using Sapiens with VACE, remove the triangle in middle and clavicule for compatibility",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use I2V lightning LoRAs with strength less than 1",
      "context": "Regular use less than 1 strength to avoid camera shake issues",
      "from": "IceAero"
    },
    {
      "tip": "Prompt text preprocessing matters for WAN",
      "details": "DiffSynth removes newlines, extra spaces, and applies whitespace cleaning to prompts",
      "from": "fredbliss"
    },
    {
      "tip": "Use specific camera movement prompts",
      "context": "For car following shots, try 'first-person perspective', 'camera follows the car side by side', or 'dynamic tracking shot following a [car model] closely'",
      "from": "Hashu"
    },
    {
      "tip": "Backup pip freeze and whole venv",
      "context": "30s of compression saves hours of fixing when things break",
      "from": "Hoernchen"
    },
    {
      "tip": "Check DDR5 RAM compatibility carefully",
      "context": "DDR5 requires specific motherboard compatibility for both size and speed, unlike DDR4",
      "from": "Canin17"
    },
    {
      "tip": "Use local LLM for action-rich prompts",
      "context": "When getting slow motion at higher frame counts, ask LLM to help create prompts with enough action",
      "from": "Hashu"
    },
    {
      "tip": "Don't use creative upscaling for likeness retention",
      "context": "When trying to maintain character likeness during upscaling",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Use CFG for fast motion generation",
      "context": "CFG is key for generating really fast motion, can also mix with high distill LoRA strengths",
      "from": "Kijai"
    },
    {
      "tip": "High lightning LoRA at 2.5 for natural walking",
      "context": "For natural paced or slightly slow walks, keep low at 1 and avoid going above 2.5 on high to prevent artifacts",
      "from": "Alpha-Neo"
    },
    {
      "tip": "Use negative prompts for unwanted elements",
      "context": "Put unwanted elements like cars in negative prompt instead of trying to prompt them away",
      "from": "aikitoria"
    },
    {
      "tip": "3-2 pulldown for 16fps to 24fps conversion",
      "context": "Interpolate 16fps by 3x to 48fps, then strip every other frame to get 24fps for sync with mmaudio",
      "from": "nacho.money"
    },
    {
      "tip": "Use higher LoRA strength on high noise model for Wan 2.1 LoRAs",
      "details": "Wan 2.1 LoRAs are weak on high noise model, use much higher strength. Low noise model works normally",
      "context": "When using Wan 2.1 LoRAs with Wan 2.2",
      "from": "Kijai"
    },
    {
      "tip": "Use 2.0-2.5 strength for high noise model",
      "details": "For forcing prompt adherence on high noise model",
      "context": "LoRA strength settings",
      "from": "DaxRedding"
    },
    {
      "tip": "Temporal mask is opposite to VACE",
      "details": "For Fun InP: black = generate, white = keep (opposite to VACE)",
      "context": "Using temporal masks with Fun InP",
      "from": "Kijai"
    },
    {
      "tip": "Give Fun 2.2 enough freedom for better motion",
      "details": "Giving enough freedom gets 2.2 motion with slight guidance, not using Fun for low noise gives better details",
      "context": "Balancing motion and detail quality",
      "from": "Kijai"
    },
    {
      "tip": "Add stationary dots to prevent unwanted camera movement",
      "details": "Adding stationary reference points helps prevent unwanted camera or scene movement",
      "context": "Controlling camera stability in generations",
      "from": "ingi // SYSTMS"
    },
    {
      "tip": "Use big swap file and --disable-smart-memory for high resolution generation",
      "context": "When running 1600x900x81 frames on lower VRAM GPUs",
      "from": "mdkb"
    },
    {
      "tip": "Don't use Fun 2.2 for low noise final step",
      "context": "Use 3rd sampler at end without Fun for better quality finish",
      "from": "Kijai"
    },
    {
      "tip": "Mix images with videos when training character LoRAs",
      "context": "Training only on images kills motion in final model",
      "from": "MilesCorban"
    },
    {
      "tip": "Use v2v with higher denoise than 0.79 for more changes",
      "context": "0.79 is tipping point - lower keeps structure, higher changes based on prompt",
      "from": "mdkb"
    },
    {
      "tip": "Use procexp64 to monitor VRAM usage per node",
      "context": "Free Microsoft tool for monitoring GPU load and memory commit",
      "from": "mdkb"
    },
    {
      "tip": "Use Lightning + LightX2V LoRA combination",
      "context": "Lightning LoRAs at strength 1.0, then LightX2V at 0.2-0.5 strength to clear up blurry results when stacking multiple LoRAs",
      "from": "screwfunk"
    },
    {
      "tip": "Use bigger model for prompt generation instead of Qwen2.5",
      "context": "Let Claude or similar make prompts, then use FL Prompt selector from ComfyUI_Fill-Nodes",
      "from": "Janosch Simon"
    },
    {
      "tip": "Use cfg + lightx2v scheduled for best results",
      "context": "Kijai's preferred setup, sometimes disable cfg for better results",
      "from": "Kijai"
    },
    {
      "tip": "More than 1 cfg step often needed",
      "context": "Motion can break with only single cfg step",
      "from": "Kijai"
    },
    {
      "tip": "Decode, upscale, encode instead of latent upscale",
      "context": "Latent upscale is terrible quality",
      "from": "Kijai"
    },
    {
      "tip": "Check LLM output for unwanted additions",
      "context": "Sometimes LLM adds 'here is my response, are you happy with it' even when told to just output the prompt",
      "from": "pagan"
    },
    {
      "tip": "Use separate block swaps for high and low noise models",
      "context": "Makes sense for upscaling workflows when models are different sizes",
      "from": "Kijai"
    },
    {
      "tip": "For video2video without lightning, bypass LoRA and use more steps",
      "context": "Put high noise sampler's CFG to 3.5, expect 4-5x slower",
      "from": "Hevi"
    },
    {
      "tip": "Use tight crop for better facial similarity",
      "context": "When providing face reference in VACE",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Consider upscaling 720p instead of generating high-res directly",
      "context": "Saves time especially when needing many attempts, 720p to 1080p takes about 500 seconds",
      "from": "Hevi"
    },
    {
      "tip": "Use normal map control with blur adjustment for 3D renders",
      "context": "For V2V from 3D renders, use normal map control and blur to adjust generation adherence while preserving shapes and movement",
      "from": "Blink"
    },
    {
      "tip": "Mix control video with straight normal color to adjust strength",
      "context": "Mix control video with normal color (128, 128, 255) to prevent bleeding into generation",
      "from": "Blink"
    },
    {
      "tip": "Use gradual blending for seamless transitions between control clips",
      "context": "Create natural transitions by letting control go off for some frames",
      "from": "Blink"
    },
    {
      "tip": "Use Wan 5B for upscaling instead of SeedVR",
      "context": "Do V2V with denoise 0.35-0.5 after simple Lanczos resize, or use I2V with 5B to guide upscale",
      "from": "Juan Gea"
    },
    {
      "tip": "Wan 2.2 Low Noise can fix wobbling before upscaling",
      "context": "Run video through Wan 2.2 Low Noise with 0.2-0.3 denoise before upscaling with SeedVR2",
      "from": "AmirKerr"
    },
    {
      "tip": "Use face detection with crop and stitch for face consistency",
      "context": "Mask face, crop to 1024x1024, do V2V to improve face, then stitch back for broken background faces",
      "from": "Juan Gea"
    },
    {
      "tip": "SeedVR needs high batch number for stable images",
      "context": "Higher batch sizes prevent instability in SeedVR upscaling",
      "from": "shockgun"
    },
    {
      "tip": "Use face target and crop then rembg for best Stand-in results",
      "context": "When preparing reference images for Stand-in LoRA",
      "from": "Kijai"
    },
    {
      "tip": "Match generation dimensions with reference image size",
      "context": "For optimal Stand-in LoRA performance",
      "from": "Kijai"
    },
    {
      "tip": "Video following camera actions depends more on motion parameters than prompting",
      "context": "Need high quality high-noise steps rather than better prompts",
      "from": "Instability01"
    },
    {
      "tip": "Use variable LoRA strengths and CFGs for better results",
      "context": "For improved video generation quality",
      "from": "Instability01"
    },
    {
      "tip": "Best temp fix for identity issues is a real 2.2 distill LoRA",
      "context": "Fixes identity issues by virtue of fewer steps",
      "from": "Instability01"
    },
    {
      "tip": "Use 'Fast shot, flying into...' instead of 'The camera flies' to avoid generating actual cameras or crew",
      "context": "When creating camera movement prompts",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Lower LightX2V to 0.6 and increase steps for less overcooking",
      "context": "When using speed LoRAs",
      "from": "Hevi"
    },
    {
      "tip": "Use 'extreme slowmotion' or 'Close-up to mid-shot, ultra slow motion, cinematic shot' for slow motion effects",
      "context": "When wanting slow motion in Wan 2.2",
      "from": "Drommen-Kille"
    },
    {
      "tip": "Stay above Q6 quantization for good results",
      "context": "Weight precision doesn't seem major as long as above Q6",
      "from": "Kijai"
    },
    {
      "tip": "Use --reserve-vram argument if getting OOM with partial loads",
      "context": "When native doesn't estimate VRAM need correctly",
      "from": "Kijai"
    },
    {
      "tip": "Use unsampling after HN model steps before LN model",
      "context": "When combining HN and LN models, doing 1-2 steps of unsampling after HN keeps motion but gives more freedom to LN model with controls",
      "from": "Ablejones"
    },
    {
      "tip": "Use higher resolution for better face consistency",
      "context": "When having face consistency issues after 1 second in I2V generations",
      "from": "N0NSens"
    },
    {
      "tip": "For best I2V results, use full model instead of distills",
      "context": "Distillation models will skew results for face likeness preservation",
      "from": "DawnII"
    },
    {
      "tip": "Reduce VACE strength when combining with other controls",
      "context": "When using VACE with other control methods, lower strength may be needed to avoid conflicts",
      "from": "Hashu"
    },
    {
      "tip": "Use white background for mask parts to keep, can draw directly on load image node",
      "context": "When creating masks for workflows",
      "from": "Kijai"
    },
    {
      "tip": "For Stand-In, use only 'man' or 'woman' in prompts without extra appearance descriptions",
      "context": "When you don't want to alter facial features",
      "from": "mdkb"
    },
    {
      "tip": "Use high-resolution frontal face images for best Stand-In results",
      "context": "Input image recommendations for Stand-In",
      "from": "mdkb"
    },
    {
      "tip": "24fps helps with Stand-In quality",
      "context": "Video generation settings",
      "from": "BobbyD4AI"
    },
    {
      "tip": "Describe the whole image with i2v as part of the prompt then the action",
      "context": "Better results with I2V generations",
      "from": "crinklypaper"
    },
    {
      "tip": "Use AIO instead of official controlnet for better performance",
      "context": "When using Canny and other controlnet duties",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Always boot ComfyUI before starting generation",
      "context": "To avoid having to restart mid-generation",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Use 1.25x latent upscale instead of 2x",
      "context": "2x latent upscale usually generates problems with weird rims around objects",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Maximize reference resolution by cropping properly",
      "context": "Don't leave too much white space around the reference image as it limits resolution",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use MAGREF when you don't want model to latch onto input image too hard",
      "context": "For context window generations where you want reference behavior rather than exact first frame matching",
      "from": "Kijai"
    },
    {
      "tip": "Enable fp8 fast mode for significant speed gains",
      "context": "Works fine with 2.2 models and can be 30-40% faster than GGUF",
      "from": "Kijai"
    },
    {
      "tip": "Use character name in prompt for better consistency",
      "context": "Give character description a name, then write the name doing something for Wan 2.2 to understand better",
      "from": "xwsswww"
    },
    {
      "tip": "Use 'fixed shot, static camera' to prevent camera movement",
      "context": "When using LoRAs that cause unwanted camera movement",
      "from": "Lodis"
    },
    {
      "tip": "Adding 'character stays in place' works better than just 'fixed shot'",
      "context": "For character rotation LoRAs to prevent unwanted movement",
      "from": "Mngbg"
    },
    {
      "tip": "Use inverted canny maps with reduced strength",
      "context": "Black lines on white background, strength 1.0 instead of 1.5 for better canny control",
      "from": "xwsswww"
    },
    {
      "tip": "Higher CFG good for fast movement generation",
      "context": "CFG 3.5 vs 1 shows dramatic difference in motion speed, but may cause artifacts",
      "from": "Mngbg"
    },
    {
      "tip": "Use 0.38 denoise for x2 latent upscale",
      "context": "Specific setting that works fine for upscaling",
      "from": "Valle"
    },
    {
      "tip": "Don't blur the mask for VACE, only blur final composition",
      "context": "When doing VACE inpainting to avoid artifacts",
      "from": "Kijai"
    },
    {
      "tip": "Use reference image and canny input as separate VACE streams",
      "context": "To control camera motion/depth and reference strength separately",
      "from": "Josiah"
    },
    {
      "tip": "Let automated memory management do its thing, don't use VRAM clear nodes",
      "context": "VRAM clear nodes are more harm than good",
      "from": "Kijai"
    },
    {
      "tip": "Use --reserve-vram argument instead of multigpu node",
      "context": "Multigpu node doesn't do anything useful",
      "from": "Kijai"
    },
    {
      "tip": "Use WanImageToVideo node for I2V models",
      "context": "Required to add proper mask, can't use I2V without it",
      "from": "Kijai"
    },
    {
      "tip": "Scale down Fantasy Portrait strength when using adult face to drive kids",
      "context": "Adult face driving kids gets too weird without scaling",
      "from": "Guey.KhalaMari"
    },
    {
      "tip": "Use Cached node for best RAM management",
      "context": "Should have zero impact on performance while saving RAM",
      "from": "Kijai"
    },
    {
      "tip": "Increase distill LoRA strength to help with leftover noise/snow artifacts",
      "context": "When using distillation LoRAs and experiencing noise issues",
      "from": "Kijai"
    },
    {
      "tip": "Images in ComfyUI should always be RGB, never RGBA",
      "context": "VAEs never support RGBA, the mask is the A channel",
      "from": "Kijai"
    },
    {
      "tip": "Don't use Uni3c for context with FantasyPortrait",
      "context": "FP doesn't need it and Uni3c ruins more of the motion",
      "from": "Kijai"
    },
    {
      "tip": "Use async offloading for minimal inference speed hit",
      "context": "When working with memory management",
      "from": "Kijai"
    },
    {
      "tip": "Use full steps without loras to reduce noisy pattern in Wan 2.2 Fun",
      "context": "When getting noisy patterns in Fun variant",
      "from": "DawnII"
    },
    {
      "tip": "Run base 2.2 low noise to address noise issues",
      "context": "Alternative to running full steps without loras",
      "from": "DawnII"
    },
    {
      "tip": "Use separate VACE embeds for different strengths",
      "context": "When needing individual control over controlnet and reference strengths",
      "from": "Nekodificador"
    },
    {
      "tip": "Set power limit to 500W instead of 600W",
      "context": "For managing GPU power consumption during generation",
      "from": "Kijai"
    },
    {
      "tip": "Use comfyui-videohelpersuite node 'Select images' with image set to -1 to extract last frame",
      "context": "When needing to extract the last frame from VAE Decode for video workflows",
      "from": "mdkb"
    },
    {
      "tip": "Quality of reference image is crucial for VACE",
      "context": "Better reference images lead to dramatically improved results, sometimes looking like lora quality",
      "from": "Nekodificador"
    },
    {
      "tip": "Use CauseVid at 0.8 strength for maintaining likeness",
      "context": "When working with 2.1 VACE workflows for better identity preservation",
      "from": "Josiah"
    },
    {
      "tip": "Use lazy switches instead of bypass/muting for more reliable workflows",
      "context": "When building complex switching workflows that need to work with API",
      "from": "Kijai"
    },
    {
      "tip": "Don't cast e4m3fn model to e5m2",
      "context": "It will be worse than using either format properly",
      "from": "Kijai"
    },
    {
      "tip": "Use reference images with white background for VACE",
      "context": "Position the object approximately where you want it to appear in output",
      "from": "Ablejones"
    },
    {
      "tip": "Use virtual environments for experimental nodes",
      "context": "Prevents conflicts when installing nodes with complex dependencies like GGUF converters",
      "from": "screwfunk"
    },
    {
      "tip": "Extract last frame and use WanVideo Blender for extension",
      "context": "For video continuation without contrast issues, use overlap of 1 frame",
      "from": "xwsswww"
    },
    {
      "tip": "Use flowmatch_pusa scheduler with PUSA",
      "context": "When using PUSA LoRA for proper functionality",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use CFG with WanFM for proper transitions",
      "context": "When using WanFM sampling method",
      "from": "Kijai"
    },
    {
      "tip": "Set audio scale to 3 for MultiTalk with MagRef",
      "context": "For v2v lip-sync with maintained face consistency",
      "from": "mdkb"
    },
    {
      "tip": "Use LCM on high noise model for prompt adherence",
      "context": "When having trouble getting Wan2.2 to follow prompts properly",
      "from": "MysteryShack"
    },
    {
      "tip": "Output at each stage for reliability",
      "context": "Due to random particle/snow blips in low noise mode",
      "from": "MysteryShack"
    },
    {
      "tip": "Use SDPA instead of SageATTN for InFrame with fp16",
      "context": "When getting poor likeness results with InFrame on fp16 models",
      "from": "hablaba"
    },
    {
      "tip": "For VACE inpainting, alignment of objects/subjects between images is crucial",
      "context": "When using First-Frame-Last-Frame functionality",
      "from": "hicho"
    },
    {
      "tip": "Use extract last frame approach for better context continuity",
      "context": "When working with context windows",
      "from": "xwsswww"
    },
    {
      "tip": "Try 5 frames for single image VACE when 1 frame fails",
      "context": "Solves weird artifacts like hands appearing in wrong places",
      "from": "mdkb"
    },
    {
      "tip": "For MAGREF, you can give different images to windows that don't include start frame and pad them to reduce sticking",
      "context": "Special case for MAGREF with context windows",
      "from": "Kijai"
    },
    {
      "tip": "Remove negative prompts to avoid skull artifacts in early steps",
      "context": "When seeing consistent skull shapes at first diffusion step",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "tip": "Use adapter_scale around 0.7 to reduce mouth over-animation",
      "context": "When Fantasy Portrait generates too extreme mouth movements",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "For LoRA training use 15 images + 15 videos for high noise, 30 images for low noise",
      "context": "Character LoRA training best practices",
      "from": "el marzocco"
    },
    {
      "tip": "Don't use CLIP vision node with Fantasy Portrait first/last frame workflow",
      "context": "Makes generation super fast when using extracted frames",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use high quality driving video with head taking most of screen",
      "context": "For better Fantasy Portrait results - good lighting, clear mouth visibility",
      "from": "seitanism"
    },
    {
      "tip": "Use repository at github.com/Wan-Video/Wan2.2 to get full quality",
      "context": "For best results instead of other implementations",
      "from": "Benjimon"
    },
    {
      "tip": "Refresh ComfyUI with CTRL+R after installing new models",
      "context": "When VACE models don't show up in model select node",
      "from": "Ryzen"
    },
    {
      "tip": "Remove negative prompts for more diverse first steps",
      "context": "Negative prompts can cause skull diffusing in first step",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "tip": "Add gray frames to end of batch to satisfy Wan requirements",
      "context": "Use get image batch range node to chop additional frames off if needed",
      "from": "Flipping Sigmas"
    },
    {
      "tip": "Fix input video framing to remove padding and frame closer to source",
      "context": "Helps with context window stability and character consistency",
      "from": "smithyIAN"
    },
    {
      "tip": "Use Set LoRA nodes instead of connecting to model loader",
      "context": "Avoids reloading whole model when changing LoRAs",
      "from": "Kijai"
    },
    {
      "tip": "Use cached encode to help with VRAM on 3090",
      "context": "Helps memory but caches prompts",
      "from": "Josiah"
    },
    {
      "tip": "Use wildcards for context windows to avoid repetitive output",
      "context": "Splits prompts so each window has own prompt",
      "from": "Kijai"
    },
    {
      "tip": "First step of high noise model should have no LoRA for T2V",
      "context": "Prevents slow motion problem, I2V doesn't have this issue",
      "from": "MysteryShack"
    },
    {
      "tip": "Use longer prompts with motion cues and cinematic elements",
      "context": "For more motion and speed in generations",
      "from": "Josiah"
    },
    {
      "tip": "Can halve block swap amount when using context windows",
      "context": "Don't need as much block swap with context windows",
      "from": "Kijai"
    },
    {
      "tip": "For skyreels lora looping issues, boost LoRA strength",
      "context": "When using skyreels lora on 2.2 and getting looping effects",
      "from": "Kijai"
    },
    {
      "tip": "Use first step with CFG then rest without for better motion",
      "context": "Wan 2.2 I2V workflow optimization",
      "from": "Kijai"
    },
    {
      "tip": "Convert 30fps video to 16fps using select_every_nth to 2",
      "context": "Making control videos compatible with Wan 2.2",
      "from": "artemonary"
    },
    {
      "tip": "Model can do most things but some actions need incredibly specific prompts",
      "context": "Getting difficult actions like striking to work",
      "from": "Ablejones"
    },
    {
      "tip": "Use Claude for prompt iteration and refinement",
      "context": "Scientific approach to prompt engineering",
      "from": "Ablejones"
    },
    {
      "tip": "Use first frame of input video to create ref image with Flux",
      "context": "For consistent style transfer when you want the ref image to match the structure",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use 25fps as sweet spot for MultiTalk, even 24fps can cause slurring",
      "context": "For lip sync quality",
      "from": "mdkb"
    },
    {
      "tip": "Set audio scale to 2.5 for better MultiTalk performance",
      "context": "When other settings aren't working well",
      "from": "mdkb"
    },
    {
      "tip": "Stick to 81 or 121 frames for talking models",
      "context": "Models were trained on these frame counts and get weird with other lengths",
      "from": "NC17z"
    },
    {
      "tip": "Use MAGREF model to maintain source image likeness",
      "context": "For better adherence to reference image in i2v workflows",
      "from": "mdkb"
    },
    {
      "tip": "Use MultiTalk + FantasyPortrait together",
      "context": "FantasyPortrait uses face landmark tracking, MultiTalk keeps lipsync on point",
      "from": "mdkb"
    },
    {
      "tip": "Use pleasant adult voices for better MultiTalk results",
      "context": "Unique/out-of-distribution voices may not work as well",
      "from": "MysteryShack"
    },
    {
      "tip": "Edit audio spacing in Audacity before lip sync",
      "context": "Fine-tune pauses and timing for better results",
      "from": "MysteryShack"
    },
    {
      "tip": "Use empty mask input for proper resize",
      "context": "Add empty mask to inputs for proper image/mask size matching",
      "from": "Kijai"
    },
    {
      "tip": "Watch frame rate settings in Video In node",
      "context": "25fps setting can cause longer generation times than expected",
      "from": "mdkb"
    },
    {
      "tip": "Don't update ComfyUI frequently to avoid breaking workflows",
      "context": "Custom nodes often break with updates",
      "from": "Kijai"
    },
    {
      "tip": "Use 25fps for audio embeddings regardless of output fps",
      "context": "MultiTalk is trained at 25fps, change only the output fps for sync",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Audio can provide more than lip sync",
      "context": "Audio also influences hand movements and overall motion",
      "from": "Dream Making"
    },
    {
      "tip": "Color match not necessary with new model",
      "context": "The colormatch doesn't seem that necessary with this new model",
      "from": "Kijai"
    },
    {
      "tip": "Every LoRA added with GGUF increases memory use and slows it down",
      "context": "When comparing to merged loras",
      "from": "Kijai"
    },
    {
      "tip": "Use disk cache in WanVideo Text Encoder",
      "context": "Saves prompt to disk so same prompt next time doesn't need encoding, saves time",
      "from": "Kijai"
    },
    {
      "tip": "Don't touch frame_window_size setting",
      "context": "Leave at default 81, motion_frame is the overlap that can be adjusted",
      "from": "Kijai"
    },
    {
      "tip": "Lower LoRA strength instead of increasing steps",
      "context": "Usually better to lower the Lora strength with distill models",
      "from": "Kijai"
    },
    {
      "tip": "Use either T2V or I2V LightX2V LoRA",
      "context": "Both work interchangeably for I2V/MagRef model",
      "from": "Kijai"
    },
    {
      "tip": "Use GitHub Desktop for keeping up with development work",
      "context": "Simple to roll back to commits and update, better than ComfyUI Manager for fast-moving repos",
      "from": "Kijai"
    },
    {
      "tip": "Use MagRef model without clip embeds",
      "context": "MagRef model doesn't require clip embeds connection unlike other models",
      "from": "DawnII"
    },
    {
      "tip": "Be careful with LoRAs on high noise side",
      "context": "Low noise side can use LoRAs freely, high noise side needs careful application to not ruin motion - best to use CFG and no lora or weak lora with CFG",
      "from": "Kijai"
    },
    {
      "tip": "Use masks for targeted lip-sync",
      "context": "Good luck with MultiTalk using VACE and masking faces, regenerating just the face area",
      "from": "Tango Adorbo"
    },
    {
      "tip": "Audio scale strength above 1 creates exaggeration",
      "context": "Tried strength of 2 and it looked too exaggerated",
      "from": "seitanism"
    },
    {
      "tip": "Don't touch frame window size parameter",
      "context": "Frame window is how many frames processed at once, leave it default",
      "from": "Kijai"
    },
    {
      "tip": "LoRA rank above 64 hits diminishing returns",
      "context": "Higher ranks are bigger memory hit when using unmerged LoRAs, especially with GGUF",
      "from": "Kijai"
    },
    {
      "tip": "Use Linux distro with bundled nvidia drivers",
      "context": "Something with nvidia drivers bundled is best to get started as manual driver install is usually huge pain",
      "from": "Kijai"
    },
    {
      "tip": "Use 2 steps with multiple speed LoRAs for fast testing",
      "context": "Using Lightx, Pusa, Accvid, and FastWan LoRAs together for 2-step generation in 319 seconds",
      "from": "VRGameDevGirl84"
    },
    {
      "tip": "Don't change the 81 frame default",
      "context": "The 81 frame setting shouldn't be changed, base model does best at 81",
      "from": "Kijai"
    },
    {
      "tip": "Audio scale shouldn't be too high",
      "context": "Audio scale of 2 is too high and can have detrimental effects, audio cfg scale of 3-5 is fine",
      "from": "Kijai, seitanism"
    },
    {
      "tip": "Need extra frames for complete audio sync",
      "context": "Need additional 28 frames beyond audio length to avoid vocal sync failure before end of video",
      "from": "NC17z"
    },
    {
      "tip": "Standard CFG for Wan is 3-6",
      "context": "For Wan models, use CFG between 3-6 range",
      "from": "Ryzen"
    },
    {
      "tip": "Keep prompts simple for lip sync models",
      "context": "Use basic prompts like 'Woman Talking' rather than detailed descriptions",
      "from": "NC17z"
    },
    {
      "tip": "Use external vocal separation tools instead of ComfyUI nodes",
      "context": "Better results for lip sync when using dedicated audio separation software",
      "from": "NC17z"
    },
    {
      "tip": "Consider first pass with low steps for speed",
      "context": "Use 1 step or low steps for initial video creation, then refine in upscaling pass",
      "from": "NC17z"
    },
    {
      "tip": "Multiple swap blocks mainly useful as OOM safeguard",
      "context": "Especially when generating at higher resolutions, doesn't significantly impact speed",
      "from": ". Not Really Human :."
    },
    {
      "tip": "Use InfiniteTalk for videos longer than 3-5 seconds, MultiTalk/Fantasy Talker for shorter clips",
      "context": "When doing lip sync work",
      "from": "NC17z"
    },
    {
      "tip": "Use --disable-smart-memory instead of --cache-none",
      "context": "For memory management - means less reloading with about same OOM reduction",
      "from": "mdkb"
    },
    {
      "tip": "Create webcam video of yourself lip-syncing for better control",
      "context": "Use 480x480 webcam capture of your lip-sync as reference for more accurate results",
      "from": "NC17z"
    },
    {
      "tip": "Use SetNode and GetNode or Anything Everywhere nodes for workflow organization",
      "context": "For managing complex workflows",
      "from": "MysteryShack"
    },
    {
      "tip": "Use vid2vid with partial denoise to fix timing issues",
      "context": "Take InfiniteTalk result and run it through vid2vid upscale with MultiTalk, then do two runs - vid2vid with partial denoise (0.65) can fix sins with timing",
      "from": "samhodge"
    },
    {
      "tip": "Combine InfiniteTalk with vid2vid for better results",
      "context": "Take the infinite talking for the whole video, then run vid2vid with same audio with 0.65 denoise to fix the spazzy motion on the InfiniteTalk output",
      "from": "samhodge"
    },
    {
      "tip": "I2V may not work well for Qwen->Wan bridge, but T2V with noise samples can",
      "context": "When bridging Qwen image to Wan video, T2V as input samples of noise works much better than I2V approach",
      "from": "fredbliss"
    },
    {
      "tip": "Use CFG 8 for more dramatic results",
      "context": "When working with image to video generation",
      "from": "fredbliss"
    },
    {
      "tip": "Crop head, process with InfiniteTalk, then paste back to original",
      "context": "For better lip-sync results with minimal background changes",
      "from": "Kijai"
    },
    {
      "tip": "Use silent audio to generate longer scenes without dialog",
      "context": "InfiniteTalk can work with silence audio for extended generations",
      "from": "mamad8"
    },
    {
      "tip": "Keep models in cache by having unused generation nodes in workflow",
      "context": "To prevent slow model reloading when switching between tools",
      "from": "mamad8"
    },
    {
      "tip": "Use 'R' keyboard shortcut for refresh node definitions",
      "context": "Alternative to finding refresh button in ComfyUI",
      "from": "Kijai"
    },
    {
      "tip": "Set Manager version to 'nightly' for WanVideoWrapper",
      "context": "Required for latest features and updates",
      "from": "Kijai"
    },
    {
      "tip": "Use 'git pull' for updating nodes",
      "context": "Best way to update WanVideoWrapper from command line",
      "from": "Kijai"
    },
    {
      "tip": "For multiple LoRAs, keep recommended strengths",
      "context": "If lora author recommends 0.7 strength, leave at 0.7 even with multiple LoRAs",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Fun Control is very flexible with inputs",
      "context": "Can input RGB or random masks and it will follow",
      "from": "Hashu"
    },
    {
      "tip": "Use select images node with '0:yourframecount' to trim extra frames",
      "context": "Removes extra frames appended at end of InfiniteTalk generation",
      "from": "ArtOfficial"
    },
    {
      "tip": "Use audio scale of 2.5 for better lip movement in MultiTalk",
      "context": "Otherwise lips don't move much",
      "from": "mdkb"
    },
    {
      "tip": "Mix MultiTalk and InfiniteTalk for best results",
      "context": "When doing lip sync work",
      "from": "mdkb"
    },
    {
      "tip": "Use 3 Sampler method with Wan-Lighting for more motion",
      "context": "Definitely makes a difference in bringing in more motion",
      "from": "ArtOfficial"
    },
    {
      "tip": "For Wan 2.2 training, split samples by 10 or 20 steps and use low vram mode",
      "context": "Trains at normal speed but quality results not yet achieved",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Interpolate Wan output to 30fps for better viewing experience",
      "context": "Raw output feels like it skips frames",
      "from": "ArtOfficial"
    },
    {
      "tip": "Fill masked area with grey (0.5/128) and provide same area as mask for VACE",
      "context": "When using VACE inpainting",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use very dim grey values (1,2,3) for segmentation masks",
      "context": "For MultiTalk masking different speakers",
      "from": "samhodge"
    },
    {
      "tip": "Connect negative prompt to NAG input when using cfg 1.0",
      "context": "Since normal negative doesn't work with cfg 1, this acts like having a negative",
      "from": "Juan Gea"
    },
    {
      "tip": "Add denoised->decode->video between every sampler for debugging",
      "context": "To see where generation breaks down and how settings affect different stages",
      "from": "Instability01"
    },
    {
      "tip": "Use 1x res_2s high step, 2x res_2s low steps with LTX 4-5 on high, 1 on low",
      "context": "For maximum motion while retaining original image",
      "from": "Instability01"
    },
    {
      "tip": "Use LoRA training with popular celebrities for better consistency",
      "context": "When creating character-consistent videos, train LoRA with recognizable celebrities since models know them well and data is available",
      "from": "Nekodificador"
    },
    {
      "tip": "Focus on fewer shots with better quality rather than full scenes",
      "context": "Better to do half a scene with quality shots than full scene with mediocre results",
      "from": "Nekodificador"
    },
    {
      "tip": "Use prompt separation/echoshot formatting for jump cuts",
      "context": "Better success getting jump cuts and hard transitions in WAN 2.2 I2V",
      "from": "DawnII"
    },
    {
      "tip": "Chain WanVideo LoRA select nodes using prev_lora input",
      "context": "When adding multiple LoRAs to workflow",
      "from": "seitanism"
    },
    {
      "tip": "Test LoRA strength between 0.5 and 1.7",
      "context": "For WAN 2.2 with LoRAs, experiment with different strengths in this range",
      "from": "seitanism"
    },
    {
      "tip": "Use start frame to guide masked generation",
      "context": "When using differential diffusion, start frame helps avoid random generation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Manually adjust mask proportions for better results",
      "context": "When using VACE with character replacement, manually adjusting masks in editing software makes huge difference",
      "from": "Nekodificador"
    },
    {
      "tip": "Lock runtime environment for reproducible results",
      "context": "Don't update until you have time to debug, version your runtime with data and workflow",
      "from": "samhodge"
    },
    {
      "tip": "Use simplified Chinese for prompts",
      "context": "When wanting to try Chinese prompts in Wan 2.2, use simplified Chinese in DeepL",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Higher CFG for 5B without acceleration",
      "context": "For 5B model without accelerators, use CFG 3.5 or 5",
      "from": "Juan Gea"
    },
    {
      "tip": "Use dpm++_sde with lightx2v loras",
      "context": "Usually gets better results with lightx2v loras especially",
      "from": "Kijai"
    },
    {
      "tip": "Loras make huge difference for single image gen",
      "context": "When doing single image generation with Wan",
      "from": "Kijai"
    },
    {
      "tip": "Use tiled VAE for 4K resolution generation",
      "context": "Required when generating at 4K resolution",
      "from": "Kijai"
    },
    {
      "tip": "Lower VACE strength to 0.5 for inpainting and rely on text prompt",
      "context": "Works best in some inpainting cases, can also add end frame to video with black mask",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "tip": "Use LightX2V LoRA for better I2V results",
      "context": "Produces better results than Lightning LoRAs for I2V generation",
      "from": "A.I.Warper"
    },
    {
      "tip": "Film grain not really needed for I2V",
      "context": "Based on testing, film grain doesn't add significant value to I2V workflows",
      "from": "Kenk"
    },
    {
      "tip": "Don't use CFG on low noise at all",
      "context": "Keep low noise at cfg 1.0 and use lightx2v or lightning at 1.0",
      "from": "Kijai"
    },
    {
      "tip": "Steps that use CFG should have LoRA disabled or at low strength",
      "context": "When using CFG scheduling on high noise",
      "from": "Kijai"
    },
    {
      "tip": "Use scheduled CFG and LoRA strength together",
      "context": "Better than using 3-4 samplers",
      "from": "Kijai"
    },
    {
      "tip": "Use prefetch_blocks and use_non_blocking for 2x speed",
      "context": "Block swap optimization settings",
      "from": "matatrata"
    },
    {
      "tip": "Don't use multitalk node for short clips",
      "context": "For clips around 5s or less, use normal I2V node",
      "from": "Kijai"
    },
    {
      "tip": "Don't mess with window_size in multitalk",
      "context": "Causes interpolation artifacting at transition points",
      "from": "DawnII"
    },
    {
      "tip": "Load models without LoRAs when using CFG",
      "context": "When using 3-4 samplers with Wan 2.2",
      "from": "DawnII"
    },
    {
      "tip": "Use latent insertion trick for better results",
      "context": "Add input image as extra latent to avoid start being noise",
      "from": "Kijai"
    },
    {
      "tip": "Use PUSA LoRA to preserve original colors",
      "context": "When doing VACE inpainting to maintain reference image colors, though it has big memory impact with GGUF",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "tip": "Work with minimal LoRAs or tweak strengths",
      "context": "LoRAs can change reference too much, better to use lightx2v and PUSA only if wanting to preserve colors",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "tip": "Provide more pose images than needed for UniAnimate",
      "details": "It's okay to give 'too many' dwpose images as input for each window is sliced from that",
      "from": "Kijai"
    },
    {
      "tip": "Use TAESD preview and CFG/LoRA scheduling",
      "context": "For previewing single step results and being able to cancel if generation looks bad",
      "from": "Kijai"
    },
    {
      "tip": "Use prefetch_block and non-blocking memory transfer together",
      "context": "For optimal block swapping performance",
      "from": "Kijai"
    },
    {
      "tip": "Don't use TeaCache with LightX2V",
      "context": "It's either one or the other, not both",
      "from": "Kijai"
    },
    {
      "tip": "Use GGUF instead of plain fp8",
      "context": "Unless using --fast flag, should use GGUF. For --fast use scaled fp8",
      "from": "Kijai"
    },
    {
      "tip": "Set prefetch blocks to 1",
      "context": "One was enough and more didn't make a difference",
      "from": "Kijai"
    },
    {
      "tip": "Use global seed for testing LoRAs effectively",
      "context": "When testing LoRAs, global seed helps with consistency",
      "from": "CJ"
    },
    {
      "tip": "Separate control inputs in VACE",
      "context": "Don't combine pose/depth/lineart into single control - use separate VACE control inputs",
      "from": "Ablejones"
    },
    {
      "tip": "Use --cache-none for memory help",
      "context": "Can help with memory issues but models reload every time if you have slow drive",
      "from": "garbus"
    },
    {
      "tip": "Wrapper + block swap + virtual memory for 16GB systems",
      "context": "Combination can help run larger models on limited RAM systems",
      "from": "hicho"
    },
    {
      "tip": "Balance memory with blocks_to_swap parameter",
      "context": "Key to running bigger models - balance VRAM and RAM usage with blocks_to_swap number",
      "from": "Kijai"
    },
    {
      "tip": "Default to core nodes when sharing workflows",
      "context": "Less that can go wrong, fewer dependencies to keep updated",
      "from": "Kijai"
    },
    {
      "tip": "Use single prompt for context options to maintain consistent motion",
      "context": "When generating long videos with context windows",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Set frame_window_size to 121 when using Skyreels",
      "context": "Skyreels 720p can handle 121 frames vs normal I2V's 81 frames",
      "from": "Kijai"
    },
    {
      "tip": "Use CFG 3.5 in high sampler and lightning lora in low sampler",
      "context": "To get same prompt following as wrapper but with native image quality",
      "from": "Drommer-Kille"
    },
    {
      "tip": "White pixels that completely surround your subject are important for VACE",
      "context": "When creating reference images for character replacement",
      "from": "Ablejones"
    },
    {
      "tip": "Don't add Lightning LoRA to high noise model in 2.2",
      "context": "Prevents high dynamic results and reduces model effectiveness",
      "from": "MysteryShack"
    },
    {
      "tip": "Don't use torch.compile with 14B model",
      "context": "On smaller models it reduces VRAM use, but on 14B it just slows it down",
      "from": "Kijai"
    },
    {
      "tip": "Use depth control only for better backgrounds, avoid aggressive canny",
      "context": "Canny can create unwanted background artifacts",
      "from": "hicho"
    },
    {
      "tip": "For single frame VACE, use 8 steps instead of 4",
      "context": "4 steps is not enough for single frame generation",
      "from": "hicho"
    },
    {
      "tip": "Mask the image area when using crop&stitch for overlays",
      "context": "More effective than relying on Wan's natural overlay handling",
      "from": "Valle"
    },
    {
      "tip": "Trigger words in LoRAs can cause unwanted text/brand generation",
      "context": "Model interprets trigger words as brand names to render",
      "from": "Kenk"
    },
    {
      "tip": "Use image with mouth slightly open for better InfiniteTalk generation",
      "context": "Helps lip sync generation significantly",
      "from": "3DBicio"
    },
    {
      "tip": "Don't use both pose and canny together in VACE",
      "context": "Creates canny imaging of stick skeleton which isn't needed",
      "from": "mdkb"
    },
    {
      "tip": "Use NAG to remove something specific rather than generic negative prompting",
      "context": "NAG works best for targeted removal",
      "from": "Kijai"
    },
    {
      "tip": "Increase NAG alpha and/or scale if effect is too weak",
      "context": "Default NAG settings can be adjusted for stronger effect",
      "from": "Kijai"
    },
    {
      "tip": "Use cached text encoder node directly with NAG for RAM issues",
      "context": "Connect text embeds to text embeds and negative embeds to nag embeds",
      "from": "Kijai"
    },
    {
      "tip": "Mix controlnets and chain VACE together for eye line correction",
      "context": "Use different strengths on dwpose, depth, ref image encoders",
      "from": "JalenBrunson"
    },
    {
      "tip": "Use Set/Get nodes instead of Anything Everywhere",
      "context": "Anything Everywhere is buggy and confusing, set/get is more reliable",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "tip": "For video upscaling challenges",
      "context": "Video upscaling is very difficult and ruins character consistency",
      "from": "Dream Making"
    },
    {
      "tip": "Removing WanVideo Long I2V Multi node for speed",
      "context": "Can speed up inference for short 3-5 second videos, but node is necessary for proper InfiniteTalk function",
      "from": "Mancho"
    },
    {
      "tip": "Use fp16 instead of bf16 for better results",
      "context": "Reference in VACE works better with fp16, and can help with VRAM issues",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use wan.video for testing - it's much faster (3-5 minutes)",
      "context": "Don't use gradio for testing",
      "from": "ZeusZeus"
    },
    {
      "tip": "Don't use both beta sigmas and beta scheduler",
      "context": "Applies beta sigma conversion twice",
      "from": "Kijai"
    },
    {
      "tip": "Use 0.6/0.6 for alpha/beta values",
      "context": "Default recommended values for scheduling",
      "from": "Ablejones"
    },
    {
      "tip": "Can offload everything to reduce VRAM usage",
      "context": "For the heavier S2V model",
      "from": "Kijai"
    },
    {
      "tip": "Try 6 or 8 steps instead of 4 for better results",
      "context": "4-step LoRAs aren't locked to 4 steps, very low in general",
      "from": "JohnDopamine"
    },
    {
      "tip": "Best not to mix LoRA loader with setLoRAs",
      "context": "When using LoRAs in workflows",
      "from": "Kijai"
    },
    {
      "tip": "Use muting instead of bypassing nodes for get/set issues",
      "context": "When ComfyUI 'fixed' get/set node behavior",
      "from": "phazei"
    },
    {
      "tip": "Minimize custom nodes for better stability",
      "context": "Too many custom nodes create dependency conflicts and workflow sharing issues",
      "from": "phazei"
    },
    {
      "tip": "Use --use-quad-cross-attention instead of --use-sage-attention",
      "context": "For AMD users, safer to start ComfyUI this way then use Kijai's sage attention node",
      "from": "patientx"
    },
    {
      "tip": "Use separate block swap settings for high and low samplers",
      "context": "When using Fun 2.2 workflow with memory constraints",
      "from": "scf"
    },
    {
      "tip": "Lower overlap on low sampler for speed",
      "context": "16 overlap fine for low, up to 48 may be needed for high noise for better blending",
      "from": "Kijai"
    },
    {
      "tip": "Don't use single sampler for longer generations",
      "context": "Not supposed to use single sampler for extended video generation",
      "from": "Kijai"
    },
    {
      "tip": "Use two-pass workflow for animation + lipsync",
      "context": "You can do that in two pass, quite slow but it works: 1st pass T2V/VACE, 2nd pass I2V/InfiniteTalk/FantasyPortrait",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use FantasyPortrait + InfiniteTalk together for better lipsync",
      "context": "I mean you can already use FantasyPortrait + InfiniteTalk together in the wrapper to get really good lipsync",
      "from": "Kijai"
    },
    {
      "tip": "Aim for 95% VRAM usage for optimal performance",
      "context": "yeah that can be more optimal, something like 95% used is generally safe",
      "from": "Kijai"
    },
    {
      "tip": "Stay under 832 resolution to avoid slowdowns",
      "context": "I found out at least in my case going over 832 makes everything extra slow",
      "from": "patientx"
    },
    {
      "tip": "Use lightx2v I2V instead of T2V in some cases",
      "context": "i was using lightx2v i2v.. as kijai said, this is a mix of all existing techs",
      "from": "Kenk"
    },
    {
      "tip": "Use DPM++ SDE Beta sampler in wrapper to match native Euler Beta results",
      "context": "When trying to match wrapper results to native",
      "from": "Draken"
    },
    {
      "tip": "Use merged LoRAs in wrapper for better compatibility",
      "context": "When custom LoRAs aren't working properly",
      "from": "Kijai, mamad8"
    },
    {
      "tip": "Unmerged LoRAs use more VRAM",
      "context": "When managing VRAM resources",
      "from": "Kenk"
    },
    {
      "tip": "Use multiple VACE encoders instead of image blend to reduce skeleton artifacts",
      "context": "When working with pose control",
      "from": "JalenBrunson"
    },
    {
      "tip": "Disable LightX LoRAs if experiencing color degradation in long videos",
      "context": "For InfiniteTalk videos longer than 1 minute",
      "from": "boorayjenkins"
    },
    {
      "tip": "Filter side bones when using Sapiens for pose control",
      "context": "To avoid headphone and clavicle artifacts in VACE",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use only first step with pose condition multiplied by 0.5",
      "context": "To reduce teeth artifacts and improve pose control",
      "from": "Kijai"
    },
    {
      "tip": "Composite original unmasked area back over video after generation",
      "context": "For V2V with masked areas to preserve unedited regions",
      "from": "ArtOfficial"
    },
    {
      "tip": "Keep LoRA strength much lower when using LightX2V",
      "context": "Use LightX2V at 0.3 strength for better results",
      "from": "gordo"
    },
    {
      "tip": "Prompt 'she opens her mouth as she sings' for better S2V results",
      "context": "To improve lip sync quality in speech-to-video generation",
      "from": "hicho"
    },
    {
      "tip": "Use PUSA for maintaining image likeness across context windows",
      "context": "Better preservation of reference images in I2V workflows",
      "from": "DawnII"
    },
    {
      "tip": "Use 1.5 lightx2v strength instead of 1.0 for better results",
      "context": "1.0 lightx2v doesn't work properly due to being too different from base model",
      "from": "Kijai"
    },
    {
      "tip": "Feed same input image into extra latent via VAE Encode for character consistency",
      "context": "When using FFLF method for maintaining character details",
      "from": "DawnII"
    },
    {
      "tip": "Use 10-15 seconds of audio for voice cloning with Chatterbox",
      "context": "For best voice cloning results with TTS systems",
      "from": "Juampab12"
    },
    {
      "tip": "Convert audio to video first, then link audio input for frame calculation",
      "context": "To automatically calculate needed frames based on audio input for S2V",
      "from": "hicho"
    },
    {
      "tip": "Use Linux for much better memory efficiency",
      "context": "64GB RAM on Linux vs 128GB on Windows for same performance level",
      "from": "Kijai"
    },
    {
      "tip": "Use default Kijai workflow with magref model for best Infinite Talk results",
      "context": "When using Infinite Talk for lip sync",
      "from": "asd"
    },
    {
      "tip": "Use QwenVL with system prompt and video input to generate mmaudio prompts",
      "context": "When needing to prompt audio events at specific times in video",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Provide more overlap frames for video continuation",
      "context": "When using Fun or VACE for continuation - with only 1 frame it doesn't have enough info",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use uni3c for camera control",
      "context": "Can record camera movements with iPhone and apply to AI generations",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "tip": "Always use offload_device for model loading",
      "context": "Should always be offload_device really to avoid memory issues",
      "from": "Kijai"
    },
    {
      "tip": "Use larger overlap for context windows with lots of movement",
      "context": "Can happen with lots of movement or if camera moves, larger overlap can help",
      "from": "Kijai"
    },
    {
      "tip": "Balance Framepack settings carefully",
      "context": "Need to find balance that gives enough motion but doesn't burn the image, too many steps can be bad especially with lightx2v",
      "from": "Kijai"
    },
    {
      "tip": "Keep shift low with Framepack",
      "context": "Shift should be pretty low when using Framepack",
      "from": "Kijai"
    },
    {
      "tip": "Use LCM sampler with Framepack",
      "context": "Some samplers destroy Framepack quickly, LCM seemed better",
      "from": "Kijai"
    },
    {
      "tip": "Save workflow images at 1:1 scale",
      "context": "Click image twice to get 1:1 view then save or it won't pick up the JSON file",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "tip": "Generate audio first then load vs running with each video gen",
      "context": "Better to make the audio first then load it vs running it with each video generation",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use NAG instead of CFG for better concept avoidance",
      "context": "Especially useful with LightX distillations and specific unwanted elements",
      "from": "Nekodificador"
    },
    {
      "tip": "Boost LoRA strength for mouth movement in S2V",
      "context": "Similar to 2.2 high noise requirements",
      "from": "Kijai"
    },
    {
      "tip": "Use stride on high noise side only with 2.2",
      "context": "Makes things jittery but blends windows better, low noise can fix it",
      "from": "Kijai"
    },
    {
      "tip": "For VACE inpainting, fill RGB areas with 128,128,128 gray",
      "context": "When using alpha masks for inpainting",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use FantasyPortrait on first step only at lower strength",
      "context": "When combining with S2V for less strict control",
      "from": "Kijai"
    },
    {
      "tip": "Don't use speedup tricks with Wan 2.2",
      "context": "Any speedup tricks kill all the quality in 2.2",
      "from": "MysteryShack"
    },
    {
      "tip": "Use full CFG with Wan 2.2",
      "context": "CFG to 1 for too many steps kills prompt adherence",
      "from": "MysteryShack"
    },
    {
      "tip": "Use CineScale LoRA at 0.7 strength",
      "context": "Full strength causes artifacting, 0.7 avoids this",
      "from": "DawnII"
    },
    {
      "tip": "Higher denoise (0.6-0.7) works with CineScale",
      "context": "Unlike UltraWan which artifacts at higher denoise",
      "from": "DawnII"
    },
    {
      "tip": "Use | symbol in WanWrapper for prompt batches",
      "context": "WanWrapper uses the | symbol to make a prompt batch that can be used to the content window",
      "from": "Dita"
    },
    {
      "tip": "Generate start image and use that instead of pure T2V",
      "context": "When wanting long generations with character consistency",
      "from": "Kijai"
    },
    {
      "tip": "Different techniques for different shots in complex projects",
      "context": "Some shots work just out the box with VACE+DWPose and inpainting, others require reference frames or depth with schedule strength",
      "from": "Nekodificador"
    },
    {
      "tip": "3 prompts with more frames can be faster than 4 prompts with fewer frames",
      "context": "3 prompts x 81 frames (14:36) was faster than 4 prompts x 65 frames (18:05) due to overlap processing",
      "from": "Dever"
    },
    {
      "tip": "Right click image and open in browser to download with metadata",
      "context": "When ComfyUI workflow metadata isn't preserved in direct download",
      "from": "Juampab12"
    },
    {
      "tip": "Google parts of model name to find download links",
      "context": "When looking for specific model downloads",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    }
  ],
  "news": [
    {
      "update": "Wan 2.1 LoRA trainer extension found",
      "details": "ComfyUI extension for training Wan 2.1 LoRAs discovered",
      "from": "Ryzen"
    },
    {
      "update": "Deforum Wan integration being explored",
      "details": "Huemin looking into using Wan as image model for Deforum animations",
      "from": "NebSH"
    },
    {
      "update": "WAN 2.2 VACE test version available",
      "details": "Test version of VACE for WAN 2.2 released on HuggingFace",
      "from": "avataraim"
    },
    {
      "update": "SageAttention3 early access available",
      "details": "Early access version available but quality is poor, public release not yet available",
      "from": "Kijai"
    },
    {
      "update": "Text encoder caching improvements",
      "details": "New node completely unloads T5 after use, leaves nothing in VRAM or RAM",
      "from": "Kijai"
    },
    {
      "update": "Bug fix for EchoShot disk cache",
      "details": "Fixed bug where echoshot wouldn't activate if prompt was loaded from cache",
      "from": "Kijai"
    },
    {
      "update": "New T5 unload node released",
      "details": "New node completely unloads T5 after use to save RAM, useful for high VRAM usage scenarios",
      "from": "Kijai"
    },
    {
      "update": "Context options support for prompt separation",
      "details": "Can now separate prompts with | symbol for different scenes in context windows, only works with wrapper encode node",
      "from": "Kijai"
    },
    {
      "update": "NAG support added to text encoding",
      "details": "New separate output for NAG (Negative Attention Guidance) support added to text encoding nodes",
      "from": "Kijai"
    },
    {
      "update": "Qwen3-coder 30B-a3b now available",
      "details": "Latest version of the code generation model",
      "from": "piscesbody"
    },
    {
      "update": "Context windows now work with 5B model",
      "details": "Kijai made it run with 5B",
      "from": "Kijai"
    },
    {
      "update": "New text encode node with NAG support",
      "details": "Fixed disk cache issue",
      "from": "Kijai"
    },
    {
      "update": "First and last frame capability mentioned in recent update",
      "details": "Functionality available in VACE and being developed for official release",
      "from": "AJO"
    },
    {
      "update": "ChatGPT custom GPT for Wan 2.2 prompts available",
      "details": "Trained on official examples and formula document, can write T2V prompts or create prompts from images",
      "from": "\u2727\u0e05\u0e42\u0e51\u2180\u11ba\u2180 \u0e51\u0e43\u0e05\u2727PookieNumnums"
    },
    {
      "update": "Test VACE model available for Wan 2.2",
      "details": "Hugging Face repo with test version",
      "from": "BondoMan"
    },
    {
      "update": "Qwen 2.5 integrated into Wan wrapper",
      "details": "Local prompt extension capability with zero memory usage after completion",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.1 Audio-14B-720P branch mentioned",
      "details": "Separate model for audio conditioning, lip-sync, talking head animation",
      "from": "thaakeno"
    },
    {
      "update": "Wan 2.2 mentioned but 2.1 version never seen",
      "details": "They mentioned 2.1 months ago, now 2.2, but 2.1 never appeared",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI native support for Wan 2.2 FLF2V",
      "details": "Official ComfyUI blog announced native support",
      "from": "BarleyFarmer"
    },
    {
      "update": "WanImageToVideo node added for FLF support",
      "details": "New node available in updated wrapper for first-last frame functionality",
      "from": "Kijai"
    },
    {
      "update": "VACE 2.2 anticipated release",
      "details": "Community excited for VACE 2.2 release, expected soon",
      "from": "pom"
    },
    {
      "update": "Wan 2.3 predicted by end of 2025",
      "details": "Speculation about next version timeline",
      "from": "Ryzen"
    },
    {
      "update": "FastWan for 5B model almost ready",
      "details": "Speed optimization LoRA for 5B model development visible in GitHub commits",
      "from": "Screeb"
    },
    {
      "update": "First-Last-Frame support added to Wan 2.2 native",
      "details": "FLF functionality now available in native ComfyUI, just needs latest update",
      "from": "Rainsmellsnice"
    },
    {
      "update": "Civitai LoRA takedowns affecting community",
      "details": "Many LoRAs removed from Civitai, community looking for alternatives",
      "from": "DaxRedding"
    },
    {
      "update": "DFloat11 compression available for Wan2.2",
      "details": "32% size reduction while maintaining bit-identical outputs, allows 24GB GPU generation",
      "from": "scf"
    },
    {
      "update": "Phr00t released new Rapid AllInOne v2",
      "details": "Claims more 2.2 features in v2, though still more 2.1 than 2.2",
      "from": "hicho"
    },
    {
      "update": "FastVideo 2.2 coming soon",
      "details": "Next version of FastVideo LoRA in development",
      "from": "Draken"
    },
    {
      "update": "FPS1 LoRA released for experimental storyboarding",
      "details": "Generates videos at 1 FPS instead of 16 FPS for creating longer scenes and storyboard frames",
      "from": "mamad8"
    },
    {
      "update": "LoRA timestep scheduling feature added",
      "details": "Added to WanVideoWrapper, only works when unmerged, allows per-step control and curves",
      "from": "Kijai"
    },
    {
      "update": "GGUF compatibility fixed",
      "details": "Should work with GGUF models now after latest update",
      "from": "Kijai"
    },
    {
      "update": "bf16 I2V models uploaded",
      "details": "High and Low bf16 models available on HuggingFace",
      "from": "Kijai"
    },
    {
      "update": "AI-TOOLKIT supports Wan 2.2 5B training",
      "details": "Very good official template available in RunPod",
      "from": "Drommer-Kille"
    },
    {
      "update": "New First-Last-Frame workflow available",
      "details": "Can be found under workflow templates flf2v after updating ComfyUI",
      "from": "Rainsmellsnice"
    },
    {
      "update": "Wan 2.2 Lightning LoRAs released",
      "details": "Official lightning LoRAs for Wan 2.2 released but had alpha value issues initially",
      "from": "yi"
    },
    {
      "update": "720P distilled I2V model released",
      "details": "LightX2V Wan2.1-I2V-14B-720P-StepDistill-CfgDistill released",
      "from": "DawnII"
    },
    {
      "update": "Wan 2.2 Lightning empty repository created",
      "details": "ModelTC/Wan2.2-Lightning repository created but initially empty",
      "from": "yi"
    },
    {
      "update": "New Wan 2.2 Lightning LoRAs released by Kijai",
      "details": "Fixed versions that work at 1.0 strength instead of 0.125, available on HuggingFace",
      "from": "Juampab12"
    },
    {
      "update": "Fake 720p I2V LoRAs uploaded then removed",
      "details": "720p I2V LoRAs were uploaded but were actually just 480p versions adapted to model diffs",
      "from": "screwfunk/Karo"
    },
    {
      "update": "Qwen image model released",
      "details": "New Qwen 20B T2I model dropped today from same company as Wan team",
      "from": "TK_999/Ada"
    },
    {
      "update": "Wan 2.2 Lightning LoRAs released",
      "details": "New 4-step distillation LoRAs available for high and low noise passes",
      "from": "Ant"
    },
    {
      "update": "5B model first/last frame support added",
      "details": "Kijai updated WanVideoWrapper to support first/last frame with 5B model",
      "from": "Kijai"
    },
    {
      "update": "FastWan2.2-TI2V-5B-FullAttn-Diffusers released",
      "details": "New full attention version available on HuggingFace",
      "from": "yi"
    },
    {
      "update": "Lightning Distill LoRAs released for Wan 2.2",
      "details": "LightX team released 4-step distill LoRA for 2.2 14B model",
      "from": "DawnII"
    },
    {
      "update": "FastWan 5B model converted to ComfyUI",
      "details": "Kijai converted FastVideo team's distilled 5B model to ComfyUI format",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.2 VACE expected in 1-2 months",
      "details": "Official Wan 2.2 VACE system expected to release in 1-2 months",
      "from": "hicho"
    },
    {
      "update": "New image guide updated",
      "details": "Alibaba updated their official image guide documentation",
      "from": "hicho"
    },
    {
      "update": "Grok Imagine coming to Premium subscribers",
      "details": "All Premium subscribers should have access to Grok Imagine for super fast picture & video creation",
      "from": "mdkb"
    },
    {
      "update": "New Lightning lora commit",
      "details": "New commit 7 mins ago with sekov1 version, removed alpha=8 so strength of 1",
      "from": "Doctor Shotgun"
    },
    {
      "update": "FastWan models released",
      "details": "FastVideo released FastWan2.2-TI2V-5B and FastWan2.1-T2V-1.3B variants",
      "from": "QANICS\ud83d\udd50"
    },
    {
      "update": "Lightning lora updated with alpha keys",
      "details": "Seko version added alpha keys so it can be used at 1.0 strength",
      "from": "Kijai"
    },
    {
      "update": "New rank64 lightning LoRAs released",
      "details": "Functionally identical to Kijai's provided lightning LoRAs, confirmed through testing",
      "from": "Kijai"
    },
    {
      "update": "Lightning 2.2 LoRAs released but with issues",
      "details": "New Lightning models uploaded but they don't work as well as previous versions and have timestep scheduling requirements",
      "from": "Kijai"
    },
    {
      "update": "LightX2V commit suggests 2.2 self-forcing distill",
      "details": "GitHub commit matches naming scheme for 2.1 self-forcing distills, though config still uses 2.1",
      "from": "JohnDopamine"
    },
    {
      "update": "Context sliding workflow available",
      "details": "Kijai shared context windowing workflow that samples model multiple times per step and blends results",
      "from": "Kijai"
    },
    {
      "update": "Torch compile support added to VAE decode",
      "details": "Recent update allows torch compile for VAE operations but can cause issues if auto-connected",
      "from": "Kijai"
    },
    {
      "update": "Official context window support in development",
      "details": "Kosinkadink started working on official context window support within ComfyUI in the context_windows branch",
      "from": "Kosinkadink"
    },
    {
      "update": "Wan 2.2 paper released",
      "details": "Official paper for Wan 2.2 has been published",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "update": "TensorRT optimization mentioned",
      "details": "Real-time Wan generation being discussed with TensorRT",
      "from": "Cubey"
    },
    {
      "update": "Gen4-Aleph already on Runway's API",
      "details": "Runway normally waits months before putting new tech on API",
      "from": "Draken"
    },
    {
      "update": "QwenImage latent support added to WAN wrapper",
      "details": "Can now use QwenImage outputs directly in wrapper and decode with native if faster",
      "from": "Kijai"
    },
    {
      "update": "Google announcing Android vibecoding in realtime generative world",
      "details": "Expected in 2-3 days according to speculation",
      "from": "Nekodificador"
    },
    {
      "update": "On-device inference getting serious investment",
      "details": "Client mentioned increased investment in local AI inference",
      "from": "fredbliss"
    },
    {
      "update": "VACE 2.1 can be merged with Wan 2.2 models",
      "details": "Community experimenting with merging VACE 2.1 components into 2.2 for reference following",
      "from": "Kijai"
    },
    {
      "update": "Juampab12 has updates coming soon",
      "details": "Mentioned having new updates to share that are looking even better",
      "from": "Juampab12"
    },
    {
      "update": "Waver 12B model announced",
      "details": "Claims to be open source, performs at Veo3 level, uses 32B text encoder, DiT architecture with flow matching",
      "from": "yi"
    },
    {
      "update": "Kijai uploaded fp8_e5m2 5B model",
      "details": "New quantized version available on HuggingFace",
      "from": "phazei"
    },
    {
      "update": "ComfyUI frontend package 1.25.5 released",
      "details": "Latest version works with updated VHS nodes",
      "from": "Kijai"
    },
    {
      "update": "New Lightning LoRA versions released",
      "details": "T2V 1.1 and I2V versions available, with fixed alpha issues but not ComfyUI key compatibility",
      "from": "Kijai"
    },
    {
      "update": "Adaptive rank LoRA has highest rank at 256",
      "details": "Available as the top-tier Lightning LoRA option",
      "from": "Kijai"
    },
    {
      "update": "Lightning 2.2 I2V LoRAs released",
      "details": "New lightning LoRAs available for Wan 2.2, but causing various issues",
      "from": "Multiple users"
    },
    {
      "update": "MagCache officially supports Wan 2.2",
      "details": "1.5-2x speedup now available for Wan 2.2",
      "from": "NebSH"
    },
    {
      "update": "ATI motion transfer trajectories tool released",
      "details": "New script for creating motion transfer trajectories from video",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.2 Fun Control released",
      "details": "New control model available at https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-Control",
      "from": "DawnII"
    },
    {
      "update": "Context window support pull request submitted",
      "details": "PR #9238 for ComfyUI to make it easier to experiment with context windows",
      "from": "Kosinkadink"
    },
    {
      "update": "New Lightning I2V LoRAs available",
      "details": "Kijai converted versions available with different strength settings",
      "from": "screwfunk"
    },
    {
      "update": "Lightning LoRA versions updated",
      "details": "Lightning 1.1 version released but may have quality issues compared to 1.0",
      "from": "WorldX"
    },
    {
      "update": "Wan 2.2 Fun Control and InP models released",
      "details": "New models supporting control conditions (Canny, Depth, Pose, MLSD) and temporal inpainting, supporting up to 81 frames at multiple resolutions",
      "from": "CJ"
    },
    {
      "update": "Kijai released fp8 scaled versions of Fun models",
      "details": "Available at 14.5GB instead of 28.6GB for better VRAM compatibility",
      "from": "Kijai"
    },
    {
      "update": "5B version of Fun models coming",
      "details": "Based on code branch analysis, a 5B parameter version is expected",
      "from": "DawnII"
    },
    {
      "update": "SkyReels A3 demo revealed",
      "details": "New model showing impressive singing and camera control capabilities, paper available on arXiv",
      "from": "DawnII"
    },
    {
      "update": "Wan Chattable Knowledge Base updated with 2.2 conversations",
      "details": "NotebookLM knowledge base now includes Wan 2.2 discussion, accessible via provided link",
      "from": "Nathan Shipley"
    },
    {
      "update": "WAN wrapper nodes updated with fixes",
      "details": "Fixed tensor size issues in Fun Control, removed unused clip vision connections",
      "from": "Kijai"
    },
    {
      "update": "Automatic CFG implementation added",
      "details": "Added automatic CFG calculation per step based on cond/uncond ratio, following arxiv.org/abs/2508.03442",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.2 Fun A14B Control models released",
      "details": "New control models available on HuggingFace alibaba-pai repo",
      "from": "Dannhauer80"
    },
    {
      "update": "Possibility of 8-step Lightning model",
      "details": "Discussion on GitHub about potential 8-step model development",
      "from": "DawnII"
    },
    {
      "update": "DiffSynth Studio commit supports arbitrary sequence length",
      "details": "Recent commit adds support for variable length sequences",
      "from": "fredbliss"
    },
    {
      "update": "Kijai updated FP8 fast support",
      "details": "Overhauled loader node, scaled + fast works with merged LoRAs, faster on 5090 than 4090",
      "from": "Kijai"
    },
    {
      "update": "Torch 2.8.0 now in stable release",
      "details": "Kijai updated to torch 2.8.0 and latest triton, fp8_fast seems faster than before",
      "from": "Kijai"
    },
    {
      "update": "New FastWan2.2-TI2V-5B-FullAttn model released",
      "details": "Available on HuggingFace in diffusers format",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "update": "FastWan LoRA extracted for 5B model",
      "details": "Available in Kijai's repository",
      "from": "Kijai"
    },
    {
      "update": "Radial attention now supports WAN 2.2",
      "details": "Updated attention code with 0 dense timesteps and decay factor of 1",
      "from": "MysteryShack"
    },
    {
      "update": "ComfyUI training nodes RFC in progress",
      "details": "RFC discussion for generic training node implementation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "Qwen Image Lightning released",
      "details": "8-step distilled model for Qwen image generation, works with 4 steps too, doesn't ruin style like Wan version",
      "from": "hicho"
    },
    {
      "update": "VACE 2.2 developers working on update",
      "details": "Kijai mentioned hearing that VACE developers are working on an update, though not confirmed officially",
      "from": "JohnDopamine"
    },
    {
      "update": "ComfyUI now in top CG academia/companies",
      "details": "ComfyUI course at SIGGRAPH 2025 conference, showing mainstream academic adoption",
      "from": "Christian Sandor"
    },
    {
      "update": "New Fun 2.2 control model available",
      "details": "5B controlnet model with improved capabilities but lost reference input support",
      "from": "Kijai"
    },
    {
      "update": "PUSA fix pushed to repository",
      "details": "Fix for something that was broken in previous commit",
      "from": "Kijai"
    },
    {
      "update": "DeepVerse world model released",
      "details": "Sunday release of new world model, though lightweight with 4 FPS on A800",
      "from": "DawnII"
    },
    {
      "update": "FastWan 5B released",
      "details": "Available for few days, works with controlnet",
      "from": "Kijai"
    },
    {
      "update": "Official Flash version announced",
      "details": "Claims 12x faster than Wan 2.1, release imminent",
      "from": "\u9752\u9f8d\u8056\u8005@bdsqlsz"
    },
    {
      "update": "5B controlnet support added",
      "details": "Traditional depth controlnet now works with 5B model",
      "from": "Kijai"
    },
    {
      "update": "Skyreels LoRA extracted and available",
      "details": "Can help break 121 frame loop limitation in 2.2",
      "from": "Kijai"
    },
    {
      "update": "Official Lightning LoRA announced",
      "details": "Official one was announced today, maybe best to wait for that",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.2 Fun A14B InP in training",
      "details": "Model is currently in training phase",
      "from": "yi"
    },
    {
      "update": "Rapid AllInOne 2.2 v6 released",
      "details": "v6 came today, merges both high/low models into one",
      "from": "Drommer-Kille"
    },
    {
      "update": "Prefetch option added to block swap",
      "details": "New prefetch option available for block swap with debug option to confirm transfer speeds",
      "from": "Kijai"
    },
    {
      "update": "FAL now has Wan 2.2 14B LoRA trainer",
      "details": "Available at https://fal.ai/models/fal-ai/wan-22-image-trainer",
      "from": "Drommer-Kille"
    },
    {
      "update": "New Stand-In identity control method released",
      "details": "Lightweight plug-and-play identity control for video generation, 600MB LoRA",
      "from": "s2k"
    },
    {
      "update": "Block swap optimization implemented",
      "details": "Recent changes make block swapping faster than before by a good margin",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "Matrix Game 2.0 from Skyreels released",
      "details": "Video + camera control model, feels like 'video model, half world model'",
      "from": "JohnDopamine"
    },
    {
      "update": "Stand-In LoRA released",
      "details": "600MB LoRA for face consistency, works like ID adapter, runs for one step using KV cache",
      "from": "Kijai"
    },
    {
      "update": "StableAvatar released from Microsoft Asia",
      "details": "New lip-sync solution, appears to be trained on Fun model",
      "from": "DawnII"
    },
    {
      "update": "Fantasy Portrait weights available",
      "details": "FantasyPortrait model weights now available on HuggingFace",
      "from": "NebSH"
    },
    {
      "update": "Multiple lip-sync releases in 48h",
      "details": "Fantasy talking, StableAvatar, Pika new lipsync, and StandIn lora all released recently",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "Stand-in LoRA now compatible with GGUF models",
      "details": "Latest update fixed using GGUF models with Stand-in, allows SageAttention without errors",
      "from": "garbus"
    },
    {
      "update": "FFv1 support merged into VideoHelperSuite",
      "details": "Allows lossless storage of picture collections as videos with workflow as mkv files",
      "from": "thaakeno"
    },
    {
      "update": "A3 (Skyworks) confirmed no plans to release in open source",
      "details": "Contact with A3 team confirmed they won't open source the model",
      "from": "NebSH"
    },
    {
      "update": "LightX2V team acknowledged aesthetic issues in Wan 2.2 Lightning",
      "details": "Team responded to feedback about aesthetic problems, may release 8-step version",
      "from": "DawnII"
    },
    {
      "update": "New Stand-In reference model released",
      "details": "Subject replacement model for Wan video generation",
      "from": "Purz"
    },
    {
      "update": "Wan 2.2 Fun Camera model released",
      "details": "Posted 8 hours ago, provides camera control capabilities for Wan 2.2",
      "from": "Kijai"
    },
    {
      "update": "Fun 2.2 GGUF quantizations available",
      "details": "GGUF quantized versions of Fun 2.2 models are now available",
      "from": "DawnII"
    },
    {
      "update": "Better sing performance models to be released",
      "details": "Pretrained models with improved singing/lip sync performance are planned for release",
      "from": "NebSH"
    },
    {
      "update": "Genie 3 local release expected next week",
      "details": "Around Genie 2 level quality expected for local deployment",
      "from": "Juampab12"
    },
    {
      "update": "Three new features released: Fun 2.2 camera control, Stand-In reference LoRA for 2.1 14B, and FantasyPortrait face motion transfer",
      "details": "Multiple new capabilities added to the ecosystem",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI started implementing context windows natively for Wan",
      "details": "Native ComfyUI support being developed",
      "from": "Lodis"
    },
    {
      "update": "Fun models now available in native ComfyUI in latest update",
      "details": "Official ComfyUI integration",
      "from": "Drommer-Kille"
    },
    {
      "update": "Released Wan 14B 2.2 version of goldenboy retro anime style LoRA",
      "details": "Includes detailed training process documentation",
      "from": "crinklypaper"
    },
    {
      "update": "Stand-in workflow updated and pushed to main",
      "details": "Mostly working, attention masking for multiple people not added yet",
      "from": "Kijai"
    },
    {
      "update": "Fantasy Portrait updated with bug fixes",
      "details": "Fixed variable naming error (z to x) and added progress bar for long encodes",
      "from": "Kijai"
    },
    {
      "update": "Fun 2.2 example workflow updated",
      "details": "Fixed issues with 153 input frames causing overblown first frame",
      "from": "Kijai"
    },
    {
      "update": "Alibaba released camera control model for Wan 2.2",
      "details": "Wan2.2-Fun-A14B-Control-Camera released recently, allows camera control via nodes instead of prompts",
      "from": "Lodis"
    },
    {
      "update": "GGUF versions available for Wan 2.2 Fun models",
      "details": "QuantStack released GGUF versions, Fun INP works but Camera Control has issues",
      "from": "Daflon"
    },
    {
      "update": "WanVideoWrapper updated to fix dependency issues",
      "details": "Kijai updated to catch exceptions and fix FantasyPortrait dependency problems",
      "from": "Kijai"
    },
    {
      "update": "Added bbox and landmark detection visualization to Fantasy Portrait",
      "details": "New visualization features for 222 keypoints detection, less guesswork if it's working",
      "from": "Kijai"
    },
    {
      "update": "Added projection scaling controls for Fantasy Portrait",
      "details": "Can now scale mouth, emotion, and adapter projections separately",
      "from": "Kijai"
    },
    {
      "update": "Fantasy Portrait branch not merged to main yet",
      "details": "Need to switch branches to access Fantasy Portrait functionality",
      "from": "A.I.Warper"
    },
    {
      "update": "Added warning detection for fp8e4nv incompatibility with RTX 3090",
      "details": "Will warn users about quantization compatibility issues",
      "from": "Kijai"
    },
    {
      "update": "New denoise method research paper released",
      "details": "Arxiv paper on potentially faster video generation algorithms",
      "from": "shockgun"
    },
    {
      "update": "Wan 2.2 Flash teased by Alibaba",
      "details": "Question about whether it will be open sourced",
      "from": "Cseti"
    },
    {
      "update": "Tencent dipping into Wan with ToonComposer",
      "details": "New development in the Wan ecosystem",
      "from": "DawnII"
    },
    {
      "update": "Dino v3 announcement",
      "details": "New version released",
      "from": "LarpsAI"
    },
    {
      "update": "i2v-Flash officially released",
      "details": "Official release from Alibaba, appears to be paid only initially",
      "from": "642326806678077441"
    },
    {
      "update": "Context windows now available in native Wan implementation",
      "details": "Manual nodes available for testing, with hooks for ContextHandler",
      "from": "Kosinkadink"
    },
    {
      "update": "ToonComposer from Tencent using Wan model",
      "details": "Professional implementation for keyframe animation, supports inbetween frames",
      "from": "GOD_IS_A_LIE"
    },
    {
      "update": "Flash-attn v2 wheels released for PyTorch 2.8",
      "details": "Fixes compatibility issues that previously required uninstalling",
      "from": "phazei"
    },
    {
      "update": "StableAvatar finetuning and lora training codes released",
      "details": "Released finetuning codes and lora training/finetuning codes for Wan2.1-Fun-V1.1-1.3B-InP, supports infinite-length videos at 480x832, 832x480, or 512x512 resolution",
      "from": "NebSH"
    },
    {
      "update": "WanFM inference optimization method discovered",
      "details": "New method does start-to-end prediction, then end-to-start prediction and blends them together each step, but takes twice the sampling time",
      "from": "Kijai"
    },
    {
      "update": "Fantasy Portrait branch merged into main",
      "details": "Main branch is now more recent, deleted the other branch to avoid confusion",
      "from": "Kijai"
    },
    {
      "update": "Nunchaku coming to Wan for 3+ months",
      "details": "Expected to provide speed improvements similar to what they did for Flux",
      "from": "Kijai"
    },
    {
      "update": "New WanFM (Frame Morphing) sampling method released",
      "details": "Model-agnostic bidirectional sampling method that samples forward and reverse on each step",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI frontend changes affect bypassed nodes",
      "details": "Recent updates now pass data through bypassed/muted nodes, breaking existing workflows",
      "from": "phazei"
    },
    {
      "update": "Fixed RoPE implementation for WanFM",
      "details": "Kijai fixed the RoPE reverse issue that was causing jittery results on low noise side",
      "from": "Kijai"
    },
    {
      "update": "Fixed Fun-Control models for fp8 compatibility",
      "details": "Updated models on HF repo to fix ref_conv layer issue with fp8 scaled models",
      "from": "Kijai"
    },
    {
      "update": "Phantom VACE fp16 full weights model released",
      "details": "Piblarg shared full fp16 model for GGUF conversion",
      "from": "Piblarg"
    },
    {
      "update": "Matrix Game 2.0 interactive model based on Wan released",
      "details": "Interactive model from Skywork based on Wan using self-forcing training technique, trained on GTA",
      "from": "hicho"
    },
    {
      "update": "Nunchaku team working on Wan quantization",
      "details": "Nunchaku team indicated they are starting work on Wan model quantization",
      "from": "aikitoria"
    },
    {
      "update": "WanFM implementation added to main branch",
      "details": "Though barely any difference vs using 2.2 normally, all examples can be done with plain 2.2",
      "from": "Kijai"
    },
    {
      "update": "Dev branch now uses less RAM when not merging LoRAs",
      "details": "Potentially 20GB less RAM usage, model loader doesn't load when not merging",
      "from": "Kijai"
    },
    {
      "update": "14B VACE Phantom v2 GGUF models available",
      "details": "Multiple sizes available on HuggingFace",
      "from": "orabazes"
    },
    {
      "update": "Phantom + MultiTalk combination fixed",
      "details": "Now working with context windows",
      "from": "Kijai"
    },
    {
      "update": "Bidirectional sampling fix merged",
      "details": "PR #9392 merged to fix OOM issues with bidirectional sampling",
      "from": "Kosinkadink"
    },
    {
      "update": "Topaz Starlight-Mini available",
      "details": "New diffusion-based video upscaler, slow but high quality",
      "from": ". Not Really Human ."
    },
    {
      "update": "InfiniteTalk repository made public then privatized again",
      "details": "Was available for 30 minutes before being pulled, repo duplicated at github.com/bmwas/InfiniteTalk but without weights",
      "from": "JohnDopamine"
    },
    {
      "update": "Kijai made fp8_scaled version of MAGREF available",
      "details": "Both fp8_e4m3fn and fp8_e5m2 scaled versions released",
      "from": "Kijai"
    },
    {
      "update": "New video grid creation node added to KJNodes",
      "details": "Uses VHS nodes and requires VHS to be installed to work",
      "from": "Kijai"
    },
    {
      "update": "InfiniteTalk release confirmed for tomorrow",
      "details": "Will be compatible with existing MultiTalk workflows by changing model",
      "from": "NebSH"
    },
    {
      "update": "MTV Crafter hardcoded to 49 frames only",
      "details": "Cannot work with 81 frames as previously thought",
      "from": "Kijai"
    },
    {
      "update": "CineTrans released for scene transitions",
      "details": "1.3B LoRA only model for scene transitions, does timesteps manipulation along with weights",
      "from": "Kijai"
    },
    {
      "update": "Qwen image edit model released",
      "details": "Edit model is available but not supported in ComfyUI yet, missing components for usage",
      "from": "Kijai"
    },
    {
      "update": "Add Memory to Reserve node added",
      "details": "New node in node-memory-reserve branch to reserve additional memory for models",
      "from": "Kosinkadink"
    },
    {
      "update": "InfiniteTalk released with ComfyUI support",
      "details": "Same developers as MultiTalk, official repo and HuggingFace models available",
      "from": "JohnDopamine"
    },
    {
      "update": "VHS Load Video node updated with drag and drop functionality",
      "details": "AustinMroz contributed the drag and drop feature",
      "from": "JohnDopamine"
    },
    {
      "update": "FastWan distillation available for 5B model",
      "details": "Speed optimization for 5B model available in Kijai's repo",
      "from": "TK_999"
    },
    {
      "update": "InfiniteTalk released with ComfyUI support",
      "details": "New lip sync model from MultiTalk authors, available in single and multi versions",
      "from": "NebSH"
    },
    {
      "update": "Qwen Image Edit ComfyUI implementation available",
      "details": "New image editing model with ComfyUI wrapper released",
      "from": "Lodis"
    },
    {
      "update": "MeiGen-AI forked KJ's wrapper",
      "details": "Created their own ComfyUI branch but conflicts with original wrapper",
      "from": "Kijai"
    },
    {
      "update": "InfiniteTalk models released",
      "details": "Single and Multi person versions available in fp8_scaled format, updated MultiTalk with infinite generation capability",
      "from": "Kijai"
    },
    {
      "update": "MAGREF integration with InfiniteTalk",
      "details": "MAGREF now works with InfiniteTalk for better character consistency",
      "from": "Kijai"
    },
    {
      "update": "Fixed merged LoRA issues with scaled models",
      "details": "Updated code to support merged LoRAs with fp8_scaled models",
      "from": "Kijai"
    },
    {
      "update": "InfiniteTalk Q8 GGUF model released",
      "details": "Took some trickery to convert but works - new Q8 GGUF version available",
      "from": "Kijai"
    },
    {
      "update": "MTVCrafter model mentions FusionX LoRA",
      "details": "New model on HuggingFace mentions FusionX lora in documentation",
      "from": "Dream Making"
    },
    {
      "update": "Removed precision setting in multitalk loader",
      "details": "Precision setting was redundant and has been removed",
      "from": "Kijai"
    },
    {
      "update": "InfiniteTalk Q4 and Q6 GGUF quantizations released",
      "details": "Added Q4 and Q6 versions to HuggingFace repo",
      "from": "Kijai"
    },
    {
      "update": "Uni3c node updated for automatic video processing",
      "details": "Modified uni3c node to work without manual latent input when using multitalk sampling",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI UI changed refresh method",
      "details": "Refresh nodes option moved from dropdown menu to 'R' shortkey",
      "from": "CJ"
    },
    {
      "update": "WanWrapper updated with multitalk improvements",
      "details": "KJ updated wrapper this morning with improvements to multitalk functionality",
      "from": "ArtOfficial"
    },
    {
      "update": "LightX2V team acknowledges static face issue",
      "details": "LightX2V developers acknowledge the issue with static faces and are working on proper distill LoRA",
      "from": "Kijai"
    },
    {
      "update": "TREAD technique for faster diffusion training",
      "details": "Claims 37x speedup, implemented by feffy380, should make training Wan much faster",
      "from": "Ada"
    },
    {
      "update": "New TLBVFI interpolation method released",
      "details": "ComfyUI implementation available, claims to be better than RIFE with latent space processing and 7 interpolated frames",
      "from": "JohnDopamine"
    },
    {
      "update": "Wan 2.2 Fun 5B Control released",
      "details": "Hugging Face release of the control version",
      "from": "DawnII"
    },
    {
      "update": "KJ updated wrapper to automatically handle MultiTalk/InfiniteTalk video input",
      "details": "Latest commit allows pulling and splicing from multitalk/infinitetalk input video automatically to replicate what meigen did in their fork",
      "from": "DawnII"
    },
    {
      "update": "Alibaba released whole suite of Fun models for 5B",
      "details": "New Fun Control models released for the 5B Wan2.2 variant",
      "from": "Kijai"
    },
    {
      "update": "Added InfiniteTalk GGUF support",
      "details": "Support for GGUF InfiniteTalk models was added to the nodes yesterday",
      "from": "Kijai"
    },
    {
      "update": "Added latent counter helper node for easier trimming",
      "details": "New node added to make trimming latents easier in workflows",
      "from": "Kijai"
    },
    {
      "update": "Lumen model released",
      "details": "1.3B text-to-video model from Kunbyte, background replacement and relighting capabilities",
      "from": "DawnII"
    },
    {
      "update": "WanVideoWrapper updated with InfiniteTalk V2V support",
      "details": "New streamlined workflow with faster sampling speeds",
      "from": "Kijai"
    },
    {
      "update": "Fun Control 5B model works with existing workflows",
      "details": "Same workflow as 14B version, supports pose control",
      "from": "Kijai"
    },
    {
      "update": "GGUF VACE modules now supported",
      "details": "Can load GGUF VACE modules, available on HuggingFace",
      "from": "Kijai"
    },
    {
      "update": "InfiniteTalk encode node added",
      "details": "New node for InfiniteTalk workflow",
      "from": "Kijai"
    },
    {
      "update": "Auto-detection from filename for InfiniteTalk mode",
      "details": "Mode automatically detected from file name",
      "from": "Kijai"
    },
    {
      "update": "Prompt travel feature added",
      "details": "Can use prompts separated by | for different sections",
      "from": "Kijai"
    },
    {
      "update": "Preview Actual Frames output added",
      "details": "New output added in latest push",
      "from": "Kijai"
    },
    {
      "update": "Kijai fixed differential diffusion in wrapper inpainting",
      "details": "Was in wrong place for a year, now working properly with significant improvement",
      "from": "Kijai"
    },
    {
      "update": "PUSA developer training Wan2.2 version mentioned 2 weeks ago",
      "details": "In process of training according to GitHub issue",
      "from": "JohnDopamine"
    },
    {
      "update": "Ostris released Wan2.2 orbit shot LoRA on HuggingFace",
      "details": "Training took days even with 96GB VRAM in RunPod",
      "from": "Drommer-Kille"
    },
    {
      "update": "New Wan2.2-Fun-A14B-InP model released",
      "details": "Supports temporal inpainting only, like first to last frame and in-between frames",
      "from": "Zabo"
    },
    {
      "update": "VACE inpainting differential diffusion code fixed",
      "details": "Results improved significantly - before result was poor, now working much better",
      "from": "Kijai"
    },
    {
      "update": "Multiple VACE usage bug fixed",
      "details": "Bug when using multiple VACEs in workflow has been resolved",
      "from": "Kijai"
    },
    {
      "update": "WanVideoWrapper updated with fixes",
      "details": "Recent commits include fixes for various issues",
      "from": "Kijai"
    },
    {
      "update": "CineScale LoRA released for 4K upscaling",
      "details": "307MB LoRA files available on HuggingFace, uses cascading sampling approach",
      "from": "MilesCorban"
    },
    {
      "update": "Waver video model paper released by ByteDance",
      "details": "Supports 10-second videos, 12B parameter model, but appears to be proprietary",
      "from": "NebSH"
    },
    {
      "update": "EasyCache and LazyCache implemented natively in ComfyUI",
      "details": "New PR supports almost all models, LazyCache for cases where EasyCache performs poorly",
      "from": "Kosinkadink"
    },
    {
      "update": "Wan 2.2 5B Turbo model released",
      "details": "Available in Kijai's HuggingFace repo, 4 steps, cfg 1.0, significant speed improvement",
      "from": "Kijai"
    },
    {
      "update": "Unofficial Wan 2.2 5B Turbo released",
      "details": "Available at quanhaol/Wan2.2-TI2V-5B-Turbo on HuggingFace, beat the lightx2v lightning team to it",
      "from": "DawnII"
    },
    {
      "update": "Tiny VAE for 2.2 may be coming",
      "details": "GitHub issue discussion about taehv implementation",
      "from": "Prelifik"
    },
    {
      "update": "CineScale LoRA released",
      "details": "New LoRA for 4K video upscaling came out, available on GitHub",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI masked conditioning fix merged",
      "details": "Fix for masked conditioning issue with video latents merged 15 minutes ago",
      "from": "Kosinkadink"
    },
    {
      "update": "Sliders for Wan 2.1 & 2.2 coming next week",
      "details": "Release expected for parameter sliders similar to baulab sliders",
      "from": "phazei"
    },
    {
      "update": "VACE 2.2 expected in July",
      "details": "Release date mentioned for VACE 2.2 version",
      "from": "JohnDopamine"
    },
    {
      "update": "UniAnimate now works with unmerged LoRAs and GGUF",
      "details": "Also works with Multi/InfiniteTalk long generation",
      "from": "Kijai"
    },
    {
      "update": "Added logging to CFG scheduler node",
      "details": "Now displays the generated CFG list like image resize node",
      "from": "Kijai"
    },
    {
      "update": "GGUF Multi versions of InfiniteTalk uploaded",
      "details": "Available at https://huggingface.co/Kijai/WanVideo_comfy_GGUF/tree/main/InfiniteTalk",
      "from": "Kijai"
    },
    {
      "update": "MelBandRoFormer vocal separation wrapped for ComfyUI",
      "details": "New audio separation tool available",
      "from": "Kijai"
    },
    {
      "update": "PUSA LoRA for Wan 2.2 is being worked on",
      "details": "In development for about 3 weeks",
      "from": "hicho"
    },
    {
      "update": "FantasyPortrait now works with multi/infinite talk loop",
      "details": "Integration allows combining head movements with lip sync",
      "from": "Kijai"
    },
    {
      "update": "VAE cache memory leak fix in development",
      "details": "Fix for InfiniteTalk VAE cache not clearing between windows",
      "from": "Kijai"
    },
    {
      "update": "Wan Nunchaku support coming",
      "details": "Nunchaku confirmed successful backend migration, focusing on video models, expected end of August",
      "from": "flo1331"
    },
    {
      "update": "New WanVideoAddControlEmbeds node",
      "details": "Alternative to previous control node that requires empty embeds input",
      "from": "Kijai"
    },
    {
      "update": "Added helper for LoRA strength scheduling",
      "details": "New node for scheduling LoRA strength, useful for bypassing LoRA on certain steps",
      "from": "Kijai"
    },
    {
      "update": "VACE 2.2 is being worked on",
      "details": "Confirmation that VACE 2.2 development is in progress",
      "from": "Kijai"
    },
    {
      "update": "Set/Get nodes may come to ComfyUI core",
      "details": "Potential integration of set/get functionality into native ComfyUI",
      "from": "Nekodificador/Kijai"
    },
    {
      "update": "New cloth adapter + controlnet repo discovered",
      "details": "Month or two old repo with dense pose controlnet and clothing replacement for Wan",
      "from": "Kijai"
    },
    {
      "update": "Fixed temporal mask casting error in image encoding",
      "details": "Reduced memory usage by half in the process",
      "from": "Kijai"
    },
    {
      "update": "NYC meetup featuring WAN 2.2 presentation",
      "details": "Community meetup with presentation focused on WAN 2.2 and community contributions",
      "from": "The Shadow (NYC)"
    },
    {
      "update": "Wan 2.2 S2V (Sound to Video) announced",
      "details": "New capability coming soon, appears to be lip sync or audio-driven generation",
      "from": "Abx"
    },
    {
      "update": "New anime style LoRA for Wan 2.2 released",
      "details": "Community creation with detailed writeup on the process",
      "from": "crinklypaper"
    },
    {
      "update": "Dev version of wrapper nodes released with refactored model loading",
      "details": "Complete refactor of model loading system",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.2 S2V (Speech to Video) model announced",
      "details": "Similar to InfiniteTalk, also supports music-driven video, uses pose video reference and motion encoder similar to UniAnimate",
      "from": "Kijai"
    },
    {
      "update": "New Wan model info released but not live yet",
      "details": "Uses Frame pack method, has causal self attention, pose control + audio control with frame pack sampling",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.2 S2V model released",
      "details": "Speech-to-video model, 14B and 5B variants available on HuggingFace, single model (not MoE)",
      "from": "DawnII"
    },
    {
      "update": "Kijai merged dev to main branch",
      "details": "Merged due to maintenance overhead, model loading now always from disk, may break things but saves RAM",
      "from": "Kijai"
    },
    {
      "update": "New VAE only for 5B model",
      "details": "There's a new VAE specifically for the 5B variant",
      "from": "daking999"
    },
    {
      "update": "Wan 2.2 S2V model released",
      "details": "Speech-to-video model available on HuggingFace",
      "from": "Impactframes"
    },
    {
      "update": "ComfyUI native audio encoder nodes available",
      "details": "New native nodes for audio encoding in ComfyUI",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI NYC meetup hosting special Wan2.2 event",
      "details": "Special event for Wan2.2 in New York area",
      "from": "ericxtang"
    },
    {
      "update": "S2V branch available for testing",
      "details": "S2V functionality added to WanVideoWrapper, work in progress with audio generation and ref image support but no long generation method yet",
      "from": "Kijai"
    },
    {
      "update": "Major merge to main repository",
      "details": "Pretty big update today, can roll back to version 1.3.1 if issues occur",
      "from": "Kijai"
    },
    {
      "update": "Nano Banana by Google available",
      "details": "Google's Gemini-based image editing tool, free during preview, good for dataset creation",
      "from": "DawnII"
    },
    {
      "update": "ComfyUI native WAN S2V implementation released",
      "details": "Work-in-progress native implementation available for testing, includes control video input",
      "from": "comfy"
    },
    {
      "update": "WAN S2V model ships as bf16 format",
      "details": "The official S2V model uses bf16 compute dtype instead of fp16",
      "from": "comfy"
    },
    {
      "update": "New AniSora V3 model released",
      "details": "https://huggingface.co/IndexTeam/Index-anisora/tree/main/V3",
      "from": "DreamWeebs"
    },
    {
      "update": "Lightning LoRAs for Wan 2.2 still in development",
      "details": "new lightx loras are still a while away https://huggingface.co/lightx2v/Wan2.2-Lightning/discussions/30",
      "from": "Karo"
    },
    {
      "update": "ComfyUI fix for S2V audio conditioning",
      "details": "reading this channel also made me realize I wasn't setting the negative audio to zero. now it is so it might work better",
      "from": "comfy"
    },
    {
      "update": "Wan team mentions ComfyUI on X",
      "details": "On X Wan they mention Comfy. https://x.com/Alibaba_Wan/status/1956250711198765362",
      "from": "Tony(5090)"
    },
    {
      "update": "ComfyUI merged S2V support",
      "details": "Official ComfyUI now has S2V nodes merged into main branch",
      "from": "comfy"
    },
    {
      "update": "HunyuanVideo-Foley released",
      "details": "New audio generation model from Tencent for video, appears to be based on MMAudio",
      "from": "JohnDopamine"
    },
    {
      "update": "Waver 1.0 announced by Bytedance",
      "details": "Was meant to be open source but they changed their minds",
      "from": "Gill Bastar, Rainsmellsnice"
    },
    {
      "update": "HunyuanVideo-Foley released",
      "details": "New audio generation model from Tencent, uses 4-5 models (sigclip, vae, transformer, syncformer), similar performance to MMAudio",
      "from": "Karo"
    },
    {
      "update": "S2V branch updated with new workflow",
      "details": "Updated S2V branch with framepack implementation and test workflow",
      "from": "Kijai"
    },
    {
      "update": "Unofficial VACE 2.2 merge available",
      "details": "Hacked together merge of 14B VACE models, not official release",
      "from": "DawnII"
    },
    {
      "update": "Updated S2V model with smaller file size and better quality",
      "details": "New S2V model is ~2GB smaller with audio injection layers in fp8 and pose conditioning fixed to not be in fp8",
      "from": "Kijai"
    },
    {
      "update": "S2V framepack working in both native ComfyUI and wrapper",
      "details": "Framepack feature from S2V now implemented, but has its own weights so only works with S2V model",
      "from": "Kijai"
    },
    {
      "update": "Fixed overbaked first frame issue in native S2V workflow",
      "details": "Updated native S2V workflow addresses the overblown first frame problem",
      "from": "comfy"
    },
    {
      "update": "GGUF offloading code added to S2V",
      "details": "DawnII copied the GGUF offloading code from main branch to s2v branch, helps significantly with memory usage",
      "from": "DawnII"
    },
    {
      "update": "e5m2 GGUF version of S2V uploaded",
      "details": "Kijai uploaded the e5m2 version to HuggingFace at Kijai/WanVideo_comfy_fp8_scaled/tree/main/S2V",
      "from": "Kijai"
    },
    {
      "update": "Recent changes to ComfyUI-WanVideoWrapper",
      "details": "Fixes for model loading bugs with merged LoRAs and other memory-related improvements",
      "from": "Kijai"
    },
    {
      "update": "Block prefetch feature change rolled back",
      "details": "A change to block prefetch feature caused issues on some systems and was rolled back",
      "from": "Kijai"
    },
    {
      "update": "Framepack technology activated in S2V",
      "details": "Framepack technology from S2V technical report has been activated and is available",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI has RAM usage improvements in latest stable",
      "details": "For Windows users on PyTorch 2.8",
      "from": "comfy"
    },
    {
      "update": "New audio volume control node added to wrapper",
      "details": "Multi and InfiniteTalk default to -23 lufs",
      "from": "Kijai"
    },
    {
      "update": "PyTorch 2.8 available with improved RAM usage",
      "details": "New pytorch update helps with memory management, reduces swap file usage",
      "from": "hicho and comfy"
    },
    {
      "update": "Torch compile fix published",
      "details": "Fixed version published that survives prompt being changed",
      "from": "phazei"
    },
    {
      "update": "CineScale already optimized for wrapper",
      "details": "CineScale from Netflix has been available and working in wrapper for weeks",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "Kijai joining comfy-org",
      "details": "I'll be working for comfy-org, and at least initially allocate half of my time on that, technically they said they just want me to keep doing what I'm doing, but I do want to do more like native implementations",
      "from": "Kijai"
    },
    {
      "update": "S2V merged to main in wrapper",
      "details": "Merged S2V to main in the wrapper and fixed bunch of bugs",
      "from": "Kijai"
    },
    {
      "update": "Pusa-Wan2.2-V1 model coming soon",
      "details": "Found repo on HuggingFace but it was deleted, suggesting imminent release",
      "from": "asd"
    },
    {
      "update": "Native S2V nodes available",
      "details": "Native S2V nodes are now in core ComfyUI",
      "from": "ArtOfficial"
    }
  ],
  "workflows": [
    {
      "workflow": "VACE 2.2 with openpose controlnet",
      "use_case": "Style transfer and character animation with pose control",
      "from": "GOD_IS_A_LIE"
    },
    {
      "workflow": "2 sampler approach vs custom single node",
      "use_case": "Comparing dual sampler workflow against custom combined sampler implementation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "VACE I2V chaining with T2V start",
      "use_case": "Creating long videos with smooth transitions using 5 I2V sections fed from initial T2V",
      "from": "seitanism"
    },
    {
      "workflow": "Multi-scene generation with context windows",
      "use_case": "Creating videos with multiple scenes using | separator in prompts",
      "from": "thaakeno"
    },
    {
      "workflow": "VACE upscaling with WAN 2.2",
      "use_case": "Using WAN 2.1 VACE then upscaling with WAN 2.2 high noise at 2 steps by 1.5x factor",
      "from": "GOD_IS_A_LIE"
    },
    {
      "workflow": "Fast video preview with frame dropping",
      "use_case": "Drop frames by 2-3, generate quickly, then interpolate and refine with high start step",
      "from": "thaakeno"
    },
    {
      "workflow": "Multi-scene generation with context windows",
      "use_case": "4 scenes in two context windows using | separator between prompts",
      "from": "Cseti"
    },
    {
      "workflow": "AMD ZLUDA setup for WAN",
      "use_case": "Successfully running on AMD Radeon 8060S with 20 minute generation times for 1280x704 121 frames",
      "from": "nacho.money"
    },
    {
      "workflow": "Two-stage resolution workflow",
      "use_case": "Generate at 640x352 then upscale to 1280x704 for better performance and quality balance",
      "from": "nacho.money"
    },
    {
      "workflow": "3-stage sampling for I2V",
      "use_case": "Better quality I2V generation with 9 steps: 3H no Lora, 3H LightX2V@3, 3L@1",
      "from": "BobbyD4AI"
    },
    {
      "workflow": "Multi-resolution generation",
      "use_case": "2 high noise passes and then low noise pass to finish, from 3 different resolutions",
      "from": "Kijai"
    },
    {
      "workflow": "Character LoRA training",
      "use_case": "59min training with high noise model, tested at 18 steps high, 2 steps low",
      "from": "Kenk"
    },
    {
      "workflow": "Three sampler setup for quality",
      "use_case": "No lora on first sampler with higher steps and high CFG, second pass high lora 3.0 and CFG 1, third sampler 1.0 lora and CFG 1",
      "from": "Simjedi"
    },
    {
      "workflow": "Four sampler setup",
      "use_case": "2/2/4/4 step distribution using kjsampler",
      "from": "Simjedi"
    },
    {
      "workflow": "Context window splitting with stride",
      "use_case": "Splitting 5B context window to 2 samplers for smoothing",
      "from": "Kijai"
    },
    {
      "workflow": "Manual frame-based prompt scheduling",
      "use_case": "Camera movement control by specifying actions at specific frame numbers in regular positive prompt",
      "from": "Bleedy (Madham)"
    },
    {
      "workflow": "Video encoding for V2V",
      "use_case": "Load video, encode with VAE Encode node, send latent as input latent for video-to-video",
      "from": "Ablejones"
    },
    {
      "workflow": "Context window setup for long videos",
      "use_case": "Use different context settings for high and low noise models to improve blending",
      "from": "Kijai"
    },
    {
      "workflow": "Multi-stage T2V generation with different resolutions",
      "use_case": "Better motion at lower res, then upscale for final quality",
      "from": "Kijai"
    },
    {
      "workflow": "Generate 640x352 then scale to 1280x704",
      "use_case": "For slower hardware, faster iteration on disappointing gens",
      "from": "nacho.money"
    },
    {
      "workflow": "V2V upscaling workflow for quality enhancement",
      "use_case": "Upscaling 480p to 1080p while fixing details like LED lightbars",
      "from": "thaakeno"
    },
    {
      "workflow": "MMaudio integration with RIFE interpolation",
      "use_case": "Generate 16fps, interpolate to 25fps for audio sync",
      "from": "thaakeno"
    },
    {
      "workflow": "High + Low + Low refinement",
      "use_case": "Using high model, then low model twice with latent fan denoise 0.5, start step 0, end -1, step 6",
      "from": "avataraim"
    },
    {
      "workflow": "V2V upscaling with Wan 14B",
      "use_case": "Upscaling videos while preserving content using high start step",
      "from": "thaakeno"
    },
    {
      "workflow": "FLF with native Wan 2.2 I2V",
      "use_case": "Creating transitions between first and last frames",
      "from": "Juampab12"
    },
    {
      "workflow": "First-Last-Frame to Video (FLF2V)",
      "use_case": "Creating videos with specific start and end frames, 10 second generations work well",
      "from": "comfy"
    },
    {
      "workflow": "T2I workflow based on Wan 2.2",
      "use_case": "1344x768 in 5sec, 2nd pass x2 upscaling to 2688x1536 in 4sec",
      "from": "N0NSens"
    },
    {
      "workflow": "Native first last image implementation",
      "use_case": "Using ComfyUI native nodes for first/last frame conditioning",
      "from": "Juampab12"
    },
    {
      "workflow": "Three K-sampler setup",
      "use_case": "Mix steps without LoRA and with LoRA for best of both worlds",
      "from": "TK_999"
    },
    {
      "workflow": "Video upscale and motion fix",
      "use_case": "Feed video latent to high-noise model with start step control for upscaling/motion fixing",
      "from": "thaakeno"
    },
    {
      "workflow": "Parallel video generation",
      "use_case": "Generate multiple videos simultaneously for efficiency",
      "from": "Fill"
    },
    {
      "workflow": "Combined MMAudio and Wan 2.2 workflow",
      "use_case": "Single workflow for video generation with audio synthesis",
      "from": "thaakeno"
    },
    {
      "workflow": "Three sampler approach for Wan 2.2",
      "use_case": "First step with no lightx and cfg, then lightx with 1 cfg for remaining steps",
      "from": "Draken"
    },
    {
      "workflow": "High resolution generation with latent upscale",
      "use_case": "Run HIGH at lower res, then latent upscale for LOW noise processing",
      "from": "Draken"
    },
    {
      "workflow": "14B to 5B upscaling pipeline",
      "use_case": "Decode 14B video, encode with VAE 2.2, feed to 5B with same starting frame and 0.3-0.5 denoise",
      "from": "Juan Gea"
    },
    {
      "workflow": "FPS1 storyboarding approach",
      "use_case": "Generate videos at 1 FPS, then use other models to fill in-between frames for longer scenes",
      "from": "mamad8"
    },
    {
      "workflow": "Three-sampler approach with euler reset",
      "use_case": "Add single euler step when switching to LN model to avoid multistep sampler artifacts",
      "from": "Ablejones"
    },
    {
      "workflow": "First frame last frame morphing",
      "use_case": "Using FPS1 LoRA for temporal consistency in guided generation",
      "from": "Fill"
    },
    {
      "workflow": "Multi-stage generation with LoRA scheduling",
      "use_case": "Use cfg for first steps without LoRA, then enable lightx2v for remaining steps",
      "from": "Kijai"
    },
    {
      "workflow": "Upscaling pipeline",
      "use_case": "Generate 1600x960 with 14B, then upscale to 3072x1840 with 5B using block swap 30",
      "from": "Juan Gea"
    },
    {
      "workflow": "Hires fix approach with high/low models",
      "use_case": "4 steps each 0.5 denoise, 140sec gen time",
      "from": "Daviejg"
    },
    {
      "workflow": "Dockerized Wan 2.2 as serverless container",
      "use_case": "Per-second billing with custom web UI dispatching jobs to H100 or 5090",
      "from": "gokuvonlange"
    },
    {
      "workflow": "Two-pass generation with LightX LoRA",
      "use_case": "First pass normal generation, second pass with LightX at 0.5 strength for added detail",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "High noise without LoRA, then low noise with LightX",
      "use_case": "Better prompt comprehension while maintaining speed benefits",
      "from": "Ablejones"
    },
    {
      "workflow": "4 high noise steps with CFG, 3 scheduled steps, 6 low noise steps",
      "use_case": "Sweet spot for quality vs speed with 2.2",
      "from": "Ada"
    },
    {
      "workflow": "3 samplers: high no LoRA, high with LoRA, low with LoRA",
      "use_case": "When you can't use LoRA scheduling but need flexibility",
      "from": "Kijai"
    },
    {
      "workflow": "Dual LoRA training for Wan 2.2 with different timesteps",
      "use_case": "Training both high and low noise experts with same dataset but different timestep ranges",
      "from": "Kijai"
    },
    {
      "workflow": "Using old 2.1 LoRAs on 2.2 low noise + training only high noise",
      "use_case": "Easier migration path - use existing 2.1 LoRAs on low, only train new high noise LoRA",
      "from": "crinklypaper"
    },
    {
      "workflow": "Hybrid LoRA approach for T2V",
      "use_case": "Use new lightning LoRA on high noise side only, old lightx2v or other LoRAs on low noise side for styling",
      "from": "Kijai"
    },
    {
      "workflow": "CFG staging for better lighting",
      "use_case": "Start with 1 CFG 3.5 step without LoRA to set tone, then use CFG 1 with LoRAs for remaining steps",
      "from": "IceAero"
    },
    {
      "workflow": "Mixed LoRA generation workflow",
      "use_case": "4-step generation using old 2.1 LoRA on high noise, new Lightning on low noise",
      "from": "Doctor Shotgun"
    },
    {
      "workflow": "CFG scheduling with Lightning",
      "use_case": "First step CFG 3.5 no LoRA, remaining steps CFG 1.0 with Lightning LoRA",
      "from": "Ablejones"
    },
    {
      "workflow": "Two-pass upscaling with 14B+5B",
      "description": "Generate with 14B model, then upscale using 5B model with block swapping",
      "use_case": "Achieving higher resolutions like 3072x with limited VRAM",
      "from": "Juan Gea"
    },
    {
      "workflow": "Multi-pass Lightning setup",
      "description": "1 step HN No LoRA CFG 3.5 => 2 step HN Light2.2 CFG 1.0 => 3 step LN Light2.2 CFG 1.0",
      "use_case": "Optimized quality with Lightning LoRAs",
      "from": "Ablejones"
    },
    {
      "workflow": "Prompt batching for variety",
      "description": "Multi prompt iteration batch using ChatGPT generated prompts",
      "use_case": "Generating large variety of test content quickly",
      "from": "Purz"
    },
    {
      "workflow": "3-3-3 split with 2 different high models",
      "use_case": "3 steps at 3 cfg with no lora, then 3 steps with lora into low model to preserve original motion before using lora",
      "from": "flo1331"
    },
    {
      "workflow": "ClownsharkSampler setup to minimize model calls",
      "use_case": "About 30 seconds longer than KSampler but results look more fully sampled, less noisy video",
      "from": "Ablejones"
    },
    {
      "workflow": "Mixed loras approach",
      "use_case": "Using both lightx and lightning loras together on different parts",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "workflow": "Ada's I2V workflow",
      "use_case": "I2V with complex prompts, 10 steps, all 1 cfg, follows complex prompts well",
      "from": "Ada"
    },
    {
      "workflow": "Lightning+Lightx+CausVid combination",
      "use_case": "Combining multiple loras for enhanced results",
      "from": "garbus"
    },
    {
      "workflow": "Homebrew Veo3 workflow",
      "use_case": "Using wan2.2, lightx2v 2.1, Multitalk and mmaudio for complete video generation",
      "from": "nacho.money"
    },
    {
      "workflow": "Context concatenation for long videos",
      "use_case": "Extending video generations with character consistency maintained across scenes",
      "from": "kendrick"
    },
    {
      "workflow": "Upscaling with Wan 2.2 14B using blockswapping",
      "use_case": "Upscaling videos from 480p to 1080p on 24GB VRAM systems",
      "from": "thaakeno"
    },
    {
      "workflow": "I2V with stacked LoRAs",
      "use_case": "High quality I2V using phantom fusionx + pusa + lightx2v LoRAs for better prompt following and motion",
      "from": "Ada"
    },
    {
      "workflow": "Wan 2.1 + 2.2 hybrid pipeline",
      "use_case": "Better I2V results by doing single frame Wan gen, then building frame sequence with prompt for FLF workflows",
      "from": "Josiah"
    },
    {
      "workflow": "Frame interpolation using Wan",
      "use_case": "Generate first video at 16 FPS with 41 frames, add gray frames between each, then regenerate at 81 frames for 32 FPS",
      "from": "mamad8"
    },
    {
      "workflow": "Qwen-Image to WAN I2V pipeline",
      "use_case": "Generate high quality first frame with Qwen-Image, then animate with WAN I2V",
      "from": "fredbliss"
    },
    {
      "workflow": "Scene transition with dual reference images",
      "use_case": "Use 2 reference images at different aspect ratios for scene cuts while maintaining character likeness",
      "from": "Jonathan"
    },
    {
      "workflow": "FFLF (First Frame Last Frame) setup for WAN 2.2",
      "use_case": "Creating video sequences with defined start and end frames",
      "from": "AJO"
    },
    {
      "workflow": "Batch video generation",
      "use_case": "Generate 20 videos in 6 minutes (49 frames each) for finding the best result",
      "from": "Fill"
    },
    {
      "workflow": "Qwen image to Wan I2V pipeline",
      "use_case": "Generate layout with Qwen image, then refine with Wan to add details",
      "from": "aikitoria"
    },
    {
      "workflow": "T2V with reference consistency",
      "use_case": "Using specific prompting techniques to get almost 100% consistent results with reference",
      "from": "Jonathan"
    },
    {
      "workflow": "Side-by-side reference technique for character consistency",
      "use_case": "Maintaining character appearance across different scenes",
      "from": "Juampab12"
    },
    {
      "workflow": "VACE inpainting for fixing video artifacts",
      "use_case": "Fixing 45-frame segments with face artifacts using mask and video input",
      "from": "Wembleycandy"
    },
    {
      "workflow": "Prompt splitting for scene changes using | separator",
      "use_case": "Creating videos with multiple distinct scenes or transitions",
      "from": "Kijai"
    },
    {
      "workflow": "VACE strength scheduling with floats",
      "use_case": "Better control over style transfer and composition",
      "from": "Nekodificador"
    },
    {
      "workflow": "Context window upscaling for high-res videos",
      "use_case": "Upscaling 10s videos with Wan 14B on limited VRAM",
      "from": "thaakeno"
    },
    {
      "workflow": "Keyframe-based generation for longer sequences",
      "use_case": "Take first frame as new keyframe, regen 5s scene change, repeat",
      "from": "Josiah"
    },
    {
      "workflow": "Two-stage generation with save/load latent",
      "use_case": "Managing VRAM limitations by splitting high noise and low noise generation",
      "from": "Mngbg"
    },
    {
      "workflow": "VACE + 2.2 HN merged workflow",
      "use_case": "Getting reference following with 2.2 motion quality",
      "from": "Kijai"
    },
    {
      "workflow": "Flux Krea + Wan 2.2 pipeline",
      "use_case": "Generate initial image with Flux Krea at 1920x1088, downscale to 1280x676 for Wan, upscale final video with Topaz",
      "from": ".: Not Really Human :."
    },
    {
      "workflow": "Using replace image in Batch node for inbetween frames with VACE",
      "use_case": "Adding interpolated frames between VACE generations",
      "from": "xwsswww"
    },
    {
      "workflow": "V2V upscaler workflow for fixing distant faces",
      "use_case": "Push to 1600x900x81 resolution and fix faces at distance using low denoise settings",
      "from": "mdkb"
    },
    {
      "workflow": "Phantom+VACE merge for character consistency with controlnet",
      "use_case": "Reliable character control coming off screen with openpose following",
      "from": "Piblarg"
    },
    {
      "workflow": "VACE character swapping with Wan 2.2",
      "use_case": "Replacing characters in videos using reference images and SAM2 points editor",
      "from": "mdkb"
    },
    {
      "workflow": "FlF2V with add noise for dreamy effects",
      "use_case": "Creating smooth morphing transitions between images with dreamy effects using LoRAs",
      "from": "piscesbody"
    },
    {
      "workflow": "USDU upscaling for video",
      "use_case": "Upscaling from 1280x768 to 2560x1536, fixes hands and hair details but may have background seams",
      "from": "Persoon"
    },
    {
      "workflow": "Beta I2V workflow for scene teleportation",
      "use_case": "Teleporting characters between different scenes while maintaining likeness",
      "from": "Juampab12"
    },
    {
      "workflow": "First-frame-last-frame morphing workflow",
      "use_case": "Video extension with close first frame matching",
      "from": "Gavmakes"
    },
    {
      "workflow": "Pre-step dark scene generation",
      "use_case": "Three methods: add 3rd sampler for single step, schedule LoRA strength to 0.0 at first step, or use CFG schedule",
      "from": "IceAero"
    },
    {
      "workflow": "162 frames in single workflow",
      "use_case": "Testing long video generation capabilities",
      "from": "Josiah"
    },
    {
      "workflow": "VACE/Phantom merge",
      "use_case": "Reliable identity preservation with control",
      "from": "Piblarg"
    },
    {
      "workflow": "MultiShot with scene changes while retaining likeness",
      "use_case": "Generating videos with camera movement and character consistency",
      "from": "Josiah"
    },
    {
      "workflow": "Batch rendering workflow",
      "use_case": "Multiple simultaneous generations for users with large memory pools",
      "from": "HeadOfOliver"
    },
    {
      "workflow": "High resolution I2V generation",
      "use_case": "1536x768 native generation using Wan 2.2 with Lightning LoRA and quantized models",
      "from": ": Not Really Human :"
    },
    {
      "workflow": "Motion control with Lightning LoRA",
      "use_case": "Using different high/low LoRA strength ratios to control motion speed",
      "from": "CaptHook"
    },
    {
      "workflow": "Uneven split generation",
      "use_case": "Running high model full steps while only distilling low noise model for efficiency",
      "from": "MysteryShack"
    },
    {
      "workflow": "First-frame-last-frame morphing with Fun InP",
      "use_case": "Creating smooth transitions between two keyframes with temporal inpainting",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "workflow": "VACE/Phantom merge workflow",
      "use_case": "Combining VACE control with Phantom consistency for better results",
      "from": "Piblarg"
    },
    {
      "workflow": "Long-form generation with context stacking",
      "use_case": "Creating 257+ frame videos using 121,4,40 context settings",
      "from": "samhodge"
    },
    {
      "workflow": "Batch processing with VLM automation",
      "use_case": "Processing multiple images (e.g., 20 images) automatically using Qwen2.5VL to minimize manual intervention",
      "from": "R."
    },
    {
      "workflow": "Matrix scene character replacement",
      "use_case": "Converting Neo shots to gorilla using WAN 2.1 + VACE inpainting with reference consistency",
      "from": "Nekodificador"
    },
    {
      "workflow": "Style mixing without LoRAs",
      "use_case": "Achieving different styles using WAN 2.2 prompting alone",
      "from": "R."
    },
    {
      "workflow": "High noise to low noise handoff using denoised samples",
      "description": "Connect denoised samples from first scheduler to second scheduler with noise added for seamless connectivity between stages",
      "use_case": "Better stage transition control",
      "from": "MysteryShack"
    },
    {
      "workflow": "V2V using encoded samples in sample slot",
      "description": "Put encoded video samples directly into sample slot, works better than expected for plain v2v",
      "use_case": "Video to video generation",
      "from": "Kijai"
    },
    {
      "workflow": "VACE single frame controlnet to video",
      "use_case": "Generate video from single controlnet depth map with prompt following and reference image support",
      "from": "Lodis"
    },
    {
      "workflow": "Qwen2.5VL with Wan2.2 for automated prompting",
      "use_case": "Automatic prompt generation from reference images with memory-efficient caching",
      "from": "Kijai"
    },
    {
      "workflow": "Chained VACE nodes for better control",
      "use_case": "Use two WanVaceToVideo nodes with positive/negative continuing to next, only last outputs latent",
      "from": "Atlas"
    },
    {
      "workflow": "Sapiens 7-stage pipeline",
      "use_case": "Comprehensive pose detection: Detect \u2192 Crop \u2192 Inference \u2192 Refine \u2192 Filter \u2192 Track \u2192 Draw",
      "from": "fredbliss"
    },
    {
      "workflow": "Fun Control with Sapiens pose",
      "use_case": "Using Sapiens-generated poses with VACE control system for better pose-driven video generation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Complete End-to-End pose detection pipeline",
      "use_case": "Multi-person tracking with temporal smoothing: Detect \u2192 Track \u2192 Crop & Pad \u2192 Sapiens Inference \u2192 Refinement \u2192 Filter \u2192 Smooth \u2192 Draw",
      "from": "fredbliss"
    },
    {
      "workflow": "Video to Video with Wan 2.2",
      "use_case": "Custom V2V workflow implementation",
      "from": "VRGameDevGirl84"
    },
    {
      "workflow": "PUSA extension workflow",
      "use_case": "Using PUSA LoRA with T2V model and extra_latents for extension without color shifting",
      "from": "Hashu"
    },
    {
      "workflow": "Clip in native, rest in wrapper",
      "use_case": "Hybrid approach for Wan 2.2 processing",
      "from": "Drommer-Kille"
    },
    {
      "workflow": "Blender + Vace then Wan 2.2 V2V",
      "use_case": "Two-stage process for enhanced realism - first pass with Vace, second pass with Wan 2.2 for added realism",
      "from": "Drommer-Kille"
    },
    {
      "workflow": "MultiTalk + MMAudio sync workflow",
      "use_case": "Process MultiTalk at 16fps, 3x interpolate to 48fps, strip every other frame to 24fps for MMAudio sync",
      "from": "nacho.money"
    },
    {
      "workflow": "Fun InP with temporal masks and VACE start/end image node",
      "description": "Using temporal mask for keyframe control with Fun InP, can do first/last frame with keyframes in middle",
      "use_case": "Video extension with keyframe control",
      "from": "Kijai"
    },
    {
      "workflow": "Point editor with ctrl + left click",
      "description": "Draw bounding boxes for control using ctrl + left click in Kijai's point editor",
      "use_case": "Creating control regions for Fun control",
      "from": "Kijai"
    },
    {
      "workflow": "Phantom object reference workflow",
      "description": "Using vanilla i2v workflow for object reference without person prompting",
      "use_case": "Object continuity across scenes",
      "from": "Juampab12"
    },
    {
      "workflow": "Multi-sampler setup for lighting LoRA",
      "description": "3 samplers: 1 step full HN, then 4 steps with LoRA HN and 4 steps with LoRA LN, LoRAs at 0.9 strength",
      "use_case": "Using lighting LoRA with uneven H/L split",
      "from": "Karo"
    },
    {
      "workflow": "Using Wan 2.2 low noise as upscaler",
      "use_case": "Fix faces at distance, upscale while maintaining structure",
      "from": "mdkb"
    },
    {
      "workflow": "Sliding context workflow for 2.2",
      "use_case": "8 and 4 split with dpm++_sde sampler, cfg 1, shift set to 8",
      "from": "kendrick"
    },
    {
      "workflow": "VACE with controlnets for v2v",
      "use_case": "Change v2v using reference image and controlnets to change structure",
      "from": "mdkb"
    },
    {
      "workflow": "Multiple sampler scheduling",
      "use_case": "Using 3+ samplers for better control, similar to cfg scheduling",
      "from": "Kijai"
    },
    {
      "workflow": "High/Low model workflow with scheduling",
      "use_case": "1 step high/low (lightning) + 2 step (old loras) configuration",
      "from": "Josiah"
    },
    {
      "workflow": "FFLF with Wan 2.2 Fun Inpaint",
      "use_case": "First-Frame-Last-Frame morphing workflow",
      "from": "R."
    },
    {
      "workflow": "Decode-upscale-encode for higher resolution",
      "details": "Use denoised output with tiny VAE, add noise in second sampler",
      "use_case": "Upscaling generated videos",
      "from": "Kijai"
    },
    {
      "workflow": "Using WanImageToVideo encode for I2V",
      "details": "ImageToVideo encode that goes into image_embeds is the main difference from T2V",
      "use_case": "Converting T2V workflow to I2V",
      "from": "patientx"
    },
    {
      "workflow": "VACE inpainting workflow",
      "use_case": "Selective video editing with masking control",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Wan 5B upscaling pipeline",
      "use_case": "Upscale 14B output by resizing then V2V with 5B at 0.35-0.5 denoise, can use I2V to guide with reference image",
      "from": "Juan Gea"
    },
    {
      "workflow": "Face cropping and stitching for background faces",
      "use_case": "Fix mutating background faces by masking, cropping to 1024x1024, V2V improvement, then stitching back",
      "from": "Juan Gea"
    },
    {
      "workflow": "FFLF (First Frame Last Frame) with Wan 2.2",
      "use_case": "Creating smooth video transitions, works well and is fast",
      "from": "Charlie"
    },
    {
      "workflow": "Extension using first few frames -> VAE Encode -> Empty Embeds -> Sampler",
      "use_case": "Video extension technique",
      "from": "daking999"
    },
    {
      "workflow": "Stand-in with face cropping and rembg",
      "use_case": "Character consistency in video generation",
      "from": "Kijai"
    },
    {
      "workflow": "Phantom + VACE + Wan2.2LN LoRA integration",
      "use_case": "Combining multiple techniques for enhanced results",
      "from": "Ablejones"
    },
    {
      "workflow": "FFLF (First Frame Last Frame) for fast face swap",
      "use_case": "Fast face replacement in videos",
      "from": "Charlie"
    },
    {
      "workflow": "Multitalk with context windows for long lip sync videos",
      "use_case": "Creating extended lip-synced videos up to minutes long",
      "from": "Kijai"
    },
    {
      "workflow": "Skyreels v2 for 121 frame generation",
      "use_case": "Extending video length beyond standard limits",
      "from": "NebSH"
    },
    {
      "workflow": "Fun Control trajectory example for complex camera movements",
      "use_case": "Controlling camera trajectories in video generation",
      "from": "Kijai"
    },
    {
      "workflow": "HN + LN model combination with unsampling",
      "use_case": "Get motion from HN model with a few steps, then use unsampling and continue with LN model for detailed control",
      "from": "Ablejones"
    },
    {
      "workflow": "Multitalk v2v with Wan 2.1 t2v VACE merge",
      "use_case": "Fast video-to-video generation with lip sync, 832x480x121 @ 24fps in under 10 minutes",
      "from": "mdkb"
    },
    {
      "workflow": "Stand-In v2v with reference image using VACE",
      "use_case": "Face replacement in existing videos, took 176 seconds on RTX 3060",
      "from": "mdkb"
    },
    {
      "workflow": "PersonMaskUltra workflow for better Stand-In quality",
      "use_case": "Segments, crops and resizes reference image to fill 512px space for improved output quality",
      "from": "BobbyD4AI"
    },
    {
      "workflow": "Medieval movie scenes using Text2Image + character rotation + inpainting + Wan2.2 i2v",
      "use_case": "Creating animated medieval scenes with consistent characters",
      "from": "loopen44"
    },
    {
      "workflow": "MAGREF + FantasyPortrait + Uni3C combination",
      "use_case": "Character consistency with camera control and face forward positioning",
      "from": "Kijai"
    },
    {
      "workflow": "MultiTalk with locked camera then use as driver",
      "use_case": "Creating driving video for stand-in without recording yourself",
      "from": "Kijai"
    },
    {
      "workflow": "Context window method for infinite generation",
      "use_case": "Endless video generation with MAGREF reference latents",
      "from": "Kijai"
    },
    {
      "workflow": "Two sampler workflow with LightX2V",
      "use_case": "First sampler without LightX2V, second sampler with 4 steps using LightX2V",
      "from": "Danial"
    },
    {
      "workflow": "FantasyPortrait + MultiTalk for lip sync",
      "use_case": "Creating talking character videos with audio sync",
      "from": "Kijai"
    },
    {
      "workflow": "Depth + DW pose with Kontext for first frame and Wan 2.2",
      "use_case": "Character animation with pose control",
      "from": "Guus"
    },
    {
      "workflow": "Stand-in with VACE for character replacement",
      "use_case": "Character swapping in videos, though results can be hit or miss",
      "from": "ArtOfficial"
    },
    {
      "workflow": "Context windows for longer video generation",
      "use_case": "Extending videos beyond normal length limits, achieved 23s generation",
      "from": "Kijai"
    },
    {
      "workflow": "T2V for first frame then I2V continuation",
      "use_case": "Creating longer videos with consistent motion",
      "from": "Daflon"
    },
    {
      "workflow": "Last frame used as first frame for next video",
      "use_case": "Creating 16+ second videos with Wan 2.2",
      "from": "Daflon"
    },
    {
      "workflow": "FFLF (First-Frame-Last-Frame) or Context for video extension",
      "use_case": "Extending videos smoothly",
      "from": "Josiah"
    },
    {
      "workflow": "Using VACE with low noise only for better consistency",
      "use_case": "More consistent results with reference image, though slower",
      "from": "Sal TK FX"
    },
    {
      "workflow": "Cascadour to Python script for 3D controlnet input into VACE",
      "use_case": "Creating controlled 3D character animations",
      "from": "mdkb"
    },
    {
      "workflow": "FantasyPortrait + MultiTalk combination",
      "use_case": "Audio-synced facial animation with better lip sync",
      "from": "Guey.KhalaMari"
    },
    {
      "workflow": "VACE + Phantom model integration",
      "use_case": "Enhanced character consistency with style control",
      "from": "Hashu"
    },
    {
      "workflow": "Long video generation up to 873 frames",
      "use_case": "Extended video sequences in single pass",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "workflow": "Triple VACE embed setup",
      "use_case": "Controlnet with low strength + Inpainting mask + Reference strength for precise control",
      "from": "Nekodificador"
    },
    {
      "workflow": "Phantom + VACE + Wan 2.2",
      "use_case": "One-shot character consistency, though very slow",
      "from": "gokuvonlange"
    },
    {
      "workflow": "MultiTalk facial performance",
      "use_case": "Improved facial performance in lip-sync applications",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "workflow": "VACE reference and Canny controlnet split workflow",
      "use_case": "Combining reference image preservation with motion control, though ref image quality issues remain",
      "from": "mdkb"
    },
    {
      "workflow": "Fantasy Portrait + MultiTalk combined workflow",
      "use_case": "Combining talking head generation with lip sync, using MAGREF checkpoint and context windows",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Downscaling + noise + SeedVR2 enhancement",
      "use_case": "Adding textures and fine-grained details to bring images to life, works well with Wan generated images",
      "from": "Shawneau \ud83c\udf41 [CA]"
    },
    {
      "workflow": "10 image Wan morphing manually",
      "use_case": "Creating morphing sequences between multiple images",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "workflow": "QwenImage to Wan I2V bridge",
      "use_case": "Using QwenImage latents as input for Wan I2V generation",
      "from": "fredbliss"
    },
    {
      "workflow": "Video extension using last frame extraction",
      "use_case": "Extract last frame, run through I2V workflow again, stitch with WanVideo Blender with 1 frame overlap",
      "from": "xwsswww"
    },
    {
      "workflow": "Single image FFLF for looping",
      "use_case": "Using same image as both first and last frame in Wan 2.2 FFLF for creating loops",
      "from": "ezMan"
    },
    {
      "workflow": "Two-pass system with high noise preview",
      "use_case": "Team workflow where high noise generates motion preview first, then continues to low noise refinement if approved",
      "from": "gokuvonlange"
    },
    {
      "workflow": "MultiTalk with three character masking",
      "use_case": "V2V lip-sync for multiple characters using separate masks and audio files (silent for non-speaking characters)",
      "from": "mdkb"
    },
    {
      "workflow": "Phantom with VACE module for character consistency",
      "use_case": "Maintaining character likeness in T2V while adding VACE control capabilities",
      "from": "mdkb"
    },
    {
      "workflow": "Save and load latent for stateless processing",
      "use_case": "Serverless workers where latents are saved as files for continuation across stateless sessions",
      "from": "gokuvonlange"
    },
    {
      "workflow": "VACE single image inpainting: 5 frames (1 with mask + 4 empty gray)",
      "use_case": "Single image inpainting with VACE",
      "from": "Nekodificador"
    },
    {
      "workflow": "Phantom + MultiTalk + FantasyPortrait combination",
      "use_case": "Dynamic talking head generation with context windows",
      "from": "Kijai"
    },
    {
      "workflow": "Qwen-image -> latent -> WAN 2.2 I2V bridge",
      "use_case": "Direct latent transfer without VAE decode",
      "from": "fredbliss"
    },
    {
      "workflow": "Multiple FUN controls can be hooked up together",
      "use_case": "Combining different control inputs",
      "from": "Flipping Sigmas"
    },
    {
      "workflow": "Qwen image to Wan latent passing",
      "use_case": "Direct latent space manipulation between models for potential long video extension",
      "from": "fredbliss"
    },
    {
      "workflow": "Fantasy Portrait with first/last frame extraction",
      "use_case": "Video-to-video conversion by extracting input video frames as start/end points",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Fantasy Portrait + MultiTalk combination",
      "use_case": "Combining video-driven and audio-driven lip sync",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "MAGREF + Fantasy Portrait + MultiTalk for long stable videos",
      "use_case": "Generate 520-700 frame stable videos with good lip sync using 81 context size, 32 overlap, 6 steps with lightx2v I2V lora",
      "from": "Kijai"
    },
    {
      "workflow": "Use Fun Control models for camera control with Blender paths",
      "use_case": "Camera trajectory control using paths created in Blender",
      "from": "Kijai"
    },
    {
      "workflow": "Context window with wildcard prompts",
      "use_case": "Long video generation without repetitive scenes",
      "from": "Kijai"
    },
    {
      "workflow": "I2V feedback loop for scene transitions",
      "use_case": "Generate I2V, extract best frames, feed into next sequence for better continuity",
      "from": "Josiah"
    },
    {
      "workflow": "LightX2V with upscale and noise injection",
      "use_case": "Better sharpness on T2V - extra low noise pass after latent upscale",
      "from": "hablaba"
    },
    {
      "workflow": "3-stage sampler approach for Wan 2.2",
      "use_case": "Better motion quality in 2.2 by using CFG in early stages",
      "from": "Kijai"
    },
    {
      "workflow": "FantasyPortrait mixed with Multitalk for lipsync",
      "use_case": "High quality talking head generation with lip sync",
      "from": "mdkb"
    },
    {
      "workflow": "Scene cuts using prompt structure",
      "use_case": "Creating scene transitions within single generation using [scene 1][cut][scene 2] format",
      "from": "NebSH"
    },
    {
      "workflow": "Style transfer using Fun Control 2.2 with ref image and input video as control",
      "use_case": "Converting videos to different artistic styles while maintaining motion",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Language replacement for commercial video",
      "use_case": "Take English speaking video, use Spanish audio driving video with face landmarks to change mouth movements to match new language",
      "from": "mdkb"
    },
    {
      "workflow": "Long-form talking head generation with InfiniteTalk",
      "use_case": "Creating videos up to 1000 frames with lip sync using static reference image and driving video",
      "from": "NC17z"
    },
    {
      "workflow": "MultiTalk + FantasyPortrait + Uni3C + UniAnimate",
      "use_case": "Complete talking head generation with camera movement",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "workflow": "InfiniteTalk with context windows",
      "use_case": "Long-form lip sync generation without degradation",
      "from": "Kijai"
    },
    {
      "workflow": "Using first/last frame for video targeting",
      "use_case": "Put target video first frame in ref, control video for lip sync",
      "from": "mdkb"
    },
    {
      "workflow": "InfiniteTalk with MAGREF",
      "use_case": "Audio-driven video generation with character consistency using image + audio input",
      "from": "Kijai"
    },
    {
      "workflow": "Audio sync setup",
      "use_case": "Use VideoInfo to extract input FPS, feed to VideoCombine output, use 25fps for embeds",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "InfiniteTalk + FantasyPortrait + MagRef",
      "use_case": "High quality lip sync with character consistency using I2V model",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "workflow": "Context windows instead of multitalk node for 2.2",
      "use_case": "Running InfiniteTalk with Wan 2.2 by avoiding the multitalk node",
      "from": "Kijai"
    },
    {
      "workflow": "V2V with driving video for lip sync",
      "use_case": "Video-to-video with InfiniteTalk using control video improves lip sync significantly",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "workflow": "InfiniteTalk V2V with uni3c integration",
      "use_case": "Video-to-video lip sync that preserves original video motion through keyframing",
      "from": "Kijai"
    },
    {
      "workflow": "VACE with face masking for targeted lip sync",
      "use_case": "Using MultiTalk with VACE and masks to regenerate only face area for lip sync",
      "from": "Tango Adorbo"
    },
    {
      "workflow": "Qwen image to Wan bridge",
      "use_case": "Generate image with Qwen, then feed to high sampler to low sampler for video generation",
      "from": "fredbliss"
    },
    {
      "workflow": "Two-step Qwen to I2V process",
      "use_case": "Generate image with Qwen in first workflow, then use second workflow to convert image to video",
      "from": "243049749014380544"
    },
    {
      "workflow": "4-stage sampling with LoRA switching",
      "use_case": "Wan 2.2 14B I2V at 1280x720 with Lightning 8 steps (2/2/2/2) - High first stage without LoRA, second with LoRA / Low first stage without LoRA, second with LoRA",
      "from": ": Not Really Human :"
    },
    {
      "workflow": "QwenImageWanBridge workflow",
      "use_case": "Creating less broken messes with Qwen integration, needs experimentation with noise settings",
      "from": "fredbliss"
    },
    {
      "workflow": "LLM description workflow",
      "use_case": "Using Qwen2.5 VL to describe image then reprompting T5 to get base image correct before feeding to Wan2.2 I2V",
      "from": "Josiah"
    },
    {
      "workflow": "Wan 2.2 upscaling workflow",
      "use_case": "720p to 1024p upscale in 15 minutes using low noise model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Endless I2V workflow adaptation",
      "use_case": "Modified POM's endless workflow to use I2V instead of VACE for continuous video generation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "InfiniteTalk with Fantasy Portrait v2v",
      "use_case": "Swapped MultTalk model with Infinite model in Fantasy Portrait workflow for driving video",
      "from": "NC17z"
    },
    {
      "workflow": "Multi-pass video creation",
      "use_case": "First pass with basic settings, second pass for upscaling and refinement",
      "from": "NC17z"
    },
    {
      "workflow": "V2V using InfiniteTalk with uni3c",
      "use_case": "Feed video input to multitalk node, ensure on infinitetalk, connect uni3c without latent hooked up, then generate",
      "from": "DawnII"
    },
    {
      "workflow": "Video inpainting with VACE and segmentation",
      "use_case": "Combining VACE inpainting with segmentation mask for object replacement in videos",
      "from": "Drommer-Kille"
    },
    {
      "workflow": "Camera motion using first and last frame",
      "use_case": "Generate jib and parallax camera movements by creating first and last frames",
      "from": "NC17z"
    },
    {
      "workflow": "Qwen Image -> Wan Video T2V pipeline",
      "use_case": "Creating video from Qwen-generated images with better alignment between image and video content",
      "from": "fredbliss"
    },
    {
      "workflow": "InfiniteTalk + vid2vid cleanup",
      "use_case": "Generate long video with InfiniteTalk, then clean up motion issues with vid2vid at 0.65 denoise",
      "from": "samhodge"
    },
    {
      "workflow": "InfiniteTalk video-to-video lip sync",
      "use_case": "Creating talking head videos from existing video + audio",
      "from": "Kijai"
    },
    {
      "workflow": "Qwen Image to Wan latent bridge",
      "use_case": "Using image generation models to drive video generation",
      "from": "fredbliss"
    },
    {
      "workflow": "Video looping for long audio",
      "use_case": "Extending short video clips to match longer audio duration",
      "from": "NebSH"
    },
    {
      "workflow": "Krita ComfyUI integration for inpainting",
      "use_case": "Live painting workflow with exposed generation parameters",
      "from": "Kijai"
    },
    {
      "workflow": "InfiniteTalk vid2vid with padding",
      "use_case": "Lip sync with video input, faster than other methods",
      "from": "NC17z"
    },
    {
      "workflow": "High noise first pass then InfiniteTalk second pass",
      "use_case": "Start with HN model normally, end early and run rest with InfiniteTalk vid2vid style",
      "from": "Josiah"
    },
    {
      "workflow": "Multiple character masking for InfiniteTalk",
      "use_case": "Splits each character's face to 16:9 B&W mask, first mask gets first audio",
      "from": "mdkb"
    },
    {
      "workflow": "Audio splitting by frame count and FPS",
      "use_case": "For workflows with multiple samplers creating multiple videos",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Two stage Wan 2.2 workflow with denoised output saving",
      "use_case": "High noise model to 50% signal noise, save denoised latents, then load in stage 2 with low noise model",
      "from": "MysteryShack"
    },
    {
      "workflow": "InfiniteTalk with multiple samplers loop",
      "use_case": "Each sampler uses previous sampler's result as I2V input, like advanced 'continue from last frame'",
      "from": "Kijai"
    },
    {
      "workflow": "VACE with differential diffusion masking",
      "use_case": "Can use latent masking alongside VACE for inpainting",
      "from": "Kijai"
    },
    {
      "workflow": "VACE inpainting with ControlNet fill",
      "use_case": "Fill mask with corresponding ControlNet data, leave outside unprocessed to avoid artifacts",
      "from": "Nekodificador"
    },
    {
      "workflow": "2.2 I2V InfiniteTalk workflow shared",
      "use_case": "Using InfiniteTalk with WAN 2.2 for video extension",
      "from": "DawnII"
    },
    {
      "workflow": "Celebrity LoRA training with Flux for keyframes, then VACE inpainting",
      "details": "Train custom LoRAs with Flux using recognizable celebrities, generate reference frames, use V2V > gen > mask > regen pipeline",
      "from": "Nekodificador"
    },
    {
      "workflow": "Blender control splines for camera movement",
      "use_case": "Creating videos with complex camera movements and object animations using grease pencil outlines, curves and spheres",
      "from": "Blink"
    },
    {
      "workflow": "Multitalk with masking integration",
      "use_case": "Lip sync with selective area masking during generation",
      "from": "Kijai"
    },
    {
      "workflow": "4-stage Lightning sampling",
      "use_case": "Stage 1 High without LoRa / Stage 2 High with LoRa / Stage 3 Low without LoRa / Stage 4 Low with LoRa for 8 steps total",
      "from": ": Not Really Human :."
    },
    {
      "workflow": "VACE + Phantom combination for character consistency",
      "use_case": "Maintaining character likeness across video generation",
      "from": "Ablejones"
    },
    {
      "workflow": "Two-stage sampling with Wan 2.2 high/low noise",
      "use_case": "Splitting steps between high noise and low noise samplers",
      "from": "ComfyCod3r"
    },
    {
      "workflow": "Manual mask creation in DaVinci Fusion for VACE",
      "use_case": "Better character replacement results with properly proportioned masks",
      "from": "Nekodificador"
    },
    {
      "workflow": "Cinescale upscaling",
      "use_case": "2x quality improvement with 1.5x upscale using two samplers with different settings and RoPE scaling",
      "from": "Kijai"
    },
    {
      "workflow": "Multi-sampler slowmo fighting",
      "use_case": "Using three different samplers with varying acceleration loras and CFG settings to combat slow motion artifacts",
      "from": "Juan Gea"
    },
    {
      "workflow": "Static camera control with Uni3C",
      "use_case": "Repeat image first, then encode to create static camera control for video generation",
      "from": "Juan Gea"
    },
    {
      "workflow": "MAGREF with context windows for long videos",
      "use_case": "Creating longer video sequences while maintaining character consistency",
      "from": "Kijai"
    },
    {
      "workflow": "I2V extension from last frame for looping",
      "use_case": "Creating smooth loops instead of using context windows",
      "from": "Kenk"
    },
    {
      "workflow": "VACE inpainting with end frame reference",
      "use_case": "Controlled transformations like dissolving effects",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "workflow": "InfiniteTalk long generation",
      "use_case": "Endless generation without context window drawbacks, no motion reduction or speed loss",
      "from": "Kijai"
    },
    {
      "workflow": "Manual chaining I2V samplers",
      "use_case": "Alternative to InfiniteTalk node for more control with different prompts/settings per clip",
      "from": "seitanism"
    },
    {
      "workflow": "Multi-sampler setup for speed LoRAs with Wan 2.2",
      "use_case": "Using speed LoRAs with high noise model",
      "from": "ArtOfficial"
    },
    {
      "workflow": "2/1 or 2/2 HN then full distill LN distribution",
      "use_case": "Preferred sampler distribution for Wan 2.2",
      "from": "DawnII"
    },
    {
      "workflow": "3 sampler setup with 1 step full HN",
      "use_case": "Making speed LoRAs work with Wan 2.2",
      "from": "Karo"
    },
    {
      "workflow": "Two-pass talking face swap",
      "use_case": "First pass on Fantasy Portrait, then vid2vid InfiniteTalk for head movements and better lip sync",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "workflow": "FantasyPortrait + InfiniteTalk combination",
      "use_case": "Using both models together with looping sampling for head pose and lip sync",
      "from": "Kijai"
    },
    {
      "workflow": "Chain samplers for early preview",
      "use_case": "Using 4 samplers with 1 step each instead of 1 sampler with 4 steps to preview and stop early if needed",
      "from": "scf"
    },
    {
      "workflow": "T2V to I2V pipeline",
      "use_case": "Save latent from T2V and move to I2V model, though it usually breaks movement dynamics",
      "from": "Draken"
    },
    {
      "workflow": "Context windows with looping",
      "use_case": "Quality takes big hit and inference time x3 or more, but enables longer generations",
      "from": "Kenk"
    },
    {
      "workflow": "High/Low noise sampling with distillation",
      "use_case": "Using Lightning on high noise, LightX2V on low noise at 0.8 weight each",
      "from": "CJ"
    },
    {
      "workflow": "FantasyPortrait + InfiniteTalk",
      "use_case": "Face-synced talking head generation with audio",
      "from": "Kijai"
    },
    {
      "workflow": "VACE + InfiniteTalk face replacement",
      "use_case": "Replacing actors in movie clips with character LoRAs and syncing audio",
      "from": "JalenBrunson"
    },
    {
      "workflow": "Multiple LoRAs for multi-clip projects",
      "use_case": "4 clips stitched together, different LoRA combinations for different segments",
      "from": "Samy"
    },
    {
      "workflow": "VACE character replacement using single frame I2V",
      "use_case": "Identity swapping for film shots using Wan 2.2 Low Noise model with lightning and fastwan loras at 1280x720",
      "from": "mdkb"
    },
    {
      "workflow": "FantasyPortrait first, then InfiniteTalk vid2vid refinement",
      "use_case": "Combining head movement projection with audio-reactive lip sync",
      "from": "Kijai"
    },
    {
      "workflow": "MagRef + InfiniteTalk for clip extension",
      "use_case": "Maintaining likeness and quality with no degradation across extended clips",
      "from": "seitanism"
    },
    {
      "workflow": "Wan VACE I2I via controlnet depth for single amazing image",
      "use_case": "Creating high-quality single images using video model with depth control",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "workflow": "Using differential diffusion for multiple LoRAs",
      "use_case": "Apply different LoRAs to different spatial regions by doing second generation with differential diffusion",
      "from": "mamad8"
    },
    {
      "workflow": "mdkb's VACE restyling workflow in zip file",
      "use_case": "For VACE restyling and decipher workflow setup",
      "from": "mdkb"
    },
    {
      "workflow": "Using Blender + OBS + Uni3c for camera motion",
      "use_case": "3D model dragged backward in Blender, captured with OBS, edited with Shotcut, controlnet into Uni3c",
      "from": "mdkb"
    },
    {
      "workflow": "FFLF (First Frame Last Frame) for video extension",
      "use_case": "Creating longer videos based on latest frame using second samplers",
      "from": "Kenk"
    },
    {
      "workflow": "T2V workflow using removed image input nodes from i2v",
      "use_case": "Messy but working T2V setup for 2.2 14b",
      "from": "crinklypaper"
    },
    {
      "workflow": "Using extra latent node with both samplers",
      "use_case": "Prevents glitches at first several frames when using Wan 2.2",
      "from": "N0NSens"
    },
    {
      "workflow": "CFG Schedule with normal steps instead of 3 samplers",
      "use_case": "Better prompt adherence and faster inference for T2V",
      "from": "Mu5hr00m_oO"
    },
    {
      "workflow": "Using ComfyUI native audio encoder nodes with S2V",
      "use_case": "Audio processing for speech-to-video generation",
      "from": "Kijai"
    },
    {
      "workflow": "S2V workflow shared",
      "use_case": "Speech-to-video generation testing",
      "from": "slmonker"
    },
    {
      "workflow": "TTS -> S2V pipeline suggestion",
      "use_case": "Text-to-speech combined with speech-to-video for complete audio-visual generation",
      "from": "daking999"
    },
    {
      "workflow": "MultiTalk with MAGREF workflow",
      "use_case": "Alternative to InfiniteTalk when noise issues occur",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "workflow": "Vid2vid VACE workflow for expression matching",
      "use_case": "Expression matching and lip-sync on video input with masking",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "workflow": "S2V with context windows for long videos",
      "use_case": "Generating longer videos without degradation",
      "from": "Kijai"
    },
    {
      "workflow": "VACE + masking for background replacement",
      "use_case": "Replacing backgrounds in existing videos",
      "from": "Dream Making"
    },
    {
      "workflow": "Multitalk + MagRef I2V with Uni3C camera control",
      "use_case": "48 second video generation with camera movement from concert video",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "workflow": "S2V with context windows for long video generation",
      "use_case": "Generating 601 frames at 960x640 in 13 minutes using audio + reference image guidance",
      "from": "Kijai"
    },
    {
      "workflow": "Infinite looping WAN workflow",
      "use_case": "Takes 4 prompts to generate seamless looping video",
      "from": "chancelor"
    },
    {
      "workflow": "InfiniteTalk with VACE inpainting",
      "use_case": "Combining lip-sync with pose control and inpainting",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "workflow": "S2V with pose control and audio",
      "use_case": "Lip sync generation with facial pose guidance",
      "from": "Kijai"
    },
    {
      "workflow": "VACE inpainting with extra frames",
      "use_case": "Video inpainting when frames not multiple of 4",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "workflow": "InfiniteTalk with I2V motion",
      "use_case": "Lip sync with additional character motion and background animation",
      "from": "fredbliss"
    },
    {
      "workflow": "Video continuation using ImageToVideo Encode",
      "use_case": "Endless video generation by stitching I2V segments",
      "from": "seb bae"
    },
    {
      "workflow": "S2V model for infinite video generation without audio",
      "use_case": "Extending videos indefinitely using S2V's reference motion capability without requiring audio input",
      "from": "comfy"
    },
    {
      "workflow": "Using separate masks for multi-character Infinite Talk",
      "use_case": "Control which faces get lip sync in videos with multiple characters using masks and separate audio tracks",
      "from": "jjtjjt"
    },
    {
      "workflow": "iPhone camera motion transfer",
      "use_case": "Using iPhone videos to drive camera movements in AI generations through uni3c",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "workflow": "2.2 HN -> S2V with context windows",
      "use_case": "Combining high noise 2.2 with S2V using context windows",
      "from": "DawnII"
    },
    {
      "workflow": "Infinite Talk with MAGREF",
      "use_case": "Best lip sync results on RTX3090 using Infinite Talk with MAGREF 14B fp8 scaled",
      "from": "NC17z"
    },
    {
      "workflow": "Long video generation with MAGREF",
      "use_case": "Using reference image for long videos with better context window performance",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "workflow": "InfiniteTalk with TTS integration",
      "use_case": "Generate audio first with TTS then use for video generation rather than running TTS with each generation",
      "from": "JohnDopamine"
    },
    {
      "workflow": "VACE with spline editor for camera control",
      "use_case": "Lock camera movement by keeping spline static on background",
      "from": "DawnII"
    },
    {
      "workflow": "Context windows for long generations",
      "use_case": "Generate 3-4 minute videos 81 frames at a time passing previous 8 frames into context",
      "from": "dipstik"
    },
    {
      "workflow": "2.2 for T2I, QwenEdit for stills, 2.2 for I2V without lipsync, 2.1 with Multi/Infinit + uni3c for lipsync",
      "use_case": "Complete video generation pipeline",
      "from": "N0NSens"
    },
    {
      "workflow": "Basic vid2vid then composite head/mouth to original",
      "use_case": "Changing lips in existing video",
      "from": "Kijai"
    },
    {
      "workflow": "Context windows with stride on high noise side only",
      "use_case": "Better window blending in 2.2 workflows",
      "from": "Kijai"
    },
    {
      "workflow": "High noise with overlap/stride then low noise without stride",
      "use_case": "Context window generation with 2.2 - use 48 overlap and 10 stride for high noise, then different settings for low noise",
      "from": "Kijai"
    },
    {
      "workflow": "FantasyPortrait + S2V combination",
      "use_case": "Combining pose control methods - FP on first step only with S2V",
      "from": "Kijai"
    },
    {
      "workflow": "Prompt travel with different prompts per context window",
      "use_case": "Each prompt for ~81 frames in context windows",
      "from": "Dever and Kijai"
    },
    {
      "workflow": "CineScale upscaling with two samplers",
      "use_case": "Latent upscale between two samplers for video upscaling",
      "from": "DawnII"
    },
    {
      "workflow": "InfiniteTalk long generation",
      "use_case": "2000 frames in 28 windows, at 832x480 with 4 steps, 12 mins",
      "from": "Kijai"
    },
    {
      "workflow": "S2V with driving video",
      "use_case": "S2V seems to like having a driving video a lot more. 4 second wan2.2 t2v gen pose video, with 10 second s2v gen",
      "from": "ArtOfficial"
    },
    {
      "workflow": "VACE with between frames setup",
      "use_case": "Connect to a VACE embed for first-last frame techniques",
      "from": "Nekodificador"
    },
    {
      "workflow": "3 KSampler workflows for Wan",
      "use_case": "Better video generation results, user wants to add second image as last frame",
      "from": "BecauseReasons"
    },
    {
      "workflow": "First/Last Frame workflow using WanFirstLastFrameToVideo node",
      "use_case": "Replaces WanImageToVideo for FLF generation",
      "from": "Juampab12"
    },
    {
      "workflow": "VACE first frame edit v2v",
      "use_case": "Take video and new first frame to edit video with new first frame as reference",
      "from": "Juampab12"
    }
  ],
  "settings": [
    {
      "setting": "LightX2V strength",
      "value": "High noise: 3, Low noise: 1",
      "reason": "Better quality and prompt adherence",
      "from": "gokuvonlange"
    },
    {
      "setting": "Steps configuration with LightX",
      "value": "High noise: 3 steps (no lightx) + 6 steps (with lightx), Low noise: 3 steps (with lightx)",
      "reason": "Optimal balance of quality and speed",
      "from": "TK_999"
    },
    {
      "setting": "CFG values",
      "value": "High noise: CFG=5 (first steps), CFG=1 (later steps), Low noise: CFG=1",
      "reason": "Better convergence",
      "from": "TK_999"
    },
    {
      "setting": "Denoise for custom nodes",
      "value": "0.5-0.7",
      "reason": "Produces dramatically different and better results",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "VACE strength",
      "value": "1.0-1.5",
      "reason": "1.0 can cause background changes, 1.5 helps but may oversaturate",
      "from": "seitanism"
    },
    {
      "setting": "Context windows frame count",
      "value": "81 frames per scene",
      "reason": "For smooth scene transitions when using | separator",
      "from": "avataraim"
    },
    {
      "setting": "Launch flag for memory issues",
      "value": "--cache-none",
      "reason": "Reduces RAM usage by disabling node caching",
      "from": "Kijai"
    },
    {
      "setting": "RadialAttention block size",
      "value": "128",
      "reason": "Image dimensions must be divisible by this value",
      "from": "NebSH"
    },
    {
      "setting": "High/Low noise model split",
      "value": "High: 6 steps (0-6, CFG 3), Low: 4 steps (6-10, CFG 1)",
      "reason": "Good balance of quality and prompt adherence",
      "from": "IceAero"
    },
    {
      "setting": "Alternative split for prompt adherence",
      "value": "High: 20 steps (0-10, CFG 3.5), Low: 6 steps (3-6, CFG 1)",
      "reason": "Better prompt following with high model getting more steps",
      "from": "gokuvonlange"
    },
    {
      "setting": "Context window overlap",
      "value": "10 stride, 48 overlap on high noise",
      "reason": "Successfully tested combination for window blending",
      "from": "Kijai"
    },
    {
      "setting": "LightX2V strength for AMD",
      "value": "Strength 1, CFG 1",
      "reason": "Works well with 5 steps (3 high, 2 low) on AMD setup",
      "from": "nacho.money"
    },
    {
      "setting": "Timestep split ratio",
      "value": "0.875 threshold",
      "reason": "Original WAN training used 875 timestep split between high/low noise models",
      "from": "Kijai"
    },
    {
      "setting": "CFG",
      "value": "3.5 for high pass, 1.5 for low pass",
      "reason": "Better prompt adherence",
      "from": "loopen44"
    },
    {
      "setting": "LightX2V LoRA strength",
      "value": "3 for high pass, 1 for low pass",
      "reason": "Optimal balance",
      "from": "loopen44"
    },
    {
      "setting": "Steps",
      "value": "5x5 steps for I2V",
      "reason": "Good balance of quality and speed",
      "from": "loopen44"
    },
    {
      "setting": "Context overlap",
      "value": "Higher overlap blends better but slows generation",
      "reason": "Quality vs speed tradeoff",
      "from": "Kijai"
    },
    {
      "setting": "Sampler",
      "value": "DDIM for faster generation",
      "reason": "Half the time of DPMPP_SDE",
      "from": "The Shadow (NYC)"
    },
    {
      "setting": "Steps for no LoRA",
      "value": "15 steps on high noise, 25 steps on low noise (40 total)",
      "reason": "Official recommendation",
      "from": "aikitoria"
    },
    {
      "setting": "CFG for no LoRA",
      "value": "3.0 to 3.5",
      "reason": "For superior motion and prompt adherence without LoRAs",
      "from": "mdkb"
    },
    {
      "setting": "LightX LoRA strength",
      "value": "0.5 on second pass",
      "reason": "Better quality with 10 steps each pass",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Timestep boundary for model switching",
      "value": "0.875 (timestep 875)",
      "reason": "Official code specification for high vs low noise model usage",
      "from": "aikitoria"
    },
    {
      "setting": "Boundary value for 40 steps",
      "value": "15",
      "reason": "Exactly correct for 40 steps/simple/shift 5",
      "from": "aikitoria"
    },
    {
      "setting": "Boundary value for 20 steps",
      "value": "8",
      "reason": "Proportionally correct for 20 step generation",
      "from": "aikitoria"
    },
    {
      "setting": "LightX LoRA for T2V only",
      "value": "2 steps low model only, 1 CFG, unipc/beta",
      "reason": "Faster inference for T2V",
      "from": "Rainsmellsnice"
    },
    {
      "setting": "Context window overlap",
      "value": "Lower values for low noise side",
      "reason": "Doesn't need as much overlap, makes generation faster",
      "from": "Kijai"
    },
    {
      "setting": "Context overlap",
      "value": "16 with 4 stride",
      "reason": "Good balance vs 48 overlap which takes much longer",
      "from": "thaakeno"
    },
    {
      "setting": "Shift parameter",
      "value": "3-5",
      "reason": "Lower values give more creativity, around 3-5 works well",
      "from": "thaakeno"
    },
    {
      "setting": "CFG for high/low noise split",
      "value": "High: 3.5, Low: 1",
      "reason": "Prevents washed out output when using lightx2v LoRAs",
      "from": "BondoMan"
    },
    {
      "setting": "Steps for T2I quality",
      "value": "30 low noise, 20 high noise",
      "reason": "Better sharpness and overall quality",
      "from": "\ud83d\udc1d bumblebee \ud83d\udc1d"
    },
    {
      "setting": "Block swap",
      "value": "20",
      "reason": "Manages VRAM usage during intensive operations",
      "from": "thaakeno"
    },
    {
      "setting": "Model quantization",
      "value": "fp8_e5m2",
      "reason": "Required for proper model loading instead of default",
      "from": "topmass"
    },
    {
      "setting": "Steps for V2V upscale",
      "value": "12 steps with start at 9",
      "reason": "Minimize changes while upscaling",
      "from": "thaakeno"
    },
    {
      "setting": "CFG for 2.2 without LightX",
      "value": "3.5 CFG with 20 steps",
      "reason": "Working parameters for 2.2 T2V",
      "from": "N0NSens"
    },
    {
      "setting": "Sampler for 5B model",
      "value": "flowmatch_pusa in wrapper, unipc in native",
      "reason": "Different optimal samplers for different implementations",
      "from": "daking999"
    },
    {
      "setting": "CFG",
      "value": "5 on high noise timesteps",
      "reason": "Works well for T2I and T2V",
      "from": "mamad8"
    },
    {
      "setting": "Steps configuration",
      "value": "3+5",
      "reason": "Ends on proper sigma the best",
      "from": "DawnII"
    },
    {
      "setting": "Sampler",
      "value": "euler + beta for T2V/I2V, res_multistep + beta57 for T2I",
      "reason": "Better results",
      "from": "mamad8"
    },
    {
      "setting": "High noise CFG + SLG first step",
      "value": "CFG + SLG first step, then 2 steps with CFG 1.0 and low side CFG 1.0",
      "reason": "Kijai's working configuration",
      "from": "Kijai"
    },
    {
      "setting": "H100 weight and compute dtype",
      "value": "fp16 to use fp16 fast accum",
      "reason": "Optimization for H100",
      "from": "Kijai"
    },
    {
      "setting": "LightX LoRA strength",
      "value": "High: 3.0, Low: 1.5",
      "reason": "Gives great results in testing",
      "from": "screwfunk"
    },
    {
      "setting": "CFG for high noise model",
      "value": "3.5",
      "reason": "Improves character motion significantly",
      "from": "Rainsmellsnice"
    },
    {
      "setting": "Steps distribution",
      "value": "8 steps total: 4 high, 4 low",
      "reason": "Balanced approach for quality and speed",
      "from": "screwfunk"
    },
    {
      "setting": "Blockswap for 4090",
      "value": "Full 40 blocks possible",
      "reason": "Takes advantage of available VRAM efficiently",
      "from": "VK"
    },
    {
      "setting": "Start step for upscaling",
      "value": "8-9 out of 12 for pure upscale, 4 for motion fixes",
      "reason": "Controls how much video changes during processing",
      "from": "thaakeno"
    },
    {
      "setting": "Block swap for 14B model",
      "value": "20 blocks",
      "reason": "Uses ~25GB VRAM for 81 frames at 1280x720 with 10 steps",
      "from": "Kijai"
    },
    {
      "setting": "lightx2v maximum steps",
      "value": "6 steps or less",
      "reason": "More than 6 steps ruins quality with lightx",
      "from": "ComfyCod3r"
    },
    {
      "setting": "lightx2v strength",
      "value": "2.0",
      "reason": "Recommended strength for both t2v and i2v variants",
      "from": "Draken"
    },
    {
      "setting": "Character LoRA training",
      "value": "min_t=0, max_t=1, 24 source images, 512px resolution",
      "reason": "Fast convergence for likeness in 9 minutes",
      "from": "Kenk"
    },
    {
      "setting": "5B upscaling denoise",
      "value": "0.3-0.5",
      "reason": "Good balance for upscaling 14B output without artifacts",
      "from": "Juan Gea"
    },
    {
      "setting": "Multitalk sync",
      "value": "High seed: specific number, Low seed: 0, shift: 8",
      "reason": "Closest to achieving audio sync",
      "from": "nacho.money"
    },
    {
      "setting": "Distill LoRA steps",
      "value": "3/3 or 5/5 for i2v, 10/10 without LoRA",
      "reason": "Better results than higher step counts which can overcook",
      "from": "Karo"
    },
    {
      "setting": "Model switching sigma",
      "value": "0.875",
      "reason": "Intended sigma value for high/low noise model transition",
      "from": "Ablejones"
    },
    {
      "setting": "lightx2v LoRA strength",
      "value": "3.0+",
      "reason": "Below 3.0 doesn't work properly, prompt only started working at 3.0",
      "from": "Kijai"
    },
    {
      "setting": "distill LoRA strength",
      "value": "0.20",
      "reason": "Works well with cfg 2, 10 steps, follows prompt very well",
      "from": "hicho"
    },
    {
      "setting": "fusion LoRA i2v sweet spot",
      "value": "4.0",
      "reason": "Found to be sweet spot for i2v with fusion lora",
      "from": "topmass"
    },
    {
      "setting": "block swap for high resolution",
      "value": "30",
      "reason": "Enables 3072x1840 generation with 5B model",
      "from": "Juan Gea"
    },
    {
      "setting": "cfg for burning prevention",
      "value": "3.5",
      "reason": "2cfg was burning too much at higher values",
      "from": "Kijai"
    },
    {
      "setting": "LoRA rank preference",
      "value": "64 or 128",
      "reason": "64 is fine, 128 can have subtle improvements, lower than 16 are awful",
      "from": "Kijai"
    },
    {
      "setting": "shift",
      "value": "1",
      "reason": "Produces perfectly sharp images for T2I generation",
      "from": "aikitoria"
    },
    {
      "setting": "LoRA rank",
      "value": "64",
      "reason": "Good balance - 256 is huge and doesn't add much, 8 too low",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "LightX LoRA strength",
      "value": "0.5",
      "reason": "Helps add details without overdoing it",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "steps for two-pass",
      "value": "20 steps each pass",
      "reason": "Works better than single 40-step pass for detail generation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Shift value",
      "value": "1 for T2I, 12 for matching original code",
      "reason": "Shift 1 works best for T2I workflows, shift 12 matches Alibaba's implementation",
      "from": "aikitoria"
    },
    {
      "setting": "CFG without LightX",
      "value": "3.5 CFG for 20 steps high noise, then 6 steps low noise",
      "reason": "Proven configuration that works well",
      "from": "TK_999"
    },
    {
      "setting": "High/Low noise steps for T2V",
      "value": "25 steps high noise, 15 steps low noise",
      "reason": "Matches official implementation boundary at 0.875",
      "from": "aikitoria"
    },
    {
      "setting": "LoRA rank",
      "value": "32 instead of 64",
      "reason": "Should be fine and reduces memory impact",
      "from": "Kijai"
    },
    {
      "setting": "Wan 2.2 Lightning LoRA strength",
      "value": "0.125",
      "reason": "Alpha 8, rank 64 = 0.125. Original had hardcoded alpha values",
      "from": "Kijai"
    },
    {
      "setting": "Shift parameter for short step counts",
      "value": "8-9",
      "reason": "Wan is linear and needs high shelf at start for short steps",
      "from": "Simjedi"
    },
    {
      "setting": "Steps for Lightning LoRAs",
      "value": "4 steps",
      "reason": "Works well with euler scheduler and 0.125 strength",
      "from": "Kijai"
    },
    {
      "setting": "CFG for Wan 2.2 with Lightning",
      "value": "High: 3.5, Low: 1.0",
      "reason": "Recommended settings for high and low noise models",
      "from": "Nemlet17"
    },
    {
      "setting": "LoRA strength for new lightning",
      "value": "1.0",
      "reason": "Fixed versions work at normal strength unlike original 0.125 requirement",
      "from": "Kijai"
    },
    {
      "setting": "Step distribution",
      "value": "3 high + 3 low minimum",
      "reason": "2+2 steps insufficient, 3+3 fine, 4+4 much better",
      "from": "Kijai/Juampab12"
    },
    {
      "setting": "Sampler",
      "value": "Euler",
      "reason": "Official training uses Euler only",
      "from": "Kijai"
    },
    {
      "setting": "Shift value",
      "value": "5",
      "reason": "Used in official training setup",
      "from": "Kijai"
    },
    {
      "setting": "CFG",
      "value": "1.0 for LoRA steps",
      "reason": "Works best with lightning LoRAs, CFG 3.5 can oversaturate",
      "from": "multiple users"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "1.0/1.0 for high/low noise",
      "reason": "Default recommended settings",
      "from": "Doctor Shotgun"
    },
    {
      "setting": "CFG for Lightning",
      "value": "CFG 1.0",
      "reason": "Designed for low CFG operation",
      "from": "gokuvonlange"
    },
    {
      "setting": "Steps for Lightning",
      "value": "4 steps (2/2 split)",
      "reason": "Official recommendation for 2.2 Lightning",
      "from": "Doctor Shotgun"
    },
    {
      "setting": "Resolution for Lightning",
      "value": "1280x720",
      "reason": "Works better than 480p, avoids slow motion artifacts",
      "from": "Kijai"
    },
    {
      "setting": "Scheduler",
      "value": "Euler",
      "reason": "Works better than LCM for Lightning LoRAs",
      "from": "piscesbody"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "1.0 for both High and Low noise",
      "reason": "Recommended starting point for new Lightning LoRAs",
      "from": "Purz"
    },
    {
      "setting": "CFG for Lightning LoRA",
      "value": "CFG 1.0 with Lightning, CFG 3.5 without",
      "reason": "Lightning LoRA is distilled to work at low CFG",
      "from": "multiple users"
    },
    {
      "setting": "FastWan 5B steps",
      "value": "3-6 steps with CFG 1.0",
      "reason": "Distilled model designed for low step count",
      "from": "Kijai"
    },
    {
      "setting": "Block swapping for 5B",
      "value": "30 blocks out",
      "reason": "Enables high resolution generation on 24GB VRAM",
      "from": "Juan Gea"
    },
    {
      "setting": "Lightning lora strength for T2I",
      "value": "CFG 1, strength 0.5, 15 steps high and low",
      "reason": "Better results than 4 steps at 1 strength and better than without loras",
      "from": "Shawneau \ud83c\udf41 [CA]"
    },
    {
      "setting": "Fast 5B T2V generation time",
      "value": "10 second generation + 17 second decode",
      "reason": "With compile optimization",
      "from": "Kijai"
    },
    {
      "setting": "Full model I2V settings",
      "value": "20 steps, CFG 3.5, no distill or lightx2v lora",
      "reason": "Really good prompt adherence but takes very long to generate",
      "from": "gokuvonlange"
    },
    {
      "setting": "Minimum steps recommendation",
      "value": "6 steps total (3+3)",
      "reason": "Less than 6 produces errors or bad gens",
      "from": "Lodis"
    },
    {
      "setting": "Lightning lora scheduler",
      "value": "Euler with shift=5, cfg=1.0",
      "reason": "Official recommendation for NFE=4 not 8",
      "from": "Kijai"
    },
    {
      "setting": "LightX2V 3+3 configuration",
      "value": "3 steps high + 3 steps low",
      "reason": "Works fantastic with old lightx",
      "from": "Lodis"
    },
    {
      "setting": "CRF encoding setting",
      "value": "CRF 10 or 17",
      "reason": "CRF 17 supposed to be visually lossless, CRF 10 for higher quality",
      "from": "N0NSens"
    },
    {
      "setting": "MMAudio configuration",
      "value": "Interpolation at 30 or 60fps, 5s config, 4-5 word prompts",
      "reason": "Works better with interpolation and short prompts",
      "from": ": Not Really Human :"
    },
    {
      "setting": "Lightning lora strength",
      "value": "Lower strength with low cfg values",
      "reason": "Works with low cfg if you lower strength",
      "from": "Kijai"
    },
    {
      "setting": "sampler_name and scheduler for lightx LoRA",
      "value": "euler beta",
      "reason": "Recommended sampler for Wan 2.2 with lightx LoRA",
      "from": "Dan"
    },
    {
      "setting": "blockswapping",
      "value": "20 blocks",
      "reason": "Enables upscaling to 1080p on 24GB VRAM",
      "from": "thaakeno"
    },
    {
      "setting": "I2V LoRA combination",
      "value": "phantom fusionx + pusa + lightx2v",
      "reason": "Provides better motion quality and prompt following than lightning LoRA alone",
      "from": "Ada"
    },
    {
      "setting": "Lightning LoRA usage",
      "value": "4 steps total, 1 cfg",
      "reason": "Optimal settings for the new lightning LoRA, though limited to simple prompts",
      "from": "Ada"
    },
    {
      "setting": "CFG scheduling for native 2.2",
      "value": "CFG 4-5 on first step only, revert to CFG 1 for remaining steps",
      "reason": "Better stability with split sampling",
      "from": "PizzaSlice"
    },
    {
      "setting": "Lightning 2.2 timestep schedule",
      "value": "euler/simple with shift 5.0 and flowmatch_distill",
      "reason": "Matches their specific timesteps for 4 steps",
      "from": "Kijai"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "0.2-0.35",
      "reason": "Prevents color burning and convergence issues",
      "from": "3DBicio"
    },
    {
      "setting": "LightX2V LoRA strength",
      "value": "2.0-3.0 strength with CFG 1",
      "reason": "Better resemblance maintenance in I2V",
      "from": "Jonathan"
    },
    {
      "setting": "WAN 2.2 Q4_K_S GGUF parameters",
      "value": "4 steps, CFG 1, LCM/simple scheduler, LightX2V 2.0 strength",
      "reason": "Works well even on lower-end hardware",
      "from": "Jonathan"
    },
    {
      "setting": "VAE tile sizes for 16GB VRAM",
      "value": "512x384 for tiles, 384 for stride",
      "reason": "Cuts VAE decoding time in half compared to default 272x144",
      "from": "patientx"
    },
    {
      "setting": "Qwen-Image precision",
      "value": "fp8_e5m2",
      "reason": "Avoids burnout errors that occur with fp8_e4m3fn",
      "from": "fredbliss"
    },
    {
      "setting": "VRAM usage",
      "value": "14GB idle, 23GB during inference",
      "reason": "Model loading/unloading based on VRAM constraints",
      "from": "642326806678077441"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "Strength 1 for old LightX2V on high noise",
      "reason": "Better motion results",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "setting": "Steps configuration",
      "value": "4+4 steps for LightX2V",
      "reason": "Commonly used configuration for good results",
      "from": ".: Not Really Human :."
    },
    {
      "setting": "Shift value",
      "value": "Shift 8 in HN (High Noise)",
      "reason": "Used in conjunction with CFG 1",
      "from": "piscesbody"
    },
    {
      "setting": "CFG",
      "value": "2.0",
      "reason": "Prevents blurry/foggy results with low contrast",
      "from": "Kijai"
    },
    {
      "setting": "Shift",
      "value": "5.0",
      "reason": "Better quality than 1.0 for general use, 0.5 for sharper images",
      "from": "Kijai"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "0.7",
      "reason": "Good compromise between speed and quality",
      "from": "Kijai"
    },
    {
      "setting": "Frame count",
      "value": "81",
      "reason": "Model cannot properly do 5 seconds at 24fps beyond this",
      "from": "Kijai"
    },
    {
      "setting": "Context overlap",
      "value": "More overlap needed",
      "reason": "For better results with context windows",
      "from": "Kijai"
    },
    {
      "setting": "CFG",
      "value": "1 for specific cases",
      "reason": "Juan Gea was using CFG=8 which might cause burning with speed LoRAs",
      "from": "Juan Gea"
    },
    {
      "setting": "Shift parameter",
      "value": "8 for I2V, 1 for T2I",
      "reason": "Shift=1 breaks I2V completely but required for T2I",
      "from": "N0NSens"
    },
    {
      "setting": "Scheduler",
      "value": "DMP++_SDE with beta",
      "reason": "Used in testing comparisons between shift values",
      "from": "Nekodificador"
    },
    {
      "setting": "Generation time on 5090",
      "value": "1 min 30s for 5 second video",
      "reason": "Performance benchmark",
      "from": "Kijai"
    },
    {
      "setting": "CFG for motion control",
      "value": "CFG 1 for T2V, higher for I2V",
      "reason": "T2V works well at low CFG, I2V needs higher CFG for motion",
      "from": "Josiah"
    },
    {
      "setting": "Lightning LoRA steps",
      "value": "4+4 steps or 3/3 steps",
      "reason": "Fast generation with Lightning LoRAs",
      "from": ".: Not Really Human :."
    },
    {
      "setting": "Non-Lightning generation",
      "value": "CFG 5+ and 20+ steps",
      "reason": "When not using Lightning LoRAs, need higher CFG and steps",
      "from": "N0NSens"
    },
    {
      "setting": "Frame limit for consistent motion",
      "value": "Up to 109 frames for I2V, 81 frames recommended",
      "reason": "Model trained on 81 frames, longer sequences may loop",
      "from": "N0NSens"
    },
    {
      "setting": "LightX LoRA strengths for HIGH and LOW models",
      "value": "HIGH (1.0, 1.0, 3.0) & LOW (3.0, 2.0, 1.0) at 6 total steps split at 3",
      "reason": "Provides good balance of speed and quality",
      "from": ": Not Really Human :."
    },
    {
      "setting": "FastWan maximum strength",
      "value": "0.35 for video",
      "reason": "Maximum recommended strength, image strength unknown",
      "from": "3DBicio"
    },
    {
      "setting": "V2V denoise level",
      "value": "Below 0.5",
      "reason": "For upscaling workflows where content already exists",
      "from": "mdkb"
    },
    {
      "setting": "Blockswap value",
      "value": "40",
      "reason": "More than 40 is useless for memory management",
      "from": ": Not Really Human :."
    },
    {
      "setting": "LightX2V high noise strength",
      "value": "2",
      "reason": "Consistently outperforms 2.2 lightning version",
      "from": "shuzhi"
    },
    {
      "setting": "VACE control strength",
      "value": "1:1",
      "reason": "Better results than 3 and 1.5 ratios",
      "from": "Lodis"
    },
    {
      "setting": "Add noise shift value",
      "value": "8-10",
      "reason": "Higher values produce cleaner results with dreamy effects",
      "from": "piscesbody"
    },
    {
      "setting": "CFG for Wan 2.2 Lightning",
      "value": "1.0",
      "reason": "Official workflow uses cfg 1.0 so negative prompts aren't used",
      "from": "Kijai"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "0.9 for high, varies for low",
      "reason": "Better results than 1.0, less artifacts",
      "from": "Kijai"
    },
    {
      "setting": "Steps for new Lightning",
      "value": "6 steps preferred over 4",
      "reason": "4 steps needs custom sigmas, 6 feels better",
      "from": "Kijai"
    },
    {
      "setting": "Flowmatch_distill with shift",
      "value": "5.0 shift (same as custom euler sigmas)",
      "reason": "Works well with 4 steps at 0.9 strength",
      "from": "Kijai"
    },
    {
      "setting": "HN model with Lightning",
      "value": "3+3, 4+4, or 5+3 steps",
      "reason": "Better than just 3+3, CFG optional on LN but helpful for certain elements",
      "from": "IceAero"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "1 for both high and low",
      "reason": "Recommended strength for new lightning LoRAs",
      "from": "Critorio"
    },
    {
      "setting": "Old Lightning LoRA strength",
      "value": "3 (high) and 1 (low)",
      "reason": "Previously working settings for older LoRAs",
      "from": "Gill Bastar"
    },
    {
      "setting": "Shift parameter for LightX",
      "value": "6",
      "reason": "Better results with LightX LoRAs",
      "from": "Karo"
    },
    {
      "setting": "CFG scheduling",
      "value": "Can define by step",
      "reason": "Available in both wrapper and native",
      "from": "Kijai"
    },
    {
      "setting": "Lightning I2V LoRA strengths",
      "value": "1.5/1 (high/low) for balanced motion",
      "reason": "Provides goldilocks balance between slow and fast motion",
      "from": "CaptHook"
    },
    {
      "setting": "Wan 2.2 optimal config",
      "value": "fp8, 1024x576, 81fr, unipc, shift 8, cfg 1, 6 steps",
      "reason": "Recommended working configuration",
      "from": "CaptHook"
    },
    {
      "setting": "CFG for Wan 2.2",
      "value": "CFG 1 on both samplers",
      "reason": "Best results with Lightning LoRA, constant 1 recommended",
      "from": "Cubey"
    },
    {
      "setting": "LightX2V adaptive rank LoRA",
      "value": "0.8 high side, 0.9 low side",
      "reason": "Better motion control than Lightning",
      "from": "MilesCorban"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "1.0",
      "reason": "Higher values make output look like 2.1 instead of 2.2",
      "from": "Ablejones"
    },
    {
      "setting": "Audio scale for MultiTalk",
      "value": "3.0",
      "reason": "Helps maintain lip sync quality for longer durations",
      "from": "samhodge"
    },
    {
      "setting": "Audio CFG",
      "value": "3.0",
      "reason": "Works well with audio scale 3 for vid2vid workflows",
      "from": "samhodge"
    },
    {
      "setting": "VRAM reservation",
      "value": "--reserve-vram 2",
      "reason": "Prevents tensor allocation errors in native ComfyUI with limited VRAM",
      "from": "Kijai"
    },
    {
      "setting": "Control end_percent",
      "value": "0.5",
      "reason": "Prevents errors when too low",
      "from": "Kijai"
    },
    {
      "setting": "Audio_scale",
      "value": "3",
      "reason": "Better MultiTalk lip sync performance",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "setting": "Block swap",
      "value": "30 consistently",
      "reason": "Prevents OOM when 20 fails, keep consistent across models",
      "from": "Kijai"
    },
    {
      "setting": "Denoise",
      "value": "< 1.0",
      "reason": "Required for proper video-to-video MultiTalk functionality",
      "from": "Kijai"
    },
    {
      "setting": "Switch DiT boundary",
      "value": "0.875",
      "reason": "Default boundary for switching from high to low noise model",
      "from": "fredbliss"
    },
    {
      "setting": "FlowMatchScheduler shift",
      "value": "5",
      "reason": "Default shift value used in modelscope diffsynth",
      "from": "fredbliss"
    },
    {
      "setting": "Lightning LoRA first step weight",
      "value": "0.0",
      "reason": "Prevents slow-motion issues",
      "from": "MysteryShack"
    },
    {
      "setting": "Fun Control steps",
      "value": "4 total steps recommended",
      "reason": "Official i2v workflow uses 4 total steps",
      "from": "Rainsmellsnice"
    },
    {
      "setting": "High/Low noise split",
      "value": "14 high / 6 low steps",
      "reason": "Better detail preservation and motion quality than 11/9 split",
      "from": "crinklypaper"
    },
    {
      "setting": "Shift value for sigma control",
      "value": "Shift 8 for higher sigma values, shift 5 for middle split",
      "reason": "Controls when high noise switches to low noise based on sigma threshold",
      "from": "Kijai"
    },
    {
      "setting": "Resolution for VRAM efficiency",
      "value": "800x480 for testing, lower res for longer frames",
      "reason": "Balance between quality and VRAM usage for extended generation",
      "from": "VK (5080 128gb)"
    },
    {
      "setting": "Input dimensions",
      "value": "Divisible by 16",
      "reason": "Prevents tensor size mismatch errors",
      "from": "Kijai"
    },
    {
      "setting": "Sapiens keypoint confidence threshold",
      "value": "0.5",
      "reason": "Filters out bad detection points",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "I2V lightning LoRA strength",
      "value": "Less than 1",
      "reason": "Prevents camera shake issues",
      "from": "XWAVE"
    },
    {
      "setting": "Radial attention dense timesteps",
      "value": "11",
      "reason": "Standard setting for WAN 2.2, 0 timesteps looks too good to be true",
      "from": "MysteryShack"
    },
    {
      "setting": "Frame count",
      "value": "77 frames instead of 81",
      "reason": "Eliminates slow motion effect, better matches Wan 2.2's 16-frame architecture",
      "from": "xwsswww"
    },
    {
      "setting": "LoRA strength for 2.1 LoRAs on 2.2",
      "value": "3.0 for high noise, 1.5 for low noise",
      "reason": "Proper strength adjustment for cross-version compatibility",
      "from": "Abyss"
    },
    {
      "setting": "RAM for Wan 2.2",
      "value": "96GB comfortable, 64GB minimum",
      "reason": "64GB can OOM after 3 generations with setting changes, 96GB is comfortable",
      "from": "Rainsmellsnice"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "High: 2.5, Low: 1.0",
      "reason": "For natural walking pace without artifacts",
      "from": "Alpha-Neo"
    },
    {
      "setting": "CFG for motion control",
      "value": "CFG 3.5 on high, 0.5 with light LoRA",
      "reason": "Finding sweet spot for decent quality with motion, keeping 0.9-1.0 on low",
      "from": "Hevi"
    },
    {
      "setting": "Lightning LoRA 4-step",
      "value": "4 steps with lightning LoRA on Wan 2.2",
      "reason": "For faster generation",
      "from": "army"
    },
    {
      "setting": "Control strength for 5B controlnet",
      "value": "0.5",
      "reason": "Good balance for first try results",
      "from": "Kijai"
    },
    {
      "setting": "VACE encode strength",
      "value": "0.7",
      "reason": "Standard setting for face swapping workflow",
      "from": "Ruairi Robinson"
    },
    {
      "setting": "Lighting LoRA strength",
      "value": "0.9",
      "reason": "Works well in multi-sampler setup",
      "from": "Karo"
    },
    {
      "setting": "ComfyUI startup parameter",
      "value": "--disable-smart-memory",
      "reason": "Helps with OOM issues on 12GB VRAM with 32GB system RAM",
      "from": "mdkb"
    },
    {
      "setting": "Denoise threshold for v2v",
      "value": "0.79",
      "reason": "Tipping point - lower keeps structure, higher allows prompt-based changes",
      "from": "mdkb"
    },
    {
      "setting": "v2v strength for good motion retention",
      "value": "0.85",
      "reason": "Maintains motion while allowing character changes",
      "from": "Benjimon"
    },
    {
      "setting": "Training frames for motion retention",
      "value": "77 frames",
      "reason": "Helps avoid slow motion issues when training on images",
      "from": "Juampab12"
    },
    {
      "setting": "cfg for speed",
      "value": "1.0",
      "reason": "cfg other than 1.0 runs model twice per step, making it slower",
      "from": "Kijai"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "1.0 each for high/low",
      "reason": "Standard strength for Lightning LoRAs",
      "from": "screwfunk"
    },
    {
      "setting": "LightX2V LoRA strength",
      "value": "0.2-0.5",
      "reason": "Works as glue to bring LoRAs together, clears up blurry results",
      "from": "screwfunk"
    },
    {
      "setting": "Skyreels LoRA strength",
      "value": "2.0 on high, 1.0 on low",
      "reason": "Allows 121 frame generation",
      "from": "Kijai"
    },
    {
      "setting": "CFG value with low steps",
      "value": "2.0",
      "reason": "Can't use very high cfg with less steps",
      "from": "Kijai"
    },
    {
      "setting": "Prefetch blocks",
      "value": "1",
      "reason": "Single block enough to offset speed loss on most systems",
      "from": "Kijai"
    },
    {
      "setting": "High model timestep range",
      "value": "0.875-1.0",
      "reason": "Default range for high noise model",
      "from": "Alisson Pereira"
    },
    {
      "setting": "Low model timestep range",
      "value": "0.0-0.875",
      "reason": "Default range for low noise model",
      "from": "Alisson Pereira"
    },
    {
      "setting": "Skyreels T2V LoRA strength",
      "value": "high = [2,2.5,2.5] (float list), low = no use",
      "reason": "For 121 frames 6 steps",
      "from": "avataraim"
    },
    {
      "setting": "Old LightX2V LoRA strength",
      "value": "3 high/1 low",
      "reason": "Better than newer versions for I2V",
      "from": "N0NSens"
    },
    {
      "setting": "Steps distribution for 2.2",
      "value": "Balance steps between low and high (e.g., 4 low 4 high for 8 total)",
      "reason": "To respect scheduler's curve",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "LoRA scheduling",
      "value": "Step 1: 2.0, Step 8: 1.2 with linear interpolation",
      "reason": "Provides varying LoRA strength over denoising steps",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "MultiTalk denoise",
      "value": "0.7 minimum for lip movement",
      "reason": "Lower values don't produce lip movement",
      "from": "AmirKerr"
    },
    {
      "setting": "Video length for quality",
      "value": "81 frames maximum",
      "reason": "Wan mostly trained on 5sec (81 frame) videos, quality goes downhill with longer sequences",
      "from": "Hevi"
    },
    {
      "setting": "Wan 5B upscaling denoise",
      "value": "0.35 or 0.5",
      "reason": "Good balance for V2V upscaling without over-processing",
      "from": "Juan Gea"
    },
    {
      "setting": "SeedVR2 sampler/scheduler",
      "value": "Heun/Beta or Euler/Beta with 4 steps",
      "reason": "Good quality-speed balance, might go lower to 1.8 denoise",
      "from": "AmirKerr"
    },
    {
      "setting": "LoRA extraction rank",
      "value": "64 usually sufficient, 128 can be better",
      "reason": "64 sufficient for most cases, 128 possibly not worth twice the size",
      "from": "Kijai"
    },
    {
      "setting": "Reference image size for Stand-in",
      "value": "640x640 or higher (768x768, 1024x1024)",
      "reason": "Better results than 512x512",
      "from": "Kijai"
    },
    {
      "setting": "Stand-in LoRA strength and latent multiplier",
      "value": "Variable based on needs",
      "reason": "Has effect on output quality",
      "from": "Kijai"
    },
    {
      "setting": "Context overlap frames",
      "value": "Lower values",
      "reason": "Higher overlap increases render time significantly",
      "from": "xwsswww"
    },
    {
      "setting": "Skyreels LoRA weight",
      "value": "1.8 on high, 1.5 on low",
      "reason": "Works well for 121 frame generation",
      "from": "NebSH"
    },
    {
      "setting": "LightX2V LoRA strength",
      "value": "0.6 with increased steps",
      "reason": "Reduces overcooking while maintaining speed benefits",
      "from": "Hevi"
    },
    {
      "setting": "Block swap",
      "value": "Enable for wrapper when using high resolution",
      "reason": "Prevents OOM errors that native handles automatically",
      "from": "Kijai"
    },
    {
      "setting": "Quantization disabled",
      "value": "Leave disabled for pre-quantized models",
      "reason": "Works as auto-select for already quantized models",
      "from": "Kijai"
    },
    {
      "setting": "LightX2V steps and CFG",
      "value": "4 steps, CFG 1.0, LoRA strength 1.00",
      "reason": "Standard settings for LightX2V acceleration",
      "from": "Danial"
    },
    {
      "setting": "VACE with guidance frames",
      "value": "8 steps without acceleration LoRAs",
      "reason": "Structure is already provided so needs fewer steps",
      "from": "pom"
    },
    {
      "setting": "Multitalk v2v denoise",
      "value": "0.7",
      "reason": "Used for video-to-video generation with Multitalk",
      "from": "mdkb"
    },
    {
      "setting": "High noise CFG",
      "value": "3.5 for 10 steps (of 20) with no LoRA",
      "reason": "Good quality balance",
      "from": "gokuvonlange"
    },
    {
      "setting": "Low noise CFG",
      "value": "1 for 2 steps (of 4) with LoRA",
      "reason": "Fast inference with LoRA",
      "from": "gokuvonlange"
    },
    {
      "setting": "Width dimensions",
      "value": "Use 832 instead of 840, follow divisible by 16 rule",
      "reason": "Avoid broadcast dimension errors",
      "from": "Kijai"
    },
    {
      "setting": "VACE strength for Stand-In v2v",
      "value": "0.2 (reduced from 1.0)",
      "reason": "Balance between original video and reference influence",
      "from": "mdkb"
    },
    {
      "setting": "Block swap",
      "value": "40 blocks for maximum memory efficiency",
      "reason": "Reduces max allocated memory from 16GB to 11GB",
      "from": "Kijai"
    },
    {
      "setting": "Prefetch blocks",
      "value": "0 for least memory use",
      "reason": "Combined with non-blocking false gives lowest VRAM usage but slower speed",
      "from": "Kijai"
    },
    {
      "setting": "Resolution for 720p generation",
      "value": "832x480",
      "reason": "Works reliably, higher resolutions may cause OOM on some cards",
      "from": "patientx"
    },
    {
      "setting": "LightX2V steps",
      "value": "4 steps in the sampler using the LoRA",
      "reason": "LoRA was trained on 4 steps",
      "from": "Danial"
    },
    {
      "setting": "Torch compile",
      "value": "Disable with stand-in method",
      "reason": "There's probably some issue with compile with the stand-in method",
      "from": "Kijai"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "1.0 for both high and low",
      "reason": "Instead of 3.0/1.0 to avoid motion destruction",
      "from": "xwsswww"
    },
    {
      "setting": "Canny control strength",
      "value": "1.0",
      "reason": "Reduced from 1.5 with inverted canny for better results",
      "from": "xwsswww"
    },
    {
      "setting": "Block swap VRAM usage",
      "value": "23G",
      "reason": "MultiTalk + Wan 2.2 with main model block swapped",
      "from": "nacho.money"
    },
    {
      "setting": "CFG for motion control",
      "value": "3.5 vs 1",
      "reason": "Higher CFG produces faster motion but may cause artifacts",
      "from": "Mngbg"
    },
    {
      "setting": "Image resize divisibility",
      "value": "32",
      "reason": "Required for proper model functioning",
      "from": "mdkb"
    },
    {
      "setting": "Block swap for 81 frames at 720x720",
      "value": "~20 blocks",
      "reason": "To fit in 24GB VRAM",
      "from": "Kijai"
    },
    {
      "setting": "Fantasy Portrait block swap",
      "value": "40 or higher",
      "reason": "Example was set for 57 frames, need more for 81 frames",
      "from": "Kijai"
    },
    {
      "setting": "Context windows node",
      "value": "Use defaults",
      "reason": "Just drag from sampler node and use default settings",
      "from": "A.I.Warper"
    },
    {
      "setting": "blocks_to_swap",
      "value": "20",
      "reason": "Puts half the model on RAM and half to VRAM for 8GB VRAM users",
      "from": "Kijai"
    },
    {
      "setting": "CFG",
      "value": "1",
      "reason": "Only runs positive conditioning, higher values run both positive and negative",
      "from": "DawnII"
    },
    {
      "setting": "reserve-vram",
      "value": "5",
      "reason": "Helps prevent ComfyUI freezing with native nodes",
      "from": "Kosinkadink"
    },
    {
      "setting": "fuse_method",
      "value": "pyramid or linear",
      "reason": "Values must be from allowed list, not numeric",
      "from": "Drommer-Kille"
    },
    {
      "setting": "--reserve-vram",
      "value": "5",
      "reason": "Prevents VRAM spillover to slow shared memory, dramatically improves speed",
      "from": "Nekodificador"
    },
    {
      "setting": "cfg",
      "value": "1.0",
      "reason": "Disables negative prompt pass for twice the speed",
      "from": "Kosinkadink"
    },
    {
      "setting": "blocks",
      "value": "40",
      "reason": "Optimal performance with sage attention 3",
      "from": "cocktailprawn1212"
    },
    {
      "setting": "power limit",
      "value": "500W",
      "reason": "Better stability vs default 600W with minimal performance impact",
      "from": "Kijai"
    },
    {
      "setting": "Ref-image VACE strength",
      "value": "0.6\u20131.0 (start at 0.8)",
      "reason": "If identity is drifting",
      "from": "Josiah"
    },
    {
      "setting": "Ref-image VACE start/end percent",
      "value": "start: 0.00, end: 0.25\u20130.40",
      "reason": "Fade out after look is established",
      "from": "Josiah"
    },
    {
      "setting": "Canny/motion VACE strength",
      "value": "0.5\u20130.8 (start ~0.7)",
      "reason": "For motion control throughout clip",
      "from": "Josiah"
    },
    {
      "setting": "Canny/motion VACE start/end percent",
      "value": "start: 0.00, end: 1.00",
      "reason": "Drive the whole clip",
      "from": "Josiah"
    },
    {
      "setting": "Lightx2v lora strength",
      "value": "2-3 strength",
      "reason": "Required strength for proper function with high noise pass",
      "from": "Kijai"
    },
    {
      "setting": "Lightning lora strength",
      "value": "1.0 strength",
      "reason": "Proper strength setting for lightning loras",
      "from": "Kijai"
    },
    {
      "setting": "Fun Control high noise steps",
      "value": "4+4 instead of 2",
      "reason": "Reduces ghosting issues in generated videos",
      "from": "Kijai"
    },
    {
      "setting": "WanFM schedulers",
      "value": "unipc works, lcm/dpm++_sde don't work",
      "reason": "Compatibility with the bidirectional sampling method",
      "from": "Kijai"
    },
    {
      "setting": "Video extension overlap",
      "value": "1 frame",
      "reason": "For smooth blending when using WanVideo Blender node",
      "from": "xwsswww"
    },
    {
      "setting": "Audio scale",
      "value": "3",
      "reason": "Best results for MultiTalk with MagRef for lip-sync accuracy",
      "from": "mdkb"
    },
    {
      "setting": "Denoise",
      "value": "0.50",
      "reason": "Used with PUSA on native using image batch repeat",
      "from": "hicho"
    },
    {
      "setting": "CFG for lightning LoRAs",
      "value": "2.0 first step, then 1.0",
      "reason": "Lightning supposed to be CFG1 but works better if first step has higher CFG without lightning",
      "from": "phazei"
    },
    {
      "setting": "Audio CFG",
      "value": "high",
      "reason": "Set high for MultiTalk, works on high noise model but most lipsync happens on low noise stage",
      "from": "MysteryShack"
    },
    {
      "setting": "LightX2V rank",
      "value": "64",
      "reason": "Used successfully with MagRef for 5 steps at 832x480x81 @ 24fps",
      "from": "mdkb"
    },
    {
      "setting": "Context overlap",
      "value": "48",
      "reason": "For better continuity with context windows",
      "from": "Kijai"
    },
    {
      "setting": "VACE frames for single image",
      "value": "5 frames",
      "reason": "First frame with change/mask, 4 empty gray frames",
      "from": "Nekodificador"
    },
    {
      "setting": "Temporal frames calculation",
      "value": "81 frames = 21 latents",
      "reason": "4x VAE compression ratio",
      "from": "Kijai"
    },
    {
      "setting": "FPS for WAN 2.2",
      "value": "12fps then RIFE doubling",
      "reason": "Outputs appear sped up at 16fps compared to 2.1",
      "from": "Fawks"
    },
    {
      "setting": "adapter_scale",
      "value": "0.7",
      "reason": "Reduces over-animated mouth movements in Fantasy Portrait",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "block_swap",
      "value": "40 blocks for 5090, 19-20 blocks for 1280x704",
      "reason": "Enables higher resolution/longer videos by offloading to CPU RAM",
      "from": "Ryzen"
    },
    {
      "setting": "steps",
      "value": "10 steps",
      "reason": "Used for 3-minute 1280x720 81-frame generation",
      "from": "Ryzen"
    },
    {
      "setting": "fuse_method",
      "value": "linear or pyramid",
      "reason": "Only accepted values for WanVideoContextOptions",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "setting": "high/low noise split",
      "value": "0.9 split (15 high, 25 low for 40 steps)",
      "reason": "Original implementation timestep distribution",
      "from": "Benjimon"
    },
    {
      "setting": "Training configuration",
      "value": "512 resolution for video, 768 and 1024 for images, 15 videos plus 2 datasets with 15 images each",
      "reason": "Consumes 75GB RAM during training",
      "from": "Ryzen"
    },
    {
      "setting": "Context window parameters",
      "value": "81 frame context size, 32 overlap, 6 steps",
      "reason": "Reduces context drift and improves stability",
      "from": "Kijai"
    },
    {
      "setting": "Lightning settings for less jiggling",
      "value": "CFG=2 on HIGH, CFG=1 on LOW",
      "reason": "Improves motion quality but takes more time",
      "from": "Ashtar"
    },
    {
      "setting": "MMAudio fps requirement",
      "value": "25 fps",
      "reason": "MMAudio expects 25fps input for proper audio generation",
      "from": "Kijai"
    },
    {
      "setting": "Context overlap",
      "value": "32 or 48",
      "reason": "Better consistency in long generations",
      "from": "Josiah"
    },
    {
      "setting": "Divisible by",
      "value": "16 or 32",
      "reason": "Prevents rounding errors in generation",
      "from": "Kijai"
    },
    {
      "setting": "LightX2V T2V",
      "value": "High pass: 0,3,3 strength, shift 8, 6 total steps split at 3",
      "reason": "Decent quality at low step count",
      "from": "hablaba"
    },
    {
      "setting": "LightX2V I2V",
      "value": "High pass: 0,4,3,1,1, Low pass: 1 strength, 10 total steps",
      "reason": "Good results for I2V",
      "from": "hablaba"
    },
    {
      "setting": "CFG for LightX2V",
      "value": "First high pass step CFG 2, all else CFG 1",
      "reason": "Better quality balance",
      "from": "hablaba"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "1.8/1.5 (high/low)",
      "reason": "Prevents looping effects with skyreels lora",
      "from": "NebSH"
    },
    {
      "setting": "Lightning LoRA application",
      "value": "Second pass of high only then on low",
      "reason": "Optimal application method for Lightning 6 steps",
      "from": ". Not Really Human ."
    },
    {
      "setting": "Scene cut limit",
      "value": "2-3 scenes maximum",
      "reason": "Best results, limited to 5 multishots in same prompt",
      "from": "NebSH"
    },
    {
      "setting": "MultiTalk FPS",
      "value": "30",
      "reason": "Better lip sync performance",
      "from": "Charlie"
    },
    {
      "setting": "Audio CFG",
      "value": "2",
      "reason": "Optimal audio control",
      "from": "Charlie"
    },
    {
      "setting": "Audio scale",
      "value": "2.5",
      "reason": "Better MultiTalk performance when other settings fail",
      "from": "mdkb"
    },
    {
      "setting": "Denoise",
      "value": "0.7",
      "reason": "When using add noise to samples true for talking models",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Add noise to samples",
      "value": "true",
      "reason": "Required for talking models to work properly",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "LightX2V strength",
      "value": "0.8",
      "reason": "Used for 6-step generation with good results",
      "from": "Kijai"
    },
    {
      "setting": "DPM++/SDE sampler",
      "value": "6 steps",
      "reason": "17 minute generation time on 5090 with quality results",
      "from": "Kijai"
    },
    {
      "setting": "MultiTalk CFG",
      "value": "3.0",
      "reason": "Good baseline for lip sync quality",
      "from": "samhodge"
    },
    {
      "setting": "MultiTalk scale",
      "value": "1.0 to 2.5",
      "reason": "Depending on how enunciated you want the result",
      "from": "samhodge"
    },
    {
      "setting": "Context windows",
      "value": "121,4,40",
      "reason": "Good settings for identity preservation",
      "from": "samhodge"
    },
    {
      "setting": "audio_scale",
      "value": "2.0",
      "reason": "Provides better audio conditioning results",
      "from": "Kijai"
    },
    {
      "setting": "fps for wav2vec embeds",
      "value": "25",
      "reason": "MultiTalk trained at 25fps",
      "from": "multiple users"
    },
    {
      "setting": "merge_loras",
      "value": "disabled",
      "reason": "Prevents noise issues with fp8_scaled models",
      "from": "Kijai"
    },
    {
      "setting": "scheduler",
      "value": "dmp++ sde over res_multistep",
      "reason": "Less noisy previews, better quality",
      "from": "Kijai"
    },
    {
      "setting": "InfiniteTalk steps with LightX2V",
      "value": "6-10 steps",
      "reason": "Optimal for distill LoRA usage",
      "from": "Kijai"
    },
    {
      "setting": "LoRA strength with LightX2V",
      "value": "0.8",
      "reason": "Better to lower strength than increase steps",
      "from": "Kijai"
    },
    {
      "setting": "Audio scale",
      "value": "1",
      "reason": "Default setting works for most cases",
      "from": "Kijai"
    },
    {
      "setting": "Blockswap for memory issues",
      "value": "20-40",
      "reason": "Helps with OOM issues, start with 20 and increase if needed",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Frame window size",
      "value": "81",
      "reason": "Should match normal generation frame count, don't change",
      "from": "Kijai"
    },
    {
      "setting": "frame_window",
      "value": "don't change default",
      "reason": "Controls how many frames processed at once",
      "from": "Kijai"
    },
    {
      "setting": "num_frames",
      "value": "total desired frames",
      "reason": "Sets the final video length",
      "from": "Kijai"
    },
    {
      "setting": "VAE precision",
      "value": "Wan2_1VAE_bf16.safettensors with bf16",
      "reason": "Correct VAE and precision combination",
      "from": "NC17z"
    },
    {
      "setting": "denoise for V2V",
      "value": "0.4",
      "reason": "Used in V2V testing",
      "from": "DawnII"
    },
    {
      "setting": "motion_frames for I2V",
      "value": "9",
      "reason": "Matching their code implementation",
      "from": "DawnII"
    },
    {
      "setting": "LoRA rank",
      "value": "up to 64",
      "reason": "Higher values hit diminishing returns and increase memory usage",
      "from": "Kijai"
    },
    {
      "setting": "Motion frame",
      "value": "9",
      "reason": "Default for new model, works fine and is faster than 25",
      "from": "Kijai"
    },
    {
      "setting": "Audio CFG scale",
      "value": "3-5",
      "reason": "Recommended range, makes generation take 3x time as it does 3 passes",
      "from": "Kijai"
    },
    {
      "setting": "Frame window",
      "value": "81",
      "reason": "Base model does best at 81 frames, shouldn't be changed",
      "from": "Kijai"
    },
    {
      "setting": "LoRA strength",
      "value": "0.8",
      "reason": "Lower strength helps with outfit variety when LoRA is overtrained",
      "from": "Ryzen"
    },
    {
      "setting": "CFG",
      "value": "3",
      "reason": "Allows negative prompts to work, CFG 1 doesn't use negatives",
      "from": "Ryzen"
    },
    {
      "setting": "Wan 2.2 upscale parameters",
      "value": "4 steps, denoise 1.00, add noise to samples = true, start at step 2",
      "reason": "For 720p to 1024p upscaling",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "InfiniteTalk long generation",
      "value": "25 steps, shift 8, cfg 3.5, audio cfg 4.0",
      "reason": "2400 frames generation taking 2.5 hours",
      "from": "seitanism"
    },
    {
      "setting": "Wan 2.2 scheduler",
      "value": "euler beta",
      "reason": "Works best according to user experience",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "LightX2V LoRA usage",
      "value": "4 steps, 1 cfg",
      "reason": "Produces less dynamic but cleaner results",
      "from": "seitanism"
    },
    {
      "setting": "Audio CFG for lip sync",
      "value": "3-5",
      "reason": "Works optimally for lip synchronization accuracy",
      "from": "NC17z"
    },
    {
      "setting": "Manual sigmas for 4-step T2V",
      "value": "[1.0000, 0.9375, 0.8750, 0.4375, 0.0000]",
      "reason": "Linear scheduling gives better results than BetaSamplingScheduler",
      "from": "phazei"
    },
    {
      "setting": "Runtime cache setting",
      "value": "--cache-none",
      "reason": "Prevents ComfyUI from tanking with high VRAM/DRAM usage",
      "from": "samhodge"
    },
    {
      "setting": "InfiniteTalk long generation",
      "value": "2400 frames, 32 steps, no lora, 480x720x81, 9 motion frames, 3.5 cfg, 7 shift, 3.5 audio cfg scale, dpm++ sde, MAGREF+infinitetalk",
      "reason": "Achieved stable 1+ minute generation with no visible degradation",
      "from": "seitanism"
    },
    {
      "setting": "Vid2vid denoise for motion cleanup",
      "value": "0.65 denoise",
      "reason": "Good balance for fixing spazzy motion from InfiniteTalk without losing quality",
      "from": "samhodge"
    },
    {
      "setting": "Wan 2.2 FLF frame requirements",
      "value": "Minimum 81 frames, maximum under 180 frames",
      "reason": "Second input goes grey outside these bounds",
      "from": "cyber jock"
    },
    {
      "setting": "InfiniteTalk V2V steps",
      "value": "4 total steps, start at 2",
      "reason": "Optimal for video-to-video processing",
      "from": "Kijai"
    },
    {
      "setting": "Light2XV LoRA strength",
      "value": "3 on high noise, 1 on low noise",
      "reason": "Better performance than new speed LoRAs",
      "from": "NebSH"
    },
    {
      "setting": "Motion frames for InfiniteTalk",
      "value": "35 motion frames",
      "reason": "Reduces dead air frames at end of clips",
      "from": "NC17z"
    },
    {
      "setting": "Audio scale adjustment",
      "value": "Turn down audio scale",
      "reason": "Mitigates prompt following issues in long generations",
      "from": "DawnII"
    },
    {
      "setting": "fps",
      "value": "25",
      "reason": "Model is really meant for 25fps",
      "from": "Kijai"
    },
    {
      "setting": "motion_frames",
      "value": "9",
      "reason": "Default overlap for smooth transitions between 81 frame windows",
      "from": "seitanism"
    },
    {
      "setting": "start_step",
      "value": "2",
      "reason": "For vid2vid, allows input video to establish before sampler takes over",
      "from": "Kijai"
    },
    {
      "setting": "control_strength",
      "value": "0.0 to 0.1",
      "reason": "Control can be overpowering, low strength works better",
      "from": "Hashu"
    },
    {
      "setting": "Audio scale",
      "value": "2.5",
      "reason": "Better lip movement in MultiTalk, otherwise lips don't move much",
      "from": "mdkb"
    },
    {
      "setting": "High noise stage",
      "value": "2 steps without lora",
      "reason": "First stage of native 2 ksampler setup",
      "from": "Not Really Human"
    },
    {
      "setting": "Low noise stage",
      "value": "2 steps with lora",
      "reason": "Second stage of native 2 ksampler setup",
      "from": "Not Really Human"
    },
    {
      "setting": "Max frames in InfiniteTalk",
      "value": "Adjustable beyond 1000",
      "reason": "Default locks at 1000 but can be increased for longer audio",
      "from": "JohnDopamine"
    },
    {
      "setting": "VACE mask fill",
      "value": "Grey 0.5/128",
      "reason": "Proper masking for inpainting",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "MultiTalk mask values",
      "value": "Very dim grey (1,2,3)",
      "reason": "For segmentation of different speakers",
      "from": "samhodge"
    },
    {
      "setting": "Motion optimization",
      "value": "LTX 4-5 on high noise, 1 on low noise, cfg=1",
      "reason": "Maximum motion while retaining original image",
      "from": "Instability01"
    },
    {
      "setting": "Audio scale for infinitetalk",
      "value": "1.5 default for vid2vid, 1.0 for i2v",
      "reason": "Effect can be too weak in vid2vid",
      "from": "Kijai"
    },
    {
      "setting": "CFG and shift for Fun 5B",
      "value": "CFG 5, shift 5",
      "reason": "Good results with flowmatch_pusa scheduler",
      "from": "Blink"
    },
    {
      "setting": "LoRA strength for WAN 2.2",
      "value": "0.5 to 1.7",
      "reason": "Range that works well, depends on LoRA epoch and quality",
      "from": "seitanism"
    },
    {
      "setting": "Lightning sampling steps",
      "value": "8 steps (2/2/2/2)",
      "reason": "4-stage approach with different LoRA applications",
      "from": ": Not Really Human :."
    },
    {
      "setting": "CineScale RoPE scaling",
      "value": "[1.0, 20.0, 20.0] for first sampler, [1.0, 25.0, 25.0] for second",
      "reason": "Proper spatial frequency scaling for resolution upscaling",
      "from": "Kijai"
    },
    {
      "setting": "Distill model parameters",
      "value": "steps 4, cfg 1, shift 1",
      "reason": "Proper settings for distilled models",
      "from": "samhodge"
    },
    {
      "setting": "Wan 2.2 render performance",
      "value": "81 frames at 720p takes 82.76 seconds",
      "reason": "Performance benchmark on fp8 scaled models",
      "from": "Drommer-Kille"
    },
    {
      "setting": "5B Turbo parameters",
      "value": "4 steps, cfg 1.0, flowmatch_distill or dpm++_sde sampler",
      "reason": "Optimized for the turbo model",
      "from": "Kijai"
    },
    {
      "setting": "Cinescale workflow",
      "value": "50 steps first sampler, 50 steps second starting at step 15, bilinear upscale, RoPE scaling applied",
      "reason": "Official implementation approach",
      "from": "Kijai"
    },
    {
      "setting": "Linear 4 step sigmas with shift 5.0",
      "value": "0.9998, 0.9375, 0.8333, 0.6250, 0.0000",
      "reason": "For custom sigma implementation",
      "from": "Kijai"
    },
    {
      "setting": "Default frames for 5B",
      "value": "121 frames",
      "reason": "Model default configuration",
      "from": "Kijai"
    },
    {
      "setting": "CineScale LoRA strength",
      "value": "0.6",
      "reason": "Used for 1.3b upscaling at 0.6 denoise",
      "from": "DawnII"
    },
    {
      "setting": "Film grain LoRA rank",
      "value": "Rank8",
      "reason": "Works for style LoRAs and files are only 73mb",
      "from": "Drommer-Kille"
    },
    {
      "setting": "VACE strength for inpainting",
      "value": "0.5",
      "reason": "Strength of 1 tries to fix subject exactly on mask area, 0.5 is better for some cases",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "setting": "WAN 2.2 5B Turbo wrapper",
      "value": "4 steps, cfg 1.0, flowmatch_distill scheduler",
      "reason": "Proper turbo model settings",
      "from": "Kijai"
    },
    {
      "setting": "WAN 2.2 5B Turbo native",
      "value": "5 steps, euler simple, cfg 1.0 or custom sigmas for 4 steps",
      "reason": "4 steps with normal schedule doesn't work well",
      "from": "Kijai"
    },
    {
      "setting": "CFG split for 20 steps without LightX",
      "value": "cfg 3.5, switch at 11 steps for dpm++sde, 2nd sampler 1 cfg",
      "reason": "Proper dual sampler setup",
      "from": "crinklypaper"
    },
    {
      "setting": "InfiniteTalk motion_frame",
      "value": "overlap amount between clips",
      "reason": "Controls frame overlap in continuous generation",
      "from": "Kijai"
    },
    {
      "setting": "InfiniteTalk frame_window_size",
      "value": "model input size",
      "reason": "Defines model context window",
      "from": "Kijai"
    },
    {
      "setting": "audio_scale",
      "value": "2.0",
      "reason": "3.0 causes issues, 2.0 works great",
      "from": "Kenk"
    },
    {
      "setting": "frame rate for multitalk",
      "value": "25fps for fast movements, 16fps acceptable for slower content",
      "reason": "Better motion handling",
      "from": "Kenk"
    },
    {
      "setting": "euler/beta with 8 steps",
      "value": "8 steps",
      "reason": "Produces superior quality with Wan 2.2 Low I2V",
      "from": "gordo"
    },
    {
      "setting": "LoRA strength with CFG Schedule",
      "value": "2.0",
      "reason": "Needed to make LoRAs work with CFG Schedule",
      "from": "Drommer-Kille"
    },
    {
      "setting": "Phantom frame count",
      "value": "121 frames",
      "reason": "Must be 121 frames else things go wrong, causes midgets at other frame counts",
      "from": "mdkb"
    },
    {
      "setting": "Loop sampling overlap calculation",
      "value": "81 + 81 - 9 = 153 for exactly 2 windows",
      "reason": "Must account for motion_frame overlap when calculating window frames",
      "from": "Kijai"
    },
    {
      "setting": "Virtual VRAM",
      "value": "14 GB",
      "reason": "Used religiously for native workflows to help with OOMs",
      "from": "mdkb"
    },
    {
      "setting": "VRAM usage limit",
      "value": "95%",
      "reason": "Above 95% on Windows causes severe slowdowns",
      "from": "Kijai"
    },
    {
      "setting": "I2V shift threshold",
      "value": "0.9",
      "reason": "Original default for I2V vs 0.875 for T2V",
      "from": "Kijai"
    },
    {
      "setting": "Block swap amount",
      "value": "40 blocks",
      "reason": "For 1280x720x121 frames on limited VRAM setups",
      "from": "Dream Making"
    },
    {
      "setting": "Prefetch blocks",
      "value": "1",
      "reason": "More didn't make a difference in testing",
      "from": "Kijai"
    },
    {
      "setting": "CFG end percent",
      "value": "0.01",
      "reason": "Applies CFG only to initial step",
      "from": "DawnII"
    },
    {
      "setting": "Control latent strength",
      "value": "0",
      "reason": "Keeps generation completely static",
      "from": "DawnII"
    },
    {
      "setting": "Lightning/LightX2V weight",
      "value": "0.8",
      "reason": "Good balance for both high and low noise",
      "from": "CJ"
    },
    {
      "setting": "MultiTalk VRAM usage",
      "value": "~2.5GB more at Q8",
      "reason": "Can be divided by block swap amount",
      "from": "Kijai"
    },
    {
      "setting": "CFG schedule",
      "value": "3.5 in high sampler, 1 in low sampler",
      "reason": "Maintains prompt following while preserving native image quality",
      "from": "Drommer-Kille"
    },
    {
      "setting": "InfiniteTalk steps",
      "value": "4-6 steps with lightx2v lora",
      "reason": "6 is better than 4, more than 6 gives minimal improvements",
      "from": "Kijai"
    },
    {
      "setting": "Frame window size for Skyreels",
      "value": "121",
      "reason": "Skyreels 720p default supports 121 frames vs normal I2V's 81",
      "from": "Kijai"
    },
    {
      "setting": "CFG",
      "value": "2",
      "reason": "Used in successful T2V low noise examples",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "setting": "Steps",
      "value": "8",
      "reason": "Better for single frame VACE generation than 4 steps",
      "from": "hicho"
    },
    {
      "setting": "Sigma split",
      "value": "0.875",
      "reason": "Optimal sigma configuration mentioned",
      "from": "JalenBrunson"
    },
    {
      "setting": "VACE strength",
      "value": "2",
      "reason": "Setting used in successful generation",
      "from": "hicho"
    },
    {
      "setting": "Wan 2.2 without Light2v LoRAs",
      "value": "6 steps total, split at middle, very first step using cfg",
      "reason": "General recommendation from Kijai",
      "from": "Kijai"
    },
    {
      "setting": "With Lightning LoRAs",
      "value": "Don't go past 10 steps, 6 can be enough, split point in middle plus/minus 1 step",
      "reason": "Optimal for distill loras",
      "from": "Kijai"
    },
    {
      "setting": "High/Low noise split for 2.2",
      "value": "8 steps total, 4 on both sides instead of 3/17 split",
      "reason": "More balanced than 3 high noise, 17 low noise",
      "from": "Kijai"
    },
    {
      "setting": "lightx2v LoRA strength for T2V",
      "value": "1.5-2 on high, 1 on low",
      "reason": "User experience for T2V, vs 3/1 for i2v",
      "from": "Gentleman bunny"
    },
    {
      "setting": "DWPose resolution",
      "value": "Ratchet up resolution in standard DWPose node",
      "reason": "Higher resolution makes difference for eye line control",
      "from": "JalenBrunson"
    },
    {
      "setting": "LoRA strength per step",
      "value": "First step no lora, 2nd step strength 2, rest at 1",
      "reason": "Allows using CFG on first step",
      "from": "Kijai"
    },
    {
      "setting": "Video generation length for speed",
      "value": "5-7 seconds",
      "reason": "Runs almost at same speed as real-time for I2V with InfiniteTalk",
      "from": "Kenk"
    },
    {
      "setting": "Audio sample rate",
      "value": "16000",
      "reason": "Required sample rate for audio processing in S2V model",
      "from": "DawnII"
    },
    {
      "setting": "Steps for Wan 2.2 I2V",
      "value": "6-8 steps instead of 4",
      "reason": "4 steps too low, causes poor quality",
      "from": "JohnDopamine"
    },
    {
      "setting": "Alpha/Beta values",
      "value": "0.6/0.6",
      "reason": "Default recommended values",
      "from": "Ablejones"
    },
    {
      "setting": "Shift value for troubleshooting",
      "value": "8 for both samplers",
      "reason": "May help with blurry results",
      "from": "screwfunk"
    },
    {
      "setting": "Input FPS",
      "value": "50 fps hardcoded",
      "reason": "Built into the model architecture",
      "from": "Kijai"
    },
    {
      "setting": "Output FPS",
      "value": "16 fps",
      "reason": "Video saved at this rate",
      "from": "Kijai"
    },
    {
      "setting": "Resolution formula for S2V",
      "value": "(width\u00f716) \u00d7 (height\u00f716) \u00d7 frames \u00f7 30 = whole number",
      "reason": "Prevents einops errors",
      "from": "patientx"
    },
    {
      "setting": "CFG Skimming with Lightning",
      "value": "4 steps (2+2)",
      "reason": "Fast generation with LoRA on HIGH & LOW",
      "from": ": Not Really Human :"
    },
    {
      "setting": "Resolution limitations for 24fps",
      "value": "Only 368x384 works reliably",
      "reason": "Higher resolutions may work but require more GPU power",
      "from": "patientx"
    },
    {
      "setting": "Frame limits",
      "value": "105 frames maximum, 109 frames causes error",
      "reason": "Model has frame count limitations",
      "from": "N0NSens"
    },
    {
      "setting": "Small resolution settings",
      "value": "192x192 145 frames, 384x384 145 frames work",
      "reason": "Lower resolutions are more stable for longer sequences",
      "from": "patientx"
    },
    {
      "setting": "VACE strength reduction",
      "value": "Reduce strength to remove openpose artifacts",
      "reason": "Prevents control inputs from appearing in output",
      "from": "xwsswww"
    },
    {
      "setting": "Steps with LightX2V",
      "value": "6 steps",
      "reason": "worked better on that native example with the lightx2v lora and 6 steps",
      "from": "Kijai"
    },
    {
      "setting": "CFG for S2V",
      "value": "3.5",
      "reason": "Used in native workflow test",
      "from": "V\u00e9role"
    },
    {
      "setting": "Block swap for WanWrapper",
      "value": "20-30 blocks",
      "reason": "I use block swap 20 with wan 2.2 workflows if I am targeting 832x480x81",
      "from": "patientx"
    },
    {
      "setting": "Custom sigmas for 4-step Lightning",
      "value": "[1.0, 0.75, 0.5, 0.25] with shift 5",
      "reason": "4 linear sigmas work better than normal schedulers",
      "from": "Kijai"
    },
    {
      "setting": "CFG and Audio CFG scale",
      "value": "2.2 for both",
      "reason": "Good balance for InfiniteTalk results",
      "from": "Antey"
    },
    {
      "setting": "Steps for S2V",
      "value": "6 steps",
      "reason": "Good quality, could potentially use less",
      "from": "Kijai"
    },
    {
      "setting": "CFG for S2V",
      "value": "1",
      "reason": "Works well with LightX2V at strength 1",
      "from": "ingi // SYSTMS"
    },
    {
      "setting": "Lightning LoRA steps",
      "value": "4 steps with 4.5/3.5 CFG",
      "reason": "Produces acceptable results",
      "from": "Mu5hr00m_oO"
    },
    {
      "setting": "Audio scale and audio_cfg_scale",
      "value": "2.2",
      "reason": "Improved InfiniteTalk results",
      "from": "Antey"
    },
    {
      "setting": "LoRA strength for LightX2V",
      "value": "0.3",
      "reason": "Much lower strength needed when combining with other LoRAs",
      "from": "gordo"
    },
    {
      "setting": "Pose condition multiplier",
      "value": "0.5",
      "reason": "Reduces teeth artifacts while maintaining pose control",
      "from": "Kijai"
    },
    {
      "setting": "Steps for S2V",
      "value": "6 steps",
      "reason": "Perfect results achievable with proper lora merge",
      "from": "mamad8"
    },
    {
      "setting": "S2V generation steps",
      "value": "8 steps",
      "reason": "Used with 3 CFG and 0.5 lora strength",
      "from": "hicho"
    },
    {
      "setting": "LCM steps",
      "value": "5 steps",
      "reason": "Fast inference with LCM sampler",
      "from": "Kijai"
    },
    {
      "setting": "LightX2V LoRA strength for High/Low noise models",
      "value": "3.0 strength at high noise, 1.0 at low noise",
      "reason": "Community consensus for best results with Wan 2.2 speed LoRAs",
      "from": "N0NSens"
    },
    {
      "setting": "S2V model frame generation",
      "value": "Can generate 35+ seconds of video",
      "reason": "Model capable of long form generation, may degrade less with guiding images at every step",
      "from": "comfy"
    },
    {
      "setting": "Training steps for character LoRAs",
      "value": "1k-3k steps",
      "reason": "Sufficient for character training, 3000 steps max recommended on fal",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Context windows bucket length",
      "value": "Set to frame length",
      "reason": "For context windows method, just set bucket length to frame length and it works",
      "from": "Kijai"
    },
    {
      "setting": "Steps for ultra HD renders",
      "value": "100+ steps",
      "reason": "At 720p+ with high motion, noticeable improvements when using ultra high steps",
      "from": "Neex"
    },
    {
      "setting": "model loader device",
      "value": "offload_device",
      "reason": "Prevents memory issues and OOM errors",
      "from": "Kijai"
    },
    {
      "setting": "video encoder tiling",
      "value": "enabled",
      "reason": "Helps with VRAM requirements and prevents OOM",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "setting": "Uni3C strength",
      "value": "3 at high pass",
      "reason": "Proper strength setting for Uni3C control",
      "from": "N0NSens"
    },
    {
      "setting": "Framepack shift",
      "value": "pretty low",
      "reason": "Higher shift values can cause issues with Framepack",
      "from": "Kijai"
    },
    {
      "setting": "ComfyUI launch flag",
      "value": "--reserve-vram 5",
      "reason": "Forces reservation of VRAM to prevent OOMing",
      "from": "Juampab12"
    },
    {
      "setting": "CFG schedule",
      "value": "cfg 5 first step, cfg 1 next 2 steps (high) with lightx 3 str, then 3 steps 2.5 lightx cfg1 (low)",
      "reason": "Working FLF setup",
      "from": "Juampab12"
    },
    {
      "setting": "Start step for vid2vid",
      "value": "2",
      "reason": "To avoid trash frames at the beginning",
      "from": "N0NSens"
    },
    {
      "setting": "Context window inference time",
      "value": "501 frames at 640x640 with 6 steps takes 5:56",
      "reason": "Performance benchmark on 5090",
      "from": "Kijai"
    },
    {
      "setting": "S2V pose strength",
      "value": "Lower strength on first step only",
      "reason": "Less strict control while still maintaining pose guidance",
      "from": "Kijai"
    },
    {
      "setting": "FantasyPortrait steps",
      "value": "First step only",
      "reason": "Reduces character changes while maintaining projection benefits",
      "from": "Kijai"
    },
    {
      "setting": "Context window overlap/stride",
      "value": "48 overlap, 10 stride",
      "reason": "For high noise pass in 2.2 context generation",
      "from": "Kijai"
    },
    {
      "setting": "I2V steps",
      "value": "30 steps 15/15, dmpp_sde/beta",
      "reason": "Settings used for good I2V results",
      "from": "ArtOfficial"
    },
    {
      "setting": "Memory allocator",
      "value": "0",
      "reason": "Makes system give memory back to OS more aggressively",
      "from": "comfy"
    },
    {
      "setting": "Lightx2v LoRA",
      "value": "8 steps, CFG 2.5 or 3.0, strength 1.0",
      "reason": "Better results with native S2V",
      "from": "Ablejones"
    },
    {
      "setting": "InfiniteTalk window size and overlap",
      "value": "Window size and motion_frame controls overlap",
      "reason": "Controls how much frames overlap in looping",
      "from": "Kijai"
    },
    {
      "setting": "Frame limits for stability",
      "value": "Max 81 frames stable without extension methods",
      "reason": "Beyond that may cause memory issues",
      "from": "Mu5hr00m_oO"
    }
  ],
  "concepts": [
    {
      "term": "MoE noise expert split",
      "explanation": "Wan 2.2 uses separate High and Low noise expert models in MoE architecture",
      "from": "general discussion"
    },
    {
      "term": "Step distribution",
      "explanation": "Steps split between models: 0-4 in first pass, 4-8 in second pass for 8 total steps",
      "from": "GOD_IS_A_LIE"
    },
    {
      "term": "Context windows in WAN",
      "explanation": "Feature allowing multiple scenes in single generation using | separator between prompts",
      "from": "avataraim"
    },
    {
      "term": "VACE strength parameter",
      "explanation": "Controls how strongly VACE modifies the video, affects background stability vs oversaturation",
      "from": "seitanism"
    },
    {
      "term": "PyTorch memory allocation behavior",
      "explanation": "Keeps RAM allocated when moving models between RAM/VRAM, can't free it while objects exist",
      "from": "Kijai"
    },
    {
      "term": "RadialAttention token divisibility",
      "explanation": "Not just resolution but final token count needs to be divisible by block size",
      "from": "Kijai"
    },
    {
      "term": "High/Low noise expert split",
      "explanation": "WAN 2.2 uses MoE architecture with separate models for high noise (early steps) and low noise (refinement steps)",
      "from": "ArtOfficial"
    },
    {
      "term": "Context windows in video generation",
      "explanation": "Method to generate longer videos by processing in overlapping segments without VRAM increase or quality degradation",
      "from": "Kijai"
    },
    {
      "term": "Blockswap",
      "explanation": "Memory optimization technique, available in native workflows but not needed with sufficient VRAM",
      "from": "IceAero"
    },
    {
      "term": "LightX2V LoRA",
      "explanation": "Distillation lora that makes the model work without using cfg and at lower step count, similar to high and low cfg",
      "from": "Kijai"
    },
    {
      "term": "Context windows with 5B",
      "explanation": "Each step does multiple model predictions with different parts of latents and blends them together, using 81 or 121 frames context vs AnimateDiff's 16",
      "from": "Kijai"
    },
    {
      "term": "Block swapping",
      "explanation": "Only reports what the weights use, rest of the processing uses additional VRAM",
      "from": "Kijai"
    },
    {
      "term": "5B timestep per frame",
      "explanation": "Works like Pusa/DiffusionForcing - technically timestep per frame, practically works with single image input and first timestep zeroed",
      "from": "Kijai"
    },
    {
      "term": "Timesteps in noise scheduling",
      "explanation": "Conventional way of splitting noise schedule into 1000 discrete steps, informs model how noisy image is at current step",
      "from": "aikitoria"
    },
    {
      "term": "High vs Low pass models",
      "explanation": "Low Pass = Wan 2.1 (refined), High Pass = new Wan 2.2 architecture",
      "from": "The Shadow (NYC)"
    },
    {
      "term": "Sigma vs Timestep",
      "explanation": "Timestep in Wan code is sigma scaled by 1000, different from ComfyUI where timesteps create evenly spaced values for sigmas",
      "from": "aikitoria"
    },
    {
      "term": "High/Low noise model split",
      "explanation": "High noise model used when timestep above boundary, low noise for below boundary",
      "from": "aikitoria"
    },
    {
      "term": "Context window overlap",
      "explanation": "How much each window overlaps with others, helps blending but makes generation slower due to more model predictions",
      "from": "Kijai"
    },
    {
      "term": "Stride in context windows",
      "explanation": "1 latent = 4 frames, so stride of 4 = disabled, goes frame by frame",
      "from": "Kijai"
    },
    {
      "term": "Pyramid vs Linear fuse",
      "explanation": "Pyramid weights middle overlap frames more heavily, Linear does straight 50/50 blend",
      "from": "thaakeno"
    },
    {
      "term": "Start step in V2V",
      "explanation": "Controls how much video changes - lower values change more, higher values stay closer to input",
      "from": "thaakeno"
    },
    {
      "term": "High noise vs low noise models",
      "explanation": "2.2 uses MoE with separate expert models for high and low noise processing",
      "from": "BondoMan"
    },
    {
      "term": "FLF (First Last Frame)",
      "explanation": "Setting first and last frames and letting the model work out what's in between",
      "from": "Juampab12"
    },
    {
      "term": "VACE strength",
      "explanation": "Controls all VACE functionality with no separation between different aspects",
      "from": "Kijai"
    },
    {
      "term": "First-Last-Frame to Video (FLF2V)",
      "explanation": "Method of conditioning video generation with specific first and last frames",
      "from": "Rainsmellsnice"
    },
    {
      "term": "Grey frames in Wan 2.2",
      "explanation": "Non-guiding frames are represented as grey, can be manually specified in input",
      "from": "comfy"
    },
    {
      "term": "High noise vs Low noise models",
      "explanation": "Wan 2.2 has different models for different noise levels, with different capabilities",
      "from": "Kijai"
    },
    {
      "term": "High/Low noise model split",
      "explanation": "High model handles motions, low model handles detail in 2.2 architecture",
      "from": "screwfunk"
    },
    {
      "term": "Blockswap",
      "explanation": "Technique to use more blocks by swapping between VRAM and RAM",
      "from": "Ryzen"
    },
    {
      "term": "TCFG",
      "explanation": "New experimental feature that fixes strobing issues while maintaining quality",
      "from": "phazei"
    },
    {
      "term": "Start/end steps in V2V",
      "explanation": "Similar to changing denoise in classic video-to-video workflows",
      "from": "Draken"
    },
    {
      "term": "FFLF (First-Frame-Last-Frame)",
      "explanation": "Feature that morphs between start and end images for video generation",
      "from": "thaakeno"
    },
    {
      "term": "fp8 matrix multiplication",
      "explanation": "fp8 * fp8 = bad quality, but (fp8 -> fp16) * (fp8 -> fp16) = good quality",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "One full block is ~700MB in fp16, swapping all blocks minimizes VRAM usage to just the active block",
      "from": "Kijai"
    },
    {
      "term": "Batch size in video generation",
      "explanation": "Number of images processed per iteration - higher batch size leads to more temporally stable videos",
      "from": "MiGrain"
    },
    {
      "term": "FPS1 LoRA concept",
      "explanation": "LoRA that generates videos at 1 frame per second instead of 16, used for storyboarding and creating longer scene structures",
      "from": "mamad8"
    },
    {
      "term": "Model switching sigma",
      "explanation": "Sigma value of 0.875 where Wan team intended the transition between high and low noise models to occur",
      "from": "Ablejones"
    },
    {
      "term": "Multistep sampler artifacts",
      "explanation": "Burning/artifacts that occur when multistep samplers use latents from different models in their calculations",
      "from": "Ablejones"
    },
    {
      "term": "LoRA timestep scheduling",
      "explanation": "Feature that allows controlling LoRA strength per step, 0 skips applying it altogether on that step, enables curves and dropoff control",
      "from": "Kijai"
    },
    {
      "term": "Self-forcing training",
      "explanation": "Training method that doesn't require a dataset, beauty of lightx2v approach, anyone with compute could do it",
      "from": "Kijai"
    },
    {
      "term": "Adaptive LoRA ranks",
      "explanation": "Each layer rank calculated based on difference, some layers rank 500+ while less important ones lower",
      "from": "Kijai"
    },
    {
      "term": "shift",
      "explanation": "Biases denoising steps - higher shift gives more steps to 'cook' before details are established, affects composition/movement/color. Shift 1 with simple scheduler removes exact same noise each step",
      "from": "Ablejones"
    },
    {
      "term": "flashboot",
      "explanation": "Runpod feature where if worker is 'warm', generation starts instantly even from idle state",
      "from": "gokuvonlange"
    },
    {
      "term": "noise boundary",
      "explanation": "Wan 2.2 has trained boundary between high noise and low noise models, but using shift 1 ignores this boundary yet produces better T2I results",
      "from": "aikitoria"
    },
    {
      "term": "High noise vs Low noise expert split",
      "explanation": "Wan 2.2 uses MoE architecture where high noise expert handles composition/structure, low noise expert handles refinement. Different step ratios needed for T2V vs I2V.",
      "from": "aikitoria"
    },
    {
      "term": "Block swap calculation",
      "explanation": "LoRA memory impact can be calculated by dividing LoRA size by block swap count",
      "from": "Kijai"
    },
    {
      "term": "MoE High/Low Noise Expert Split",
      "explanation": "Wan 2.2's architecture splits processing between high noise (motion/big concepts) and low noise (details/style) experts",
      "from": "Juampab12"
    },
    {
      "term": "VACE Strength",
      "explanation": "Controls how strictly the system applies reference/prompt to masked area - high strength (1.0) forces subject placement, lower gives more realistic freedom",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "term": "720P Distilled Model Formula",
      "explanation": "720P distilled model = 720P original model + (480P step distillation - 480P original model), not a native 720P model",
      "from": "Dan"
    },
    {
      "term": "High/Low noise expert split",
      "explanation": "Wan 2.2 uses different models for high and low noise levels, allowing different LoRAs on each",
      "from": "Kijai"
    },
    {
      "term": "Sigma cutoff for I2V",
      "explanation": "I2V has sigma cutoff at 875, affecting how steps are distributed between high/low noise models",
      "from": "DawnII"
    },
    {
      "term": "Lightning LoRA",
      "explanation": "4-step distillation LoRAs for faster generation, separate high and low noise versions",
      "from": "gokuvonlange"
    },
    {
      "term": "Mixed LoRA approach",
      "explanation": "Using different LoRAs for high noise vs low noise passes to optimize quality",
      "from": "Doctor Shotgun"
    },
    {
      "term": "GGUF model sizing",
      "explanation": "GGUF is just a storage format, higher size means higher quality packing, not more training data",
      "from": "Kijai"
    },
    {
      "term": "FastWan model",
      "explanation": "Distilled version of Wan 5B that works at 3-6 steps without CFG, converted from FastVideo team's diffusers version",
      "from": "Kijai"
    },
    {
      "term": "Lightning vs LightX2V",
      "explanation": "Lightning is new 4-step distill LoRA, LightX2V is older speed LoRA - they are different approaches",
      "from": "DawnII"
    },
    {
      "term": "Low noise model seeding",
      "explanation": "Low noise model is fixed and not really seeded in typical sense, can be 0 fixed or same seed as high",
      "from": "Lodis"
    },
    {
      "term": "work_device_cpu",
      "explanation": "Keeps as many large tensors on CPU (system RAM) as possible until needed for actual model inference",
      "from": "Ablejones"
    },
    {
      "term": "FLF (First-Frame-Last-Frame)",
      "explanation": "Morphing technique that interpolates between a first and last frame to create video transitions",
      "from": "thaakeno"
    },
    {
      "term": "Blockswapping",
      "explanation": "Memory management technique that swaps model blocks to reduce VRAM usage during inference",
      "from": "screwfunk"
    },
    {
      "term": "High/Low noise expert split",
      "explanation": "Wan 2.2's MoE architecture separates processing between high and low noise regions",
      "from": "context"
    },
    {
      "term": "Scene cutting for subject preservation",
      "explanation": "Technique using prompts like 'scene abruptly hard cuts to' to maintain subject resemblance across different contexts without specialized models",
      "from": "Jonathan"
    },
    {
      "term": "Split sampling 2.2",
      "explanation": "Using separate high noise and low noise models in Wan 2.2 architecture for better generation quality",
      "from": "Kijai"
    },
    {
      "term": "Context windowing",
      "explanation": "Samples the model multiple times per step and blends results, slows down generation but enables longer sequences",
      "from": "Kijai"
    },
    {
      "term": "Scene transition prompting",
      "explanation": "Using phrases like 'the scene immediately and abruptly cuts to' to maintain character consistency across scene changes",
      "from": "Jonathan"
    },
    {
      "term": "VAE compatibility",
      "explanation": "WAN and Qwen-Image use identical VAE architectures allowing cross-model latent space operations",
      "from": "fredbliss"
    },
    {
      "term": "High/Low Pass sampling",
      "explanation": "Wan 2.2 uses separate high and low noise samplers that can be configured independently with different LoRAs and settings",
      "from": "various"
    },
    {
      "term": "Context windows",
      "explanation": "Method for generating longer videos, but challenging with I2V models since they need image input in each window",
      "from": "Kijai"
    },
    {
      "term": "Quantile LoRA",
      "explanation": "Alternative to numbered rank versions (64/128/256) with self-forcing behavior",
      "from": "The Shadow (NYC)"
    },
    {
      "term": "Self attention splitting",
      "explanation": "Method to control scene transitions by splitting attention at specific steps, stronger split good for scene changes",
      "from": "Kijai"
    },
    {
      "term": "Patch embedding",
      "explanation": "Critical layer in diffusion models that differs significantly between Wan 2.1 and 2.2, affecting VACE compatibility",
      "from": "Kijai"
    },
    {
      "term": "Context window",
      "explanation": "Technique for processing high-resolution videos within VRAM limitations by processing in chunks",
      "from": "thaakeno"
    },
    {
      "term": "DisableNoise node",
      "explanation": "Node that prevents adding new noise when continuing a generation, used instead of RandomNoise for save/load latent workflows",
      "from": "Ablejones"
    },
    {
      "term": "--cache-none flag",
      "explanation": "ComfyUI flag that automatically clears memory after each node to manage RAM usage",
      "from": "Kijai"
    },
    {
      "term": "Block merging",
      "explanation": "Technique to merge specific transformer blocks between models, with early blocks affecting composition and later blocks affecting style",
      "from": "Kijai"
    },
    {
      "term": "HN/LN models",
      "explanation": "High Noise and Low Noise model variants in the two-stage generation process",
      "from": "Kijai"
    },
    {
      "term": "fp8 quantization conversion",
      "explanation": "Converting between e4m3 and e5m2 formats loses quality on both sides, should be avoided",
      "from": "Kijai"
    },
    {
      "term": "Add noise in video generation",
      "explanation": "Setting that adds noise to the process, only has effect when using LoRAs, changes results significantly",
      "from": "Kijai"
    },
    {
      "term": "VACE control strength ratios",
      "explanation": "Settings that control how strongly VACE applies its effects, 1:1 ratio works better than 3:1.5",
      "from": "Lodis"
    },
    {
      "term": "Adaptive rank LoRA",
      "explanation": "Highest rank Lightning LoRA variant at around rank 256, with FFN layers up to rank 319",
      "from": "Kijai"
    },
    {
      "term": "Lightning LoRA brightness forcing",
      "explanation": "New Lightning LoRAs force daylight scenes due to dataset limitations, unlike older LightX2V versions",
      "from": "Multiple users"
    },
    {
      "term": "VACE architecture advantage",
      "explanation": "VACE doesn't fine-tune base WAN - works as extra layers, allowing combination with other models like Phantom",
      "from": "Piblarg"
    },
    {
      "term": "Video token calculation",
      "explanation": "For Wan 2.1/2.2 14B: width/16 * height/16 * (length+3)/4. For Wan 2.2 5B: width/32 * height/32 * (length+3)/4",
      "from": ".: Not Really Human :."
    },
    {
      "term": "Block swapping impact",
      "explanation": "Absolute hell for speed when using rented GPUs",
      "from": "MysteryShack"
    },
    {
      "term": "MoE (Mixture of Experts) in Wan 2.2",
      "explanation": "High/low noise expert split architecture, though the use of MoE terminology may be a stretch",
      "from": "TK_999"
    },
    {
      "term": "Sigma boundary thresholds",
      "explanation": "Set to not split in half, with boundary at 0.875 for T2V and 0.9 for I2V",
      "from": "DawnII"
    },
    {
      "term": "Merge LoRA function",
      "explanation": "Merges LoRA with model before inference, slower first time but faster subsequent runs and saves VRAM",
      "from": "pagan"
    },
    {
      "term": "Temporal inpainting",
      "explanation": "InP models can take any number of input frames and fill in empty frames between them, like start/end frame interpolation but more flexible",
      "from": "Kijai"
    },
    {
      "term": "52 channel input",
      "explanation": "New Fun Control models use 52 input channels instead of standard 48, with unknown purpose for extra 4 channels",
      "from": "Kijai"
    },
    {
      "term": "Dual sampler setup",
      "explanation": "2.2 models require specific dual sampler configuration for high/low noise processing",
      "from": "Kijai"
    },
    {
      "term": "Signal to noise ratio threshold",
      "explanation": "The boundary point at 0.5 SNR where model switches from high noise to low noise expert, occurs around step 875 in original implementation",
      "from": "MysteryShack"
    },
    {
      "term": "4n+1 frame format",
      "explanation": "WAN automatically trims frame count to fit this mathematical requirement",
      "from": "Nekodificador"
    },
    {
      "term": "High/Low noise expert split",
      "explanation": "WAN 2.2 uses different models for high sigma (high noise) and low sigma (low noise) denoising steps",
      "from": "Kijai"
    },
    {
      "term": "Modified sigma calculation",
      "explanation": "modified_sigma = (shift * base_sigma) / (1 + (shift - 1) * base_sigma) - used to determine model switching point",
      "from": "fredbliss"
    },
    {
      "term": "Denoise parameter",
      "explanation": "Sets start step to fraction of total steps, e.g., 4 * 0.4 = 1.6 rounded up to 2 steps",
      "from": "Kijai"
    },
    {
      "term": "50% Signal-to-Noise Ratio threshold",
      "explanation": "Theoretical optimal point for switching between high and low noise models, but varies for distilled models",
      "from": "MysteryShack"
    },
    {
      "term": "Sigma threshold switching",
      "explanation": "T2V uses 0.875 threshold, I2V uses 0.9 threshold to determine when to switch from high to low noise model based on noise levels in generation",
      "from": "Kijai"
    },
    {
      "term": "FP8 weight limitations",
      "explanation": "PyTorch only supports FP8 matrix multiplication, not FP8+FP8 addition or multiplication, requiring upcasting for LoRA application",
      "from": "Kijai"
    },
    {
      "term": "UDP refinement",
      "explanation": "Unbiased Data Processing refinement in Sapiens including distribution-aware coordinate decoding, sub-pixel accuracy using Gaussian blur and Taylor expansion",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "Radial attention",
      "explanation": "Sparse attention with mask specialized for video, requires tuning for which steps and blocks it's applied to",
      "from": "Kijai"
    },
    {
      "term": "Split-screen generation",
      "explanation": "Prompting technique for creating synchronized dual perspective videos with detailed scene descriptions",
      "from": "\ud83e\udd99rishappi"
    },
    {
      "term": "PUSA intended use",
      "explanation": "Designed to cheaply convert Wan 14B T2V to I2V model using only $500 and 4000 videos. Works with T2V model + LoRA + extra_latents, not just slapping it with other LoRAs",
      "from": "Kijai"
    },
    {
      "term": "Context windows in Wan",
      "explanation": "Background differs a lot between context windows because each window uses different seed. More overlap and stride can help consistency",
      "from": "Kijai"
    },
    {
      "term": "High/Low noise models in Wan 2.2",
      "explanation": "High noise model brings very noisy image to slightly less noisy (basis for video), low noise model brings noisy image to proper end result. Two-stage process designed to work together",
      "from": "izashin"
    },
    {
      "term": "3-2 pulldown",
      "explanation": "Video to film conversion technique: 3x interpolate source fps to higher fps, then strip out every other frame to reach target fps (e.g., 16fps -> 48fps -> 24fps)",
      "from": "nacho.money"
    },
    {
      "term": "VAE approximation",
      "explanation": "vae_approx used for previews - approximates VAE giving rough view, pretty much instant so can be used for live previews",
      "from": "Kijai"
    },
    {
      "term": "Attention sink",
      "explanation": "Phenomenon where models latch onto initial tokens/frames as a 'sink' for attention, similar to BOS tokens in LLMs. Could be key to enabling video extension",
      "from": "fredbliss"
    },
    {
      "term": "Temporal mask",
      "explanation": "Mask used with Fun InP where black areas are generated and white areas are kept (opposite to VACE)",
      "from": "Kijai"
    },
    {
      "term": "Fun 2.2 control model",
      "explanation": "5B controlnet model that supports both I2V and T2V, can do temporal inpainting but lost reference input capability",
      "from": "Kijai"
    },
    {
      "term": "Fun 2.2 low noise limitation",
      "explanation": "Cannot finish at good quality, better to use 3rd sampler without Fun at end",
      "from": "Kijai"
    },
    {
      "term": "Style bias in Lightning LoRA",
      "explanation": "2.2 Lightning makes everything bright and oversaturated, especially problematic for dark scenes",
      "from": "Kijai"
    },
    {
      "term": "MoE architecture difference",
      "explanation": "5B uses MoE with High/Low noise expert split, different capabilities than 14B",
      "from": "community discussion"
    },
    {
      "term": "Block swap",
      "explanation": "System for swapping model blocks between GPU and CPU to manage VRAM usage, with transfer and compute time profiling",
      "from": "Kijai"
    },
    {
      "term": "High/Low model split",
      "explanation": "MoE architecture splits processing between high noise (0.875-1.0 timesteps) and low noise (0.0-0.875 timesteps) expert models",
      "from": "Alisson Pereira"
    },
    {
      "term": "Prefetch",
      "explanation": "Option to preload blocks for block swap to reduce transfer time impact",
      "from": "Kijai"
    },
    {
      "term": "FFLF",
      "explanation": "First-Frame-Last-Frame morphing technique",
      "from": "R."
    },
    {
      "term": "LoRA strength scheduling",
      "explanation": "Varying LoRA strength over denoising steps, supports any node that outputs float list",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "Split attention in Stand-In",
      "explanation": "Uses rope split where LoRA is applied to selfattn q k v only",
      "from": "Kijai"
    },
    {
      "term": "Dequant operation",
      "explanation": "Operation that slows down GGUF models by ~20%, has custom kernel for Linux only",
      "from": "Kijai"
    },
    {
      "term": "LoRA scheduling",
      "explanation": "Varying LoRA strength over denoising steps with interpolation between values",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "Differential diffusion",
      "explanation": "Technique using masks for selective inpainting to control which areas get denoised",
      "from": "Juan Gea"
    },
    {
      "term": "Shared attention mechanism",
      "explanation": "Stand-In's method of incorporating face reference without adding extra latents to the full sequence",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Memory optimization technique that swaps model blocks to manage VRAM usage",
      "from": "patientx"
    },
    {
      "term": "FFLF",
      "explanation": "First Frame Last Frame - a morphing technique for video generation",
      "from": "Charlie"
    },
    {
      "term": "Stand-in LoRA",
      "explanation": "A LoRA that allows using reference images for character consistency, similar to Phantom but with different training",
      "from": "Kijai"
    },
    {
      "term": "res_2s sampler",
      "explanation": "Higher order method with smaller truncation error, does double the steps essentially",
      "from": "Instability01"
    },
    {
      "term": "TeaCache",
      "explanation": "Method for skipping steps in model inference for speed optimization",
      "from": "Kijai"
    },
    {
      "term": "Block swapping",
      "explanation": "Memory management technique that automatically moves model blocks between VRAM and RAM as needed",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Technique for extending video generation by overlapping and blending segments",
      "from": "Kijai"
    },
    {
      "term": "Uni3c",
      "explanation": "Control method to lock camera position for stable extended generation",
      "from": "Kijai"
    },
    {
      "term": "FFLF",
      "explanation": "First Frame Last Frame - technique for video morphing between start and end frames",
      "from": "AR"
    },
    {
      "term": "Wan v3 is Wan 1.3B (WanFun)",
      "explanation": "Clarification that the v3 model refers to the 1.3B parameter WanFun model",
      "from": "Kijai"
    },
    {
      "term": "Fun Control patch embedding",
      "explanation": "Uses 52 input channels combining noise (16) + image conditioning (16) + control (16) + mask (4)",
      "from": "Kijai"
    },
    {
      "term": "Wan 2.2 LN as LoRA",
      "explanation": "The Low Noise model can be extracted as a LoRA and applied to other Wan 2.1 models to transfer improvements",
      "from": "Ablejones"
    },
    {
      "term": "kv_cache feature",
      "explanation": "Performance optimization that applies model only once then caches and reuses k and v values on remaining steps, but may affect quality",
      "from": "Kijai"
    },
    {
      "term": "pdf in FantasyPortrait context",
      "explanation": "Unclear face detection system, possibly 'point-detection'",
      "from": "Kijai"
    },
    {
      "term": "Stand-in",
      "explanation": "New lip sync model similar to Live Portrait but as Wan adapter for 2.1 14B I2V",
      "from": "Kijai"
    },
    {
      "term": "Reference latent",
      "explanation": "What is used as the first latent in each context window that doesn't include the actual first frame",
      "from": "Kijai"
    },
    {
      "term": "MAGREF",
      "explanation": "I2V model for character subject consistency that works as reference-to-video instead of start-frame-to-video",
      "from": "gokuvonlange"
    },
    {
      "term": "fp8 scaled models",
      "explanation": "Quantized models with specific scaling to minimize quality loss compared to standard fp8",
      "from": "Kijai"
    },
    {
      "term": "VSA model",
      "explanation": "Some sort of sparse attention architecture, wouldn't work with regular workflows",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Method for generating longer videos beyond normal limits",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Memory management technique to fit large models in limited VRAM",
      "from": "nacho.money"
    },
    {
      "term": "Higher rank LoRA",
      "explanation": "Closer to the original model it's extracted from, stronger but not always better",
      "from": "Kijai"
    },
    {
      "term": "FusionX model",
      "explanation": "Wan 2.1 with 5-6 LoRAs baked in, then saved, then single LoRA extracted from that",
      "from": "Kijai"
    },
    {
      "term": "PD-FGC",
      "explanation": "222 keypoint face detection system used in Fantasy Portrait",
      "from": "Kijai"
    },
    {
      "term": "Block offloading",
      "explanation": "Memory management technique that can sometimes cause allocation crashes",
      "from": "Obsolete"
    },
    {
      "term": "Block swap",
      "explanation": "Manual memory management that reduces constant memory use underneath VRAM peaks",
      "from": "Kijai"
    },
    {
      "term": "NAG (Negative prompting)",
      "explanation": "Method to have negative prompting when not using CFG, mostly when CFG is set to 1",
      "from": "Kijai"
    },
    {
      "term": "FLF (First-Last-Frame)",
      "explanation": "Morphing technique where video transitions from first frame to last frame",
      "from": "Kijai"
    },
    {
      "term": "Graph breaks in torch.compile",
      "explanation": "Points where compilation is interrupted and resumed, can cause memory issues",
      "from": "Kijai"
    },
    {
      "term": "Reserve VRAM",
      "explanation": "Tells model loading code to assume it needs X GB more memory than estimated, preventing OOM or Windows shared memory spillover",
      "from": "Kosinkadink"
    },
    {
      "term": "NAG (Negative Augmented Generation)",
      "explanation": "Allows negative conditioning at CFG 1.0 while maintaining speed benefits",
      "from": "DawnII"
    },
    {
      "term": "Block swapping",
      "explanation": "Automatic VRAM management that moves model blocks between VRAM and RAM as needed",
      "from": "Persoon"
    },
    {
      "term": "VACE reference behavior",
      "explanation": "Works more like FaceID V2 than a simple IPAdapter, requiring high quality reference images for best results",
      "from": "Nekodificador"
    },
    {
      "term": "Lightning lora censoring",
      "explanation": "At high strengths, lightning loras generate objects to block body parts, possibly due to distillation differences from lightx2v",
      "from": "MysteryShack"
    },
    {
      "term": "WanFM method",
      "explanation": "Does start-to-end prediction, then end-to-start prediction with reverse RoPE and blends them together each step",
      "from": "Kijai"
    },
    {
      "term": "WanFM (Frame Morphing)",
      "explanation": "Sampling method that does forward and reverse passes on each step, reversing input and RoPE frequencies for reverse pass, then blending results",
      "from": "Kijai"
    },
    {
      "term": "First frame encoding difference",
      "explanation": "VAE encodes first frame differently (partial latent) vs full latents, causing flash effect if you use full latents for continuation",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "ComfyUI feature that uses disk space to handle memory overflow, but doesn't automatically clear after generation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "Self-forcing",
      "explanation": "A training technique, not something you 'use' - the model is trained with self-forcing method",
      "from": "Kijai"
    },
    {
      "term": "WanFM sampling",
      "explanation": "Method that does 2 model predictions per step, forwards and backwards, then blends those results",
      "from": "Kijai"
    },
    {
      "term": "Causal sampling",
      "explanation": "Different from self-forcing, was used for first causvid and first self-forcing models, causes motion quality degradation when used with 21 latents instead of intended 3",
      "from": "Kijai"
    },
    {
      "term": "Context stride",
      "explanation": "Controls frame movement to next batch in context windows - documented in AnimateDiff evolved",
      "from": "Kijai"
    },
    {
      "term": "I2V limitations with context windows",
      "explanation": "I2V can't work well with context as video never finished and every window needs start image",
      "from": "Kijai"
    },
    {
      "term": "VACE control video behavior",
      "explanation": "Never adds anything, only for replacing. VACE only adds reference image to latents",
      "from": "Kijai"
    },
    {
      "term": "Start to End Frame node usage",
      "explanation": "Only for I2V models, adds frames to latents for 5B model",
      "from": "Kijai"
    },
    {
      "term": "CFG early step behavior",
      "explanation": "Classifier-free guidance shows negative prompt content in early diffusion steps before converging to positive",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Technique to offload model blocks to CPU RAM to fit larger models/higher resolutions in limited VRAM",
      "from": "Ryzen"
    },
    {
      "term": "GGUF quantization overhead",
      "explanation": "GGUF models are compressed and need decompression during inference, making them slower despite smaller size",
      "from": ". Not Really Human ."
    },
    {
      "term": "High/low noise expert split",
      "explanation": "Wan 2.2 uses different model experts for different noise levels in the denoising process",
      "from": "Benjimon"
    },
    {
      "term": "Context windows for I2V models",
      "explanation": "Each window is a separate model prediction on every step. I2V models requiring start images try to start from it for each window, but can't use last frame as it's never fully denoised",
      "from": "Kijai"
    },
    {
      "term": "VACE compatibility",
      "explanation": "VACE is an addon for T2V models only, cannot work with I2V models unless retrained. Phantom works with VACE because it's a T2V model finetuned to use references",
      "from": "Kijai"
    },
    {
      "term": "Control signals for stability",
      "explanation": "Anything that controls generation like VACE control, audio control, uni3c - plain text conditioning works very poorly for long stable generations",
      "from": "Kijai"
    },
    {
      "term": "Context frames vs total frames",
      "explanation": "Memory usage based on context frames * width * height, not total output frames",
      "from": "Kijai"
    },
    {
      "term": "FP8 matmul vs emulated FP8",
      "explanation": "True FP8 matmul only on supported cards, otherwise upcast to base precision",
      "from": "Kijai"
    },
    {
      "term": "GGUF dequantization",
      "explanation": "GGUF has to be dequantized on the fly, adding extra calculations",
      "from": "Kijai"
    },
    {
      "term": "Diffusion forcing in Skyreels",
      "explanation": "Method that batches images into latent instead of single one for better contextual understanding of motion",
      "from": "sawlike"
    },
    {
      "term": "Multi-stage sampling",
      "explanation": "Using different CFG settings across sampling stages, typically CFG for early steps, no CFG for later steps",
      "from": "Kijai"
    },
    {
      "term": "Block swapping",
      "explanation": "Memory management technique that only loads needed blocks directly and swaps the rest",
      "from": "Kijai"
    },
    {
      "term": "MoE (Mixture of Experts)",
      "explanation": "Wan 2.2 architecture with High/Low noise expert split",
      "from": "MysteryShack"
    },
    {
      "term": "Context windows vs looping method",
      "explanation": "Two different approaches - context windows samples in chunks, looping samples whole thing in loop",
      "from": "Kijai"
    },
    {
      "term": "Scaled models",
      "explanation": "Modified precision versions that must be used consistently across main and extra models",
      "from": "Kijai"
    },
    {
      "term": "InfiniteTalk",
      "explanation": "Updated MultiTalk model that allows infinite/endless generation with audio control, works as endless I2V",
      "from": "Kijai"
    },
    {
      "term": "MAGREF",
      "explanation": "Reference system that provides more freedom and stability, works at 480p resolution",
      "from": "Kijai"
    },
    {
      "term": "Audio scale vs Audio CFG scale",
      "explanation": "Audio scale multiplies conditioning in every case, audio_cfg_scale only works when main CFG is used",
      "from": "Kijai"
    },
    {
      "term": "Motion frame",
      "explanation": "The overlap setting in multitalk processing, can be adjusted but best to leave at default",
      "from": "Kijai"
    },
    {
      "term": "Frame window size",
      "explanation": "Similar to context window length, should be set to frame number you generate normally",
      "from": "DawnII"
    },
    {
      "term": "Multitalk node vs context windows",
      "explanation": "Multitalk node is alternative to context windows for long video gen, not needed for 81 frames and under",
      "from": "Kijai"
    },
    {
      "term": "V2V keyframing approach",
      "explanation": "Instead of true video-to-video, system takes keyframes every 72 frames and uses uni3c + audio to generate between them",
      "from": "DawnII"
    },
    {
      "term": "High/Low noise split",
      "explanation": "Wan 2.2 MoE architecture separates high noise and low noise processing, with different LoRA compatibility",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Memory management technique that keeps data in RAM rather than writing to disk",
      "from": "Kijai"
    },
    {
      "term": "LoRA rank",
      "explanation": "Amount of parameters per layer - higher rank means bigger effect but also higher memory usage",
      "from": "Kijai"
    },
    {
      "term": "Motion frame overlap",
      "explanation": "Number of frames used from previous generation when it continues - window of 81 with overlap 25 means 81-25=56 new images per loop",
      "from": "Kijai"
    },
    {
      "term": "Audio start index",
      "explanation": "Index showing where audio processing begins in frame sequence during generation",
      "from": "seitanism"
    },
    {
      "term": "NAG",
      "explanation": "Negative prompt guidance system that helps when CFG=1 isn't using negative prompts",
      "from": "mamad8"
    },
    {
      "term": "Swap blocks",
      "explanation": "Feature mainly useful as safeguard against OOM errors, especially at higher resolutions, may only take effect on new prompts",
      "from": ". Not Really Human :."
    },
    {
      "term": "V2V with InfiniteTalk",
      "explanation": "Not currently fully implemented, requires input sample length to equal frame_window_size",
      "from": "MysteryShack"
    },
    {
      "term": "Temporal mask for Fun",
      "explanation": "Black frames are changed, white frames unchanged, opposite of VACE masking",
      "from": "DawnII"
    },
    {
      "term": "SceneCut support in InfiniteTalk",
      "explanation": "Input video can contain multiple shots, convenient for remaking video with input audio while maintaining camera movement and scene layout",
      "from": "NebSH"
    },
    {
      "term": "V2V implementation in InfiniteTalk",
      "explanation": "Chops up video every 72 frames and uses uni3c to bridge the gap, not true V2V",
      "from": "DawnII"
    },
    {
      "term": "InfiniteTalk first frame noise issue",
      "explanation": "InfiniteTalk model cannot handle full noise like it encounters in non-loop sampling, causing noisy first frames. Fixed by encoding init image and injecting it to first latent on every step.",
      "from": "Kijai"
    },
    {
      "term": "Context windows in video generation",
      "explanation": "Method for processing longer videos by breaking them into overlapping segments, useful with the new InfiniteTalk noise fix",
      "from": "Kijai"
    },
    {
      "term": "Motion frames in InfiniteTalk",
      "explanation": "Frame window is fixed at 81, motion frames are overlap between windows. Formula: total_frames = (x * 81) - ((x-1) * motion_frames)",
      "from": "DawnII"
    },
    {
      "term": "Frame padding",
      "explanation": "Video is padded to account for looping windows, causing lingering gibberish at end",
      "from": "DawnII"
    },
    {
      "term": "start_step in vid2vid",
      "explanation": "First N steps considered already done, input video used instead, noise added based on what noise should be on those steps",
      "from": "Kijai"
    },
    {
      "term": "motion_frames",
      "explanation": "Context window overlap, every batch of 81 frames uses 9 frames from last clip as first clip for smooth transition",
      "from": "seitanism"
    },
    {
      "term": "InfiniteTalk context windows",
      "explanation": "Each window is 81 frames with overlap, sampling shows audio indices for each window",
      "from": "Kijai"
    },
    {
      "term": "Differential diffusion",
      "explanation": "Mixes the init with the noise per step using mask as thresholds, so mask can be blurred and blends better than normal latent masking",
      "from": "Kijai"
    },
    {
      "term": "Temporal inpainting",
      "explanation": "Inpainting that works on first to last frame and in-between frames, not spatial inpainting like VACE",
      "from": "Kijai"
    },
    {
      "term": "I2V chaining in loops",
      "explanation": "Multiple samplers where each uses previous sampler's result as I2V input, requires finished denoised output",
      "from": "Kijai"
    },
    {
      "term": "NAG (Negative Attention Guidance)",
      "explanation": "Applied to positive conditioning, not negative. With cfg 1.0, normal negative isn't used",
      "from": "Kijai"
    },
    {
      "term": "Mixed precision models",
      "explanation": "99% fp16 weights with fp32 bias, adds minimal size (~6mb) with barely noticeable quality difference",
      "from": "Kijai"
    },
    {
      "term": "MultiTalk masking",
      "explanation": "Limits where each audio can affect result, not precise mouth targeting but broader area control",
      "from": "Kijai"
    },
    {
      "term": "Fun InP model",
      "explanation": "Model for first-last frame temporal inpainting, confusing naming",
      "from": "Blink"
    },
    {
      "term": "Extra latent technique",
      "explanation": "Adding latent to I2V changes the noise pattern, not intended way but can improve certain prompting scenarios",
      "from": "Kijai"
    },
    {
      "term": "Differential diffusion",
      "explanation": "Code technique for inpainting that was incorrectly placed, now fixed for better masking results",
      "from": "Kijai"
    },
    {
      "term": "Differential diffusion",
      "explanation": "Uses grayscale masks for blending, works poorly with low steps due to limited adjustment room",
      "from": "Kijai"
    },
    {
      "term": "RoPE frequency scaling",
      "explanation": "Spatial base frequency scaling for resolution changes while temporal frequency stays at 1.0",
      "from": "Kijai"
    },
    {
      "term": "Context windows with prompt changes",
      "explanation": "Prompts separated by | are spread evenly across context windows, last prompt used for remaining frames",
      "from": "Kijai"
    },
    {
      "term": "RoPE scaling",
      "explanation": "A technique that can improve results even without cinescale, applied to both samplers in upscaling workflow",
      "from": "Kijai"
    },
    {
      "term": "Flowmatch_distill",
      "explanation": "A sampler that's only really necessary for 4 steps, gets errors if you try more steps",
      "from": "Kijai"
    },
    {
      "term": "Set Lora node",
      "explanation": "Sets loras after loading model so you don't have to reload the model, uses more VRAM but allows dynamic lora application",
      "from": "Kijai"
    },
    {
      "term": "MAGREF",
      "explanation": "A model with I2V structure trained differently that allows context windows and character consistency",
      "from": "Kijai"
    },
    {
      "term": "Context windows with I2V models",
      "explanation": "I2V models being start image models by nature makes them incompatible with context windows concept",
      "from": "Kijai"
    },
    {
      "term": "CUDA out of memory vs Allocation on device",
      "explanation": "CUDA out of memory = RAM error, Allocation on device = VRAM error",
      "from": "Kijai"
    },
    {
      "term": "InfiniteTalk vs MultiTalk",
      "explanation": "InfiniteTalk doesn't degrade when continuing from last frame, allows endless generation without context window drawbacks",
      "from": "Kijai"
    },
    {
      "term": "Block swap settings",
      "explanation": "non_blocking increases RAM use but speeds transfers; prefetch_blocks moves blocks asynchronously from RAM to VRAM; block_swap_debug reports transfer times",
      "from": "Kijai"
    },
    {
      "term": "Torch compile caching",
      "explanation": "Compile is cached per input - same resolution and frame count shouldn't re-compile",
      "from": "Kijai"
    },
    {
      "term": "CFG interpolation",
      "explanation": "Start and end CFG being same disables interpolation, range outside start/end percent is put to 1.0",
      "from": "Kijai"
    },
    {
      "term": "first_sampler variable",
      "explanation": "Used as an 'end_sampling_before_final_timestep' check, badly named leftover variable that skips slicing of timesteps",
      "from": "Kijai"
    },
    {
      "term": "latent insertion trick",
      "explanation": "Adding input image as extra latent to avoid start being noise",
      "from": "Kijai"
    },
    {
      "term": "Loop sampling window overlap",
      "explanation": "When using loop sampling with window size 81 and motion_frames 9, it creates overlap requiring more total frames than just the window size",
      "from": "Kijai"
    },
    {
      "term": "InfiniteTalk latent training",
      "explanation": "InfiniteTalk is trained to expect the image in the latents, which is why first latent in normal I2V workflow is just noise",
      "from": "Kijai"
    },
    {
      "term": "Block swapping memory calculation",
      "explanation": "Divide model size by 40 on 14B models, by 30 on 1.3B and 5B models to get per-block memory usage",
      "from": "Kijai"
    },
    {
      "term": "MoE High/Low noise expert split",
      "explanation": "Wan 2.2 uses different experts for high and low noise, can hook different inputs to each",
      "from": "Draken"
    },
    {
      "term": "SNR threshold splitting",
      "explanation": "New node splits steps based on sigma threshold (0.875) instead of manually selected step count",
      "from": "Kijai"
    },
    {
      "term": "Block swap memory management",
      "explanation": "Balances memory use between VRAM and RAM using blocks_to_swap parameter to run larger models",
      "from": "Kijai"
    },
    {
      "term": "VACE keyframe interpolation",
      "explanation": "Basic VACE feature that allows smooth transitions between keyframes, often overlooked",
      "from": "Kijai"
    },
    {
      "term": "Dense pose controlnet",
      "explanation": "More detailed pose control compared to regular openpose, available in cloth adapter repos",
      "from": "Kijai"
    },
    {
      "term": "Fakevace",
      "explanation": "Alternative VACE implementation that can work with extend nodes and InfiniteTalk",
      "from": "JalenBrunson"
    },
    {
      "term": "Frame window size in InfiniteTalk",
      "explanation": "Size of a single generation when splitting process to multiple generations, with motion_frame as overlap between windows",
      "from": "Kijai"
    },
    {
      "term": "Fun InP vs Fun-Control channel requirements",
      "explanation": "Fun InP = 36 channels like normal I2V, Fun-Control 2.2 = 52 channels (16+16+16+4)",
      "from": "Kijai"
    },
    {
      "term": "InfiniteTalk first latent expectation",
      "explanation": "InfiniteTalk model is trained to expect the first latent to be an image, that's why it works for continuous I2V generation",
      "from": "Kijai"
    },
    {
      "term": "Crop&stitch",
      "explanation": "Powerful technique for handling overlays and masked regions",
      "from": "Valle"
    },
    {
      "term": "Stand-in rope",
      "explanation": "Feature implemented for ComfyUI compatibility, comfy version is better than original",
      "from": "Kijai"
    },
    {
      "term": "Frame pack method",
      "explanation": "Sampling method used in new Wan S2V model with causal self attention",
      "from": "Kijai"
    },
    {
      "term": "Non-blocking in block swap",
      "explanation": "Reserves more RAM but is faster, uses more CPU RAM for speed",
      "from": "xwsswww"
    },
    {
      "term": "Tea Cache compatibility",
      "explanation": "Doesn't work with LoRA accelerators, causes pixelation issues",
      "from": ".: Not Really Human :."
    },
    {
      "term": "Framepack",
      "explanation": "Technology that allows video extension and potentially infinite generation, used in S2V model",
      "from": "Kijai"
    },
    {
      "term": "Context window size",
      "explanation": "Frame window size that affects inference speed - larger windows make generation slower",
      "from": "Mancho"
    },
    {
      "term": "Beta sigma conversion",
      "explanation": "A scheduling technique that can be applied in samplers, shouldn't be used twice",
      "from": "Kijai"
    },
    {
      "term": "Reference latent",
      "explanation": "Used in S2V model exactly like Phantom model for consistency",
      "from": "Kijai"
    },
    {
      "term": "CFG Schedule",
      "explanation": "Method to vary CFG values across sampling steps for better results",
      "from": "Mu5hr00m_oO"
    },
    {
      "term": "CFG Skimming",
      "explanation": "CFG interpolation technique, same as advanced CFG interpolation",
      "from": "DawnII"
    },
    {
      "term": "Framepack extension technique",
      "explanation": "Technique used for longer video generation beyond 5 seconds",
      "from": "ArtOfficial"
    },
    {
      "term": "Framepack",
      "explanation": "Architecture used by WAN S2V for video generation, allows consistent VRAM usage regardless of frame count",
      "from": "Kijai"
    },
    {
      "term": "Context overlap",
      "explanation": "Higher overlap means more model passes needed and slower generation, but better blending between windows",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Method for generating longer videos by processing in segments",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Manual VRAM management technique in WanWrapper for memory optimization",
      "from": "Kijai"
    },
    {
      "term": "Uni3C",
      "explanation": "Component that provides camera control functionality",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "term": "Framepack",
      "explanation": "A technique that initializes motion in input latents to allow infinite generation, used by S2V model. Different from direct guidance, more like projection",
      "from": "Kijai, MilesCorban"
    },
    {
      "term": "Context windows vs Framepack",
      "explanation": "Context windows use overlapping frames for continuity, while framepack uses motion initialization for infinite generation",
      "from": "Kijai"
    },
    {
      "term": "Merged vs Unmerged LoRAs",
      "explanation": "Native always merges LoRAs, while wrapper can use them unmerged. GGUF is always unmerged. Behavior differs significantly between modes",
      "from": "Kijai, Draken"
    },
    {
      "term": "PUSA",
      "explanation": "Serves as a way to expand t2v model capabilities for frame extension, I2V, extended generation. Helps maintain likeness across context windows due to how it attends images",
      "from": "DawnII"
    },
    {
      "term": "Framepack implementation",
      "explanation": "One of three motioner methods in S2V, doesn't have context window burning issues but reduces lipsync quality due to overlap",
      "from": "Kijai"
    },
    {
      "term": "Pose condition layer",
      "explanation": "Layer in S2V model that shouldn't be quantized to fp8 for proper functionality",
      "from": "Kijai"
    },
    {
      "term": "mmproj",
      "explanation": "The visual component of VLM (Vision Language Models), not a text encoder",
      "from": "DawnII"
    },
    {
      "term": "Framepack",
      "explanation": "Context sliding technique used by S2V model with its own weights, allows for extended video generation",
      "from": "Kijai"
    },
    {
      "term": "MAGREF",
      "explanation": "I2V model trained to use the image more as a reference than as a start image, so it has lot more freedom to work with",
      "from": "Kijai"
    },
    {
      "term": "Framepack continuation method",
      "explanation": "Method for video generation where bucket function determines frame handling - different from context windows approach",
      "from": "Kijai"
    },
    {
      "term": "MAGREF",
      "explanation": "A full Wan I2V model that works like Phantom, using image as reference rather than direct I2V conversion",
      "from": "blake37"
    },
    {
      "term": "Framepack",
      "explanation": "Technology from S2V that enables longer generation but quality degrades over time, requires careful balance of settings",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Method for generating longer videos by passing previous frames as context, better quality than Framepack but worse lipsync",
      "from": "Kijai"
    },
    {
      "term": "Beta scheduler differences",
      "explanation": "ComfyUI and diffusers handle beta scheduling differently - diffusers applies shift first then beta, ComfyUI may do opposite",
      "from": "Kijai"
    },
    {
      "term": "Context windows vs extension",
      "explanation": "Context windows do single generation in windows with no quality degradation, unlike extension methods",
      "from": "Kijai"
    },
    {
      "term": "Alpha channel handling",
      "explanation": "Alpha isn't special to models, just extra channel with information. VAE strips alpha away",
      "from": "Kijai"
    },
    {
      "term": "CFG with empty conditioning",
      "explanation": "Empty conditioning still runs model as if without prompt, then subtracts that result",
      "from": "Ablejones"
    },
    {
      "term": "Context window stride artifacts",
      "explanation": "High noise output shows stride artifacts that get cleaned up in low noise pass",
      "from": "Kijai"
    },
    {
      "term": "MAGREF vs regular I2V for context",
      "explanation": "MAGREF works with context windows while regular I2V works poorly due to training to reference first frame",
      "from": "daking999"
    },
    {
      "term": "Prompt travel",
      "explanation": "Different prompts can be used for different segments of context windows, each covering ~81 frames",
      "from": "Kijai"
    },
    {
      "term": "InfiniteTalk looping",
      "explanation": "The infinite talk node simply continues from last frame in a loop, expects the image encoded as first latent",
      "from": "Kijai"
    },
    {
      "term": "Context window stitching",
      "explanation": "Different context prompts with new scenes, infinitetalk looping is better for this",
      "from": "DawnII"
    },
    {
      "term": "VACE embed technique",
      "explanation": "Connect between frames setup to a VACE embed for advanced control",
      "from": "Nekodificador"
    },
    {
      "term": "VACE first frame edit",
      "explanation": "VACE can take a video and a new first frame and edit the video using that new first frame as reference",
      "from": "Juampab12"
    }
  ],
  "resources": [
    {
      "resource": "Rapid Wan 2.2 All-in-One model",
      "url": "https://civitai.com/models/1824594/rapid-wan-22-all-in-one?modelVersionId=2064884",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Wan2.2 ControlNet implementation",
      "url": "https://github.com/TheDenk/wan2.2-controlnet",
      "type": "repo",
      "from": "A.I.Warper"
    },
    {
      "resource": "WAN 2.2 14B (GGUF) V2V workflow",
      "url": "https://huggingface.co/thaakeno46/ComfyUI-Workflows/blob/main/WAN%202.2%2014B%20(GGUF)%20V2V.json",
      "type": "workflow",
      "from": "thaakeno"
    },
    {
      "resource": "Wan 2.1 LoRA trainer",
      "url": "https://comfy.icu/extension/jaimitoes__ComfyUI_Wan2_1_lora_trainer",
      "type": "tool",
      "from": "Ryzen"
    },
    {
      "resource": "WSL2 ComfyUI setup guide",
      "url": "https://www.youtube.com/watch?v=ZBgfRlzZ7cw",
      "type": "tutorial",
      "from": "seitanism"
    },
    {
      "resource": "Triton and SageAttention installer",
      "url": "https://civitai.com/articles/12851/easy-installation-triton-and-sageattention",
      "type": "tool",
      "from": "Ruairi Robinson"
    },
    {
      "resource": "WAN 2.2 VACE test version",
      "url": "https://huggingface.co/lym00/Wan2.2_T2V_A14B_VACE-test/tree/main",
      "type": "model",
      "from": "avataraim"
    },
    {
      "resource": "RadialAttention ComfyUI implementation",
      "url": "https://github.com/woct0rdho/ComfyUI-RadialAttn",
      "type": "repo",
      "from": "Lumi"
    },
    {
      "resource": "Block Sparse SageAttention 2.0",
      "url": "https://github.com/Radioheading/Block-Sparse-SageAttention-2.0",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "SpargeAttn wheels",
      "url": "https://github.com/woct0rdho/SpargeAttn/releases",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "SageAttention3 early access",
      "url": "https://huggingface.co/jt-zhang/SageAttention3",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "LightX2V LoRA comparison",
      "url": "https://www.reddit.com/r/comfyui/comments/1meszym/21_lightx2v_lora_will_make_wan22_more_like_wan21/",
      "type": "resource",
      "from": "toyxyz"
    },
    {
      "resource": "Triton Windows build",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "NSFW LoRA for WAN 2.2",
      "url": "civitai.com/models/1476909",
      "type": "lora",
      "from": "crinklypaper"
    },
    {
      "resource": "WAN 2.2 ControlNet Depth v1",
      "url": "https://huggingface.co/TheDenk/wan2.2-ti2v-5b-controlnet-depth-v1",
      "type": "model",
      "from": "orabazes"
    },
    {
      "resource": "Prompt generator tool",
      "url": "https://dengeai.com/prompt-generator",
      "type": "tool",
      "from": "avataraim"
    },
    {
      "resource": "Official Alibaba source materials",
      "url": "",
      "type": "documentation",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "WAN camera prompting guide",
      "url": "https://alidocs.dingtalk.com/i/nodes/EpGBa2Lm8aZxe5myC99MelA2WgN7R35y",
      "type": "documentation",
      "from": "crinklypaper"
    },
    {
      "resource": "HSD LoRA for high speed dynamic movement",
      "url": "https://civitai.com/models/1698719/high-speed-dynamic",
      "type": "lora",
      "from": "Jonathan"
    },
    {
      "resource": "LLM prompt generation system guide",
      "url": "https://pastebin.com/raw/nFjbM7mJ",
      "type": "workflow",
      "from": "Simjedi"
    },
    {
      "resource": "Ollama workflow for prompt generation",
      "url": "workflow shared in chat",
      "type": "workflow",
      "from": "Simjedi"
    },
    {
      "resource": "Mobius looping/long video strategy",
      "url": "https://github.com/YisuiTT/Mobius",
      "type": "repo",
      "from": "daking999"
    },
    {
      "resource": "Wan 2.2 official code repository",
      "url": "https://github.com/Wan-Video/Wan2.2/tree/main/wan/configs",
      "type": "repo",
      "from": "aikitoria"
    },
    {
      "resource": "H1111 implementation for full quality",
      "url": "https://github.com/maybleMyers/H1111/tree/wan2.2b",
      "type": "repo",
      "from": "Benjimon"
    },
    {
      "resource": "I2V workflow for 14B",
      "url": "",
      "type": "workflow",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "T2V workflow with auto-calculation",
      "url": "",
      "type": "workflow",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "Wan 2.2 Prompt Creator ChatGPT",
      "url": "https://chatgpt.com/g/g-688805b301ec81918e3a5a45dbb8405b-wan2-2-prompt-creator",
      "type": "tool",
      "from": "\u2727\u0e05\u0e42\u0e51\u2180\u11ba\u2180 \u0e51\u0e43\u0e05\u2727PookieNumnums"
    },
    {
      "resource": "VACE Test model for Wan 2.2",
      "url": "https://huggingface.co/lym00/Wan2.2_T2V_A14B_VACE-test/tree/main",
      "type": "model",
      "from": "BondoMan"
    },
    {
      "resource": "Sage Attention installation guide",
      "url": "https://www.youtube.com/watch?v=CgLL5aoEX-s",
      "type": "tutorial",
      "from": "xwsswww"
    },
    {
      "resource": "Wan 2.2 system prompt",
      "url": "https://github.com/Wan-Video/Wan2.2/blob/main/wan/utils/system_prompt.py",
      "type": "code",
      "from": "Kijai"
    },
    {
      "resource": "MMaudio ComfyUI nodes",
      "url": "",
      "type": "tool",
      "from": "nacho.money"
    },
    {
      "resource": "Wan 2.2 i2v crush LoRA",
      "url": "https://huggingface.co/ostris/wan22_5b_i2v_crush_it_lora",
      "type": "lora",
      "from": "thaakeno"
    },
    {
      "resource": "EchoShot model",
      "url": "https://huggingface.co/JonneyWang/EchoShot",
      "type": "model",
      "from": "Juampab12"
    },
    {
      "resource": "MOSS-TTSD",
      "url": "https://github.com/OpenMOSS/MOSS-TTSD",
      "type": "repo",
      "from": "EKvan"
    },
    {
      "resource": "Prompt generator tool",
      "url": "https://prompt.dengeai.com/prompt-generator",
      "type": "tool",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "Wan 2.2 ComfyUI Repackaged",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/tree/main/split_files/text_encoders",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Wan prompting guide",
      "url": "https://alidocs.dingtalk.com/i/nodes/EpGBa2Lm8aZxe5myC99MelA2WgN7R35y",
      "type": "guide",
      "from": "Ada"
    },
    {
      "resource": "Wan 2.2 prompter with video input",
      "url": "https://prompter.on.websim.com/",
      "type": "tool",
      "from": "thaakeno"
    },
    {
      "resource": "ComfyUI Wan 2.2 FLF2V blog post",
      "url": "https://blog.comfy.org/p/wan22-flf2v-comfyui-native-support",
      "type": "guide",
      "from": "BarleyFarmer"
    },
    {
      "resource": "CondensedMovies dataset",
      "url": "https://github.com/m-bain/CondensedMovies",
      "type": "dataset",
      "from": "mamad8"
    },
    {
      "resource": "RES4LYF custom node",
      "url": "https://github.com/ClownsharkBatwing/RES4LYF",
      "type": "node",
      "from": "mamad8"
    },
    {
      "resource": "Wan2.2_T2V_A14B_VACE-test",
      "url": "https://huggingface.co/lym00/Wan2.2_T2V_A14B_VACE-test/tree/main",
      "type": "model",
      "from": "Ashtar"
    },
    {
      "resource": "SageAttention installation scripts",
      "url": "https://huggingface.co/lym00/scripts/tree/main",
      "type": "scripts",
      "from": "hicho"
    },
    {
      "resource": "UMT5 encoders",
      "url": "https://huggingface.co/city96/umt5-xxl-encoder-gguf/tree/main",
      "type": "model",
      "from": "xwsswww"
    },
    {
      "resource": "Reddit discussion on Wan 2.2 settings",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1mfzvl5/debate_best_wan_22_t2v_settings_steps_sampler_cfg/?share_id=Z4koU6SN5YPpZUm7SgVRk&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=10",
      "type": "guide",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "MMAudio + Wan 2.2 workflow",
      "url": "https://huggingface.co/thaakeno46/ComfyUI-Workflows/blob/main/Wan2.2-MMAudio_GGUF_T2V.json",
      "type": "workflow",
      "from": "thaakeno"
    },
    {
      "resource": "FastVideo GitHub commit",
      "url": "https://github.com/hao-ai-lab/FastVideo/commit/5f42748ed11bd1be9929d7b3de42691f6eac553a",
      "type": "repo",
      "from": "Screeb"
    },
    {
      "resource": "DFloat11 compressed Wan2.2",
      "url": "https://huggingface.co/DFloat11/Wan2.2-T2V-A14B-2-DF11",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Phr00t Rapid AllInOne v2",
      "url": "https://huggingface.co/Phr00t/WAN2.2-14B-Rapid-AllInOne/tree/main/v2",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Mixed precision weights",
      "url": "https://huggingface.co/maybleMyers/wan_files_for_h1111/tree/main",
      "type": "model",
      "from": "Benjimon"
    },
    {
      "resource": "Official Wan 2.2 code",
      "url": "https://github.com/Wan-Video/Wan2.2",
      "type": "repo",
      "from": "aikitoria"
    },
    {
      "resource": "FusionX ingredients and workflows",
      "url": "https://civitai.com/models/1736052/fusionxlightning-ingredientsworkflows",
      "type": "workflow",
      "from": "Kenk"
    },
    {
      "resource": "FPS1 experimental LoRAs",
      "url": "Wan22_I2V_FPS1_lownoise_s13000.safetensors: https://pixeldrain.com/u/eXypSibw, Wan22_I2V_FPS1_highnoise_s9000.safetensors: https://pixeldrain.com/u/ei2FRBkU",
      "type": "lora",
      "from": "mamad8"
    },
    {
      "resource": "Wan official prompt guide",
      "url": "https://alidocs.dingtalk.com/i/nodes/EpGBa2Lm8aZxe5myC99MelA2WgN7R35y",
      "type": "documentation",
      "from": "crinklypaper"
    },
    {
      "resource": "ClownShark samplers",
      "url": "https://github.com/ClownsharkBatwing/RES4LYF/tree/main",
      "type": "sampler",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "LightX2V I2V 14B distill LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Lightx2v/lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors",
      "type": "lora",
      "from": "Ablejones"
    },
    {
      "resource": "Quantile adaptive T2V LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Lightx2v/lightx2v_14B_T2V_cfg_step_distill_lora_adaptive_rank_quantile_0.15_bf16.safetensors",
      "type": "lora",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "bf16 I2V models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_2-I2V-A14B-HIGH_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "bf16 I2V Low model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_2-I2V-A14B-LOW_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Self-Forcing Plus repo",
      "url": "https://github.com/GoatWu/Self-Forcing-Plus",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "h1111 gradio app",
      "url": "https://github.com/maybleMyers/h1111",
      "type": "tool",
      "from": "Alisson Pereira"
    },
    {
      "resource": "A1111 WebUI",
      "url": "https://github.com/AUTOMATIC1111/stable-diffusion-webui",
      "type": "tool",
      "from": "Rainsmellsnice"
    },
    {
      "resource": "Modal Labs examples repository",
      "url": "https://github.com/modal-labs/modal-examples",
      "type": "repo",
      "from": "Karo"
    },
    {
      "resource": "Runpod ComfyUI worker template",
      "url": "https://github.com/runpod-workers/worker-comfyui",
      "type": "repo",
      "from": "gokuvonlange"
    },
    {
      "resource": "Wan 2.2 config showing shift 12 for T2V",
      "url": "https://github.com/Wan-Video/Wan2.2/blob/main/wan/configs/wan_t2v_A14B.py#L34",
      "type": "repo",
      "from": "aikitoria"
    },
    {
      "resource": "Video about noise scheduling",
      "url": "https://www.youtube.com/watch?v=egn5dKPdlCk",
      "type": "educational",
      "from": "Ablejones"
    },
    {
      "resource": "First-Last-Frame ComfyUI node",
      "url": "https://github.com/stduhpf/ComfyUI--Wan22FirstLastFrameToVideoLatent",
      "type": "repo",
      "from": "hicho"
    },
    {
      "resource": "Aether Crash Telephoto LoRA",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1mf1wqp/lora_release_aether_crash_telephoto_crashzoom/",
      "type": "lora",
      "from": "NebSH"
    },
    {
      "resource": "T2I Civitai workflow",
      "url": "https://civitai.com/models/1834338?modelVersionId=2079614",
      "type": "workflow",
      "from": "Juampab12"
    },
    {
      "resource": "Fixed Wan 2.2 Lightning LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Lightning",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Triton for Windows",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "SageAttention for Windows",
      "url": "https://github.com/woct0rdho/SageAttention",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.2 Lightning Official Repo",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "720P Distilled I2V Model",
      "url": "https://huggingface.co/lightx2v/Wan2.1-I2V-14B-720P-StepDistill-CfgDistill-Lightx2v/commits/main",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Kijai's Wan 2.2 Lightning LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Lightning",
      "type": "model",
      "from": "Juampab12"
    },
    {
      "resource": "Lightx2v I2V 720P (fake)",
      "url": "https://huggingface.co/lightx2v/Wan2.1-I2V-14B-720P-StepDistill-CfgDistill-Lightx2v",
      "type": "model",
      "from": "Doctor Shotgun"
    },
    {
      "resource": "Wan 2.2 Lightning LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Lightning",
      "type": "model",
      "from": "Ant"
    },
    {
      "resource": "FastWan2.2-TI2V-5B-FullAttn-Diffusers",
      "url": "https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "LightX2V Lightning repo",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning",
      "type": "repo",
      "from": "Luis Clement"
    },
    {
      "resource": "Wan2.2 Lightning GitHub",
      "url": "https://github.com/ModelTC/Wan2.2-Lightning/tree/fxy/distill",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.2 Lightning LoRAs",
      "url": "https://github.com/ModelTC/Wan2.2-Lightning",
      "type": "repo",
      "from": "hicho"
    },
    {
      "resource": "Lightning LoRA models",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-rank64-V1",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "FastWan 5B converted model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/FastWan/Wan2_2-TI2V-5B-FastWanFullAttn_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Claude custom project for prompting",
      "url": "",
      "type": "tool",
      "from": "Juampab12"
    },
    {
      "resource": "Wan prompting guide",
      "url": "https://prompter.on.websim.com",
      "type": "tool",
      "from": "gokuvonlange"
    },
    {
      "resource": "Alibaba image guide",
      "url": "https://alidocs.dingtalk.com/i/nodes/amweZ92PV6DbOdgzUZZe0YxN8xEKBD6p",
      "type": "documentation",
      "from": "hicho"
    },
    {
      "resource": "ClownsharkSampler workflow",
      "url": "shared as PNG",
      "type": "workflow",
      "from": "Ablejones"
    },
    {
      "resource": "Kijai's Lightning loras",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Lightning",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "MMAudio space",
      "url": "https://huggingface.co/spaces/hkchengrex/MMAudio",
      "type": "tool",
      "from": ": Not Really Human :"
    },
    {
      "resource": "FastVideo FastWan models",
      "url": "https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers",
      "type": "model",
      "from": "s2k"
    },
    {
      "resource": "PSNbst Accelerator lora",
      "url": "https://huggingface.co/PSNbst/Wan22-Accelerator-PAseer",
      "type": "lora",
      "from": "piscesbody"
    },
    {
      "resource": "Qwen Image VAE",
      "url": "https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/vae/qwen_image_vae.safetensors",
      "type": "model",
      "from": "Draken"
    },
    {
      "resource": "Wan 2.2 GGUF Upscaler 14B workflow",
      "url": "https://huggingface.co/thaakeno46/ComfyUI-Workflows/blob/main/WAN2.2_GGUF_UPSCALER_14B.json",
      "type": "workflow",
      "from": "thaakeno"
    },
    {
      "resource": "Wan 2.2 GGUF V2V workflow",
      "url": "https://huggingface.co/thaakeno46/ComfyUI-Workflows/blob/main/Wan2.2_GGUF_V2V.json",
      "type": "workflow",
      "from": "thaakeno"
    },
    {
      "resource": "Wan 2.2 GGUF FLF workflow",
      "url": "https://huggingface.co/thaakeno46/ComfyUI-Workflows/blob/main/WAN2.2_GGUF_FFLF.json",
      "type": "workflow",
      "from": "thaakeno"
    },
    {
      "resource": "Wan22-Accelerator-PAseer",
      "url": "https://huggingface.co/PSNbst/Wan22-Accelerator-PAseer/tree/main",
      "type": "model",
      "from": "N0NSens"
    },
    {
      "resource": "Qwen3 prompt enhancement node",
      "url": "",
      "type": "node",
      "from": "shockgun"
    },
    {
      "resource": "ComfyUI-AutomaticCFG",
      "url": "https://github.com/Extraltodeus/ComfyUI-AutomaticCFG",
      "type": "tool",
      "from": "PizzaSlice"
    },
    {
      "resource": "WanVideo_comfy_fp8_scaled",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Video prompter tool",
      "url": "https://prompter.on.websim.com/",
      "type": "tool",
      "from": "thaakeno"
    },
    {
      "resource": "TheDenk 5B ControlNets",
      "url": "https://huggingface.co/TheDenk",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Pixelle-MCP ComfyUI server",
      "url": "https://github.com/AIDC-AI/Pixelle-MCP",
      "type": "tool",
      "from": "AJO"
    },
    {
      "resource": "WAN 2.2 high noise 14B fp8_scaled model",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/tree/main/split_files/diffusion_models",
      "type": "model",
      "from": "hiroP"
    },
    {
      "resource": "Kijai's fp8_scaled model collection",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI file formatting documentation",
      "url": "https://comfyuidoc.com/Interface/SaveFileFormatting.html",
      "type": "documentation",
      "from": "Kijai"
    },
    {
      "resource": "Caching node pack for text encoders",
      "url": "https://github.com/alastor-666-1933/caching_to_not_waste",
      "type": "tool",
      "from": "patientx"
    },
    {
      "resource": "VRGameDevGirl84 workflows",
      "url": "",
      "type": "workflow",
      "from": "Gill Bastar"
    },
    {
      "resource": "Batch video generation node",
      "url": "",
      "type": "tool",
      "from": "Fill"
    },
    {
      "resource": "Wan 2.2 Lightning LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Lightning",
      "type": "lora",
      "from": "avataraim"
    },
    {
      "resource": "LightX2V LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Lightx2v",
      "type": "lora",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "resource": "WAN 2.2 14B Arcane Jinx LoRA",
      "url": "https://huggingface.co/Cseti/wan2.2-14B-Arcane_Jinx-lora-v1",
      "type": "model",
      "from": "Cseti"
    },
    {
      "resource": "SageAttention repository",
      "url": "https://github.com/thu-ml/SageAttention",
      "type": "repo",
      "from": "Josiah"
    },
    {
      "resource": "Vibecoding examples collection",
      "url": "https://ballad.microapp.me",
      "type": "tool",
      "from": "fredbliss"
    },
    {
      "resource": "Wan2.2_T2V_A14B_VACE-test checkpoint",
      "url": "https://huggingface.co/lym00/Wan2.2_T2V_A14B_VACE-test",
      "type": "model",
      "from": "Piblarg"
    },
    {
      "resource": "Model merging tutorial",
      "url": "https://discord.com/channels/1076117621407223829/1386453178240733235/1402513536068096063",
      "type": "tutorial",
      "from": "Ablejones"
    },
    {
      "resource": "YouTube video by NerdyRodent",
      "url": "https://www.youtube.com/watch?v=MkAuWTLJp1s",
      "type": "tutorial",
      "from": "Draken"
    },
    {
      "resource": "spacepxl Wan 2.1 control LoRAs",
      "url": "https://huggingface.co/spacepxl/Wan2.1-control-loras",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "N0NSens T2I workflow",
      "url": "https://discord.com/channels/1076117621407223829/1401393279790088213",
      "type": "workflow",
      "from": "N0NSens"
    },
    {
      "resource": "FakeVace2.2 experimental model",
      "url": "https://huggingface.co/CCP6/FakeVace2.2/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Reddit comparison post",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1mhosa2/update_qwenimage_vs_flux_1d_vs_krea_1d_vs_wan_22/",
      "type": "comparison",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "resource": "Spline-Path-Control",
      "url": "https://github.com/WhatDreamsCost/Spline-Path-Control",
      "type": "repo",
      "from": "Jonathan"
    },
    {
      "resource": "ComfyUI-GIMM-VFI",
      "url": "https://github.com/kijai/ComfyUI-GIMM-VFI",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI-Rife-Tensorrt",
      "url": "https://github.com/yuvraj108c/ComfyUI-Rife-Tensorrt",
      "type": "repo",
      "from": "aipmaster"
    },
    {
      "resource": "Wan2.2_T2V_A14B_VACE-test merge",
      "url": "https://huggingface.co/lym00/Wan2.2_T2V_A14B_VACE-test",
      "type": "model",
      "from": "JalenBrunson"
    },
    {
      "resource": "ComfyUI-RadialAttn workflow",
      "url": "https://github.com/woct0rdho/ComfyUI-RadialAttn",
      "type": "workflow",
      "from": ": Not Really Human :."
    },
    {
      "resource": "WAN 2.1 Makoto Shinkai style with res_2s_bong_tangent sampler",
      "url": "https://civitai.com/models/1766551/wan21-your-name-makoto-shinkai-style",
      "type": "model",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "resource": "FakeVace2.2 for Wan 2.2",
      "url": "https://huggingface.co/CCP6/FakeVace2.2/tree/main",
      "type": "model",
      "from": "Lodis"
    },
    {
      "resource": "Waver project page",
      "url": "http://www.waver.video/",
      "type": "project",
      "from": "yi"
    },
    {
      "resource": "Waver GitHub repository",
      "url": "https://github.com/FoundationVision/Waver?tab=readme-ov-file",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "Dream LoRA for morphing",
      "url": "https://civitai.com/models/1811141?modelVersionId=2049587",
      "type": "lora",
      "from": "piscesbody"
    },
    {
      "resource": "Video arena leaderboard",
      "url": "https://artificialanalysis.ai/text-to-video/arena?tab=leaderboard",
      "type": "benchmark",
      "from": "yi"
    },
    {
      "resource": "Wan 2.2 Lightning discussion",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/discussions/14",
      "type": "discussion",
      "from": "Screeb"
    },
    {
      "resource": "New Lightning LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Lightning",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "LoRA rank inspection",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Lightx2v",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "WanVideo I2I LoRA for similarity",
      "url": "https://civitai.com/models/1421989/wanvideo-i2i-480p-expression-changer-perspective-changer",
      "type": "model",
      "from": "Juampab12"
    },
    {
      "resource": "Lightning team discussion",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/discussions/13",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "Lightning 2.2 LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Lightning",
      "type": "model",
      "from": "Josiah"
    },
    {
      "resource": "ATI Motion Transfer",
      "url": "https://github.com/bytedance/ATI?tab=readme-ov-file#motion-transfer",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "MagCache",
      "url": "https://github.com/Zehong-Ma/MagCache",
      "type": "tool",
      "from": "NebSH"
    },
    {
      "resource": "fp8 scaled models comparison",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.2 Fun Control",
      "url": "https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-Control",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Kijai Lightning LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Lightning",
      "type": "lora",
      "from": ": Not Really Human :"
    },
    {
      "resource": "LightX2V adaptive rank LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Lightx2v/lightx2v_14B_T2V_cfg_step_distill_lora_adaptive_rank_quantile_0.15_bf16.safetensors",
      "type": "lora",
      "from": "MilesCorban"
    },
    {
      "resource": "Lightning discussion thread",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/discussions/13",
      "type": "discussion",
      "from": "CaptHook"
    },
    {
      "resource": "Wan2.2-Fun-A14B-Control",
      "url": "https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-Control",
      "type": "model",
      "from": "CJ"
    },
    {
      "resource": "Wan2.2-Fun-A14B-InP",
      "url": "https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-InP",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Kijai's fp8 scaled Fun models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/Fun",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "V2V workflows",
      "url": "https://huggingface.co/thaakeno46/ComfyUI-Workflows/tree/main",
      "type": "workflow",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "resource": "Native Wan 2.2 14B FLF2V workflow",
      "url": "https://docs.comfy.org/tutorials/video/wan/wan2_2#wan2-2-14b-flf2v-workflow-example",
      "type": "workflow",
      "from": "JohnDopamine"
    },
    {
      "resource": "SkyReels A3 demo",
      "url": "https://github.com/SkyworkAI/skyreels-a3.github.io",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "SkyReels paper",
      "url": "https://arxiv.org/abs/2506.00830",
      "type": "paper",
      "from": "yi"
    },
    {
      "resource": "Wan Chattable Knowledge Base",
      "url": "https://notebooklm.google.com/notebook/a08901b9-0511-4926-bbf8-3c86a12dc306",
      "type": "knowledge base",
      "from": "Nathan Shipley"
    },
    {
      "resource": "Sapiens pose detector CLI",
      "url": "not provided",
      "type": "tool",
      "from": "fredbliss"
    },
    {
      "resource": "Automatic CFG paper",
      "url": "https://arxiv.org/abs/2508.03442",
      "type": "research",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.1 Knowledge Base",
      "url": "https://nathanshipley.notion.site/Wan-2-1-Knowledge-Base-1d691e115364814fa9d4e27694e9468f",
      "type": "knowledge base",
      "from": "DawnII"
    },
    {
      "resource": "Sapiens ComfyUI node pack",
      "url": "https://github.com/smthemex/ComfyUI_Sapiens",
      "type": "node pack",
      "from": "NebSH"
    },
    {
      "resource": "Wan2.2-Lightning LoRA v1.1",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Wan2.2-Fun-A14B-Control",
      "url": "https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-Control",
      "type": "model",
      "from": "Dannhauer80"
    },
    {
      "resource": "8-step model discussion",
      "url": "https://github.com/ModelTC/Wan2.2-Lightning/discussions/4",
      "type": "discussion",
      "from": "DawnII"
    },
    {
      "resource": "DiffSynth Studio FlowMatchScheduler",
      "url": "https://github.com/modelscope/DiffSynth-Studio/blob/main/diffsynth/schedulers/flow_match.py",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "Scheduler explanation video",
      "url": "https://m.youtube.com/watch?v=egn5dKPdlCk",
      "type": "educational",
      "from": "artemonary"
    },
    {
      "resource": "thaakeno ComfyUI Workflows",
      "url": "https://huggingface.co/thaakeno46/ComfyUI-Workflows/blob/main/WAN2.2_GGUF_I2V_LIGHTNING.json",
      "type": "workflow",
      "from": "thaakeno"
    },
    {
      "resource": "Kijai Github Sponsor",
      "url": "https://github.com/sponsors/kijai#sponsors",
      "type": "support",
      "from": "\ud83e\udd99rishappi"
    },
    {
      "resource": "Joy Caption Beta",
      "url": "https://huggingface.co/spaces/fancyfeast/joy-caption-beta-one",
      "type": "tool",
      "from": "screwfunk"
    },
    {
      "resource": "ComfyUI JoyCaption Node",
      "url": "https://github.com/1038lab/ComfyUI-JoyCaption/tree/main",
      "type": "node",
      "from": "screwfunk"
    },
    {
      "resource": "Wan Knowledge Base",
      "url": "https://nathanshipley.notion.site/Wan-2-1-Knowledge-Base-1d691e115364814fa9d4e27694e9468f",
      "type": "documentation",
      "from": "mdkb"
    },
    {
      "resource": "FastWan2.2-TI2V-5B model",
      "url": "https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers",
      "type": "model",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "resource": "Sapiens CLI implementation",
      "url": "https://github.com/fblissjr/sapiens_cli",
      "type": "tool",
      "from": "fredbliss"
    },
    {
      "resource": "Google Vids",
      "url": "https://workspace.google.com/products/vids/",
      "type": "tool",
      "from": "fredbliss"
    },
    {
      "resource": "ComfyUI pose interpolation",
      "url": "https://github.com/toyxyz/ComfyUI_pose_inter",
      "type": "node",
      "from": "DawnII"
    },
    {
      "resource": "BFloat16 Sapiens model",
      "url": "https://huggingface.co/melmass/sapiens/blob/main/sapiens_1b_goliath_best_goliath_AP_639_bfloat16.pt2",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Sapiens 2B COCO model",
      "url": "https://huggingface.co/noahcao/sapiens-pose-coco/tree/main/sapiens_lite_host/torchscript/pose/checkpoints/sapiens_2b",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Co-tracker",
      "url": "https://github.com/facebookresearch/co-tracker",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "DOT tracker",
      "url": "https://github.com/16lemoing/dot",
      "type": "repo",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Sapiens CLI",
      "url": "https://github.com/fblissjr/sapiens_cli",
      "type": "tool",
      "from": "fredbliss"
    },
    {
      "resource": "Qwen Image Lightning",
      "url": "https://huggingface.co/lightx2v/Qwen-Image-Lightning/tree/main",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "WAN resources ChatGPT bot",
      "url": "https://discord.com/channels/1076117621407223829/1400081347678306304/1400081347678306304",
      "type": "tool",
      "from": "Dorksense"
    },
    {
      "resource": "PUSA workflow example",
      "url": "https://discord.com/channels/1076117621407223829/1386453178240733235/1398123737257083032",
      "type": "workflow",
      "from": "Hashu"
    },
    {
      "resource": "TLBVFI interpolation method",
      "url": "https://github.com/ZonglinL/TLBVFI?tab=readme-ov-file",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Tiny VAE model for previews",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/taew2_1.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SeedVR2 setup video",
      "url": "https://youtu.be/I0sl45GMqNg?si=fc0Xci7XNeG-nSQb",
      "type": "tutorial",
      "from": "nacho.money"
    },
    {
      "resource": "DLoRAL for frame interpolation",
      "url": "https://github.com/yjsunnn/DLoRAL?tab=readme-ov-file",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI LayerForge for cropping",
      "url": "https://github.com/Azornes/Comfyui-LayerForge",
      "type": "tool",
      "from": "gordo"
    },
    {
      "resource": "FFmpeg frame rate changing guide",
      "url": "https://trac.ffmpeg.org/wiki/ChangingFrameRate",
      "type": "documentation",
      "from": "Kijai"
    },
    {
      "resource": "Triton Windows installation",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "SageAttention",
      "url": "https://github.com/woct0rdho/SageAttention",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Sapiens CLI with multi-person tracking",
      "url": "https://github.com/fblissjr/sapiens_cli/blob/main/cli/README.md",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "DeepVerse world model",
      "url": "https://github.com/SOTAMak1r/DeepVerse",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "Wan lighting workflow issue",
      "url": "https://github.com/ModelTC/Wan2.2-Lightning/issues/3",
      "type": "repo",
      "from": "flo1331"
    },
    {
      "resource": "StreamingLLM paper",
      "url": "https://github.com/mit-han-lab/streaming-llm",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "ComfyUI-TLBVFI",
      "url": "https://github.com/BobRandomNumber/ComfyUI-TLBVFI",
      "type": "repo",
      "from": "phazei"
    },
    {
      "resource": "SeedVR2-7B model",
      "url": "https://huggingface.co/ByteDance-Seed/SeedVR2-7B/tree/main",
      "type": "model",
      "from": "Karo"
    },
    {
      "resource": "FastWan 5B model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/FastWan",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Skyreels LoRA for 121 frame fix",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Skyreels/Wan2_1_Skyreels-v2-I2V-720P_LoRA_rank_64_fp16.safetensors",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "5B controlnet implementation",
      "url": "https://github.com/TheDenk/wan2.2-controlnet",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.2 GGUF V2V workflow",
      "url": "https://huggingface.co/thaakeno46/ComfyUI-Workflows/blob/main/Wan2.2_GGUF_V2V.json",
      "type": "workflow",
      "from": "thaakeno"
    },
    {
      "resource": "mdkb's workflows",
      "url": "referenced on mdkb's website footprints and current projects pages",
      "type": "workflow",
      "from": "mdkb"
    },
    {
      "resource": "Community knowledge notebook",
      "url": "https://notebooklm.google.com/notebook/a08901b9-0511-4926-bbf8-3c86a12dc306",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "WAN 2.2 system prompt",
      "url": "In qwen subfolder in the wrapper",
      "type": "code",
      "from": "Kijai"
    },
    {
      "resource": "Wan2.2-Fun-A14B-InP discussions",
      "url": "https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-InP/discussions/1#6899d52a63714925142812f3",
      "type": "discussion",
      "from": "yi"
    },
    {
      "resource": "ComfyUI-WanMoeKSampler",
      "url": "https://github.com/stduhpf/ComfyUI-WanMoeKSampler/",
      "type": "repo",
      "from": "Tomber"
    },
    {
      "resource": "Rapid AllInOne 2.2",
      "url": "https://huggingface.co/Phr00t/WAN2.2-14B-Rapid-AllInOne",
      "type": "model",
      "from": "Drommer-Kille"
    },
    {
      "resource": "Skyreels LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Skyreels/Wan2_1_Skyreels-v2-I2V-720P_LoRA_rank_adaptive_quantile_0.20_fp16.safetensors",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "Wan2_1_Skyreels-v2-T2V-720P_LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Skyreels/Wan2_1_Skyreels-v2-T2V-720P_LoRA_rank_64_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Stand-In identity control",
      "url": "https://huggingface.co/BowenXue/Stand-In",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Matrix Game 2.0",
      "url": "https://huggingface.co/Skywork/Matrix-Game-2.0",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Fantasy Portrait",
      "url": "https://github.com/Fantasy-AMAP/fantasy-portrait",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Radial Attention workflow",
      "url": "https://github.com/woct0rdho/ComfyUI-RadialAttn/blob/main/example_workflows/radial_attn.json",
      "type": "workflow",
      "from": ": Not Really Human :"
    },
    {
      "resource": "Stand-In LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Stand-In",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "StableAvatar",
      "url": "https://huggingface.co/FrancisRing/StableAvatar",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Fantasy Portrait weights",
      "url": "https://huggingface.co/acvlab/FantasyPortrait/tree/main",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "SeedVR2 ComfyUI nodes",
      "url": "https://github.com/numz/ComfyUI-SeedVR2_VideoUpscaler",
      "type": "repo",
      "from": "\ud83e\udd99rishappi"
    },
    {
      "resource": "SeedVR2 GGUF models",
      "url": "https://huggingface.co/cmeka/SeedVR2-GGUF/tree/main",
      "type": "model",
      "from": "AmirKerr"
    },
    {
      "resource": "Wan2.2 T2V GGUF models",
      "url": "https://huggingface.co/QuantStack/Wan2.2-T2V-A14B-GGUF/tree/main",
      "type": "model",
      "from": "Josiah"
    },
    {
      "resource": "NotebookLM model tracker",
      "url": "https://notebooklm.google.com/notebook/a08901b9-0511-4926-bbf8-3c86a12dc306",
      "type": "tool",
      "from": "DawnII"
    },
    {
      "resource": "Vibe-coded kernel blog",
      "url": "https://benanderson.work/blog/vibe-coded-kernel/",
      "type": "resource",
      "from": "642326806678077441"
    },
    {
      "resource": "Wan 2.2 5B model",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/blob/main/split_files/diffusion_models/wan2.2_ti2v_5B_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.2 VAE",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/blob/main/split_files/vae/wan2.2_vae.safetensors",
      "type": "model",
      "from": "humangirltotally"
    },
    {
      "resource": "FantasyPortrait for face driving",
      "url": "https://huggingface.co/acvlab/FantasyPortrait/tree/main",
      "type": "model",
      "from": "DeeX"
    },
    {
      "resource": "Higgs Audio Voice Clone WebUI",
      "url": "https://github.com/Saganaki22/higgs-audio-WebUI",
      "type": "tool",
      "from": "drbaph"
    },
    {
      "resource": "All-in-one Wan 2.2 workflow for AMD GPUs",
      "url": "comfyui-zluda fork github",
      "type": "workflow",
      "from": "patientx"
    },
    {
      "resource": "Skyreels v2 LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Skyreels/Wan2_1_Skyreels-v2-T2V-720P_LoRA_rank_64_fp16.safetensors",
      "type": "lora",
      "from": "NebSH"
    },
    {
      "resource": "Wan 2.1 Knowledge Base",
      "url": "https://nathanshipley.notion.site/Wan-2-1-Knowledge-Base-1d691e115364814fa9d4e27694e9468f",
      "type": "knowledge base",
      "from": "DeepSeaCatz"
    },
    {
      "resource": "ComfyUI Wan 2.2 First Last Frame node",
      "url": "https://github.com/stduhpf/ComfyUI--Wan22FirstLastFrameToVideoLatent",
      "type": "custom node",
      "from": "QANICS\ud83d\udd50"
    },
    {
      "resource": "Stand-In reference example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_Stand-In_reference_example_01.json",
      "type": "workflow",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "NotebookLM Discord Knowledge Base",
      "url": "https://notebooklm.google.com/notebook/a08901b9-0511-4926-bbf8-3c86a12dc306?pli=1",
      "type": "knowledge base",
      "from": "JohnDopamine"
    },
    {
      "resource": "LightX2V LoRAs for Wan 2.1",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Lightx2v",
      "type": "model",
      "from": "Danial"
    },
    {
      "resource": "Fun 2.2 GGUF quantizations",
      "url": "https://huggingface.co/QuantStack/Wan2.2-Fun-A14B-InP-GGUF",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "EchoMimic ComfyUI node",
      "url": "https://github.com/smthemex/ComfyUI_EchoMimic",
      "type": "repo",
      "from": "hicho"
    },
    {
      "resource": "ComfyUI GGUF quantization tools",
      "url": "https://github.com/city96/ComfyUI-GGUF/tree/main/tools",
      "type": "tool",
      "from": "Ablejones"
    },
    {
      "resource": "Fun 2.2 Camera Control models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/blob/main/Fun/",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Stand-In Preprocessor",
      "url": "https://github.com/WeChatCV/Stand-In_Preprocessor_ComfyUI",
      "type": "repo",
      "from": "orabazes"
    },
    {
      "resource": "NotebookLM with latest Wan info",
      "url": "https://notebooklm.google.com/notebook/a08901b9-0511-4926-bbf8-3c86a12dc306",
      "type": "resource",
      "from": "Nathan Shipley"
    },
    {
      "resource": "MultiTalk workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_multitalk_test_02.json",
      "type": "workflow",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "FantasyPortrait branch and models",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/fantasy_portrait and https://huggingface.co/Kijai/WanVideo_comfy/tree/main/FantasyPortrait",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI blog post on Wan2.2 Fun InP support",
      "url": "https://blog.comfy.org/p/comfyui-wan22-fun-inp-support",
      "type": "documentation",
      "from": "xwsswww"
    },
    {
      "resource": "mxToolkit with Slider2D node",
      "url": "https://github.com/Smirnov75/ComfyUI-mxToolkit",
      "type": "tool",
      "from": "phazei"
    },
    {
      "resource": "LayerStyle node pack for masking",
      "url": "https://github.com/chflame163/ComfyUI_LayerStyle",
      "type": "tool",
      "from": "corza"
    },
    {
      "resource": "Stand-In preprocessor ComfyUI",
      "url": "https://github.com/WeChatCV/Stand-In_Preprocessor_ComfyUI",
      "type": "tool",
      "from": "mdkb"
    },
    {
      "resource": "Goldenboy retro anime style LoRA for Wan 14B 2.2",
      "url": "https://civitai.com/models/1671285?modelVersionId=2112013",
      "type": "lora",
      "from": "crinklypaper"
    },
    {
      "resource": "Kijai's fp8 scaled models repository",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled",
      "type": "models",
      "from": "Kijai"
    },
    {
      "resource": "Kijai's main WanVideo repository",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy",
      "type": "models",
      "from": "screwfunk"
    },
    {
      "resource": "Comfy-Org repackaged models",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models",
      "type": "models",
      "from": "Kijai"
    },
    {
      "resource": "MAGREF model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-Wan-I2V-MAGREF-14B_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Stand-in reference example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_Stand-In_reference_example_01.json",
      "type": "workflow",
      "from": "pewpewpew"
    },
    {
      "resource": "Wan2.2-Fun-A14B-Control-Camera",
      "url": "https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-Control-Camera",
      "type": "model",
      "from": "Lodis"
    },
    {
      "resource": "Stand-In LoRA",
      "url": "https://huggingface.co/BowenXue/Stand-In",
      "type": "model",
      "from": "ArtOfficial"
    },
    {
      "resource": "GGUF Camera Control",
      "url": "https://huggingface.co/QuantStack/Wan2.2-Fun-A14B-Control-Camera-GGUF/tree/main",
      "type": "model",
      "from": "Daflon"
    },
    {
      "resource": "VSA paper",
      "url": "https://arxiv.org/pdf/2525.13389",
      "type": "research",
      "from": "Kijai"
    },
    {
      "resource": "Cat singing workflow",
      "url": "https://discord.com/channels/1076117621407223829/1403263501421776977/1405598984600551575",
      "type": "workflow",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Photopea",
      "url": "",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "New denoise algorithm paper",
      "url": "https://arxiv.org/pdf/2504.17033",
      "type": "research",
      "from": "shockgun"
    },
    {
      "resource": "PD-FGC inference repository",
      "url": "https://github.com/Dorniwang/PD-FGC-inference",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Fantasy Portrait repository",
      "url": "https://github.com/Fantasy-AMAP/fantasy-portrait",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "ONNX Runtime nightly install",
      "url": "https://onnxruntime.ai/docs/install/#install-nightly-2",
      "type": "installation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Google MediaPipe face landmarks",
      "url": "https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker",
      "type": "tool",
      "from": "mdkb"
    },
    {
      "resource": "umt5-xxl-enc-bf16.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/umt5-xxl-enc-bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Qwen3-4B official models",
      "url": "https://huggingface.co/Qwen/Qwen3-4B",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "seed-vc for voice swapping",
      "url": "https://github.com/Plachtaa/seed-vc",
      "type": "tool",
      "from": "LarpsAI"
    },
    {
      "resource": "janky_memory_patcher",
      "url": "https://github.com/drozbay/janky_memory_patcher",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "Wan22_A14B_T2V_lora_extract_r64.safetensors",
      "url": "https://huggingface.co/drozbay/Wan2.2_A14B_lora_extract/blob/main/Wan22_A14B_T2V_lora_extract_r64.safetensors",
      "type": "lora",
      "from": "gokuvonlange"
    },
    {
      "resource": "ToonComposer",
      "url": "https://github.com/TencentARC/ToonComposer",
      "type": "repo",
      "from": "GOD_IS_A_LIE"
    },
    {
      "resource": "NAG paper",
      "url": "https://chendaryen.github.io/NAG.github.io/",
      "type": "research",
      "from": "DawnII"
    },
    {
      "resource": "Janky Memory Patcher",
      "url": "https://github.com/drozbay/janky_memory_patcher",
      "type": "tool",
      "from": "Ablejones"
    },
    {
      "resource": "Flash-attention prebuild wheels",
      "url": "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/tag/v0.4.10",
      "type": "tool",
      "from": "phazei"
    },
    {
      "resource": "Intel shared GPU memory override",
      "url": "https://videocardz.com/newz/intel-adds-shared-gpu-memory-override-feature-for-core-ultra-systems-enables-larger-vram-for-ai",
      "type": "news",
      "from": "hicho"
    },
    {
      "resource": "StableAvatar repository",
      "url": "https://github.com/Francis-Rings/StableAvatar",
      "type": "repo",
      "from": "NebSH"
    },
    {
      "resource": "ComfyUI StableAvatar implementation",
      "url": "https://github.com/smthemex/ComfyUI_StableAvatar",
      "type": "repo",
      "from": "NebSH"
    },
    {
      "resource": "WanFM repository",
      "url": "https://github.com/ff2416/WanFM",
      "type": "repo",
      "from": "dir2050"
    },
    {
      "resource": "MAGREF Wan2.1 I2V 14B GGUF with workflow",
      "url": "https://huggingface.co/QuantStack/MAGREF_Wan2.1_I2V_14B-GGUF/tree/main",
      "type": "model",
      "from": "mdkb"
    },
    {
      "resource": "Execution inversion demo for lazy switches",
      "url": "https://github.com/BadCafeCode/execution-inversion-demo-comfyui",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "SeedVR2 upscaling workflow discussion",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1mqnlnf/adding_textures_and_finegrained_details_with/",
      "type": "workflow",
      "from": "Shawneau \ud83c\udf41 [CA]"
    },
    {
      "resource": "Wan2_1-T2V-14B-Phantom_fp8_e5m2_scaled_KJ.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/blob/main/T2V/Wan2_1-T2V-14B-Phantom_fp8_e5m2_scaled_KJ.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Wan22-FastMix",
      "url": "https://huggingface.co/Zuntan/Wan22-FastMix/tree/main",
      "type": "model",
      "from": "l\u0488u\u0488c\u0488i\u0488f\u0488e\u0488r\u0488"
    },
    {
      "resource": "ComfyUI-QwenImageWanBridge",
      "url": "https://github.com/fblissjr/ComfyUI-QwenImageWanBridge",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "WanFM",
      "url": "https://github.com/ff2416/WanFM?tab=readme-ov-file",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "AutoGGUF",
      "url": "https://github.com/leafspark/AutoGGUF",
      "type": "tool",
      "from": "Tony(5090)"
    },
    {
      "resource": "ComfyUI-ModelQuantizer",
      "url": "https://github.com/lum3on/ComfyUI-ModelQuantizer",
      "type": "node",
      "from": "orabazes"
    },
    {
      "resource": "SDMatte",
      "url": "https://github.com/vivoCameraResearch/SDMatte",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "First frame object lora",
      "url": "https://civitai.com/models/1863941?modelVersionId=2109632",
      "type": "lora",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "resource": "Phantom VACE fp16 model",
      "url": "https://huggingface.co/Inner-Reflections/Wan2.1_VACE_Phantom/blob/main/wan-14B_vace_phantom_v2_fp16.safetensors",
      "type": "model",
      "from": "Piblarg"
    },
    {
      "resource": "Fixed Fun-Control models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/Fun",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Matrix Game 2.0 interactive model",
      "url": "https://huggingface.co/Skywork/Matrix-Game-2.0/tree/main",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Wan LoRA trainer node",
      "url": "https://github.com/jaimitoes/ComfyUI_Wan2_1_lora_trainer",
      "type": "tool",
      "from": "shockgun"
    },
    {
      "resource": "QwenVL ComfyUI nodes",
      "url": "https://github.com/alexcong/ComfyUI_QwenVL",
      "type": "node",
      "from": "MilesCorban"
    },
    {
      "resource": "AutoGGUF tool",
      "url": "https://github.com/leafspark/AutoGGUF/releases/tag/v2.0.1",
      "type": "tool",
      "from": "Tony(5090)"
    },
    {
      "resource": "StableAvatar",
      "url": "https://github.com/Francis-Rings/StableAvatar",
      "type": "repo",
      "from": "army"
    },
    {
      "resource": "14B VACE Phantom v2 GGUF models",
      "url": "https://huggingface.co/orabazes/wan-14B_vace_phantom_v2_GGUF",
      "type": "model",
      "from": "orabazes"
    },
    {
      "resource": "Context Options documentation",
      "url": "https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved/tree/main/documentation/nodes#context-optionsstandard-static",
      "type": "documentation",
      "from": "Kijai"
    },
    {
      "resource": "Qwen-Image to WAN Bridge",
      "url": "https://github.com/fblissjr/ComfyUI-QwenImageWanBridge",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "Git rebase vs merge visualization",
      "url": "https://miro.medium.com/v2/1*cEXnJtDL2tGeoN3KoCJSIw.png",
      "type": "documentation",
      "from": "scf"
    },
    {
      "resource": "Stand-In Preprocessor fix",
      "url": "https://github.com/WeChatCV/Stand-In_Preprocessor_ComfyUI/issues/3#issuecomment-3193744986",
      "type": "repo",
      "from": "mdkb"
    },
    {
      "resource": "ComfyUI-TopazVideoAI node",
      "url": "https://github.com/sh570655308/ComfyUI-TopazVideoAI",
      "type": "node",
      "from": ". Not Really Human ."
    },
    {
      "resource": "ComfyUI-MultiGPU for VRAM management",
      "url": "https://github.com/pollockjj/ComfyUI-MultiGPU",
      "type": "node",
      "from": "Jonathan"
    },
    {
      "resource": "FP32 text encoder",
      "url": "https://huggingface.co/Nap/umt5-xxl-encoder-only-fp32-safetensors/tree/main",
      "type": "model",
      "from": "Benjimon"
    },
    {
      "resource": "Topaz Astra app",
      "url": "https://astra.app",
      "type": "tool",
      "from": ". Not Really Human ."
    },
    {
      "resource": "ComfyUI PR #9392",
      "url": "https://github.com/comfyanonymous/ComfyUI/pull/9392",
      "type": "repo",
      "from": "Kosinkadink"
    },
    {
      "resource": "Wan 2.2 official repository",
      "url": "https://github.com/Wan-Video/Wan2.2",
      "type": "repo",
      "from": "Benjimon"
    },
    {
      "resource": "Modified H1111 implementation for small GPU",
      "url": "https://github.com/maybleMyers/H1111/tree/wan2.2b",
      "type": "repo",
      "from": "Benjimon"
    },
    {
      "resource": "Mixed weights model files",
      "url": "https://huggingface.co/maybleMyers/wan_files_for_h1111/tree/main",
      "type": "model",
      "from": "Benjimon"
    },
    {
      "resource": "MAGREF fp8 scaled models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/blob/main/I2V/Wan2_1-I2V-14B-MAGREF_fp8_e4m3fn_scaled_KJ.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "ShotVL multimodal LLMs for cinematic understanding",
      "url": "https://huggingface.co/Vchitect/ShotVL-3B",
      "type": "model",
      "from": "s2k"
    },
    {
      "resource": "InfiniteTalk duplicated repository",
      "url": "https://github.com/bmwas/InfiniteTalk",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "RealisMotion (works with Wan)",
      "url": "https://github.com/JingyunLiang/RealisMotion",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Wan prompt guide",
      "url": "https://www.instasd.com/post/wan2-2-whats-new-and-how-to-write-killer-prompts",
      "type": "guide",
      "from": "Josiah"
    },
    {
      "resource": "Prompt generator examples",
      "url": "https://dengeai.com/prompt-generator",
      "type": "tool",
      "from": "Josiah"
    },
    {
      "resource": "InfiniteTalk examples",
      "url": "https://github.com/bmwas/InfiniteTalk/tree/main/examples",
      "type": "repo",
      "from": "NebSH"
    },
    {
      "resource": "Wan 2.2 merged models",
      "url": "https://huggingface.co/Phr00t/WAN2.2-14B-Rapid-AllInOne",
      "type": "model",
      "from": "Gateway"
    },
    {
      "resource": "Kenk's workflow",
      "url": "https://civitai.com/models/1824027",
      "type": "workflow",
      "from": "Kenk"
    },
    {
      "resource": "CineTrans",
      "url": "https://github.com/UknowSth/CineTrans",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "MTV Crafter",
      "url": "https://dingyanb.github.io/MTVCrafter-/",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Lightning LoRA usage guide",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/discussions/20",
      "type": "workflow",
      "from": ". Not Really Human ."
    },
    {
      "resource": "Runpod tutorial featuring Kijai's work",
      "url": "https://www.youtube.com/watch?v=w9wlVeEa610",
      "type": "tutorial",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "InfiniteTalk HuggingFace",
      "url": "https://huggingface.co/MeiGen-AI/InfiniteTalk/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "InfiniteTalk GitHub repo",
      "url": "https://github.com/MeiGen-AI/InfiniteTalk",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "InfiniteTalk ComfyUI branch",
      "url": "https://github.com/MeiGen-AI/InfiniteTalk/tree/comfyui",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "FastWan 5B distillation",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/FastWan",
      "type": "model",
      "from": "TK_999"
    },
    {
      "resource": "VRGameDevGirl84's Fun Control workflow",
      "url": "https://discord.com/channels/1076117621407223829/1373520070596231251",
      "type": "workflow",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "InfiniteTalk models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/blob/main/InfiniteTalk/",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Qwen Image Edit workflow",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1mu8ccu/comfyorgqwenimageedit_comfyui_hugging_face/",
      "type": "workflow",
      "from": "Lodis"
    },
    {
      "resource": "InfiniteTalk GitHub",
      "url": "https://github.com/MeiGen-AI/InfiniteTalk",
      "type": "repo",
      "from": "NebSH"
    },
    {
      "resource": "HiggsAudio ComfyUI wrapper",
      "url": "https://github.com/Yuan-ManX/ComfyUI-HiggsAudio/",
      "type": "tool",
      "from": "NebSH"
    },
    {
      "resource": "ResolutionMaster node",
      "url": "https://www.reddit.com/r/comfyui/comments/1mtzfyx/resolutionmaster_a_new_node_for_precise/",
      "type": "tool",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "resource": "Wan2_1-InfiniTetalk models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Standard Wan models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "InfiniteTalk issue discussion",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/issues/1069#issuecomment-3200088392",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "InfiniteTalk Q8 GGUF",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/blob/main/InfiniteTalk/Wan2_1-InfiniteTalk_Single_Q8.gguf",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "MagRef Q8 GGUF",
      "url": "https://huggingface.co/QuantStack/MAGREF_Wan2.1_I2V_14B-GGUF/blob/main/MAGREF_Wan2.1_I2V_14B-Q8_0.gguf",
      "type": "model",
      "from": "seitanism"
    },
    {
      "resource": "MagRef Q6 GGUF",
      "url": "https://huggingface.co/QuantStack/MAGREF_Wan2.1_I2V_14B-GGUF/blob/main/MAGREF_Wan2.1_I2V_14B-Q6_K.gguf",
      "type": "model",
      "from": "seitanism"
    },
    {
      "resource": "Wan2.1 I2V 480P GGUF models",
      "url": "https://huggingface.co/city96/Wan2.1-I2V-14B-480P-gguf/tree/main",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "MTVCrafter model",
      "url": "https://huggingface.co/yanboding/MTVCrafter",
      "type": "model",
      "from": "Dream Making"
    },
    {
      "resource": "InfiniteTalk GGUF Q4/Q6",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/tree/main/InfiniteTalk",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Speed LoRAs discussion",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1mujk6a/psa_speed_up_loras_for_wan_22_kill_everything/",
      "type": "guide",
      "from": "Dream Making"
    },
    {
      "resource": "V2V examples",
      "url": "https://drive.google.com/drive/folders/1lG4A-VhL1QtSueMTe7D06fsQwof6eK8F",
      "type": "examples",
      "from": "NebSH"
    },
    {
      "resource": "ComfyUI-QwenImageWanBridge",
      "url": "https://github.com/fblissjr/ComfyUI-QwenImageWanBridge/",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "CineTrans",
      "url": "https://github.com/UknowSth/CineTrans",
      "type": "repo",
      "from": "Josiah"
    },
    {
      "resource": "ComfyUI-WanMoeKSampler",
      "url": "https://github.com/stduhpf/ComfyUI-WanMoeKSampler",
      "type": "repo",
      "from": "xwsswww"
    },
    {
      "resource": "Wan21_Uni3C_controlnet_fp16.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_Uni3C_controlnet_fp16.safetensors",
      "type": "model",
      "from": "Ubertummen"
    },
    {
      "resource": "TREAD diffusion training speedup",
      "url": "https://github.com/feffy380/diffusion-pipe/tree/tread",
      "type": "repo",
      "from": "Ada"
    },
    {
      "resource": "TREAD paper",
      "url": "https://arxiv.org/pdf/2501.04765",
      "type": "paper",
      "from": "Ada"
    },
    {
      "resource": "TLBVFI interpolation ComfyUI implementation",
      "url": "https://github.com/BobRandomNumber/ComfyUI-TLBVFI",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Kijai sponsor page",
      "url": "https://github.com/sponsors/kijai",
      "type": "sponsor",
      "from": "seitanism"
    },
    {
      "resource": "ComfyUI educational stream",
      "url": "https://www.youtube.com/watch?v=TZIijn-tvoc",
      "type": "video",
      "from": "Fill"
    },
    {
      "resource": "Ultimate Vocal Remover",
      "url": "https://ultimatevocalremover.com/",
      "type": "tool",
      "from": "NC17z"
    },
    {
      "resource": "Wan 2.2 Fun 5B Control",
      "url": "https://huggingface.co/alibaba-pai/Wan2.2-Fun-5B-Control",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "InfiniteTalk GitHub issue with tips",
      "url": "https://github.com/MeiGen-AI/InfiniteTalk/issues/6#issuecomment-3204108512",
      "type": "repo",
      "from": "NebSH"
    },
    {
      "resource": "South Park Alias Wavefront tutorials",
      "url": "https://www.youtube.com/playlist?list=PL8F5B91C31A4FEE17",
      "type": "tutorial",
      "from": "samhodge"
    },
    {
      "resource": "Wan2.2-Fun-5B-Control models",
      "url": "https://huggingface.co/alibaba-pai/Wan2.2-Fun-5B-Control",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "WanVideo GGUF InfiniteTalk models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/tree/main/InfiniteTalk",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "LightX2V Wan2.2 Lightning LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1",
      "type": "lora",
      "from": "blird"
    },
    {
      "resource": "ComfyUI-QwenImageWanBridge",
      "url": "https://github.com/fblissjr/ComfyUI-QwenImageWanBridge",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "Banodoco Wan Video Discussion KB",
      "url": "https://notebooklm.google.com/notebook/a08901b9-0511-4926-bbf8-3c86a12dc306?pli=1",
      "type": "knowledge_base",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "resource": "Qwen Image dataset for video prompts",
      "url": "https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset",
      "type": "dataset",
      "from": "fredbliss"
    },
    {
      "resource": "InfiniteTalk GGUF models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/tree/main/InfiniteTalk",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Wan2.1-I2V-14B GGUF",
      "url": "https://huggingface.co/city96/Wan2.1-I2V-14B-480P-gguf/tree/main",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Lumen T2V model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Fun/Lumen/Wan2_1_Lumen-T2V-1.3B-V1.0_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Lumen GitHub repository",
      "url": "https://github.com/Kunbyte-AI/Lumen/tree/main",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "GGUF VACE modules",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/blob/main/VACE/Wan2_1-VACE_module_14B_Q8_0.gguf",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "InfiniteTalk single character model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/InfiniteTalk",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "MultiTalk weights",
      "url": "https://huggingface.co/MeiGen-AI/MeiGen-MultiTalk/blob/main/multitalk.safetensors",
      "type": "model",
      "from": "samhodge"
    },
    {
      "resource": "InfiniteTalk workflow examples",
      "url": "",
      "type": "workflow",
      "from": "Kijai"
    },
    {
      "resource": "Ostris Wan2.2 orbit shot LoRA",
      "url": "https://huggingface.co/ostris/wan22_i2v_14b_orbit_shot_lora",
      "type": "model",
      "from": "Drommer-Kille"
    },
    {
      "resource": "Wan2.2-Fun-A14B-InP model",
      "url": "https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-InP",
      "type": "model",
      "from": "Zabo"
    },
    {
      "resource": "PUSA-VidGen Wan2.2 version development",
      "url": "https://github.com/Yaofang-Liu/Pusa-VidGen/issues/34",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "ComfyUI-for-Nuke integration",
      "url": "https://github.com/vinavfx/ComfyUI-for-Nuke",
      "type": "tool",
      "from": "samhodge"
    },
    {
      "resource": "InfiniteTalk I2V example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_I2V_InfiniteTalk_example_01.json",
      "type": "workflow",
      "from": "JohnDopamine"
    },
    {
      "resource": "InfiniteTalk V2V example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_InfiniteTalk_V2V_example_01.json",
      "type": "workflow",
      "from": "JohnDopamine"
    },
    {
      "resource": "Qwen text enhancement for WanVideo",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Qwen",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "2.2 I2V InfiniteTalk workflow",
      "url": "https://discord.com/channels/1076117621407223829/1396263390296674324/1408070176095735890",
      "type": "workflow",
      "from": "DawnII"
    },
    {
      "resource": "InfiniteTalk GGUF models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/tree/main/InfiniteTalk",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "ComfyUI QwenImage WanBridge",
      "url": "https://github.com/fblissjr/ComfyUI-QwenImageWanBridge",
      "type": "tool",
      "from": "fredbliss"
    },
    {
      "resource": "Execution blocker for ComfyUI debugging",
      "url": "https://github.com/BadCafeCode/execution-inversion-demo-comfyui/blob/d9eebfaa1a6a33067e8c9108ef093b48279c4cbb/flow_control.py#L133-L158",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "ComfyUI-QwenImageWanBridge",
      "url": "https://github.com/fblissjr/ComfyUI-QwenImageWanBridge",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "WanVideo_comfy models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy",
      "type": "model",
      "from": "Josiah"
    },
    {
      "resource": "WanVideoWrapper workflows",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/main/example_workflows",
      "type": "workflow",
      "from": "JohnDopamine"
    },
    {
      "resource": "PUSA VidGen update",
      "url": "https://github.com/Yaofang-Liu/Pusa-VidGen/commit/4b664d213c5d6a0c141c5e7e8e8d3e9fb5391182",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "3-alpha-ultra model",
      "url": "https://huggingface.co/deca-ai/3-alpha-ultra",
      "type": "model",
      "from": "Ada"
    },
    {
      "resource": "CineScale LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/CineScale",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "CineScale original repo",
      "url": "https://github.com/Eyeline-Labs/CineScale",
      "type": "repo",
      "from": "s2k"
    },
    {
      "resource": "FreeScale predecessor",
      "url": "https://github.com/ali-vilab/FreeScale",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Blender OpenPose addons",
      "url": "https://impactframes.gumroad.com/l/fxnyez and https://toyxyz.gumroad.com/l/ciojz",
      "type": "tool",
      "from": "xwsswww"
    },
    {
      "resource": "InfiniteTalk example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_InfiniteTalk_V2V_example_01.json",
      "type": "workflow",
      "from": "samhodge"
    },
    {
      "resource": "Waver video project page",
      "url": "http://www.waver.video/",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "Ultimate Vocal Remover 5",
      "url": "",
      "type": "tool",
      "from": "samhodge"
    },
    {
      "resource": "Wan 2.2 5B Turbo",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Turbo",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Unofficial Wan 2.2 5B Turbo",
      "url": "https://huggingface.co/quanhaol/Wan2.2-TI2V-5B-Turbo",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Qwen text encoders",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Qwen",
      "type": "model",
      "from": "Dita"
    },
    {
      "resource": "UniAnimate Lora",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/UniAnimate-Wan2.1-14B-Lora-12000-fp16.safetensors",
      "type": "lora",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "LTXV Instagram examples",
      "url": "https://www.instagram.com/reel/DJUHfgso5yx/?utm_source=ig_web_copy_link&igsh=MzRlODBiNWFlZA==",
      "type": "example",
      "from": "NebSH"
    },
    {
      "resource": "CineScale LoRA",
      "url": "https://github.com/Eyeline-Labs/CineScale/tree/main",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "MediaSyncer tool",
      "url": "https://whatdreamscost.github.io/MediaSyncer/",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "Kenk's big workflow",
      "url": "https://civitai.com/models/1824027?modelVersionId=2137850",
      "type": "workflow",
      "from": "Kenk"
    },
    {
      "resource": "Sliders project",
      "url": "https://sliders.baulab.info/",
      "type": "tool",
      "from": "phazei"
    },
    {
      "resource": "360 degree rotation LoRA",
      "url": "https://civitai.com/models/1346623/360-degree-rotation-microwave-rotation-wan21-i2v-lora",
      "type": "lora",
      "from": "xwsswww"
    },
    {
      "resource": "InfiniteTalk models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/InfiniteTalk",
      "type": "model",
      "from": "seitanism"
    },
    {
      "resource": "DawnII's CausVid testing thread",
      "url": "https://discord.com/channels/1076117621407223829/1399888174519685224",
      "type": "discussion",
      "from": "DawnII"
    },
    {
      "resource": "MelBandRoFormer ComfyUI wrapper",
      "url": "https://github.com/kijai/ComfyUI-MelBandRoFormer",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "InfiniteTalk GGUF models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/tree/main/InfiniteTalk",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Kijai's Wan 2.2 scaled models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main",
      "type": "model",
      "from": "Kenk"
    },
    {
      "resource": "Wan 2.2 5B Turbo FP8 E5M2",
      "url": "https://huggingface.co/patientxtr/wan22ti2v5bturbofp8e5m2",
      "type": "model",
      "from": "patientx"
    },
    {
      "resource": "WAN-14B VACE Phantom v2 GGUF Q8",
      "url": "https://huggingface.co/orabazes/wan-14B_vace_phantom_v2_GGUF/blob/main/wan-14B_vace_phantom_v2_Q8.gguf",
      "type": "model",
      "from": "xwsswww"
    },
    {
      "resource": "WAN-14B VACE Phantom v2 GGUF Q5_K_M",
      "url": "https://huggingface.co/orabazes/wan-14B_vace_phantom_v2_GGUF/blob/main/wan-14B_vace_phantom_v2_Q5_K_M.gguf",
      "type": "model",
      "from": "orabazes"
    },
    {
      "resource": "ComfyUI-MultiGPU",
      "url": "https://github.com/pollockjj/ComfyUI-MultiGPU",
      "type": "repo",
      "from": "Jonathan"
    },
    {
      "resource": "Mellon project",
      "url": "https://github.com/cubiq/Mellon/",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "WanVideo GGUF repository",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/tree/main",
      "type": "model",
      "from": "xwsswww"
    },
    {
      "resource": "MAGREF I2V model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/blob/main/I2V/Wan2_1-I2V-14B-MAGREF_fp8_e4m3fn_scaled_KJ.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Fantasy Portrait + InfiniteTalk workflow",
      "url": "",
      "type": "workflow",
      "from": "Kijai"
    },
    {
      "resource": "Audio separation node",
      "url": "",
      "type": "node",
      "from": "Kenk"
    },
    {
      "resource": "HXHY-Realistic_Kontext_LoRA",
      "url": "https://huggingface.co/AlekseyCalvin/HXHY-Realistic_Kontext_LoRA_byXiaolxl",
      "type": "lora",
      "from": "Akumetsu971"
    },
    {
      "resource": "Magic-TryOn repository",
      "url": "github.com/vivoCameraResearch/Magic-TryOn",
      "type": "repo",
      "from": "Prelifik"
    },
    {
      "resource": "ComfyUI_WanVace-pipeline",
      "url": "https://github.com/tarkansarim/ComfyUI_WanVace-pipeline",
      "type": "node pack",
      "from": "xwsswww"
    },
    {
      "resource": "MTVCrafter info page",
      "url": "https://dingyanb.github.io/MTVCrafter-/",
      "type": "documentation",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "Tarkan Sarim LinkedIn demo",
      "url": "https://www.linkedin.com/posts/tarkan-sarim-a069347_comfyui-ugcPost-7365141303698886656-LuOF",
      "type": "demo",
      "from": "NebSH"
    },
    {
      "resource": "VACE methods knowledge base",
      "url": "https://nathanshipley.notion.site/Wan-2-1-Knowledge-Base-1d691e115364814fa9d4e27694e9468f#1d691e11536481f380e4cbf7fa105c05",
      "type": "documentation",
      "from": "mdkb"
    },
    {
      "resource": "Video-to-audio-through-text alternative",
      "url": "https://github.com/DragonLiu1995/video-to-audio-through-text",
      "type": "repo",
      "from": "sneha1743"
    },
    {
      "resource": "WanMoeKSampler for automating H/L sampler split",
      "url": "https://github.com/stduhpf/ComfyUI-WanMoeKSampler",
      "type": "tool",
      "from": "BobbyD4AI"
    },
    {
      "resource": "Magic-TryOn repository",
      "url": "https://github.com/vivoCameraResearch/Magic-TryOn",
      "type": "repo",
      "from": "sneha1743"
    },
    {
      "resource": "New anime style LoRA for Wan 2.2",
      "url": "https://discord.com/channels/1076117621407223829/1409548743425917041",
      "type": "lora",
      "from": "crinklypaper"
    },
    {
      "resource": "Wan 2.2 S2V announcement",
      "url": "https://x.com/Alibaba_Wan/status/1960012297059057935",
      "type": "news",
      "from": "Abx"
    },
    {
      "resource": "Wan audio announcement",
      "url": "https://x.com/Alibaba_Wan/status/1959963989703880866",
      "type": "news",
      "from": "Abx"
    },
    {
      "resource": "ComfyUI blog on masking and scheduling LoRA weights",
      "url": "https://blog.comfy.org/p/masking-and-scheduling-lora-and-model-weights",
      "type": "tutorial",
      "from": "Draken"
    },
    {
      "resource": "Meetup presentation",
      "url": "https://lu.ma/62hfwf86",
      "type": "event",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "wav2vec2 safetensors conversion",
      "url": "https://huggingface.co/Kijai/wav2vec2_safetensors/tree/main",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "mdkb's VACE workflows zip",
      "url": "https://markdkberry.com/workflows/footprints/",
      "type": "workflow",
      "from": "mdkb"
    },
    {
      "resource": "T2V 2.2 14b workflow on Civitai",
      "url": "https://civitai.com/models/1868641?modelVersionId=2147818",
      "type": "workflow",
      "from": "crinklypaper"
    },
    {
      "resource": "ATI model and tracking points",
      "url": "https://github.com/bytedance/ATI",
      "type": "repo",
      "from": "mdkb"
    },
    {
      "resource": "Mel-Band Roformer Vocal Model",
      "url": "https://github.com/KimberleyJensen/Mel-Band-Roformer-Vocal-Model",
      "type": "tool",
      "from": "DawnII"
    },
    {
      "resource": "Wrapper example workflows",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/main/example_workflows",
      "type": "workflow",
      "from": "Mu5hr00m_oO"
    },
    {
      "resource": "Wan 2.2 S2V 14B model",
      "url": "https://huggingface.co/Wan-AI/Wan2.2-S2V-14B",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Wan S2V demo page",
      "url": "https://humanaigc.github.io/wan-s2v-webpage/",
      "type": "demo",
      "from": "Screeb"
    },
    {
      "resource": "HuggingFace demo space",
      "url": "https://huggingface.co/spaces/Wan-AI/Wan2.2-S2V",
      "type": "demo",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "resource": "Fixed HotReloadHack",
      "url": "https://github.com/kijai/ComfyUI-HotReloadHack",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.2 S2V model",
      "url": "https://huggingface.co/Wan-AI/Wan2.2-S2V-14B",
      "type": "model",
      "from": "Impactframes"
    },
    {
      "resource": "Wan S2V demo space",
      "url": "https://huggingface.co/spaces/Wan-AI/Wan2.2-S2V",
      "type": "demo",
      "from": "NebSH"
    },
    {
      "resource": "WanVideo S2V models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/S2V",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "CFG Skimming branch",
      "url": "https://github.com/Mu5hr00moO/ComfyUI-WanVideoWrapper/tree/CFG_Skimming",
      "type": "repo",
      "from": "Mu5hr00m_oO"
    },
    {
      "resource": "NYC ComfyUI meetup",
      "url": "https://luma.com/62hfwf86",
      "type": "event",
      "from": "ericxtang"
    },
    {
      "resource": "S2V wav2vec2 model",
      "url": "https://huggingface.co/Wan-AI/Wan2.2-S2V-14B/blob/main/wav2vec2-large-xlsr-53-english/model.safetensors",
      "type": "model",
      "from": "slmonker"
    },
    {
      "resource": "ComfyUI Windows Portable",
      "url": "https://github.com/YanWenKun/ComfyUI-Windows-Portable",
      "type": "tool",
      "from": "Kenk"
    },
    {
      "resource": "Skimmed CFG",
      "url": "https://github.com/Extraltodeus/Skimmed_CFG",
      "type": "repo",
      "from": "MilesCorban"
    },
    {
      "resource": "KokoroTTS ComfyUI node",
      "url": "https://github.com/benjiyaya/ComfyUI-KokoroTTS",
      "type": "node",
      "from": "daking999"
    },
    {
      "resource": "Nano Banana",
      "url": "https://aistudio.google.com/apps/bundled/past_forward?showPreview=true&showAssistant=true",
      "type": "tool",
      "from": "asd"
    },
    {
      "resource": "Wan S2V paper",
      "url": "https://humanaigc.github.io/wan-s2v-webpage/content/wan-s2v.pdf",
      "type": "paper",
      "from": "MilesCorban"
    },
    {
      "resource": "Kijai VACE models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/tree/main/VACE",
      "type": "model",
      "from": "xwsswww"
    },
    {
      "resource": "ComfyUI native S2V PR",
      "url": "https://github.com/comfyanonymous/ComfyUI/pull/9568",
      "type": "repo",
      "from": "comfy"
    },
    {
      "resource": "Audio Duration Node documentation",
      "url": "https://comfyai.run/documentation/AudioDurationNode",
      "type": "tool",
      "from": "samhodge"
    },
    {
      "resource": "S2V workflow example",
      "url": "https://github.com/user-attachments/files/22001431/sound_to_video_wan_example.json",
      "type": "workflow",
      "from": "Kijai"
    },
    {
      "resource": "AniSora V3 model",
      "url": "https://huggingface.co/IndexTeam/Index-anisora/tree/main/V3",
      "type": "model",
      "from": "DreamWeebs"
    },
    {
      "resource": "WanVideo ComfyUI fp8 scaled models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/I2V/AniSora",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Background replacement workflow",
      "url": "https://discord.com/channels/1076117621407223829/1376383134303784970/1376383134303784970",
      "type": "workflow",
      "from": "Dream Making"
    },
    {
      "resource": "ElevenLabs video to sound generator",
      "url": "https://videotosfx.elevenlabs.io/",
      "type": "tool",
      "from": "Ablejones"
    },
    {
      "resource": "S2V branch pull request",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/pull/1122",
      "type": "repo",
      "from": "Mu5hr00m_oO"
    },
    {
      "resource": "Wan 2.2 full safetensor models",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/tree/main/split_files/diffusion_models",
      "type": "model",
      "from": "ingi // SYSTMS"
    },
    {
      "resource": "HunyuanVideo-Foley",
      "url": "https://huggingface.co/tencent/HunyuanVideo-Foley",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "HunyuanVideo-Foley GitHub",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Ultimate Vocal Remover",
      "url": "https://ultimatevocalremover.com/",
      "type": "tool",
      "from": "NC17z"
    },
    {
      "resource": "MelBandRoFormer for ComfyUI",
      "url": "https://github.com/kijai/ComfyUI-MelBandRoFormer",
      "type": "repo",
      "from": "Kenk"
    },
    {
      "resource": "SAM2 video masking node",
      "url": "https://github.com/kijai/ComfyUI-segment-anything-2",
      "type": "repo",
      "from": "JalenBrunson"
    },
    {
      "resource": "CFG Skimming workflow",
      "url": "https://drive.google.com/file/d/1aHVLpLlzJ4805L5IwCmKabb5COxX2ZPo/view?usp=sharing",
      "type": "workflow",
      "from": "Mu5hr00m_oO"
    },
    {
      "resource": "Skimmed CFG documentation",
      "url": "https://deepwiki.com/Extraltodeus/Skimmed_CFG",
      "type": "documentation",
      "from": "Mu5hr00m_oO"
    },
    {
      "resource": "ComfyUI-Sapiens pose detection",
      "url": "https://github.com/smthemex/ComfyUI_Sapiens",
      "type": "repo",
      "from": "Impactframes."
    },
    {
      "resource": "ComfyUI-Rife-Tensorrt",
      "url": "https://github.com/yuvraj108c/ComfyUI-Rife-Tensorrt",
      "type": "repo",
      "from": "Mu5hr00m_oO"
    },
    {
      "resource": "HunyuanVideo-Foley",
      "url": "https://github.com/Tencent-Hunyuan/HunyuanVideo-Foley",
      "type": "repo",
      "from": "Karo"
    },
    {
      "resource": "HunyuanVideo-Foley demo",
      "url": "https://szczesnys.github.io/hunyuanvideo-foley/",
      "type": "tool",
      "from": "pom"
    },
    {
      "resource": "QwenImageWanBridge research nodes",
      "url": "https://github.com/fblissjr/ComfyUI-QwenImageWanBridge/tree/main/nodes/research",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "PUSA-VidGen update",
      "url": "https://github.com/Yaofang-Liu/Pusa-VidGen/commit/4b664d213c5d6a0c141c5e7e8e8d3e9fb5391182",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "Krea realtime video",
      "url": "https://www.krea.ai/blog/announcing-realtime-video",
      "type": "tool",
      "from": "Nekodificador"
    },
    {
      "resource": "Unofficial VACE 2.2 merge",
      "url": "https://huggingface.co/lym00/Wan2.2_T2V_A14B_VACE-test/tree/main",
      "type": "model",
      "from": "screwfunk"
    },
    {
      "resource": "ComfyUI-VibeVoice wrapper",
      "url": "https://github.com/wildminder/ComfyUI-VibeVoice",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "ComfyUI-Chatterbox for voice cloning",
      "url": "https://github.com/wildminder/ComfyUI-Chatterbox",
      "type": "repo",
      "from": "N0NSens"
    },
    {
      "resource": "Higgs Audio TTS model",
      "url": "https://github.com/boson-ai/higgs-audio",
      "type": "model",
      "from": "Juampab12"
    },
    {
      "resource": "MiniMax-bmo removal node",
      "url": "https://github.com/casterpollux/MiniMax-bmo",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Native S2V test workflow (35 second generation)",
      "url": "",
      "type": "workflow",
      "from": "comfy"
    },
    {
      "resource": "Updated native S2V workflow with first frame fix",
      "url": "",
      "type": "workflow",
      "from": "comfy"
    },
    {
      "resource": "Qwen2.5-VL-7B-Instruct-GGUF models",
      "url": "https://huggingface.co/unsloth/Qwen2.5-VL-7B-Instruct-GGUF/tree/main",
      "type": "model",
      "from": "xwsswww"
    },
    {
      "resource": "Fixed S2V workflow for color mismatch",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1410771695022637187",
      "type": "workflow",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "S2V model checkpoint",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/blob/main/split_files/diffusion_models/wan2.2_s2v_14B_bf16.safetensors",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "diffusion-pipe-TREAD",
      "url": "https://github.com/Ada123-a/diffusion-pipe-TREAD",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "HunyuanVideo-Foley wrapper",
      "url": "https://github.com/if-ai/ComfyUI_HunyuanVideoFoley",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Kijai example workflows",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/main/example_workflows",
      "type": "workflow",
      "from": "JohnDopamine"
    },
    {
      "resource": "LoRA scheduling article",
      "url": "https://blog.comfy.org/p/masking-and-scheduling-lora-and-model-weights",
      "type": "resource",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "CineScale super-resolution",
      "url": "https://github.com/Eyeline-Labs/CineScale/",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "ComfyUI-WanVideoWrapper commits",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/commits/main/",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "ComfyUI-VibeVoice",
      "url": "https://github.com/wildminder/ComfyUI-VibeVoice",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "MAGREF model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-Wan-I2V-MAGREF-14B_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Mancho"
    },
    {
      "resource": "Scaled MAGREF model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/blob/main/I2V/Wan2_1-I2V-14B-MAGREF_fp8_e4m3fn_scaled_KJ.safetensors",
      "type": "model",
      "from": "Mancho"
    },
    {
      "resource": "Wan 2.2 LoRAs",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/tree/main/split_files/loras",
      "type": "model",
      "from": "Ashtar"
    },
    {
      "resource": "ComfyUI HunyuanFoley",
      "url": "https://github.com/aistudynow/Comfyui-HunyuanFoley",
      "type": "repo",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "resource": "ComfyUI HunyuanVideoFoley",
      "url": "https://github.com/if-ai/ComfyUI_HunyuanVideoFoley",
      "type": "repo",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "resource": "ComfyUI Custom Sigma Editor",
      "url": "https://github.com/JoeNavark/comfyui_custom_sigma_editor",
      "type": "tool",
      "from": "Nekodificador"
    },
    {
      "resource": "ComfyUI HunyuanVideo Foley",
      "url": "https://github.com/if-ai/ComfyUI_HunyuanVideoFoley",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "SageAttention releases",
      "url": "https://github.com/woct0rdho/SageAttention/releases",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "ComfyUI-AnimateDiff-Evolved context documentation",
      "url": "https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved/tree/main/documentation/nodes#context-options-and-view-options",
      "type": "documentation",
      "from": "Mu5hr00m_oO"
    },
    {
      "resource": "ComfyUI portable build with PyTorch 2.8",
      "url": "https://huggingface.co/Nakamotosatoshi/ComfyUI_0.3.55",
      "type": "tool",
      "from": "hicho"
    },
    {
      "resource": "FrameUtilitys custom nodes",
      "url": "https://github.com/lum3on/ComfyUI-FrameUtilitys",
      "type": "tool",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "resource": "CineScale implementation",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/CineScale",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "WAN workflows collection",
      "url": "https://github.com/bluespork",
      "type": "workflow",
      "from": "hicho"
    },
    {
      "resource": "MAGREF project page",
      "url": "https://magref-video.github.io/",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Lightx2v I2V LoRA",
      "url": "https://huggingface.co/lightx2v/Wan2.1-I2V-14B-480P-StepDistill-CfgDistill-Lightx2v",
      "type": "model",
      "from": "Ablejones"
    },
    {
      "resource": "ComfyUI API nodes for nano banana",
      "url": "https://blog.comfy.org/p/nano-banana-via-comfyui-api-nodes",
      "type": "workflow",
      "from": "DawnII"
    },
    {
      "resource": "Reddit workflow for Wan 2.2",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1mxu5tq/wan_22_text2video_with_ultimate_sd_upscaler_the/",
      "type": "workflow",
      "from": "chrisd0073"
    },
    {
      "resource": "CineScale workflows",
      "url": "https://civitai.com/models/1893519/cinescalewan22-highlow-noise-three-shot-intelligent-prompt-word-to-video-high-detail-workflow-v1",
      "type": "workflow",
      "from": "BecauseReasons"
    },
    {
      "resource": "Qwen2.5-7B-Instruct GGUF",
      "url": "huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF",
      "type": "model",
      "from": "Prelifik"
    },
    {
      "resource": "Wan prompting documentation",
      "url": "https://www.instasd.com/post/wan2-2-whats-new-and-how-to-write-killer-prompts",
      "type": "documentation",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "resource": "Wan camera moves and controls documentation",
      "url": "https://alidocs.dingtalk.com/i/nodes/EpGBa2Lm8aZxe5myC99MelA2WgN7R35y",
      "type": "documentation",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "resource": "VACE examples workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_1_3B_VACE_examples_03.json",
      "type": "workflow",
      "from": "Juampab12"
    },
    {
      "resource": "Chinese wav2vec2 model",
      "url": "https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn/tree/main",
      "type": "model",
      "from": "piscesbody"
    }
  ],
  "limitations": [
    {
      "limitation": "24fps generations often sped up",
      "details": "Videos at 24fps randomly correct speed or feel sped up, not consistent",
      "from": "Ruairi Robinson"
    },
    {
      "limitation": "Poor timestamping control",
      "details": "Wan 2.2 not good with simple timestamping like 'this appears at 3s mark' unlike Veo",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Cannot control light colors effectively",
      "details": "Impossible to control light colors for specific cinematography like Blade Runner style",
      "from": "GOD_IS_A_LIE"
    },
    {
      "limitation": "Character LoRA applies to all faces",
      "details": "Person LoRAs apply to every character's face in generation, difficult to limit to specific characters",
      "from": "Ruairi Robinson"
    },
    {
      "limitation": "WAN 2.2 generation times still too long",
      "details": "15+ minute runs make it less practical than closed models despite better quality",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "SageAttention3 poor quality",
      "details": "Early access version has terrible quality, needs step/block restrictions to be usable",
      "from": "Kijai"
    },
    {
      "limitation": "RadialAttention strict resolution requirements",
      "details": "Only works with resolutions where token count is divisible by block size",
      "from": "Kijai"
    },
    {
      "limitation": "VACE 2.1 with 2.2 not perfect",
      "details": "Using 2.1 VACE with 2.2 models doesn't work perfectly, waiting for 2.2 VACE release",
      "from": "seitanism"
    },
    {
      "limitation": "bf16 required for original T5 implementation",
      "details": "Original WAN T5 code doesn't work properly in fp16, requires bf16 or ComfyUI bridge",
      "from": "Kijai"
    },
    {
      "limitation": "Camera prompt adherence poor in I2V",
      "details": "Even with detailed prompts, CFG 3.5, and various tricks, camera movement prompts don't work well in I2V mode",
      "from": "Juan Gea"
    },
    {
      "limitation": "Male genitalia generation issues",
      "details": "WAN 2.2 struggles with male genitalia generation, often creates fingers instead. Training LoRAs for this is difficult",
      "from": "Kenk"
    },
    {
      "limitation": "Context options don't work well with I2V",
      "details": "Context window functionality designed primarily for T2V, limited effectiveness in I2V workflows",
      "from": "Kijai"
    },
    {
      "limitation": "Motion varies significantly with resolution",
      "details": "Higher resolutions like 576x1024 produce slow-motion effects, while 480x720 has more natural motion",
      "from": "shockgun"
    },
    {
      "limitation": "Camera control ignores prompts",
      "details": "Even on their website, camera movements like 'camera overpassing a runner' don't work reliably",
      "from": "Juan Gea"
    },
    {
      "limitation": "5B model has inferior motion quality",
      "details": "Motion is far from 2.2 A14B quality, generates weird motion or bad faces",
      "from": "Lodis"
    },
    {
      "limitation": "Loop args don't work well with newer models",
      "details": "Only worked on 1.3B model, didn't blend well with 2.2",
      "from": "Kijai"
    },
    {
      "limitation": "Context generation can have scene jumps",
      "details": "Sometimes overlap is not enough to blend and you get huge jumps",
      "from": "Kijai"
    },
    {
      "limitation": "Color correction node oversaturates",
      "details": "Often 'corrects' otherwise ok input to be oversaturated",
      "from": "lostintranslation"
    },
    {
      "limitation": "No LightX2V I2V LoRA for 14B at 720p",
      "details": "Only 480p version available, but works with 480p",
      "from": "Lodis"
    },
    {
      "limitation": "Context overlap technique has limits",
      "details": "Won't work well with I2V, has limits when used with pure T2V without control",
      "from": "Kijai"
    },
    {
      "limitation": "Current LoRAs pull 2.2 back to 2.1 quality",
      "details": "Need proper 2.2 LoRAs to fully unlock 2.2 capabilities",
      "from": "The Shadow (NYC)"
    },
    {
      "limitation": "50/50 step split may not be optimal",
      "details": "Should probably be more like 25% high noise, 75% low noise",
      "from": "ArtOfficial"
    },
    {
      "limitation": "Context windows don't work well with I2V",
      "details": "Due to strong image conditioning, causes video to replay after frame count",
      "from": "Kijai"
    },
    {
      "limitation": "VACE 2.1 poor performance on high noise model",
      "details": "Works fine on low noise model but terrible results on high noise model",
      "from": "Kijai"
    },
    {
      "limitation": "GGUF quantized models poor quality",
      "details": "Only FP8 or FP16 models work well, quantized models don't work good",
      "from": "avataraim"
    },
    {
      "limitation": "Sage Attention limited benefits on older GPUs",
      "details": "Cards lower than RTX 3000 series won't see much improvement",
      "from": "xwsswww"
    },
    {
      "limitation": "5B model doesn't work well with existing LoRAs",
      "details": "LightX and other 14B LoRAs incompatible due to size differences",
      "from": "WorldX"
    },
    {
      "limitation": "Context windows don't maintain action coherence for long videos",
      "details": "Guy running will change pace, action differs, background changes over 15 seconds",
      "from": "Juan Gea"
    },
    {
      "limitation": "Model doesn't handle 24fps well",
      "details": "Causes burnt frames, better to generate at 16fps and interpolate",
      "from": "thaakeno"
    },
    {
      "limitation": "9:16 aspect ratio produces weird character motions",
      "details": "Walking characters do weird motions in vertical format, works better in higher ratios",
      "from": "scf"
    },
    {
      "limitation": "5B model very slow on 3090",
      "details": "7 minutes for generation on RTX 3090",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Phantom not compatible with 2.2 high noise model",
      "details": "Seems fully incompatible when tested",
      "from": "Kijai"
    },
    {
      "limitation": "5B model extension doesn't work well",
      "details": "Tested extension with 5B model, results not good",
      "from": "Kijai"
    },
    {
      "limitation": "5B first to last frame by giving last latent doesn't work",
      "details": "Attempted FLF approach failed with 5B model",
      "from": "Kijai"
    },
    {
      "limitation": "Character LoRAs from 2.1 don't trigger properly in 2.2",
      "details": "LoRAs that work in 2.1 fail to trigger in 2.2 I2V",
      "from": "screwfunk"
    },
    {
      "limitation": "2.2 T2V produces noisy/fuzzy results in wrapper",
      "details": "Multiple users report poor quality compared to 2.1",
      "from": "AffenBrot"
    },
    {
      "limitation": "VACE 2.2 high noise model has limited compatibility",
      "details": "Some modalities don't work at all, some work weakly. Not worth using over 2.1 VACE",
      "from": "Kijai"
    },
    {
      "limitation": "Middle frame transitions rarely produce proper motions",
      "details": "While it works technically, proper motions between images are rare",
      "from": "Kijai"
    },
    {
      "limitation": "Depth control with VACE 2.2 not very effective",
      "details": "Depth does something but results are not very great",
      "from": "Kijai"
    },
    {
      "limitation": "2.2 not drop-in replacement for 2.1",
      "details": "LoRAs and workflows from 2.1 don't directly work with 2.2",
      "from": "Karo"
    },
    {
      "limitation": "5B model quality issues",
      "details": "Generally pretty garbage quality compared to 14B model",
      "from": "Screeb"
    },
    {
      "limitation": "Camera motion keywords ignored",
      "details": "Model tends to ignore camera movement prompts in some workflows",
      "from": "Rainsmellsnice"
    },
    {
      "limitation": "LoRA masking not supported for Wan",
      "details": "Advanced masking techniques for selective training not available yet",
      "from": "Ryzen"
    },
    {
      "limitation": "Quality degradation above 81 frames",
      "details": "Anything above 81 frames starts to really suffer in quality",
      "from": "\u2727\u0e05\u0e42\u0e51\u2180\u11ba\u2180 \u0e51\u0e43\u0e05\u2727PookieNumnums"
    },
    {
      "limitation": "Motion quality suffers at higher resolutions",
      "details": "Higher resolution causes motion to suffer, may need more steps or different params",
      "from": "Kijai"
    },
    {
      "limitation": "fp8 can hallucinate incorrect objects",
      "details": "fp8 precision can change subject matter completely (divers becoming seaweed)",
      "from": "hicho"
    },
    {
      "limitation": "High noise layer difficult to work with",
      "details": "Messing with the high layer is problematic even with 3 sampler approach",
      "from": "Karo"
    },
    {
      "limitation": "Model struggles with object state changes",
      "details": "Difficult to show actions like stealing ice cream - tends to maintain original object states",
      "from": "Kijai"
    },
    {
      "limitation": "Multitalk doesn't work well with Wan 2.2",
      "details": "Audio sync issues, works better with 2.1",
      "from": "Kijai"
    },
    {
      "limitation": "First frame last frame doesn't work with 5B model",
      "details": "Only available for 14B model currently",
      "from": "Kijai"
    },
    {
      "limitation": "Frames over 81 cause weird twisting artifacts",
      "details": "Generation quality degrades significantly beyond 81 frames",
      "from": "Kijai"
    },
    {
      "limitation": "Wan doesn't handle destruction/gore well",
      "details": "Struggles with realistic blood/flesh texturing and destruction concepts",
      "from": "DevouredBeef"
    },
    {
      "limitation": "lightx2v doesn't work properly with high noise model",
      "details": "Requires using cfg for first steps, fine balance needed between cfg and LoRA strength",
      "from": "Kijai"
    },
    {
      "limitation": "Hard limit is VRAM for frame count",
      "details": "Without guidance, generation just loops after 81 frames",
      "from": "Kijai"
    },
    {
      "limitation": "LoRA extraction only works against training model",
      "details": "Can't extract from one model and apply to another effectively",
      "from": "Kijai"
    },
    {
      "limitation": "Caching doesn't work with distillations",
      "details": "Most don't use easycache because it doesn't really work with the distillations",
      "from": "Kijai"
    },
    {
      "limitation": "lightx2v adds same 1girl face in 2.2",
      "details": "Tends to add the same face, making model more 2.1-like",
      "from": "RRR"
    },
    {
      "limitation": "Wan 2.2 doesn't like violence",
      "details": "Model tends to ignore violent actions in prompts like hitting",
      "from": "mamad8"
    },
    {
      "limitation": "High shift values can cause distortions",
      "details": "Increasing shift too much makes it harder for video to find coherent path, leading to distortions or garbled mess",
      "from": "gokuvonlange"
    },
    {
      "limitation": "2.1 LoRAs don't work optimally with 2.2",
      "details": "Require higher strength settings which harms model quality, but necessary unless using more steps",
      "from": "Juampab12"
    },
    {
      "limitation": "Camera controls lost with LoRA usage",
      "details": "Excellent at character motion but camera controls except zoom are gone when using LoRA",
      "from": "Rainsmellsnice"
    },
    {
      "limitation": "Native nodes don't support wrapper scheduling features",
      "details": "String to float conversion and advanced scheduling only work with wrapper nodes",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.2 5B model has poor motion quality",
      "details": "Motion doesn't come close to what high noise expert can achieve despite good image quality",
      "from": "Kijai"
    },
    {
      "limitation": "VACE character consistency requires LoRA",
      "details": "Nearly impossible to get character consistency without a character LoRA",
      "from": "Nekodificador"
    },
    {
      "limitation": "Multitalk not compatible with 720p models",
      "details": "Multitalk works with 480p but generates noise with 720p models",
      "from": "Juan Gea"
    },
    {
      "limitation": "Enhance-a-video bad for high noise model",
      "details": "In experience it's bad for the high noise, but somewhat useful for the low noise",
      "from": "Kijai"
    },
    {
      "limitation": "New lightning LoRAs only work for T2V",
      "details": "I2V compatibility is broken - no motion, poor prompt adherence. I2V-specific LoRAs not released yet",
      "from": "Kijai/DawnII"
    },
    {
      "limitation": "Reduced dynamic range in 2.2",
      "details": "Wan 2.2 Lightning has significantly reduced dynamic performance compared to 2.1 version",
      "from": "wange1002"
    },
    {
      "limitation": "Struggles with dark/moody scenes",
      "details": "New lightning LoRAs produce overly bright results, lose cinematic/moody tones",
      "from": "gokuvonlange"
    },
    {
      "limitation": "Guide image support buggy in 2.2",
      "details": "Guide images have issues especially in I2V mode",
      "from": "TK_999/NebSH"
    },
    {
      "limitation": "New Lightning LoRA high noise pass produces poor results",
      "details": "Weird camera fade effects, poor prompt adherence compared to old LoRAs",
      "from": "Doctor Shotgun"
    },
    {
      "limitation": "Lightning LoRAs lack text embedding layers",
      "details": "Missing important layers that affect LightX2V performance",
      "from": "Kijai"
    },
    {
      "limitation": "480p resolution produces slow motion and artifacts",
      "details": "Lower resolutions don't work well with Lightning LoRAs",
      "from": "Kijai"
    },
    {
      "limitation": "5B model behaves differently from 14B models",
      "details": "Not interchangeable, 5B handles first/last frame differently",
      "from": "seitanism"
    },
    {
      "limitation": "Lightning LoRA breaks complex camera movements",
      "details": "Lightning LoRA on high noise model causes issues with complex camera movements and motion quality",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "limitation": "Camera fade out issue with 2.2 LoRA",
      "details": "2.2 LoRA causes unwanted camera fade out effects that weren't present in Pisces",
      "from": "Doctor Shotgun"
    },
    {
      "limitation": "Lightning LoRA has strong bias",
      "details": "Lightning LoRA has way too big bias, doesn't handle lower strengths well like LightX2V could",
      "from": "Kijai"
    },
    {
      "limitation": "Split screen with 'cut' keyword",
      "details": "Using the word 'cut' in prompts causes unwanted split-screen effects",
      "from": "IceAero"
    },
    {
      "limitation": "New lightning loras destroy motion and prompt following",
      "details": "Particularly bad prompt adherence when using high+low, drops half the prompt even with 2 CFG",
      "from": "flo1331"
    },
    {
      "limitation": "Fast 5B model has poor prompt following",
      "details": "About zero prompt following, quality degraded",
      "from": "N0NSens"
    },
    {
      "limitation": "VAE decode time is very slow",
      "details": "60 seconds to generate, 80 seconds to decode. Decoding longer than generating",
      "from": "Kijai"
    },
    {
      "limitation": "2.2 new loras currently only work for T2V",
      "details": "The new ones for 2.2 are T2V only, 2.1 ones kind of work for I2V",
      "from": "Immac"
    },
    {
      "limitation": "Lightning lora changes faces and brightness",
      "details": "Makes videos brighter than wanted and changes faces",
      "from": "screwfunk"
    },
    {
      "limitation": "Lightning lora bad at cfg distillation",
      "details": "Seems pretty bad at cfg distillation, doesn't work well with very low cfg",
      "from": "Kijai"
    },
    {
      "limitation": "5B model turns everything into humans",
      "details": "Anything other than a human gets turned into a human",
      "from": "Rainsmellsnice"
    },
    {
      "limitation": "Context windows work poorly with I2V",
      "details": "Sliding context doesn't work well with I2V workflows",
      "from": "Kijai"
    },
    {
      "limitation": "VAE grid stepping artifacts",
      "details": "When characters with hair move, VAE creates weird grid stepping because it cannot render motion smoothly",
      "from": "aikitoria"
    },
    {
      "limitation": "Lightning LoRA fails on complex prompts",
      "details": "Works fine for simple prompts but falls apart on complex scenarios that base Wan 2.2 handles",
      "from": "Kijai"
    },
    {
      "limitation": "Character LoRAs tend to stifle motion",
      "details": "Character LoRAs reduce motion quality, creating a trade-off between character consistency and animation",
      "from": "screwfunk"
    },
    {
      "limitation": "Going through doors is difficult",
      "details": "The model struggles with generating characters moving through doorways",
      "from": "kendrick"
    },
    {
      "limitation": "Masking with WanVideo Encode didn't work",
      "details": "User reported that masking rendered from Blender didn't function properly with WanVideo Encode",
      "from": "xwsswww"
    },
    {
      "limitation": "No true 2.2 VACE support",
      "details": "Most VACE modes either are super weak or don't work at all with the high noise model",
      "from": "Kijai"
    },
    {
      "limitation": "Frame interpolation doesn't work well with Wan",
      "details": "Attempted frame interpolation using all frames to double FPS but results weren't great",
      "from": "Kijai"
    },
    {
      "limitation": "Lightning kills motion significantly",
      "details": "Lightning LoRAs reduce motion so much that lower strength + CFG feels better, quickly reaching point where lightx2v is better",
      "from": "Kijai"
    },
    {
      "limitation": "WAN 2.2 lightning LoRAs are not great",
      "details": "Current lightning LoRAs for 2.2 don't work as well as older LightX2V LoRAs",
      "from": "Kijai"
    },
    {
      "limitation": "Hard to avoid offloading with 2.2 14B model",
      "details": "Model is too large to fit entirely in VRAM on most consumer cards",
      "from": "Kijai"
    },
    {
      "limitation": "Context windowing slows generation significantly",
      "details": "Multiple sampling per step creates unavoidable slowdown for longer sequences",
      "from": "Kijai"
    },
    {
      "limitation": "[CUT] prompt doesn't work reliably",
      "details": "Sometimes just displays text [CUT] on screen instead of creating scene transitions",
      "from": "FancyJustice"
    },
    {
      "limitation": "Wan 2.2 heavily biased toward realism",
      "details": "Very difficult to get stylized art - wasn't trained on stylized art at all",
      "from": "Fill"
    },
    {
      "limitation": "Context windows don't work well with I2V",
      "details": "I2V models need image input in each window, causing snapping or full T2V behavior",
      "from": "Kijai"
    },
    {
      "limitation": "90+ frames causes looping and quality degradation",
      "details": "Generating more than 81 frames generally works poorly with visual quality issues",
      "from": "Kijai"
    },
    {
      "limitation": "Bong_tangent scheduler not good for low steps",
      "details": "Not recommended for <20 steps, especially not for Wan 2.2. Beta is best for <10 steps",
      "from": "Ablejones"
    },
    {
      "limitation": "Cannot properly generate 5 seconds at 24fps beyond 81 frames",
      "details": "Model has difficulty with longer generations",
      "from": "Kijai"
    },
    {
      "limitation": "5B model has issues at large resolutions like 1920x1080",
      "details": "Hallucinates more than Kontext/Krea at large resolutions",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Character consistency not perfect without training",
      "details": "Even with reference techniques, character retention is imperfect",
      "from": "Juampab12"
    },
    {
      "limitation": "VACE doesn't work properly with Wan 2.2",
      "details": "Due to different patch_embedding architecture in high noise model",
      "from": "Kijai"
    },
    {
      "limitation": "Wan2.1 VACE quality degrades significantly for longer generations",
      "details": "Style transfer quality drops heavily when going from 20 to 60 frames",
      "from": "Poppi"
    },
    {
      "limitation": "2.2 speed LoRAs performance",
      "details": "Not better than old LightX2V, can cause bleached/burned appearance",
      "from": "Kijai"
    },
    {
      "limitation": "EchoShot implementation unreliable",
      "details": "Often doesn't trigger, panda prompt won't work with their method",
      "from": "Kijai"
    },
    {
      "limitation": "VACE blocks don't work with 2.2 HN model",
      "details": "Need to merge 2.1 patch embedding and block 0 to get reference following",
      "from": "Kijai"
    },
    {
      "limitation": "Camera prompting difficult with I2V",
      "details": "Camera movement prompting is challenging to control with image-to-video",
      "from": "N0NSens"
    },
    {
      "limitation": "Motion loops back after 81 frames",
      "details": "Model trained on 81 frames, longer sequences may repeat or undo previous actions",
      "from": "Hoernchen"
    },
    {
      "limitation": "Poor performance at higher resolutions without proper settings",
      "details": "Higher resolutions like 1280x720 need increased CFG and steps",
      "from": "Mngbg"
    },
    {
      "limitation": "5B model requires more effort",
      "details": "5B model needs more careful prompting and parameter tuning compared to 14B",
      "from": "DawnII"
    },
    {
      "limitation": "Speed LoRAs significantly change model style",
      "details": "Lightning and LightX have colossal effect on image style, LightX makes everything anime-like",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "limitation": "2.1 LoRAs cause poor temporal continuity on 2.2",
      "details": "Jerky frames and strange generations when using 2.1 trained LoRAs on 2.2 models",
      "from": "mamad8"
    },
    {
      "limitation": "FP8 requires newer GPU architecture",
      "details": "3000 series lacks native fp8 support, needs at least 4000 series. A5000 is compute 8.6 but needs 8.9",
      "from": "Lodis"
    },
    {
      "limitation": "Wan 2.2 Lightning LoRA quality",
      "details": "Performs worse than 2.1 LightX2V, limited to specific 4-step setup usecase",
      "from": "Kijai"
    },
    {
      "limitation": "Frame count affects style consistency",
      "details": "Different frame counts produce different styles - fewer frames tend toward comic/anime, more frames toward realism",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "limitation": "USDU upscaling background seams",
      "details": "While it fixes hands and hair details, it creates seams in backgrounds that need more tile blending",
      "from": "Persoon"
    },
    {
      "limitation": "Add noise LoRA only works in Low Noise model",
      "details": "LoRA effects only appear in LN model, not in HN model",
      "from": "piscesbody"
    },
    {
      "limitation": "Lightning LoRAs cannot generate dark/night scenes",
      "details": "Force daylight scenes, break on dark prompts, likely due to dataset limitations",
      "from": "Kijai"
    },
    {
      "limitation": "New LoRAs have compatibility issues with GGUF quants",
      "details": "Don't work as well with quantized models, need more steps",
      "from": "Josiah"
    },
    {
      "limitation": "Scene teleportation inconsistency",
      "details": "Characters sometimes only appear on one side of scene, requires specific prompting techniques",
      "from": "FancyJustice"
    },
    {
      "limitation": "New I2V LoRA redraws first frame more than old T2V version",
      "details": "Less preservation of input image compared to using T2V LoRA for I2V",
      "from": "N0NSens"
    },
    {
      "limitation": "Lightning LoRAs completely destroy lighting capabilities",
      "details": "No dark scenes, always full bright, can't make scenes with dim lights",
      "from": "Critorio"
    },
    {
      "limitation": "Small context windows in MultiTalk don't work",
      "details": "With small windows like 25 frames, can't create many new frames per iteration and may not finish",
      "from": "Kijai"
    },
    {
      "limitation": "MultiTalk quality degradation with too many windows",
      "details": "Image quality degrades based on number of windows, not video length",
      "from": "Kijai"
    },
    {
      "limitation": "RadialAttention changes input image in I2V",
      "details": "Strays too far from source image making it useless for I2V",
      "from": "N0NSens"
    },
    {
      "limitation": "Going over 81 frames affects prompt adherence and motion",
      "details": "Longer videos may have reduced prompt following and motion quality",
      "from": "MysteryShack"
    },
    {
      "limitation": "Lightning LoRA prompt adherence issues",
      "details": "Low noise model affects prompt adherence, doesn't work well with complex or weird prompts",
      "from": "MysteryShack"
    },
    {
      "limitation": "Latent composite masked doesn't support video",
      "details": "Only works for images, not video latents",
      "from": "xwsswww"
    },
    {
      "limitation": "Lightning quality issues with v1.1",
      "details": "Lightning 1.1 version appears to have worse quality than previous versions",
      "from": "MysteryShack"
    },
    {
      "limitation": "Fun Control model size",
      "details": "Very large file size making disk space management difficult",
      "from": "xwsswww"
    },
    {
      "limitation": "Fun Control limited to 4 seconds maximum",
      "details": "Unlike VACE which can go longer with sufficient VRAM",
      "from": "Draken"
    },
    {
      "limitation": "Lightning LoRAs force bright daytime aesthetic",
      "details": "Cannot easily achieve dark or night scenes even with strong negative prompting",
      "from": "Ablejones"
    },
    {
      "limitation": "VAE cannot be chunked without seams",
      "details": "Temporal VAE must process full sequence, causing memory issues with long videos",
      "from": "Kijai"
    },
    {
      "limitation": "MultiTalk trained with Chinese wav2vec",
      "details": "Other wav2vec models don't work - they don't sync to audio at all",
      "from": "Kijai"
    },
    {
      "limitation": "MultiTalk lip sync timing issues",
      "details": "Tends to be off-sync for first 2 seconds and last 2 seconds of clips, works well in middle portion",
      "from": "hiroP"
    },
    {
      "limitation": "Reference image quality in WAN 2.2",
      "details": "Reference does not work very well, start image works better",
      "from": "DawnII"
    },
    {
      "limitation": "FP8 quality degradation",
      "details": "FP8 scaling still changes output quality compared to FP16",
      "from": "Kijai"
    },
    {
      "limitation": "VACE control breakdown with long videos",
      "details": "Using VACE first frame with long control video starts breaking down after 100 frames",
      "from": "TheSwoosh"
    },
    {
      "limitation": "Lightning LoRA can't handle black colors properly",
      "details": "This lora can't even do black, changes model aesthetics unlike previous distilled models",
      "from": "Kijai"
    },
    {
      "limitation": "2-step processing through 14B network insufficient",
      "details": "2 steps through a giant 14B network seems pointless, not enough processing power",
      "from": "Draken"
    },
    {
      "limitation": "Fun Control quality not as good as base model",
      "details": "Quality degrades compared to normal text to video, gets better with fewer high noise steps but never matches base quality",
      "from": "Canin17"
    },
    {
      "limitation": "Lightning LoRA doesn't work well with CFG > 1",
      "details": "Actively detrimental when CFG is higher than 1",
      "from": "Canin17"
    },
    {
      "limitation": "Wan2.2 lightning LoRAs have style bias",
      "details": "Not well trained and introduce unwanted style changes, better to use LightX LoRAs or mix",
      "from": "Kijai"
    },
    {
      "limitation": "I2V sometimes doesn't follow reference image",
      "details": "May only pick up color scheme or specific elements like smoke, inconsistent adherence to reference",
      "from": "VK (5080 128gb)"
    },
    {
      "limitation": "Fun models consistently lower quality",
      "details": "Lack compute and dataset resources to match original model quality across all releases",
      "from": "aikitoria"
    },
    {
      "limitation": "FP8 memory leak with LoRA changes",
      "details": "Changing LoRAs or porq strength causes 3x VRAM usage increase on subsequent runs without block swap",
      "from": "Kijai"
    },
    {
      "limitation": "Radial attention ruins motion quality",
      "details": "Changes movement for worse, especially problematic on early steps and low step counts",
      "from": "N0NSens"
    },
    {
      "limitation": "Sapiens pose consistency issues",
      "details": "No time consistency, won't handle complex shots with frame drops properly",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Q3 quantized models have poor quality",
      "details": "T2V generations are quite 'artistic' with low quantization",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Meta's BF16 Sapiens incompatible with modern PyTorch",
      "details": "Shouldn't work for PyTorch > 2.0",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Fire/explosion quality in Wan 2.2",
      "details": "Fire is one thing Wan 2.2 doesn't do particularly well, and 2.1 LoRAs haven't helped much",
      "from": "JohnDopamine"
    },
    {
      "limitation": "VACE color shifting",
      "details": "VACE introduces color shifting issues, especially problematic for temporal inpainting with lightx2v generated videos",
      "from": "daking999"
    },
    {
      "limitation": "Masks don't work properly in Wan 2.2",
      "details": "Masks in WanVideo Encode act more like control net or attention masks rather than true masks for i2v/t2v",
      "from": "xwsswww"
    },
    {
      "limitation": "Wan 2.2 wrapper doesn't fully follow input or change style",
      "details": "V2V with wrapper has limitations compared to Vace for style changes",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "16fps is problematic for film production",
      "details": "16fps is not a multiple of standard fps rates, creates issues for professional film work requiring 24fps",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Interpolation fails on fast action",
      "details": "All interpolation methods struggle with kinetic fast-paced action, sword fights, liquid splashing - missing subframe motion information",
      "from": "Ruairi Robinson"
    },
    {
      "limitation": "Wan 2.2 cannot mix 5B and 14B models",
      "details": "Different latent spaces prevent using 5B for first pass and 14B for second pass",
      "from": "Kijai"
    },
    {
      "limitation": "SeedVR2 extremely slow",
      "details": "Can upscale 1080p to 4K but takes couple of days to process",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Cars keep appearing despite negative prompts",
      "details": "Persistent issue where cars appear in generated videos regardless of prompting strategy",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Fun 2.2 control model lost reference input",
      "details": "The new Fun 2.2 control model no longer supports reference inputs that worked with Fun 1.1",
      "from": "Kijai"
    },
    {
      "limitation": "Fun 2.2 refuses to work beyond 81 frames",
      "details": "Model won't generate any movement when trying more than 81 frames",
      "from": "Kijai"
    },
    {
      "limitation": "Fun low noise model quality issues",
      "details": "Fun low noise doesn't finish properly, could be LoRA compatibility issue",
      "from": "Kijai"
    },
    {
      "limitation": "Wan doesn't understand object physics",
      "details": "Example: doesn't understand when skateboard is upside down",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Point editor lacks relative scaling",
      "details": "Editor shows images at original size, sucks on large images due to no relative scaling",
      "from": "Kijai"
    },
    {
      "limitation": "2.2 Lightning LoRA has severe style bias",
      "details": "Makes everything bright and oversaturated, especially bad for dark scenes, changes clothing inappropriately",
      "from": "Kijai"
    },
    {
      "limitation": "14B model cannot do 1080p while 5B can",
      "details": "Counterintuitive limitation of larger model",
      "from": "QuintForms"
    },
    {
      "limitation": "121 frame looping issue in 2.2",
      "details": "Without Skyreels LoRA, subjects won't leave frame properly in longer generations",
      "from": "Kijai"
    },
    {
      "limitation": "Fun 2.2 cannot finish at good quality",
      "details": "Needs additional sampler without Fun at end for proper quality",
      "from": "Kijai"
    },
    {
      "limitation": "14B 2.2 is not 24fps like 5B",
      "details": "Easily seen when trying I2V for 121 frames",
      "from": "Kijai"
    },
    {
      "limitation": "MultiTalk resolution limit",
      "details": "Can't use 1280x720, only works at lower resolutions like 832x480 on 5090",
      "from": "NebSH"
    },
    {
      "limitation": "2.2 I2V loops at 121 frames",
      "details": "Model tends to loop content at 121 frame length",
      "from": "Kijai"
    },
    {
      "limitation": "Skyreels LoRA may affect 2.2 motion quality",
      "details": "Probably has negative effect on the 2.2 motion, but still better than plain 2.2 on 121 frames",
      "from": "Kijai"
    },
    {
      "limitation": "Karras/exponential doesn't work with flowmatch",
      "details": "It's just opposite of what it should be with flowmatch models",
      "from": "Kijai"
    },
    {
      "limitation": "Latent upscale quality",
      "details": "Latent upscale is terrible quality",
      "from": "Kijai"
    },
    {
      "limitation": "No native VACE for Wan 2.2",
      "details": "Have to wait for official VACE for Wan 2.2",
      "from": "AmirKerr"
    },
    {
      "limitation": "Skyreels LoRA breaks looping in T2V",
      "details": "Requires increased strength but disrupts 121 frame loop functionality",
      "from": "Kijai"
    },
    {
      "limitation": "LightX2V LoRA changes faces in I2V",
      "details": "Tends to change the face of the subject when used with Wan 2.2",
      "from": "RRR"
    },
    {
      "limitation": "Upscaling without training is poor quality",
      "details": "Always gonna be terrible without training model for it",
      "from": "Kijai"
    },
    {
      "limitation": "Higher resolutions cause slow motion",
      "details": "At higher resolutions there is less motion or slow motion effects",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "limitation": "GGUF models don't work with batch size > 1",
      "details": "Problem with GGUF is batch size limitation, works with batch 1 but takes longer for videos",
      "from": "Alisson Pereira"
    },
    {
      "limitation": "Wan 2.2 doesn't have VACE equivalent",
      "details": "No official VACE release for 2.2, only experimental workflows with merged models and hacks",
      "from": "Josiah"
    },
    {
      "limitation": "Background faces mutate due to insufficient resolution",
      "details": "Model doesn't have enough pixel space for stable results in backgrounds, needs 1152p+ resolution",
      "from": "Juan Gea"
    },
    {
      "limitation": "SeedVR adds inter-frame flicker",
      "details": "SeedVR upscaling introduces temporal inconsistency, needs better temporal addon",
      "from": "HeadOfOliver"
    },
    {
      "limitation": "Torch compile first run takes more VRAM on Windows",
      "details": "Weird issue with compile in Windows - first run uses more VRAM, doesn't happen in Linux",
      "from": "Kijai"
    },
    {
      "limitation": "ComfyUI LoRA extraction doesn't do 5D tensors",
      "details": "Probably only extracts patch embed, not full model differences",
      "from": "Kijai"
    },
    {
      "limitation": "Stand-in LoRA has limited effect on Wan 2.2 high noise",
      "details": "Doesn't work well with high noise settings in 2.2",
      "from": "Kijai"
    },
    {
      "limitation": "Stand-in not working well with whole body references",
      "details": "Examples show whole body but couldn't get it to work in practice",
      "from": "Kijai"
    },
    {
      "limitation": "Motion vs likeness trade-off",
      "details": "More motion results in less likeness and vice versa",
      "from": "Instability01"
    },
    {
      "limitation": "Less motion at higher resolutions",
      "details": "Above 900x900 tends to produce less motion in videos",
      "from": "Instability01"
    },
    {
      "limitation": "Stand-in causing videos to match reference too closely in last frame",
      "details": "Not very versatile/flexible, tends to revert to reference appearance",
      "from": "Juampab12"
    },
    {
      "limitation": "Stand-in model doesn't work with Wan 2.2 LOW",
      "details": "Compatibility issue specifically with the LOW noise model in 2.2",
      "from": "Hevi"
    },
    {
      "limitation": "Wan 2.2 5B has artifacts in video output",
      "details": "Quality issues reported with the 5B model",
      "from": "army"
    },
    {
      "limitation": "LightX2V has aesthetic issues",
      "details": "Team acknowledged aesthetic problems with current Lightning LoRA",
      "from": "DawnII"
    },
    {
      "limitation": "Multitalk incredibly slow for full potential usage",
      "details": "Performance limitation prevents full utilization of multitalk capabilities",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows can cause ghosting and instability",
      "details": "Without proper camera control, context windows produce poor blending",
      "from": "Juan Gea"
    },
    {
      "limitation": "LightX2V motion issues with Wan 2.2",
      "details": "Current LightX2V distillation has acknowledged motion problems with Wan 2.2, team working on better solution",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.2 Fun Control lacks reference image support",
      "details": "Unlike Fun 2.1 v1.1, the 2.2 version doesn't support reference images in the wrapper",
      "from": "Kijai"
    },
    {
      "limitation": "Particle simulations don't work well",
      "details": "AI ignores particles in simulations, even with full model unlikely to change",
      "from": "Gill Bastar"
    },
    {
      "limitation": "Combining StandIn and VACE reference breaks generation",
      "details": "Using both controls together with same reference causes issues",
      "from": "Hashu"
    },
    {
      "limitation": "StandIn gets worse at higher resolutions",
      "details": "Performance degrades when increasing resolution beyond certain point",
      "from": "Hashu"
    },
    {
      "limitation": "FantasyPortrait struggles with anime eyes and extreme driving videos",
      "details": "Model has difficulty with certain art styles and extreme facial expressions",
      "from": "Kijai"
    },
    {
      "limitation": "Stand-In works better with single person prompts",
      "details": "Multiple people in prompts causes confusion and reduces likeness quality",
      "from": "mdkb"
    },
    {
      "limitation": "Wan 2.2 struggles with complex camera movements on certain images",
      "details": "Model unable to handle specific camera movement requests like circular tracking shots for some image types",
      "from": "\u4f0a\u7199\u5c14\u675c\u7684\u514b\u661f"
    },
    {
      "limitation": "VACE can't do lip sync, only expressions",
      "details": "VACE can handle expressions and pose control but not lip synchronization",
      "from": "mdkb"
    },
    {
      "limitation": "fp8 scaled Fun 2.2 control models don't work for reference",
      "details": "The ref_conv layer in fp8 causes silent failures for reference functionality",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows work poorly with start image",
      "details": "Context windows work better with reference image than start image for I2V",
      "from": "Kijai"
    },
    {
      "limitation": "GGUF doesn't allow LoRA merging",
      "details": "Must use LoRAs unmerged which uses more VRAM and slows down generation",
      "from": "Kijai"
    },
    {
      "limitation": "720p cannot generate small faces properly",
      "details": "Higher resolution needed for detailed facial features",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Stand-in attention masking may be very slow with SageAttention",
      "details": "SageAttention doesn't support attention masking used by stand-in",
      "from": "Kijai"
    },
    {
      "limitation": "Fun InP sliding timestep window doesn't work with released 5B weights",
      "details": "Theory is correct but implementation doesn't work in practice with current weights",
      "from": "Kijai"
    },
    {
      "limitation": "MultiTalk doesn't work well with Wan 2.2",
      "details": "Works well with 2.1 but compatibility issues with 2.2",
      "from": "Kijai"
    },
    {
      "limitation": "MultiTalk requires more than 8GB VRAM",
      "details": "Cannot run on 8GB VRAM systems",
      "from": "xwsswww"
    },
    {
      "limitation": "Wan 2.2 Lightning LoRAs destroy motion quality",
      "details": "Barely any motion despite being faster with 4 steps",
      "from": "Lodis"
    },
    {
      "limitation": "Fun Control Camera GGUF has compatibility issues",
      "details": "Tensor type mismatches and gradient errors",
      "from": "Kijai"
    },
    {
      "limitation": "2.1 LoRAs on 2.2 cause camera movement",
      "details": "Results work but camera may move away from intended position",
      "from": "Mngbg"
    },
    {
      "limitation": "Wan 2.2 High noise tends towards slow motion",
      "details": "At 81 frames sometimes produces slow motion effects",
      "from": "Ablejones"
    },
    {
      "limitation": "Fun 2.2 trajectory control doesn't work with reference input",
      "details": "Only works when using start image input",
      "from": "Kijai"
    },
    {
      "limitation": "More than 81 frames usually causes color fading",
      "details": "Light color fading occurs after a few frames in longer generations",
      "from": "Kijai"
    },
    {
      "limitation": "Fun InP model extension makes first half too static",
      "details": "When placing image in middle of video, first half remains almost static",
      "from": "Kijai"
    },
    {
      "limitation": "Fun Control model reference only in Control model",
      "details": "Reference input not available in InP or Control-Camera models",
      "from": "Kijai"
    },
    {
      "limitation": "RTX 3090 cannot use native 2.2 reliably",
      "details": "User literally cannot use native 2.2 on RTX 3090, wrapper crashes but can work",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "fp8e4nv not supported on RTX 3090 architecture",
      "details": "Must use e5m2 quantization instead for torch.compile compatibility",
      "from": "Kijai"
    },
    {
      "limitation": "WanVideoSampler extremely slow without block swap",
      "details": "Saturates VRAM fully and uses shared VRAM which barely runs",
      "from": "Kijai"
    },
    {
      "limitation": "VACE + Phantom model integration issues",
      "details": "Can completely mess up the image, suspect both VACE and Phantom being baked in causes handling differences",
      "from": "Nekodificador"
    },
    {
      "limitation": "Native nodes cause frequent recompilation",
      "details": "Recompiles even when just changing prompt, not optimal performance",
      "from": "Kijai"
    },
    {
      "limitation": "MMaudio limited to 5-8 second videos",
      "details": "Not suitable for talking people, mainly for sound effects",
      "from": ". Not Really Human ."
    },
    {
      "limitation": "700px limit for Wan 2.2 with block swap",
      "details": "1000px takes very long time on WanVideo Sampler",
      "from": "xwsswww"
    },
    {
      "limitation": "Wan 2.2 transitions happen too late in video",
      "details": "Changes occur closer to end rather than smooth transitions throughout duration",
      "from": "dir2050"
    },
    {
      "limitation": "Color shifting when saving videos",
      "details": "Videos don't preserve true colors when saved, affects both native nodes and video suite",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "limitation": "Noisy pattern in Wan 2.2 Fun variant",
      "details": "Persistent noise even with full steps, may require base 2.2 low noise instead",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "limitation": "Context windows implementation still early",
      "details": "Manual nodes only, may have errors with longer sequences",
      "from": "Lodis"
    },
    {
      "limitation": "VACE modules don't work with GGUF files",
      "details": "Cannot use VACE with Phantom GGUF, no way to include it in workflow as VACE modules don't work with GGUF files",
      "from": "mdkb"
    },
    {
      "limitation": "Context transitions not smooth in Wan 2.2",
      "details": "Flow between context slots always restarts from first frame, unlike AnimateDiff which was smooth",
      "from": "xwsswww"
    },
    {
      "limitation": "Lightning lora prompt adherence poor",
      "details": "Lightning lora doesn't listen to directional prompts like 'turns head right, then left' while lightx2v technically works",
      "from": "MysteryShack"
    },
    {
      "limitation": "Fantasy Portrait stuttering with open mouth",
      "details": "Fantasy Portrait creates involuntary movements and stuttering when source mouth stays open for extended periods",
      "from": "smithyIAN - 4080ti Super 16gig"
    },
    {
      "limitation": "WanFM requires end frame",
      "details": "The whole point of Frame Morphing is to morph from start to end frame, can't work with just start frame",
      "from": "Kijai"
    },
    {
      "limitation": "VACE and Phantom don't support GGUF",
      "details": "Can't plug non-GGUF VACE module into Phantom GGUF model",
      "from": "mdkb"
    },
    {
      "limitation": "Latent extraction for video continuation doesn't work",
      "details": "First frame encoding is handled differently, causes flash effect when using full latents",
      "from": "Kijai"
    },
    {
      "limitation": "Endless generation has poor motion quality",
      "details": "While it can go endlessly, motion quality degrades significantly, making it impractical",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.2 lacks dynamics with single image input",
      "details": "When using same image as first and last frame, output may lack dynamics compared to 2.1",
      "from": "ezMan"
    },
    {
      "limitation": "MultiTalk quality degrades over longer generations",
      "details": "Quality starts degrading before 1000 frames, 1000 frame limit exists for quality reasons",
      "from": "seitanism"
    },
    {
      "limitation": "Phantom difficult to use with other models",
      "details": "Combining Phantom with VACE can cause character consistency issues, nasty flashes and artifacts because VACE's referencing might override Phantom's",
      "from": "mdkb"
    },
    {
      "limitation": "Fighting scenes don't work well in Wan",
      "details": "Wan avoids actual fist connections in fighting sequences",
      "from": "mdkb"
    },
    {
      "limitation": "Can't combine Phantom and MultiTalk embeds",
      "details": "Each has their own embeds node feeding into same image_embeds input, no clear way to combine them",
      "from": "seitanism"
    },
    {
      "limitation": "Wan2.2 random particle/snow blips in low noise mode",
      "details": "Weird particle or snow blips happen randomly in low noise mode, making it unreliable",
      "from": "MysteryShack"
    },
    {
      "limitation": "VACE single image doesn't work with native nodes",
      "details": "Only works with wrapper implementation",
      "from": "Nekodificador"
    },
    {
      "limitation": "Context windows don't work well with I2V models",
      "details": "Every window needs start image, causing consistency issues",
      "from": "Kijai"
    },
    {
      "limitation": "VACE control doesn't follow well for single image",
      "details": "Doesn't follow control effectively with just one image",
      "from": "Kijai"
    },
    {
      "limitation": "Fast motions don't work well with last frame context approach",
      "details": "Jank between moving into next batch",
      "from": "xwsswww"
    },
    {
      "limitation": "People appear too wide/thin at 720p and other resolutions",
      "details": "Possibly caused by 2.1 LoRAs not adapting properly to 2.2",
      "from": "Fawks"
    },
    {
      "limitation": "Fantasy Portrait doesn't support video input natively",
      "details": "Only accepts image input, requires workaround with frame extraction for video-to-video",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "Fantasy Portrait arms don't move",
      "details": "Only face/mouth animation, arms remain static unless positioned in end frame",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Video LoRA training struggles with outfit variations",
      "details": "Video-only datasets insufficient for outfit diversity, needs mixed image/video training data",
      "from": "Ryzen"
    },
    {
      "limitation": "Wan 2.2 tends to over-generate talking people",
      "details": "Model bias toward generating people with open mouths/talking poses",
      "from": "Lodis"
    },
    {
      "limitation": "VACE cannot work with I2V models",
      "details": "VACE is specifically an addon for T2V models and cannot be used with I2V models unless someone retrains the whole system",
      "from": "Kijai"
    },
    {
      "limitation": "High noise extraction from 2.2 not reliable",
      "details": "People have tried to extract 2.2 as a LoRA but results are mixed at best, no reliable way to incorporate high noise yet",
      "from": "DawnII"
    },
    {
      "limitation": "AI video clips are 8-bit and fail Netflix QA",
      "details": "8-bit AI clips cannot pass automatic QA process for Netflix, faking 10-bit by adding noise may still result in rejection",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Context window creates fade in/out at 5 seconds",
      "details": "With static_standard and 6 steps, creates fade effect combining two separate clips rather than following first clip",
      "from": "Lodis"
    },
    {
      "limitation": "No Wan 2.2 MAGREF model exists yet",
      "details": "Only 2.1 MAGREF available, workflows showing 2.2 contain typos",
      "from": "Kijai"
    },
    {
      "limitation": "MTV Crafter hardcoded to 49 frames",
      "details": "Cannot generate 81 frame outputs",
      "from": "Kijai"
    },
    {
      "limitation": "Context window degrades over time",
      "details": "Current method degrades with longer generations, InfiniteTalk should fix this",
      "from": "Kijai"
    },
    {
      "limitation": "Scene cuts difficult to achieve",
      "details": "Hard to maintain causal consistency between separate generations",
      "from": "Ablejones"
    },
    {
      "limitation": "T2V tends to be repetitive without wildcards",
      "details": "Single prompt produces repetitive output in long context",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.2 avoids striking/combat actions",
      "details": "Model has restrictions with prompting for striking, likely a T5 issue, needs very specific prompts",
      "from": "Ablejones"
    },
    {
      "limitation": "MTV Crafter limited to 49 frames",
      "details": "Training limitation prevents longer generations, examples don't exceed 49 frames",
      "from": "Kijai"
    },
    {
      "limitation": "Fun Control 14B can't do hard poses",
      "details": "Model struggles with difficult pose requirements",
      "from": "Kijai"
    },
    {
      "limitation": "I2V plain generation generally just loops",
      "details": "Without proper techniques, I2V tends to produce looping content",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.2 I2V doesn't support inbetween frames",
      "details": "Can only use first and last frames, not intermediate keyframes",
      "from": "xwsswww"
    },
    {
      "limitation": "Fun Control models are 30GB",
      "details": "Too large for some users to download and use",
      "from": "mdkb"
    },
    {
      "limitation": "VACE for 5B model not available yet",
      "details": "Only base 5B model available, which has quality issues",
      "from": "Lodis"
    },
    {
      "limitation": "InfiniteTalk video-to-video not properly implemented yet",
      "details": "Glitchy results when trying v2v, needs proper developer implementation",
      "from": "JohnDopamine"
    },
    {
      "limitation": "InfiniteTalk fork conflicts with main wrapper",
      "details": "Cannot install both MeiGen-AI fork and KJ's wrapper simultaneously",
      "from": "Kijai"
    },
    {
      "limitation": "F5-TTS language limitations",
      "details": "Only handles 2 languages well, French doesn't work, has catastrophic forgetting",
      "from": "MysteryShack"
    },
    {
      "limitation": "Qwen Image Edit makes things plastic looking",
      "details": "Initial tests show plastic appearance, may need sampler experimentation",
      "from": "Lodis"
    },
    {
      "limitation": "VACE with multiple people identity issues",
      "details": "Difficulty maintaining likeness of 3 people simultaneously",
      "from": "mdkb"
    },
    {
      "limitation": "Hand hallucinations in InfiniteTalk",
      "details": "Model tends to generate incorrect hand movements and positions",
      "from": "Kijai"
    },
    {
      "limitation": "Green screen effect with MAGREF",
      "details": "Characters do not blend well with background when using MAGREF",
      "from": "Hevi"
    },
    {
      "limitation": "Transition artifacts in high motion",
      "details": "InfiniteTalk transition between windows becomes apparent with more motion",
      "from": "DawnII"
    },
    {
      "limitation": "Image degradation over time",
      "details": "Progressive blurriness and quality loss in longer generations",
      "from": "multiple users"
    },
    {
      "limitation": "8GB VRAM insufficient for most InfiniteTalk workflows",
      "details": "3070 8GB struggles even with Q6 model and blockswap, needs very low resolution and RAM",
      "from": "seitanism"
    },
    {
      "limitation": "Extra frames generated beyond audio length",
      "details": "InfiniteTalk generates extra frames causing sync issues when audio is shorter than video",
      "from": "BobbyD4AI"
    },
    {
      "limitation": "Lip movement continues after audio ends",
      "details": "Model continues lip movement during silence periods at end of audio clips",
      "from": "Tony(5090)"
    },
    {
      "limitation": "Q8 I2V model compatibility issues",
      "details": "Some Q8 I2V models not compatible, getting 'blocks.0.cross_attn.ip_adapter_single_stream_k_proj.weight' errors",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "InfiniteTalk V2V is not true V2V",
      "details": "Uses keyframing every 72 frames rather than processing full video continuously",
      "from": "DawnII"
    },
    {
      "limitation": "No proper distill for Wan 2.2 yet",
      "details": "Speed LoRAs like LightX2V work but no proper distilled version available",
      "from": "Kijai"
    },
    {
      "limitation": "VACE doesn't follow text prompts well",
      "details": "Better to grab motion with I2V workflow rather than relying on VACE text prompts",
      "from": "Piblarg"
    },
    {
      "limitation": "Context options don't work well with InfiniteTalk",
      "details": "User couldn't make context work properly, always produced strange results",
      "from": "mamad8"
    },
    {
      "limitation": "Speed LoRAs kill quality on high noise side",
      "details": "Low noise side can use them freely, but high noise side needs careful application to avoid ruining motion",
      "from": "Kijai"
    },
    {
      "limitation": "No prompt word scheduling support",
      "details": "Cannot change prompts every N frames like some users want",
      "from": "1081570872994824212"
    },
    {
      "limitation": "InfiniteTalk not officially supported on Wan 2.2",
      "details": "It's very much made for 2.1, you can brute force whatever but it's not officially supported",
      "from": "Kijai"
    },
    {
      "limitation": "No CLIP vision model works with Wan 2.2",
      "details": "When asked about CLIP vision compatibility with Wan 2.2, response was 'None for 2.2'",
      "from": "Kijai"
    },
    {
      "limitation": "Model not meant for 5 second videos",
      "details": "The whole model and multitalk node are not designed for short 5 second videos",
      "from": "Kijai"
    },
    {
      "limitation": "LightX2V LoRA causes static faces",
      "details": "Using LightX2V LoRA results in very static faces with limited movement",
      "from": "seitanism"
    },
    {
      "limitation": "InfiniteTalk doesn't support start and end images",
      "details": "Designed for continuous generation with context windows, not short clips with defined endpoints",
      "from": "seitanism"
    },
    {
      "limitation": "InfiniteTalk V2V not fully implemented",
      "details": "Current code branch doesn't handle start/end frames or proper v2v functionality",
      "from": "MysteryShack"
    },
    {
      "limitation": "Wan models not good for long detailed prompts",
      "details": "Designed for simple prompts, not for complex detailed descriptions",
      "from": "NC17z"
    },
    {
      "limitation": "Upscaling reduces expression and emotion",
      "details": "Multiple denoising passes with low model loses expressions and magic of high model",
      "from": "mamad8"
    },
    {
      "limitation": "FusionX LoRA issues with long videos",
      "details": "Exacerbates color shift over 1 minute and reduces ID preservation in videos",
      "from": "NC17z"
    },
    {
      "limitation": "I2V color shifts beyond 1 minute",
      "details": "Color shifts become more pronounced in I2V generation beyond 1 minute",
      "from": "NC17z"
    },
    {
      "limitation": "InfiniteTalk speed",
      "details": "Takes significantly longer to render compared to MultiTalk, especially for 30-second 480x480 clips",
      "from": "NC17z"
    },
    {
      "limitation": "Qwen Edit detail loss",
      "details": "Easily loses details from source image that Kontext rarely loses",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "5B model VAE is slow despite fast generation",
      "details": "The 5B model generates quickly but VAE processing remains slow, converting to fp8 won't help much as VAE weights aren't large",
      "from": "hicho"
    },
    {
      "limitation": "Wan latents don't like when you cut the first frame off",
      "details": "Trimming first frames from Wan latents can cause issues, making skip latent parameter less useful than expected",
      "from": "Kijai"
    },
    {
      "limitation": "V2V as noise not implemented in InfiniteTalk loop",
      "details": "Video-to-video with noise/denoise functionality is not implemented in the loop method and wouldn't obey denoise settings anyway",
      "from": "Kijai"
    },
    {
      "limitation": "GGUF and non-GGUF models cannot be mixed",
      "details": "Must use matching format for InfiniteTalk and base Wan models",
      "from": "Kijai"
    },
    {
      "limitation": "Background changes too much without masking in InfiniteTalk",
      "details": "Need to crop head and paste back for clean results",
      "from": "Kijai"
    },
    {
      "limitation": "Lumen is text-conditioned only",
      "details": "1.3B model only currently available",
      "from": "Kijai"
    },
    {
      "limitation": "InfiniteTalk doesn't work properly with Wan 2.2",
      "details": "Not trained for 2.2, functional but not optimal",
      "from": "DawnII"
    },
    {
      "limitation": "GGUF models don't allow merging LoRAs",
      "details": "Cannot merge LoRAs when using GGUF quantized models",
      "from": "Kijai"
    },
    {
      "limitation": "Wan doesn't work well with every different frame count",
      "details": "Model input size changes affect performance",
      "from": "Kijai"
    },
    {
      "limitation": "Vid2vid denoises whole video",
      "details": "Unlike approaches like latentsync, entire video gets denoised causing background changes",
      "from": "DawnII"
    },
    {
      "limitation": "Majority of samplers are cursed for InfiniteTalk",
      "details": "Many samplers don't work well with InfiniteTalk",
      "from": "MysteryShack"
    },
    {
      "limitation": "Fun-InP model is temporal inpainting only",
      "details": "Not spatial inpainting like VACE, only works on first to last frame transitions",
      "from": "Kijai"
    },
    {
      "limitation": "Differential diffusion works poorly for low frame counts",
      "details": "Reason unclear but consistent issue with low frame count videos",
      "from": "Kijai"
    },
    {
      "limitation": "Can't do proper I2V chaining with noisy input",
      "details": "Must finish previous window sampling before using as input for next I2V window",
      "from": "Kijai"
    },
    {
      "limitation": "Normal map control in VACE causes color bleeding",
      "details": "Makes normal map control unusable due to heavy color bleeding into other areas",
      "from": "Blink"
    },
    {
      "limitation": "Context windows work poorly with I2V",
      "details": "Each window uses input image as starting point, works better with MAGREF as reference",
      "from": "Mattis"
    },
    {
      "limitation": "VACE doesn't handle blurred masks properly",
      "details": "Only diff diff part would work with blurred masks, binary masks recommended",
      "from": "Kijai"
    },
    {
      "limitation": "Cannot separate reference and control scheduling in single VACE encode",
      "details": "Not possible to schedule reference differently from control in single encode",
      "from": "Kijai"
    },
    {
      "limitation": "VACE better for inpainting existing subjects than adding new ones",
      "details": "Works well where subject already exists, not good for adding subjects where they don't exist",
      "from": "Dream Making"
    },
    {
      "limitation": "InfiniteTalk looping doesn't work with T2V",
      "details": "The model works with T2V but the looping sampling method does not",
      "from": "Kijai"
    },
    {
      "limitation": "Fantasy Portrait fails when no face detected",
      "details": "Skips frames without detected faces, can cause processing to fail entirely",
      "from": "fredbliss"
    },
    {
      "limitation": "Infinitetalk doesn't work well on high noise model",
      "details": "Gets good prompt adherence but quality suffers significantly",
      "from": "MysteryShack"
    },
    {
      "limitation": "I2V can't do proper pull-focus or pan to person",
      "details": "Starting with image makes these camera techniques very difficult",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "WAN 2.1 LoRAs compatibility with WAN 2.2 variable",
      "details": "Most work on low noise, high noise can be variable and may need strength adjustment",
      "from": "DawnII"
    },
    {
      "limitation": "Lightning LoRA is censored",
      "details": "Provides better prompt adherence but censors content and reduces character consistency",
      "from": "MysteryShack"
    },
    {
      "limitation": "Large workflow metadata can crash Windows Explorer",
      "details": "Workflows over 200KB+ metadata can cause system crashes when viewing files",
      "from": "Ashtar"
    },
    {
      "limitation": "CineScale requires high VRAM for 4K generation",
      "details": "Runs at full 4K resolution without tiling, not practical for consumer GPUs",
      "from": "Kijai"
    },
    {
      "limitation": "Models work worse outside intended frame count",
      "details": "Phantom designed for 121 frames, performance degrades significantly at lower frame counts",
      "from": "Kijai"
    },
    {
      "limitation": "UniAnimate incompatible with GGUF models",
      "details": "UniAnimate LoRA won't work with GGUF format models",
      "from": "Kijai"
    },
    {
      "limitation": "High resolution generation causes artifacts",
      "details": "At 1080p and above, models produce extra heads and elongated limbs similar to SD1.5 issues",
      "from": "ingi // SYSTMS"
    },
    {
      "limitation": "Changing reference images per frame not easily possible",
      "details": "Would require splitting attention in code, no current setup available for frame-wise reference changes",
      "from": "Kijai"
    },
    {
      "limitation": "T2V rough with 5B turbo",
      "details": "Text-to-video generation is rough with the 5B turbo model, I2V works better",
      "from": "Kijai"
    },
    {
      "limitation": "GGUF can't merge loras",
      "details": "More loras you add, more overhead since GGUF can't merge loras unlike other formats",
      "from": "Kijai"
    },
    {
      "limitation": "Cinescale VAE has no temporal coherency",
      "details": "Can't use it for more than 1 frame as it has no temporal coherency",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows don't work properly with Wan 2.2",
      "details": "Context windows not intended to work on 2.2 model at least for now",
      "from": "Mngbg"
    },
    {
      "limitation": "Film grain node can't process 800+ frame videos",
      "details": "Throws OOM errors when trying to process very long videos",
      "from": "Kenk"
    },
    {
      "limitation": "Bidirectional sampling causes OOM even on high-end GPUs",
      "details": "Always runs into OOM errors even at low resolutions on RTX 5090",
      "from": "Roman_S"
    },
    {
      "limitation": "Wan 2.2 has ping pong effect after 81 frames",
      "details": "Video tries to revert back toward original input image after 81 frames",
      "from": "JohnDopamine"
    },
    {
      "limitation": "LightX2V LoRA doesn't work properly at 4K",
      "details": "LoRA has limitations when used at 4K resolution",
      "from": "Instability01"
    },
    {
      "limitation": "WAN 2.2 5B Turbo T2V poor performance",
      "details": "Couldn't get much quality out of the Turbo 5B T2V model",
      "from": "Kijai"
    },
    {
      "limitation": "Lightning LoRA works poorly on high noise model",
      "details": "Works fine on low noise though",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows make 2.2 unusable for some",
      "details": "Almost 3x the rendering time, described as 'like rendering 2 videos at the same time'",
      "from": "DawnII"
    },
    {
      "limitation": "Speed LoRAs incompatible with high noise model",
      "details": "LightX2V and other speedup loras don't work with high noise unless specifically trained for it",
      "from": "MysteryShack"
    },
    {
      "limitation": "Color degradation with Wan 2.2",
      "details": "Washed out colors compared to 2.1, especially noticeable in multitalk workflows",
      "from": "Kenk"
    },
    {
      "limitation": "Wan lacks good image editing capabilities",
      "details": "Insanely good as T2I model but not as good for image editing",
      "from": "Dream Making"
    },
    {
      "limitation": "FantasyPortrait doesn't combine well with InfiniteTalk",
      "details": "Head movements and expressions not being copied properly",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "limitation": "Loop sampling not meant for short videos",
      "details": "Loop sampling method not meant for 81 frame or less generations, same as context windows",
      "from": "Kijai"
    },
    {
      "limitation": "InfiniteTalk doesn't work well with smaller faces",
      "details": "InfiniteTalk alone doesn't always produce best results when face is smaller in frame",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "limitation": "VACE + Phantom overwhelming lower VRAM",
      "details": "VACE introduces too much work for Phantom workflow on 3060, runs like a dog and keels over even with reduced character count",
      "from": "mdkb"
    },
    {
      "limitation": "Context windows don't work with I2V",
      "details": "Only works with T2V generations",
      "from": "Kijai"
    },
    {
      "limitation": "Face detection failures",
      "details": "Fantasy face detector sometimes fails even on seemingly fine images, dependent on underlying face detector",
      "from": "Kijai"
    },
    {
      "limitation": "Color degradation with InfiniteTalk",
      "details": "Colors get washed out when using InfiniteTalk, especially on both high and low samplers",
      "from": "N0NSens"
    },
    {
      "limitation": "African language lip sync not working",
      "details": "Certain languages don't work well with the lip sync models",
      "from": "Kenk"
    },
    {
      "limitation": "16GB RAM insufficient for Wan 2.1 14B",
      "details": "Even with Q4 GGUF, system freezes due to RAM limitations",
      "from": "Akumetsu971/Kijai"
    },
    {
      "limitation": "InfiniteTalk context windows don't work with extensions",
      "details": "Context windows non-functional when using with extend nodes",
      "from": "JalenBrunson"
    },
    {
      "limitation": "InfiniteTalk coherence degrades toward end",
      "details": "Loses coherence near end of extended generations",
      "from": "JalenBrunson"
    },
    {
      "limitation": "LoRA training limitations",
      "details": "Text encoder training not happening, so CLIP output likely does nothing",
      "from": "Kijai"
    },
    {
      "limitation": "VACE underutilized",
      "details": "Many users not taking full advantage of VACE capabilities",
      "from": "Nekodificador"
    },
    {
      "limitation": "VACE struggles with position/angle changes from reference image",
      "details": "Cannot handle replacing characters if reference image is at different angle than target position",
      "from": "mdkb"
    },
    {
      "limitation": "Lightning 2.2 LoRAs don't work well on high noise model",
      "details": "Can't achieve high dynamic results when adding lightning lora to high noise model",
      "from": "piscesbody"
    },
    {
      "limitation": "InfiniteTalk needs new model to work properly with 2.2",
      "details": "Can sort of work already but has very weak effect on the high noise model",
      "from": "Kijai"
    },
    {
      "limitation": "VACE temporal extension causes facial features drift",
      "details": "Faces change and grow makeup in porn-ish overdone way even without porn loras",
      "from": "lostintranslation"
    },
    {
      "limitation": "2.2 Lightning LoRAs don't work well yet",
      "details": "Still a harder problem to solve, took months for 2.1 distill LoRAs",
      "from": "Kijai"
    },
    {
      "limitation": "Infinite Talk causes gradual video degradation in I2V mode",
      "details": "Quality degrades over time, possibly due to the LoRA",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "limitation": "Wan 2.2 commercial overlay prompting not available in open source",
      "details": "Text overlay removal and instructions only work in commercial version",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "limitation": "VACE 2.1 has ghosting effects and control input mixing with output",
      "details": "Sometimes control input mixed with output result, inconsistent quality",
      "from": "Lodis"
    },
    {
      "limitation": "Lipsync with VACE has drift problems",
      "details": "Can almost nail it but doesn't quite work perfectly, similar issue with fakevace",
      "from": "mdkb"
    },
    {
      "limitation": "Gender role reversal impossible",
      "details": "No AI model can generate woman lifting man, all default to man lifting woman instead",
      "from": "seitanism"
    },
    {
      "limitation": "2.2 + infinite context causes memory issues",
      "details": "Won't work in general with any 2 sampler setup, causes OOM",
      "from": "Kijai"
    },
    {
      "limitation": "New S2V model likely incompatible with other components",
      "details": "Little chance for compatibility with other stuff due to frame pack method",
      "from": "Kijai"
    },
    {
      "limitation": "T2V quality inconsistent with current distill LoRAs",
      "details": "Can get good outputs then next prompt is crap, balance between motion/speed/quality difficult",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.2 I2V cannot generate lightning",
      "details": "If there's no lightning in the original image, you can't get lightning to appear in wan 2.2 i2v",
      "from": "MysteryShack"
    },
    {
      "limitation": "S2V model produces static results",
      "details": "Tests show just static dolls talking, does little motion on its own",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "limitation": "WanFM has VRAM requirements",
      "details": "Causes OOM even on A100 80GB in some configurations",
      "from": "Yan"
    },
    {
      "limitation": "Color shift in long generations",
      "details": "28-second examples show slight color shift over time (gets more cyan)",
      "from": "Screeb"
    },
    {
      "limitation": "Wan.video refuses to generate without human detected in input",
      "details": "For S2V, requires human detection in input image",
      "from": "Juampab12"
    },
    {
      "limitation": "S2V model heavier on VRAM than 2.2 A14B",
      "details": "Requires more VRAM despite being newer",
      "from": "Kijai"
    },
    {
      "limitation": "Hardcoded FPS values causing confusion",
      "details": "Input fps locked to 50, unclear why this value was chosen",
      "from": "Kijai"
    },
    {
      "limitation": "S2V focused on human speech only",
      "details": "Model's clear intent is humans talking, not general environmental sounds",
      "from": "MilesCorban"
    },
    {
      "limitation": "S2V no long generation method yet",
      "details": "Can do audio gen and ref image but missing long video generation capability",
      "from": "Kijai"
    },
    {
      "limitation": "S2V lower quality than InfiniteTalk",
      "details": "Current S2V implementation produces worse results than InfiniteTalk",
      "from": "ArtOfficial"
    },
    {
      "limitation": "S2V resolution constraints",
      "details": "Specific formula required for resolutions to work, many combinations fail with einops errors",
      "from": "patientx"
    },
    {
      "limitation": "S2V mouth movement looks weird",
      "details": "Mouth generally looks off with S2V and 16fps output feels unnatural",
      "from": "Kijai"
    },
    {
      "limitation": "VACE+Phantom model compatibility",
      "details": "Combined VACE+Phantom GGUF model doesn't work well with WanVideoWrapper or isn't compatible with WAN 2.2",
      "from": "xwsswww"
    },
    {
      "limitation": "S2V framepack implementation incomplete",
      "details": "Framepack implementation is beyond current capabilities and not fully working",
      "from": "Kijai"
    },
    {
      "limitation": "Model has strict resolution auto-selection",
      "details": "The code has strict automatic resolution selection that limits flexibility",
      "from": "Kijai"
    },
    {
      "limitation": "Model has degradation in long generation",
      "details": "these methods always have some degradation when it's constantly encoded and decoded",
      "from": "Kijai"
    },
    {
      "limitation": "S2V has distortions at start",
      "details": "distortions on start are caused by static img to vid, it's because of the extension method, in the original code they just drop the first frames",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows with Fun Control causes errors",
      "details": "When scaling back the end percent below .5, it throws errors. With just 81 frames its fine",
      "from": "Guey.KhalaMari"
    },
    {
      "limitation": "Tongue never moves in lipsync models",
      "details": "One thing I notice with all these lip-sync model is that the tongue never moves. lol Makes it look wierder when you study it",
      "from": "Guey.KhalaMari"
    },
    {
      "limitation": "Start-end frame morphing issues",
      "details": "the video stays the same up until the last frame where it changes to the end image",
      "from": "Dream Making"
    },
    {
      "limitation": "S2V doesn't blink naturally",
      "details": "Character doesn't blink during speech generation",
      "from": "Kijai"
    },
    {
      "limitation": "S2V quality degrades over long videos",
      "details": "Quality bakes and degrades significantly over 800 frames, becoming very bad",
      "from": "Kijai"
    },
    {
      "limitation": "Context window overlap bad for lipsync",
      "details": "The overlap in context windows negatively affects lip synchronization quality",
      "from": "Kijai"
    },
    {
      "limitation": "S2V movement less natural than InfiniteTalk",
      "details": "Character movement appears more stiff and less natural compared to InfiniteTalk",
      "from": "Rainsmellsnice, Impactframes."
    },
    {
      "limitation": "InfiniteTalk color degradation after 1 minute",
      "details": "Starts falling apart at 1 minute, becomes very weird by 2 minutes",
      "from": "boorayjenkins"
    },
    {
      "limitation": "Custom LoRAs need 33+ frames in wrapper but only 5 in native",
      "details": "LoRAs trained on 5 frames don't work with small frame counts in wrapper",
      "from": "mamad8"
    },
    {
      "limitation": "S2V has stop motion/lagging issues",
      "details": "16fps output creates choppy motion compared to 25fps MultiTalk",
      "from": "hicho"
    },
    {
      "limitation": "VACE color shift issues",
      "details": "Color Match helps but doesn't solve extreme cases, training seepage affects functionality",
      "from": "pom"
    },
    {
      "limitation": "S2V code feels unfinished",
      "details": "Includes lots of unused code, messy implementation compared to other Wan models",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows reduce lipsync quality with framepack",
      "details": "Overlap in framepack method degrades lip sync accuracy",
      "from": "Kijai"
    },
    {
      "limitation": "HunyuanVideo-Foley not significantly better than MMAudio",
      "details": "Similar quality, uses 4-5 models making it complex to implement properly",
      "from": "Kijai"
    },
    {
      "limitation": "PUSA limited diversity",
      "details": "Only finetuned on 4000 videos so diversity is probably limited",
      "from": "fredbliss"
    },
    {
      "limitation": "S2V model poor at T2V generation and motion",
      "details": "Model opposed to motions like walking, video extensions don't follow prompts, speed LoRAs cause blur",
      "from": "flo1331"
    },
    {
      "limitation": "Control signals only work with Fun-Control model",
      "details": "Cannot use control inputs like Canny, Depth, Pose with regular 1.3B or other base models",
      "from": "N0NSens"
    },
    {
      "limitation": "ComfyUI inefficient with long videos",
      "details": "Stores all frames in RAM simultaneously in fp32, making 4K video handling nearly impossible",
      "from": "Kijai"
    },
    {
      "limitation": "S2V model limited motion compared to other lip sync methods",
      "details": "S2V produces stiffer results compared to MAGREF + Multi or InfiniteTalk for lip sync",
      "from": "Kijai"
    },
    {
      "limitation": "Framepack only works with S2V model",
      "details": "Cannot apply framepack context sliding to Wan 2.1/2.2 models as it has its own specific weights",
      "from": "Kijai"
    },
    {
      "limitation": "Action LoRAs don't work with S2V",
      "details": "Motion/action loras won't work with S2V, only style and character loras work",
      "from": "Kenk"
    },
    {
      "limitation": "Infinite Talk works badly on high noise model",
      "details": "Infinite talk works very badly on the high noise model, but works fine on low noise - more like vid2vid",
      "from": "Kijai"
    },
    {
      "limitation": "Index out of bounds error over 100 steps",
      "details": "Getting 'index out of bounds' error when using over 100 steps with Wan and VACE",
      "from": "Neex"
    },
    {
      "limitation": "uni3c affects whole generation",
      "details": "uni3c has effect on whole thing and not only the camera, also kills some motion sometimes",
      "from": "N0NSens"
    },
    {
      "limitation": "uni3c only works with I2V",
      "details": "uni3c doesn't work with T2V, only with I2V for camera control",
      "from": "hicho"
    },
    {
      "limitation": "Framepack degrades over longer generations",
      "details": "Goes bad the longer it goes, need weak settings or careful balance",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows have worse lipsync",
      "details": "Better quality for long gens but lipsync isn't as good as Framepack",
      "from": "Kijai"
    },
    {
      "limitation": "Uni3C only works with I2V",
      "details": "Cannot be used with VACE workflows, only I2V",
      "from": "DawnII"
    },
    {
      "limitation": "AccVid LoRA causes artifacts",
      "details": "Creates overlap artifacts in InfiniteTalk outputs",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "InfiniteTalk creates noise frames",
      "details": "Makes weird noise frames at head of shot despite better mouth movements",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "limitation": "Context windows ghosting with movement",
      "details": "Can have ghosting transitions with lots of movement or camera movement",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "VACE doesn't continue movement",
      "details": "Quality is also really bad for video continuation",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Context windows increase inference time significantly",
      "details": "All overlap areas have extra computational cost",
      "from": "Kijai"
    },
    {
      "limitation": "S2V severely impacted by distillations",
      "details": "Changes significantly from original with distilled models",
      "from": "Ablejones"
    },
    {
      "limitation": "VACE masks only support binary",
      "details": "Cannot use blurred masks",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Context windows don't work with bridge",
      "details": "Would need a node to combine embeds for bridge compatibility",
      "from": "Kijai"
    },
    {
      "limitation": "I2V works poorly with context windows",
      "details": "Due to training to always reference first frame at beginning of each window",
      "from": "Kijai"
    },
    {
      "limitation": "FantasyPortrait doesn't follow iris well",
      "details": "FP projection doesn't track eye movements accurately",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Wan 2.2 denoises input images",
      "details": "I2V removes grain/noise from input images, trained on high quality material only",
      "from": "Lodis"
    },
    {
      "limitation": "Speed optimizations break Wan 2.2 quality",
      "details": "Any speedup tricks completely destroy quality in 2.2",
      "from": "MysteryShack"
    },
    {
      "limitation": "InfiniteTalk doesn't work with long gen loop",
      "details": "For T2V, InfiniteTalk works for short gens, maybe with context windows for long gens, but doesn't work with the long gen loop",
      "from": "Kijai"
    },
    {
      "limitation": "Beyond 45-81 frames causes quality degradation",
      "details": "When increasing frame number beyond 45 frames, results get really bad with blurred smoke-like diffusion. Max 81 is stable without extension methods",
      "from": "ByArlooo/Mu5hr00m_oO"
    },
    {
      "limitation": "ComfyUI native Latent nodes not compatible with video latents",
      "details": "Most of the ComfyUI native Latent modification nodes are not compatible with video latents, like Crop latent wasn't working because it wasn't considering the frames dimension",
      "from": "Ablejones"
    },
    {
      "limitation": "Context window issues with dynamic camera movements",
      "details": "Context windows work really bad with camera moves. When camera movements are too dynamic, it gets confused when pose isn't long enough",
      "from": "Kijai/ArtOfficial"
    },
    {
      "limitation": "InfiniteTalk affects rest of video beyond face",
      "details": "Need automated way to mask just face/lower face back in, currently labor intensive in After Effects",
      "from": "Geoff"
    },
    {
      "limitation": "Save Image With Alpha might not work with animated sequences",
      "details": "Kijai notes it might not work at all for PNG sequences with alpha channel",
      "from": "Kijai"
    }
  ],
  "hardware": [
    {
      "requirement": "RAM usage for training",
      "details": "81GB RAM used for HiDream training batch size 8 at 1024p",
      "from": "tarn59"
    },
    {
      "requirement": "Generation performance",
      "details": "121 frames 512x512 with LightX2V in 30s, uses 70% RAM caching both High and Low noise models",
      "from": "tarn59"
    },
    {
      "requirement": "VRAM for different model sizes",
      "details": "Q6 quantized models work well at 720x480, takes 2 minutes to generate with 16GB VRAM",
      "from": "1013738790742925343"
    },
    {
      "requirement": "Memory consumption scaling",
      "details": "User experiencing 27GB->29GB->32GB RAM usage progression leading to system freeze",
      "from": "homem desgraca"
    },
    {
      "requirement": "Storage space for models",
      "details": "User filled 2TB drive in a week with WAN-related models and training",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "VRAM for upscaling",
      "details": "RTX 5090 can handle 1.5x upscaling for 77 frames, 2x possible for 16 frames",
      "from": "GOD_IS_A_LIE"
    },
    {
      "requirement": "Memory management with --cache-none",
      "details": "Using --cache-none flag reduces RAM usage significantly but requires reloading models",
      "from": "Kijai"
    },
    {
      "requirement": "bf16 support for T5",
      "details": "AMD RX 6800 falls back to fp32 when bf16 not supported, making T5 very slow",
      "from": "patientx"
    },
    {
      "requirement": "VRAM for context windows",
      "details": "Context windows don't increase VRAM usage - that's the main benefit of the feature",
      "from": "Kijai"
    },
    {
      "requirement": "AMD ZLUDA performance",
      "details": "AMD Radeon 8060S can run 1280x704 121 frames in 20 minutes with proper setup",
      "from": "nacho.money"
    },
    {
      "requirement": "RTX 5090 native vs wrapper",
      "details": "Native workflows much slower than wrapper on RTX 5090 - 20+ minutes vs 400 seconds for same output",
      "from": "IceAero"
    },
    {
      "requirement": "RAM management options",
      "details": "Can use --cache-none launch option to disable all caching and free RAM completely between model switches",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM for 5B model",
      "details": "RTX 5070 Ti with 128GB RAM can handle I2V at 2.5 minutes for 5x5 steps",
      "from": "loopen44"
    },
    {
      "requirement": "Virtual memory for crashes",
      "details": "Should set virtual memory higher than 64GB unless you have more than 64GB RAM",
      "from": "Ablejones"
    },
    {
      "requirement": "14B model VRAM",
      "details": "Works on systems, maxes out 5090 without headroom during ksampler",
      "from": "hicho"
    },
    {
      "requirement": "L40S performance",
      "details": "445 seconds for 832x480 generation",
      "from": "AJO"
    },
    {
      "requirement": "5090 performance benchmarks",
      "details": "30 mins for 5 seconds native 14B at 1280x703, 2-3 mins with wrapper+LightX",
      "from": "AJO"
    },
    {
      "requirement": "4080 12GB performance",
      "details": "2 minutes max for 5 second videos",
      "from": "Lodis"
    },
    {
      "requirement": "RAM configuration",
      "details": "Z790 boards support up to 256GB DDR5, but speed drops with more RAM. 4x64GB may drop to 5600-5200MHz from rated 6400MHz",
      "from": "AJO"
    },
    {
      "requirement": "RTX 3060 12GB performance",
      "details": "15 minutes for T2V generation, 9 minutes when models already loaded",
      "from": "Abx"
    },
    {
      "requirement": "Memory management for low VRAM",
      "details": "32GB swap files, --disable-smart-memory and --cache-none flags help prevent OOM",
      "from": "mdkb"
    },
    {
      "requirement": "AMD 8060S performance",
      "details": "20 minutes for 7 second video at 640x352, with 64GB dedicated + 32GB shared GPU memory",
      "from": "nacho.money"
    },
    {
      "requirement": "Sage Attention GPU compatibility",
      "details": "Version 2++ requires RTX 30XX and above, minimal benefit on RTX 20XX series",
      "from": "mdkb"
    },
    {
      "requirement": "VRAM management for upscaling",
      "details": "Need block swap at 20 for high-res processing to avoid choking VRAM",
      "from": "thaakeno"
    },
    {
      "requirement": "5B model performance on 3090",
      "details": "Takes 7 minutes for generation, quite slow compared to other models",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "30 second generation time",
      "details": "Kijai managing 30-35 second generations, likely on high-end hardware",
      "from": "Kijai"
    },
    {
      "requirement": "H100 performance",
      "details": "1x H100 can run 25 step generation, SeedVR2 7B brutal even on H100 for 720 to 1080 upscaling with max batch size around 40",
      "from": "topmass"
    },
    {
      "requirement": "3090 VRAM limits",
      "details": "Max resolution 960x656 for 49 frames without blockswap/gguf on 3090",
      "from": "daking999"
    },
    {
      "requirement": "5090 training limitations",
      "details": "Using 320 resolution for low model due to 5090 VRAM constraints",
      "from": "mamad8"
    },
    {
      "requirement": "VRAM for native 1920x1088, 81 frames",
      "details": "52GB VRAM, 400 seconds render time on RTX 6000 Pro",
      "from": "Fill"
    },
    {
      "requirement": "System RAM usage",
      "details": "Up to 96GB system RAM used with certain configurations, 60GB typical with Wan",
      "from": "Fill"
    },
    {
      "requirement": "Sage attention performance",
      "details": "2 minutes for full video at 960x720, 5 minutes at 1280x720",
      "from": "Ryzen"
    },
    {
      "requirement": "H100 optimization",
      "details": "Use auto mode for sage attention, fp16 dtype for fast accumulation",
      "from": "Kijai"
    },
    {
      "requirement": "RAM for Wan 2.2",
      "details": "User upgraded to 128GB RAM due to OOM issues, average use around 98GB with full 40 block swap",
      "from": "VK"
    },
    {
      "requirement": "RTX 4090 capabilities",
      "details": "Can run 14B with blockswap, 1920x1080 generation takes 8 minutes",
      "from": "Ryzen"
    },
    {
      "requirement": "AM5 motherboard RAM compatibility",
      "details": "AM5 doesn't like 4 sticks of RAM very much, may need specific configurations",
      "from": "phazei"
    },
    {
      "requirement": "Performance scaling",
      "details": "640x480 to 704x1280 (3x pixels) took 4x time instead of expected 3x",
      "from": "phazei"
    },
    {
      "requirement": "14B model VRAM usage",
      "details": "20 block swap uses max 24.916 GB VRAM for 81 frames at 1280x720",
      "from": "Kijai"
    },
    {
      "requirement": "fp16 on 6GB VRAM",
      "details": "6GB VRAM + 64GB RAM requires increasing virtual memory to avoid disconnection",
      "from": "hicho"
    },
    {
      "requirement": "B200 performance",
      "details": "25 minutes generation time on B200, slower than expected due to model chunks",
      "from": "aikitoria"
    },
    {
      "requirement": "VRAM for character LoRA training",
      "details": "L40S getting OOM errors, specific requirements unclear",
      "from": "Santoshyandhe"
    },
    {
      "requirement": "Maximum resolution tested",
      "details": "2560x1536 for 81 frames using 5B model upscaling",
      "from": "Juan Gea"
    },
    {
      "requirement": "3072x1840 resolution achievable",
      "details": "Using 5B model with block swap 30, 30 steps, 16 iterations, 53 seconds per iteration",
      "from": "Juan Gea"
    },
    {
      "requirement": "1280x780 generation time",
      "details": "1 min 26s on 5090",
      "from": "Kijai"
    },
    {
      "requirement": "Training compute estimate",
      "details": "About $10k run for 14B model training, $20k for both high/low models",
      "from": "aikitoria"
    },
    {
      "requirement": "h1111 generation time",
      "details": "1 hour with 4090 for generation",
      "from": "Benjimon"
    },
    {
      "requirement": "KJ wrapper performance improvement",
      "details": "Times went down from 125s to 80s for 5s 512x512 gen, using 6 steps vs 4 steps",
      "from": "Rainsmellsnice"
    },
    {
      "requirement": "Cold boot time on Modal",
      "details": "20 seconds cold boot into UI",
      "from": "Karo"
    },
    {
      "requirement": "Model loading time on network volume",
      "details": "Network volume causes 1-2 minutes billing just for loading model into memory",
      "from": "gokuvonlange"
    },
    {
      "requirement": "RAM usage with LoRA scheduling",
      "details": "64GB starting to look like minimum rather than comfortable amount. Memory usage varies: 27 blocks with merge, 33-35 without merge",
      "from": "Rainsmellsnice"
    },
    {
      "requirement": "Ampere card compatibility",
      "details": "3090s and other Ampere cards need e5 fp8 weights or they freak out",
      "from": "samhodge"
    },
    {
      "requirement": "5090 performance with Lightning",
      "details": "1280x720x81, 4/4 euler, new loras, 262 seconds generation time",
      "from": "IceAero"
    },
    {
      "requirement": "5B model VRAM for reasonable times",
      "details": "Card doesn't have enough VRAM for reasonable generation times with 5B model",
      "from": "QANICS\ud83d\udd50"
    },
    {
      "requirement": "VAE decode performance with 5.5B model",
      "details": "VAE decode takes very long time with the 5.5B model due to large VAE file size",
      "from": "cocktailprawn1212"
    },
    {
      "requirement": "Generation time",
      "details": "4-step generation in 36.92 seconds, 14 minute gen time for 720x1280",
      "from": "TK_999"
    },
    {
      "requirement": "VRAM usage",
      "details": "11GB for T5 text encoder when not using cached embeds",
      "from": "Doctor Shotgun"
    },
    {
      "requirement": "FastWan 5B VRAM usage",
      "details": "Can generate 121 frames at 1280x704 in ~34 seconds total (14s sampling + 20s decode)",
      "from": "Kijai"
    },
    {
      "requirement": "High resolution with 5B model",
      "details": "24GB VRAM with 30 blocks out enables 3072 resolution generation",
      "from": "Juan Gea"
    },
    {
      "requirement": "RTX 5090 performance",
      "details": "Used for multi-prompt iteration batching and fast generation testing",
      "from": "Purz"
    },
    {
      "requirement": "Fast 5B VRAM usage",
      "details": "6 steps on 1280x704x121 frames uses max 13.043 GB allocated memory, 13.875 GB reserved",
      "from": "N0NSens"
    },
    {
      "requirement": "3060 12GB can run high res",
      "details": "832x480 done in 8 mins, 1600x960 possible but challenging on 3060 12GB with 32GB system RAM",
      "from": "mdkb"
    },
    {
      "requirement": "3090 performance",
      "details": "1600x960 resolution takes 10 minutes on 3090 with 64GB RAM",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "requirement": "Compilation time impact",
      "details": "Compiling VAE took almost 2 mins, transformer blocks about 10 seconds first run",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM usage",
      "details": "720p takes 40 mins, Q5 models give 2GB VRAM headroom vs Q6",
      "from": "mdkb"
    },
    {
      "requirement": "Generation speed",
      "details": "832x832x81 frames in 4:32 on optimized setup",
      "from": "Ada"
    },
    {
      "requirement": "4090 performance",
      "details": "337 frames with 8 steps takes ~10 mins on 4090",
      "from": "Kijai"
    },
    {
      "requirement": "FastWan performance",
      "details": "9 second generation but 20 second decode time",
      "from": "Kijai"
    },
    {
      "requirement": "Upscaling to 1080p VRAM usage",
      "details": "Uses 23.5GB VRAM when upscaling to 1080p with Wan 2.2 14B on L4 GPU",
      "from": "thaakeno"
    },
    {
      "requirement": "Context generation speed",
      "details": "337 frames generated in 10 minutes on 4090 with lightx2v LoRA",
      "from": "kendrick"
    },
    {
      "requirement": "5B model on 5080",
      "details": "I2V generation time varies significantly based on settings and complexity",
      "from": "lomerio"
    },
    {
      "requirement": "3090 torchcompile compatibility",
      "details": "fp8_e4m3fn hangs on 3090s due to inductor issue, need to use e5 instead",
      "from": "Kijai"
    },
    {
      "requirement": "Storage consumption",
      "details": "ComfyUI folder reached 700GB in 7 days, filled entire 2TB drive",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "24GB VRAM for WAN 2.2 fp8",
      "details": "Can fit 81 frames at 480p without block swap on RTX 4090",
      "from": "BobbyD4AI"
    },
    {
      "requirement": "16GB VRAM VAE optimization",
      "details": "Using 512x384 tile sizes cuts VAE decode time in half on 16GB cards",
      "from": "patientx"
    },
    {
      "requirement": "Torch compile increases disk usage",
      "details": "Compiled models cache to disk but size is negligible compared to model loading time savings",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM for 5B model",
      "details": "Different VRAM requirements due to different latent space compared to other Wan models",
      "from": "Kijai"
    },
    {
      "requirement": "24GB VRAM sufficient for batch generation",
      "details": "Can run batch video generation with 24GB VRAM",
      "from": "Gill Bastar"
    },
    {
      "requirement": "12GB 3060 performance",
      "details": "Can upscale to 1600x900x81 frames in 27 minutes, improvement over previous 49 frame limit",
      "from": "mdkb"
    },
    {
      "requirement": "RTX 4090 VRAM for WAN 2.2 5B",
      "details": "Can run 1280x720 locally, but struggles with higher resolutions",
      "from": "Persoon"
    },
    {
      "requirement": "Mac M4 compatibility issues with 5B",
      "details": "Even with 128GB RAM, OOMs on VAE due to fp32 requirements",
      "from": "wwlee."
    },
    {
      "requirement": "RTX 3090 confirmed working",
      "details": "With SageAttention, WAN 2.2 Q8 GGUF confirmed working",
      "from": "Josiah"
    },
    {
      "requirement": "VRAM for context window upscaling",
      "details": "High-res 10s video upscaling with Wan 14B requires context windows and is very slow",
      "from": "thaakeno"
    },
    {
      "requirement": "VRAM for extended generations",
      "details": "Can do more than 81 frames with prompt splitting if you have the VRAM",
      "from": "Kijai"
    },
    {
      "requirement": "5090 generation time",
      "details": "1 min 30s for 5 second video on RTX 5090",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM for VAE precision",
      "details": "FP32 VAE uses twice the memory of FP16, rent 80GB GPU for testing",
      "from": "Kijai"
    },
    {
      "requirement": "High resolution generation",
      "details": "16GB VRAM can handle 1344x768 at 5 seconds",
      "from": "N0NSens"
    },
    {
      "requirement": "Rife TensorRT performance",
      "details": "5 seconds for 720p 16->32 frame conversion on RTX 4090, 2 seconds to process 10 seconds of video",
      "from": "aipmaster"
    },
    {
      "requirement": "1600x900x81 generation on 3060 12GB",
      "details": "Achieved with fp8_e5m2, 32GB system RAM, swap file on SSD, --disable-smart-memory. Takes 27 minutes",
      "from": "mdkb"
    },
    {
      "requirement": "1280x614px generation on 4060TI 16GB",
      "details": "350 seconds render time with 64GB RAM",
      "from": ": Not Really Human :."
    },
    {
      "requirement": "Memory usage with 128GB RAM",
      "details": "Usually 56% used when generating, upgrade from 64GB for comfort",
      "from": "phazei"
    },
    {
      "requirement": "RAM speed limitations with 4 sticks",
      "details": "Can't use max speed with 4 sticks, had to choose between faster 64GB or regular 128GB",
      "from": "kendrick"
    },
    {
      "requirement": "3000 series GPU compilation",
      "details": "3000 series NVIDIA GPUs can't use torch.compile with e4m3fn weights",
      "from": "Kijai"
    },
    {
      "requirement": "4K upscaling VRAM",
      "details": "Even with blockswap on 40, 4K upscaling took more than 50GB VRAM",
      "from": "thaakeno"
    },
    {
      "requirement": "RTX 5090 upgrade consideration",
      "details": "User considering 4-5k upgrade from 3090 to 5090 for better performance",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "RTX PRO 6000 performance",
      "details": "1280x720, 81 frames took 2 minutes",
      "from": "HeadOfOliver"
    },
    {
      "requirement": "5090 rental performance",
      "details": "1024x576 takes 10 minutes, 20 minutes estimated for 1280x720 81 frames due to block swapping",
      "from": "MysteryShack"
    },
    {
      "requirement": "H200 capability",
      "details": "Can do 500 frames at 960x544 without context",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "fp16 vs fp8 VRAM usage",
      "details": "fp16 requires more offloading, fp8 is faster due to less offloading needed",
      "from": "Kijai"
    },
    {
      "requirement": "High resolution I2V",
      "details": "1280x1400p achievable with fp16 and block swap 40",
      "from": "xwsswww"
    },
    {
      "requirement": "High resolution generation",
      "details": "1536x768 generation completed in 11 minutes using quantized Q5_K_M model",
      "from": ": Not Really Human :"
    },
    {
      "requirement": "Phantom workflow resource usage",
      "details": "Very resource intensive according to user testing",
      "from": "samhodge"
    },
    {
      "requirement": "VRAM optimization",
      "details": "Merge LoRA setting saves VRAM, quantized models help with large resolutions",
      "from": "pagan"
    },
    {
      "requirement": "Fun Control model VRAM",
      "details": "Original model needs 28.6GB, fp8 scaled version needs 14.5GB",
      "from": "Kijai"
    },
    {
      "requirement": "3090 compatibility",
      "details": "Can run fp8 scaled Fun models at 14.5GB",
      "from": "Rainsmellsnice"
    },
    {
      "requirement": "Model switching speed",
      "details": "High to low model switching takes 15-25 seconds with 12GB VRAM and 24GB system RAM",
      "from": "PizzaSlice"
    },
    {
      "requirement": "AMD optimization",
      "details": "96GB VRAM AMD card can remove block transfer and force unload for better utilization",
      "from": "nacho.money"
    },
    {
      "requirement": "RAM usage for WAN 2.2 Fun",
      "details": "Can reach 60-80GB RAM usage, T5 removal saves 10-20GB",
      "from": "Gonzo"
    },
    {
      "requirement": "VRAM for WAN 2.2 Fun",
      "details": "RTX 3090 gets OOM at 720x480 81 frames with FP8 e5m2 scaled",
      "from": "Josiah"
    },
    {
      "requirement": "Performance on RTX 5090",
      "details": "Native: 5 seconds, Wrapper: 10 seconds for same generation",
      "from": "pagan"
    },
    {
      "requirement": "4K upscaling with Wan 2.2 14B",
      "details": "NOT recommended - causes system overload and overheating",
      "from": "thaakeno"
    },
    {
      "requirement": "VRAM for 600 frame generation",
      "details": "Would need 96GB VRAM for extended frame generation at good quality",
      "from": "VK (5080 128gb)"
    },
    {
      "requirement": "5090 vs 4090 performance",
      "details": "FP8_fast significantly faster on RTX 5090 than RTX 4090 percentually",
      "from": "Kijai"
    },
    {
      "requirement": "sageattn compatibility",
      "details": "sageattn_qk_int8_pv_fp16_triton only option working with cuda128+torch2.8.0, other options cause bfloat16 errors",
      "from": "trax"
    },
    {
      "requirement": "Sapiens processing speed",
      "details": "5 minutes for 192 frame video on RTX 4090, can achieve 57.2 FPS average with optimizations",
      "from": "fredbliss"
    },
    {
      "requirement": "BFloat16 GPU compatibility",
      "details": "Works best with Ampere+ GPUs (RTX 3090, A100) for 50% memory reduction",
      "from": "fredbliss"
    },
    {
      "requirement": "Sapiens batch processing",
      "details": "Can handle 48 bboxes in parallel, batch processing crucial for multi-person scenarios",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "VRAM for Wan 2.2",
      "details": "Can run on RTX 2080 with 8GB VRAM, but requires 64GB system RAM. RTX 6000 Pro can run everything native",
      "from": "xwsswww"
    },
    {
      "requirement": "RAM requirements",
      "details": "32GB RAM is very low for Wan 2.2, 64GB minimum recommended, 96GB comfortable for frequent generation",
      "from": "Rainsmellsnice"
    },
    {
      "requirement": "High resolution processing",
      "details": "1920x1080 v2v took over an hour on step 0 with 100GB RAM usage, while 1280x720 took 15 minutes",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "SeedVR2 VRAM scaling",
      "details": "RTX 4090: 720x720 to 960x960 max, RTX 3090 with 64GB RAM: 720p->1080p uses 98% VRAM",
      "from": "Gill Bastar"
    },
    {
      "requirement": "Wan 2.2 5B performance",
      "details": "22 minutes for 1080x1920 on RTX 4090, ~20 seconds VAE decoding for 1280x704 121 frames",
      "from": "QuintForms"
    },
    {
      "requirement": "Topaz VRAM usage",
      "details": "Heaviest model uses maybe 16GB VRAM, uses 20-30% GPU on 5090, can run on 4GB VRAM cards",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "RAM for fp8 vs GGUF models",
      "details": "64+ GB RAM can use fp8 with offloading, 32GB or less should use GGUF quantized models",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 3060 12GB setup",
      "details": "Can run fp8_e5m2 Wan 2.2 14B with workflow tweaks, --disable-smart-memory, and large swap file on SSD. Slower but works for upscaling to 900p",
      "from": "mdkb"
    },
    {
      "requirement": "SeedVR2 VRAM usage",
      "details": "Takes 'infinite VRAM' - model is 33GB, causes high VRAM requirements",
      "from": "Karo"
    },
    {
      "requirement": "fp16 model with 32GB VRAM",
      "details": "Can handle 81 frames at 720p, which is maximum official support",
      "from": "Shawneau \ud83c\udf41 [CA]"
    },
    {
      "requirement": "3060 GPU capability",
      "details": "Can achieve 1600x900x81 frames in 27 minutes with proper optimization",
      "from": "mdkb"
    },
    {
      "requirement": "3090 performance with 5B",
      "details": "Gives solid video in 40 seconds",
      "from": "Kiwv"
    },
    {
      "requirement": "Storage issues with frequent model releases",
      "details": "New models coming out every other day causing storage problems",
      "from": "Kijai"
    },
    {
      "requirement": "3090 torch compile",
      "details": "Need fp8e5m2 models for torch compile, fp8e4nv not supported",
      "from": "Josiah"
    },
    {
      "requirement": "SageAttention compatibility",
      "details": "2000 series can't use sage2, 3000 series can use SageAttention regardless of model precision",
      "from": "Kijai"
    },
    {
      "requirement": "5090 VRAM usage",
      "details": "WAN fp8 fits nicely on 5090 without block swap needed",
      "from": "pagan"
    },
    {
      "requirement": "A100 recommendations",
      "details": "Don't need GGUF on A100 40GB, use fp8 models",
      "from": "Juampab12"
    },
    {
      "requirement": "Block swap memory impact",
      "details": "Prefetch uses additional VRAM about the size of single block",
      "from": "Kijai"
    },
    {
      "requirement": "Rapid AllInOne efficiency",
      "details": "Can do 121 frames at 720p without OOM",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "VRAM for high resolution I2V",
      "details": "Full HD (1920x1080) I2V likely not possible with 24GB, may need GGUF Q3 with heavy offloading",
      "from": "Hevi"
    },
    {
      "requirement": "Compute capability for FP8",
      "details": "A5000 with compute 8.6 cannot do 14B FP8, needs higher compute capability",
      "from": "Slavrix"
    },
    {
      "requirement": "5B model performance",
      "details": "5B model supports 1024x1024 at 24 FPS and is very fast",
      "from": "Gill Bastar"
    },
    {
      "requirement": "Native ComfyUI needs 64GB RAM",
      "details": "For non-GGUF models on 4070ti super, native workflow faster but requires 64GB RAM vs 32GB for GGUF",
      "from": "pewpewpew"
    },
    {
      "requirement": "SeedVR extremely VRAM hungry",
      "details": "Even 32GB VRAM struggles with SeedVR, need GGUF models and offloading",
      "from": "Gill Bastar"
    },
    {
      "requirement": "GGUF Q8 works on 3090",
      "details": "Can run GGUF Q8 on 3090, Q6 recommended for 32GB VRAM",
      "from": "Hevi"
    },
    {
      "requirement": "Block swap performance impact",
      "details": "Each block computed individually, block swap adds transfer time on top of compute",
      "from": "patientx"
    },
    {
      "requirement": "VRAM savings with wrapper vs native",
      "details": "Wrapper saves 3GB VRAM and several GB RAM compared to native implementation",
      "from": "Jonathan"
    },
    {
      "requirement": "Reference image processing",
      "details": "Larger reference images may use more VRAM but not confirmed since it's single image",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM management",
      "details": "Native implementation uses ~88% of 24GB VRAM for 720p 121 I2V, wrapper needs block swapping",
      "from": "Hevi"
    },
    {
      "requirement": "4090 capability",
      "details": "Can run minute-long multitalk generations and Q8 models effectively",
      "from": "Kijai"
    },
    {
      "requirement": "RAM usage concern",
      "details": "RAM usage more concerning than VRAM when using offloading with fp16",
      "from": "Kijai"
    },
    {
      "requirement": "Multitalk performance",
      "details": "6 minutes for 1 person on 3060 with 832x480x121 frames at 24fps",
      "from": "mdkb"
    },
    {
      "requirement": "GPU compatibility for fp8_e4m3fn",
      "details": "GPUs prior to 4000 series don't work with fp8_e4m3fn with torch compile, need to use fp8_e5m2",
      "from": "Kijai"
    },
    {
      "requirement": "Fun models storage",
      "details": "Each Fun model is 30GB (15GB x2 for high/low noise variants)",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "Stand-In v2v processing time",
      "details": "176 seconds on RTX 3060 for video processing",
      "from": "mdkb"
    },
    {
      "requirement": "Torch 2.9 compatibility issues",
      "details": "Flash attention and custom nodes break with PyTorch 2.9, uninstalling flash attention recommended",
      "from": "pagan"
    },
    {
      "requirement": "Stand-in workflow VRAM usage",
      "details": "16GB VRAM easily enough with proper block swap settings",
      "from": "Kijai"
    },
    {
      "requirement": "Memory usage for 81 frames at 832x480",
      "details": "Max 16.362GB with 15 blocks swapped, 11.657GB with 40 blocks swapped",
      "from": "Kijai"
    },
    {
      "requirement": "Memory usage with optimizations",
      "details": "7.136GB max allocated with chunked_rope enabled",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 3090 generation speed",
      "details": "Minimum 80s/it for 720p, varies significantly with resolution",
      "from": "MiGrain"
    },
    {
      "requirement": "RAM requirements for Wan 2.2",
      "details": "32GB RAM causes swapping issues, 64GB recommended for smooth operation",
      "from": "Zueuk"
    },
    {
      "requirement": "VRAM for MultiTalk",
      "details": "Needs more than 8GB VRAM, fits in 23G with block swap",
      "from": "nacho.money"
    },
    {
      "requirement": "Dual GPU setup",
      "details": "2x 5090 available on RunPod for $1.75/hour",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "Storage needs",
      "details": "15GB per Wan 2.2 model file, 4 models = 60GB disk space",
      "from": "xwsswww"
    },
    {
      "requirement": "Heat generation",
      "details": "Hot spot behind PC reaches 93F/34C during generation, cable burning reported",
      "from": "garbus"
    },
    {
      "requirement": "VRAM for 81 frames at 720x720",
      "details": "Needs ~20 block swap to fit in 24GB, max allocated 18.914GB",
      "from": "Kijai"
    },
    {
      "requirement": "RAM usage for Wan 2.2",
      "details": "Hitting 80GB reserved with 128GB RAM on 2.2 workflow",
      "from": "Kijai"
    },
    {
      "requirement": "RAM usage can exceed 100GB",
      "details": "Workflows can take up to 100GB+ RAM with paging",
      "from": "Obsolete"
    },
    {
      "requirement": "RTX 3090 quantization compatibility",
      "details": "Must use fp8e5m2 instead of fp8e4nv for torch.compile",
      "from": "Kijai"
    },
    {
      "requirement": "Disk space for memory allocation",
      "details": "Need 10% disk space available for proper memory allocation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "8GB VRAM challenges",
      "details": "Tough to work with, requires block swap around 20 for basic functionality",
      "from": "Kijai"
    },
    {
      "requirement": "CUDA Compute Capability 8.6 limitations",
      "details": "Torch.compile with fp8_e4m3fn not supported, need to use fp8_e5m2, GGUF or higher precision",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "873 frames generation possible",
      "details": "Successfully generated on single GPU but with noticeable artifacts",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "requirement": "64GB RAM + 16GB VRAM can still run out",
      "details": "RAM issues after second sampler finishes even with 64GB RAM",
      "from": "3Dmindscaper2000"
    },
    {
      "requirement": "VRAM management critical for Windows users",
      "details": "Windows CUDA does slow swap to shared memory instead of OOM, reserve-vram flag essential",
      "from": "Kosinkadink"
    },
    {
      "requirement": "13GB shared RAM usage observed",
      "details": "5090 user seeing significant shared memory usage during generation",
      "from": "Kosinkadink"
    },
    {
      "requirement": "AMD Ryzen 9 9900X achieves 40 it/s",
      "details": "For FantasyPortrait node running on CPU",
      "from": "Kijai"
    },
    {
      "requirement": "Power consumption varies significantly",
      "details": "4090: 10-20W variance, 5090: 80W variance during generation",
      "from": "IceAero"
    },
    {
      "requirement": "720p 5 second clip on 4090",
      "details": "Takes about 2 minutes for 1044x788 resolution on RTX 4090",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "requirement": "3060 VRAM limitations",
      "details": "3060 requires reduction from 720p to 832x480 resolution and various optimizations like distorch loading to avoid OOM",
      "from": "mdkb"
    },
    {
      "requirement": "3060 cards can't run e4m3fn models",
      "details": "Need GGUF versions or fp16 with on-the-fly quantization for 3060 compatibility",
      "from": "mdkb"
    },
    {
      "requirement": "Phantom fp16 is 30GB",
      "details": "Too large for on-the-fly quantization on 3060 cards",
      "from": "mdkb"
    },
    {
      "requirement": "GGUF conversion needs CUDA toolkit",
      "details": "Requires nvcc and full CUDA toolkit installation for llama.cpp compilation",
      "from": "orabazes"
    },
    {
      "requirement": "VRAM usage with PUSA",
      "details": "PUSA sampling uses more VRAM regardless of model format or merging",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 3060 optimization",
      "details": "Needs --disable-smart-memory flag, 32GB static swap file, offload everything possible to RAM. OOMs first run but works second time",
      "from": "mdkb"
    },
    {
      "requirement": "Model loading memory management",
      "details": "Set second model loader to offload_device instead of main_device to prevent OOM, though main_device is faster",
      "from": "phazei"
    },
    {
      "requirement": "5090 VRAM limitations",
      "details": "Not enough VRAM for MagRef + MultiTalk combination even on 5090 when renting",
      "from": "MysteryShack"
    },
    {
      "requirement": "RAM usage reduction in dev branch",
      "details": "20GB less RAM when not merging LoRAs",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM optimization",
      "details": "Workflow that was OOMing now uses GPU to fullest capacity",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "RTX 5090 VRAM limits",
      "details": "Can handle up to 1280x720 97 frames, anything above causes OOM",
      "from": "WorldX"
    },
    {
      "requirement": "Block swap performance",
      "details": "3-minute generation time for 1280x720 81 frames with full FP16 model using block swap",
      "from": "Ryzen"
    },
    {
      "requirement": "RTX 6000 Pro performance",
      "details": "300% faster than RTX 5090 when models fit in VRAM, 48GB capacity vs 32GB",
      "from": "WorldX"
    },
    {
      "requirement": "8GB VRAM compatibility",
      "details": "GGUF versions work on 8GB cards, FP8 scaled versions may cause OOM",
      "from": "xwsswww"
    },
    {
      "requirement": "Full model size too large for 32GB VRAM",
      "details": "Original combined model pieces would exceed 32GB VRAM capacity",
      "from": "Tony(5090)"
    },
    {
      "requirement": "Training memory usage",
      "details": "75GB RAM consumed while training with 15 videos plus 2 datasets with 15 images each at 512/768/1024 resolutions",
      "from": "Ryzen"
    },
    {
      "requirement": "fp8 variants for different GPU generations",
      "details": "fp8 e5m2 works with 30xx cards, fp8 e4m3fn requires 40xx cards or above",
      "from": "mdkb"
    },
    {
      "requirement": "Depth Crafter VRAM usage",
      "details": "Consumes significant VRAM, causes OOM on 24GB VRAM with 125 frame videos",
      "from": "Gill Bastar"
    },
    {
      "requirement": "3090 FP8 support",
      "details": "No real FP8 support on 3090, uses emulated FP8",
      "from": "Josiah"
    },
    {
      "requirement": "Context window VRAM",
      "details": "Memory usage = context frames * width * height, need sufficient RAM for frame count after decode",
      "from": "Kijai"
    },
    {
      "requirement": "Training speed",
      "details": "RTX card doing 1.5s/it for video training, 7000-10000 steps taking ~4 hours",
      "from": "Ryzen"
    },
    {
      "requirement": "System memory usage",
      "details": "Image concat multi uses almost 120GB system memory",
      "from": "Gateway"
    },
    {
      "requirement": "VRAM usage optimization",
      "details": "Dev branch uses different block swapping method with no init, loads only needed blocks directly",
      "from": "Kijai"
    },
    {
      "requirement": "Memory reservation capability",
      "details": "New node allows reserving additional memory beyond model estimates",
      "from": "Kosinkadink"
    },
    {
      "requirement": "Performance variance on AMD/ZLUDA",
      "details": "Dev branch showing slower performance (31-33 sec/it vs expected improvements) on Windows AMD systems",
      "from": "patientx"
    },
    {
      "requirement": "RTX 5090 performance",
      "details": "1024x576 video took 18 minutes for 873 frames",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "requirement": "RTX 3090 performance",
      "details": "16 seconds at 480x480 took 18:34, 33 frames took 32 minutes, can do up to 1000 frames",
      "from": "NC17z"
    },
    {
      "requirement": "RTX 3060 performance",
      "details": "500 frames took 35 minutes",
      "from": "mdkb"
    },
    {
      "requirement": "Fun Control VRAM needs",
      "details": "FP32/FP16 mixed model should use 30GB dedicated VRAM and 40GB total VRAM",
      "from": "seitanism"
    },
    {
      "requirement": "InfiniteTalk VRAM usage",
      "details": "Same VRAM as MultiTalk despite larger model size due to looping method",
      "from": "Kijai"
    },
    {
      "requirement": "5090 generation time",
      "details": "17 minutes for 6-step generation with LightX2V",
      "from": "Kijai"
    },
    {
      "requirement": "3060 capabilities",
      "details": "Can generate 8 seconds reasonably, 20 seconds in less reasonable time",
      "from": "mdkb"
    },
    {
      "requirement": "VRAM for InfiniteTalk",
      "details": "3060 struggles with InfiniteTalk, works better on higher end GPUs",
      "from": "mdkb"
    },
    {
      "requirement": "FP8 performance on 3090",
      "details": "No speed difference between FP8 and GGUF Q8 on RTX 3090",
      "from": "iShootGood"
    },
    {
      "requirement": "RTX 3090 memory limits",
      "details": "Q8 GGUF works better than fp8 for 3090, e5m2 not worth quality loss",
      "from": "Kijai"
    },
    {
      "requirement": "InfiniteTalk + FantasyPortrait VRAM",
      "details": "Requires significant VRAM - grows model by 4GB, 24GB VRAM + 128GB RAM still getting OOM",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 3070 8GB limitations",
      "details": "Need Q6 model with blockswap, low resolution, and sufficient RAM to work",
      "from": "Kijai"
    },
    {
      "requirement": "4090/5090 fp8 performance",
      "details": "Fp8 is faster on 4090 and 5090, but Q8 still better quality",
      "from": "Kijai"
    },
    {
      "requirement": "GPU compatibility for torch.compile",
      "details": "Need RTX 4000 series or higher for fp8_e4m3fn weights, RTX 3090 users should use e5m2 or GGUF Q8",
      "from": "Kijai"
    },
    {
      "requirement": "Storage needs",
      "details": "Users reporting need for 4TB+ NVMe storage for ComfyUI models, model storage growing rapidly",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "requirement": "HDD vs SSD performance",
      "details": "WAN takes forever to load on HDD, SSD strongly recommended",
      "from": "Draken"
    },
    {
      "requirement": "Network speed for NAS",
      "details": "1GB network not sufficient for NAS model storage, need 2.5G or 10G fiber",
      "from": "piscesbody"
    },
    {
      "requirement": "VRAM for Wan 2.2 14B",
      "details": "Possible to run on 16GB VRAM using KJ's wrapper with proper optimization",
      "from": ": Not Really Human :"
    },
    {
      "requirement": "Generation time scaling",
      "details": "Using audio CFG makes generation take 3x the time due to 3 passes (positive, negative, audio)",
      "from": "Kijai"
    },
    {
      "requirement": "Long generation times for quality",
      "details": "1 minute 36 second song with 25 steps takes approximately 2 hours 30 minutes on 5090",
      "from": "seitanism"
    },
    {
      "requirement": "Long generation times",
      "details": "2400 frames took 2.5 hours, 720p to 1024p upscale took 15 minutes",
      "from": "seitanism"
    },
    {
      "requirement": "VRAM issues with mixed precision model",
      "details": "Wan 2.2 fp32/fp16 mixed stuck at 22GB VRAM usage with poor performance",
      "from": "seitanism"
    },
    {
      "requirement": "OOM with low noise upscale",
      "details": "Wan 2.2 low noise upscale causes OOM on RTX 5090",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "VRAM usage with cache-none",
      "details": "48GB VRAM, 128GB DRAM, 2GB swap - ComfyUI stable all day with --cache-none",
      "from": "samhodge"
    },
    {
      "requirement": "SeedVR2 VRAM requirements",
      "details": "Not viable for 24GB cards, constantly runs out of VRAM",
      "from": "646594572499025921"
    },
    {
      "requirement": "InfiniteTalk generation time",
      "details": "2400 frames at 480x720 with MagRef+InfiniteTalk executed in 6 hours 12 minutes",
      "from": "seitanism"
    },
    {
      "requirement": "InfiniteTalk GGUF model performance",
      "details": "Q4 quality seems fine, Q8 very close to fp16 quality with fast dequant",
      "from": "Kijai"
    },
    {
      "requirement": "Fantasy Portrait Face Detector",
      "details": "Should process 850 frames in few seconds on GPU with proper onnxruntime-gpu setup",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM for 3000 frames",
      "details": "Question about whether VAE decode will run out of VRAM at end with max 3000 frames",
      "from": "Yae"
    },
    {
      "requirement": "Wan 2.2 14B training VRAM",
      "details": "5 days to train LoRA with 5090, can use 32GB with split samples and low VRAM mode",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "RunPod L40S performance",
      "details": "Good performance for generation tasks, used for quick turnaround testing",
      "from": "Santoshyandhe"
    },
    {
      "requirement": "RTX 3090 limitations",
      "details": "17 minutes for single generation when Torch Compile not connected, can struggle with newer workflows",
      "from": "NC17z"
    },
    {
      "requirement": "Storage costs on RunPod",
      "details": "$7 for 100GB storage per month, considered expensive",
      "from": "Santoshyandhe"
    },
    {
      "requirement": "RTX A6000 48GB performance",
      "details": "997 frames at 720p takes about 40 minutes, using e5m2 quantization on Ampere architecture",
      "from": "samhodge"
    },
    {
      "requirement": "Precision considerations for different architectures",
      "details": "Ampere RTX A6000 (2020) uses e5m2, Ada (2022) and Blackwell (2025) support better precisions",
      "from": "samhodge"
    },
    {
      "requirement": "WAN 2.2 on 5090",
      "details": "Can run fp32/fp16 mixed model with 20 blocks swapped, hardly need for Pro 6000 except training",
      "from": "seitanism"
    },
    {
      "requirement": "FP16 WAN 2.2 VRAM usage",
      "details": "Uses 15GB VRAM but runs extremely slow, similar issues to 1% fp32/99% fp16 model",
      "from": "seitanism"
    },
    {
      "requirement": "CineScale VRAM needs",
      "details": "Requires same VRAM as running at target resolution normally, tested at 1632x2880 on 96GB VRAM",
      "from": "Kijai"
    },
    {
      "requirement": "Wan 2.2 performance",
      "details": "81 frames at 720p renders in 82.76 seconds on fp8 scaled models",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "VRAM for 10s video at 24fps",
      "details": "Would need VRAM for 242 frames at some point in pipeline, at full offloading barely any difference to Wan 14B",
      "from": "Kijai"
    },
    {
      "requirement": "5070TI insufficient for 83 frame video with sound",
      "details": "Takes ages to load, need to increase block swap and use TE on CPU",
      "from": "hicho"
    },
    {
      "requirement": "Torch compile compatibility",
      "details": "Works on 3000 series and up, E5 compiles on 3000, E4 on 4000/5000 series",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "Turbo model performance",
      "details": "9s to sample, 24s to decode for 5B turbo model",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 6000 performance",
      "details": "1280x1280, 30 seconds rendered in 45 minutes",
      "from": "Fill"
    },
    {
      "requirement": "High VRAM for CineScale 4K",
      "details": "Speculation that 8xA100 needed for proper 4K generation",
      "from": "scf"
    },
    {
      "requirement": "Tiled VAE required for 4K",
      "details": "Must use tiled VAE when generating at 4K resolution",
      "from": "Kijai"
    },
    {
      "requirement": "3090 torch compile compatibility",
      "details": "Works with GGUF or fp8_e5m2, no reason to disable compile on 3090",
      "from": "Kijai"
    },
    {
      "requirement": "Block swap usually needed on 3090",
      "details": "Amount depends on resolution, compile reduces the need by reducing peak VRAM",
      "from": "Kijai"
    },
    {
      "requirement": "VAE decoding performance",
      "details": "Default res takes ~20 seconds on 4090 while generation is 9 seconds",
      "from": "Kijai"
    },
    {
      "requirement": "InfiniteTalk speed benchmark",
      "details": "~10 minutes for 600 frames on 4090 without compile (640x640, 7 steps)",
      "from": "Kijai"
    },
    {
      "requirement": "FP8 E5M2 model for older hardware",
      "details": "Converted for AMD and Nvidia 3000 series users to use with torch.compile",
      "from": "patientx"
    },
    {
      "requirement": "Processing time improvements",
      "details": "5B turbo model: 6 minutes with fp16, 4 minutes with e5m2 + torch.compile, down to 57sec sampling with unipc+beta",
      "from": "patientx"
    },
    {
      "requirement": "InfiniteTalk VRAM usage",
      "details": "12GB VRAM may be too low for InfiniteTalk, especially with VAE cache leak causing 2-3GB extra usage from 2nd window",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "requirement": "FP8 scaled_fast optimal for RTX 50 series",
      "details": "FP8 scaled_fast mode is best for speed and slightly lower memory use than GGUF Q8, 16GB VRAM should be enough with 64GB RAM",
      "from": "Kijai"
    },
    {
      "requirement": "PUSA LoRA memory impact",
      "details": "PUSA LoRA is huge and has pretty big memory impact when used with GGUF or as unmerged LoRA",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM management",
      "details": "Keep below 95% VRAM on Windows to avoid shared memory slowdowns. 25GB max allocated on 5090 for 696 frames at 832x512",
      "from": "Kijai"
    },
    {
      "requirement": "Block swapping stats",
      "details": "Block transfer times around 0.0003s, compute time 1.3s, to_cpu transfer 0.2743s per block",
      "from": "Draken"
    },
    {
      "requirement": "Speed comparison",
      "details": "3 minutes for 5 seconds at 1200x600 on RTX 3090 with Wan 2.2 + Lightning",
      "from": "Ashtar"
    },
    {
      "requirement": "RAM for Wan 2.1",
      "details": "16GB RAM insufficient for 14B model, 64GB recommended",
      "from": "Akumetsu971/SonidosEnArmon\u00eda"
    },
    {
      "requirement": "InfiniteTalk VRAM on 5090",
      "details": "Max allocated 28.411GB, max reserved 29.938GB for 297 frames at 960x640",
      "from": "Kijai"
    },
    {
      "requirement": "MultiTalk additional VRAM",
      "details": "~2.5GB more at Q8, divisible by block swap amount",
      "from": "Kijai"
    },
    {
      "requirement": "Block swap rendering time",
      "details": "7515 seconds for rendering with 17712.77MB total transformer block memory",
      "from": "SwampMonster"
    },
    {
      "requirement": "VRAM usage for image encoding",
      "details": "1536x1536 is pretty much the limit with 24GB VRAM without tiling",
      "from": "Kijai"
    },
    {
      "requirement": "Performance difference between model formats",
      "details": "GGUF takes ~10 seconds to load, fp8 takes ~2 seconds",
      "from": "Kijai"
    },
    {
      "requirement": "3060 12GB performance",
      "details": "Can run VACE identity swap at 1280x720 with 1 frame in about 3 minutes",
      "from": "mdkb"
    },
    {
      "requirement": "VRAM usage with LightX2V",
      "details": "81 frames at 832x480 with 6 steps: max 11.910 GB allocated, 12.750 GB reserved",
      "from": "Kijai"
    },
    {
      "requirement": "RAM savings with dev version",
      "details": "Saves up to 30GB RAM with 2.2 workflows",
      "from": "Kijai"
    },
    {
      "requirement": "Paging file needed even with large RAM",
      "details": "Even with 128GB RAM, need paging file to prevent crashes when going 1MB over",
      "from": "Ablejones"
    },
    {
      "requirement": "RAM for video work",
      "details": "64GB seems like minimum, considering 128GB for high level work",
      "from": "Lodis"
    },
    {
      "requirement": "VRAM for video generation",
      "details": "8GB VRAM not enough for video work, 3060 good value at <$400",
      "from": "Lodis"
    },
    {
      "requirement": "CPU recommendation",
      "details": "9950x good combo with 5090",
      "from": "Lodis"
    },
    {
      "requirement": "3060 performance",
      "details": "8 mins for 81 frames at 832x480 resolution",
      "from": "mdkb"
    },
    {
      "requirement": "Color Match node performance",
      "details": "Takes all day on large/long videos, needs GPU acceleration",
      "from": "\uac10\uc790"
    },
    {
      "requirement": "VRAM usage optimization",
      "details": "Using iGPU for display saves 4% GPU VRAM, 2.2 14B fp16 peaks at 64% vs previous 90%+ usage",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "Model loading from disk",
      "details": "New loading method reserves much less RAM but may be slower on some setups",
      "from": "Kijai"
    },
    {
      "requirement": "Inference speed",
      "details": "11x slower than real-time (12 second video takes 60 seconds to generate)",
      "from": "Mancho"
    },
    {
      "requirement": "VRAM usage",
      "details": "S2V model is heavier on VRAM than Wan 2.2 A14B but easier on RAM",
      "from": "Kijai"
    },
    {
      "requirement": "PyTorch version",
      "details": "Need PyTorch 2.7+ for proper functionality, 2.3.1 causes issues",
      "from": "patientx"
    },
    {
      "requirement": "VRAM for attention operations",
      "details": "512\u00d7512: ~1GB base, 1024\u00d71024: ~4GB, 2048\u00d72048: ~16GB (calculated with 3x modifier)",
      "from": "Mu5hr00m_oO"
    },
    {
      "requirement": "Block swap and torch compile compatibility",
      "details": "Cannot use compile torch and blockswap together currently with S2V",
      "from": "slmonker"
    },
    {
      "requirement": "Frame count vs VRAM",
      "details": "1000 frames uses same VRAM as 81 frames due to context window architecture",
      "from": "Kijai"
    },
    {
      "requirement": "Memory issues with latest wrapper",
      "details": "Latest WanVideoWrapper version shows unusual VRAM spikes and instability",
      "from": "Kenk"
    },
    {
      "requirement": "GPU requirements for fp8_e5m2_fast",
      "details": "you need 4000+ series GPU to use the _fast mode",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM optimization target",
      "details": "95% VRAM usage is generally safe for optimal performance",
      "from": "Kijai"
    },
    {
      "requirement": "Processing speed benchmark",
      "details": "48 seconds video took 58 minutes processing, so a little over a minute a second generation times",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "requirement": "Resolution performance impact",
      "details": "going over 832 makes everything extra slow",
      "from": "patientx"
    },
    {
      "requirement": "S2V generation speed",
      "details": "RTX 4090: 13 minutes for 601 frames at 960x640 with 6 steps",
      "from": "Kijai"
    },
    {
      "requirement": "InfiniteTalk generation time",
      "details": "RTX 3090 24GB + 32GB RAM: 15-18 minutes for 17 seconds at 832x592",
      "from": "Antey"
    },
    {
      "requirement": "S2V long video generation",
      "details": "1000 frames at 720p in 25 minutes",
      "from": "ingi // SYSTMS"
    },
    {
      "requirement": "A5000 performance issues",
      "details": "First sampler pass took 24 hours for 501 frames, indicating severe performance problems",
      "from": "Slavrix"
    },
    {
      "requirement": "VACE GGUF Q4_K_S",
      "details": "OOM issues reported with 48GB VRAM on 119 frames 1280x720",
      "from": "AmirKerr"
    },
    {
      "requirement": "S2V fp8 model",
      "details": "2GB smaller after fixing pose condition layer, audio injection layers work fine in fp8",
      "from": "Kijai"
    },
    {
      "requirement": "HunyuanVideo-Foley VRAM usage",
      "details": "No reason for it to use that much VRAM according to analysis",
      "from": "Kijai"
    },
    {
      "requirement": "Blackwell GPU requirements",
      "details": "PyTorch 2.8 + CUDA 12.8 minimum requirement for Blackwell series GPUs",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 6000 Pro performance for T2V",
      "details": "About 200 seconds for 720x1280x81 frames T2V generation, 64GB DDR4 RAM becomes bottleneck",
      "from": "dg1860"
    },
    {
      "requirement": "Linux vs Windows memory efficiency",
      "details": "64GB RAM on Linux equivalent to 128GB on Windows for same performance, much more memory efficient",
      "from": "Kijai"
    },
    {
      "requirement": "H100 capability for long generation",
      "details": "Can generate 47 second videos on H100, though VRAM/RAM usage not specified",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "S2V VRAM usage",
      "details": "S2V is a hefty beast even at q4k_m quantization, GGUF offloading helps significantly",
      "from": "DawnII"
    },
    {
      "requirement": "Memory leak in Framepack",
      "details": "Before fix: Max allocated memory: 15.313 GB, Max reserved memory: 22.531 GB - 7GB wasted due to accumulation",
      "from": "Kijai"
    },
    {
      "requirement": "RTX3090 performance",
      "details": "50 minutes generation time for Infinite Talk with MAGREF 14B fp8 on RTX3090",
      "from": "NC17z"
    },
    {
      "requirement": "VACE encoding VRAM",
      "details": "Peaks at 15GB VRAM for 85 frames at 1280x720 with 6 steps",
      "from": "Kijai"
    },
    {
      "requirement": "Long video generation",
      "details": "Successfully generated 1600 frames on RTX6000Pro",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "requirement": "128GB RAM with 3090",
      "details": "Still experienced crashes at 500 frames until using offload_device",
      "from": "\ud835\udd6f\ud835\udd97. \ud835\udd78\ud835\udd86\ud835\udd88\ud835\udd86\ud835\udd87\ud835\udd97\ud835\udd8a \u2620"
    },
    {
      "requirement": "RTX 3060 model choice",
      "details": "FP8 usually works better in wrapper, GGUF in native workflows",
      "from": "mdkb"
    },
    {
      "requirement": "Context windows performance",
      "details": "501 frames at 640x640, 6 steps: 5:56 on RTX 5090, max 12.775GB VRAM",
      "from": "Kijai"
    },
    {
      "requirement": "Memory management",
      "details": "PyTorch reserves RAM even after moving objects, Windows reserves 20GB before runs",
      "from": "Kijai"
    },
    {
      "requirement": "Python 3.13 benefits",
      "details": "No obvious performance benefits noticed yet",
      "from": "Ablejones"
    },
    {
      "requirement": "VRAM for upscaling workflow",
      "details": "Regular 14B high/low noise models cause VRAM issues even on RTX 5090, Q8 models recommended",
      "from": "WorldX and Gill Bastar"
    },
    {
      "requirement": "RAM usage improvement",
      "details": "64GB RAM users can avoid swap files with new pytorch 2.8 update",
      "from": "hicho"
    },
    {
      "requirement": "CineScale VRAM",
      "details": "V2V VRAM requirements not higher due to being a LoRA",
      "from": "DawnII"
    },
    {
      "requirement": "14B model requirements",
      "details": "14b requires 8xh100, probably for 4k",
      "from": "[\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-]"
    },
    {
      "requirement": "InfiniteTalk generation speed",
      "details": "2000 frames in 28 windows, at 832x480 with 4 steps, 12 mins",
      "from": "Kijai"
    },
    {
      "requirement": "L40s performance",
      "details": "Generated video on l40s which took around 20 minutes for Bollywood movie dialogue",
      "from": "Santoshyandhe"
    },
    {
      "requirement": "Increased VRAM usage in recent versions",
      "details": "Need to increase blockswaps from 20 to 26 for 480x720 resolution, OOM issues even with 96GB VRAM",
      "from": ".: Not Really Human :., HeadOfOliver"
    }
  ],
  "community_creations": [
    {
      "creation": "VACE 2.2 workflow with openpose",
      "type": "workflow",
      "description": "Working VACE implementation using openpose controlnet",
      "from": "GOD_IS_A_LIE"
    },
    {
      "creation": "Custom combined sampler node",
      "type": "node",
      "description": "Single node combining high and low noise sampling with denoise control",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "T5 unload node",
      "type": "node",
      "description": "Completely unloads T5 after use, leaves nothing in VRAM or RAM",
      "from": "Kijai"
    },
    {
      "creation": "Latest frame grabber node",
      "type": "node",
      "description": "Grabs the latest last frame from a directory for chaining workflows",
      "from": "Flipping Sigmas"
    },
    {
      "creation": "VACE multi-scene chaining workflow",
      "type": "workflow",
      "description": "Complex workflow with T2V start feeding into 5 I2V sections with frame interpolation",
      "from": "seitanism"
    },
    {
      "creation": "WAN 2.2 upscaling workflow",
      "type": "workflow",
      "description": "Uses WAN 2.1 VACE then WAN 2.2 high noise for upscaling",
      "from": "GOD_IS_A_LIE"
    },
    {
      "creation": "T5 Memory Unload Node",
      "type": "node",
      "description": "Completely unloads T5 encoder after use to save RAM",
      "from": "Kijai"
    },
    {
      "creation": "NAG Text Encoding Node",
      "type": "node",
      "description": "Separate text encoding node with NAG (Negative Attention Guidance) support",
      "from": "Kijai"
    },
    {
      "creation": "CustomGPT for WAN prompting",
      "type": "tool",
      "description": "GPT trained on official Alibaba materials for generating cinematic prompts",
      "from": "The Shadow (NYC)"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "Kijai's ComfyUI wrapper for Wan models with context windows support",
      "from": "Kijai"
    },
    {
      "creation": "LLM prompt generation system",
      "type": "workflow",
      "description": "System guide for LLM to create prompts for T2I generation with negative prompts",
      "from": "Simjedi"
    },
    {
      "creation": "Character LoRA",
      "type": "lora",
      "description": "Trained on specific face for 59 minutes with high noise model",
      "from": "Kenk"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "Kijai's ComfyUI wrapper for Wan models",
      "from": "Multiple users"
    },
    {
      "creation": "Blockswap node",
      "type": "node",
      "description": "Saves VRAM by offloading to CPU",
      "from": "Ryzen"
    },
    {
      "creation": "Automatic context frame calculator",
      "type": "workflow",
      "description": "Automatically calculates context frames and num_frames based on number of '|' separators in prompt",
      "from": "avataraim"
    },
    {
      "creation": "Ollama prompt preprocessing workflow",
      "type": "workflow",
      "description": "Uses Ollama nodes to process prompts and images, returns English and Chinese prompts using multimodal models",
      "from": "Juan Gea"
    },
    {
      "creation": "Qwen 2.5 integration",
      "type": "node",
      "description": "Local prompt extension with zero memory usage after completion",
      "from": "Kijai"
    },
    {
      "creation": "V2V upscaling workflow",
      "type": "workflow",
      "description": "Upscales 480p to 1080p while fixing specific details",
      "from": "thaakeno"
    },
    {
      "creation": "MMaudio + RIFE workflow",
      "type": "workflow",
      "description": "Integrates audio generation with frame interpolation for sync",
      "from": "thaakeno"
    },
    {
      "creation": "Custom VFI batching node",
      "type": "node",
      "description": "VFI node with batch_size 40 for faster interpolation (4 seconds vs 15-20s)",
      "from": "gokuvonlange"
    },
    {
      "creation": "14B I2V movie training experiment",
      "type": "training",
      "description": "Training 14B I2V on 3500 movie extracts at FPS=1, 5-81 frames at 192 resolution for high model, 5-21 frames at 320 for low model",
      "from": "mamad8"
    },
    {
      "creation": "Animorphs-type transformation lora",
      "type": "lora",
      "description": "Early 250 step checkpoint for transformation effects on 2.2 5B i2v",
      "from": "orabazes"
    },
    {
      "creation": "Background training loras",
      "type": "lora",
      "description": "City centre backgrounds and background removal techniques",
      "from": "Ryzen"
    },
    {
      "creation": "Middle frame implementation",
      "type": "workflow",
      "description": "Modified WanFirstLastImageToVideo code for middle frame conditioning",
      "from": "Juampab12"
    },
    {
      "creation": "Video upscale + motion fix workflow",
      "type": "workflow",
      "description": "Takes video latent and feeds to high-noise model with start step control",
      "from": "thaakeno"
    },
    {
      "creation": "MMAudio integration workflow",
      "type": "workflow",
      "description": "Combined video generation and audio synthesis in single workflow",
      "from": "thaakeno"
    },
    {
      "creation": "FPS=1 I2V training approach",
      "type": "training method",
      "description": "Training LoRA on slideshow-type video for sequential scene generation",
      "from": "mamad8"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "ComfyUI wrapper for Wan models",
      "from": "Kijai"
    },
    {
      "creation": "FPS1 LoRA",
      "type": "lora",
      "description": "Custom LoRA for specific frame rate control, showing promising results",
      "from": "mamad8"
    },
    {
      "creation": "FPS1 experimental LoRA",
      "type": "lora",
      "description": "Generates videos at 1 FPS instead of 16 for storyboarding and longer scene creation",
      "from": "mamad8"
    },
    {
      "creation": "Character transformation LoRA",
      "type": "lora",
      "description": "Nick Cage transforming into elephant, trained at 3000 steps",
      "from": "orabazes"
    },
    {
      "creation": "LoRA timestep scheduling",
      "type": "feature",
      "description": "Allows per-step LoRA strength control with curves and dropoff",
      "from": "Kijai"
    },
    {
      "creation": "Context window workflow",
      "type": "workflow",
      "description": "Wan 2.2 with context window implementation",
      "from": "thaakeno"
    },
    {
      "creation": "Multi-sampler approach",
      "type": "workflow",
      "description": "3 ksamplers: 1 step 3.5cfg, 2 steps cfg1 ltrx 3.0, final cfg1 ltrx 0.7",
      "from": "3DBicio"
    },
    {
      "creation": "Custom dockerized Wan 2.2 serverless setup",
      "type": "workflow",
      "description": "Complete Docker container with ComfyUI, pytorch, models, sage attention for serverless deployment",
      "from": "gokuvonlange"
    },
    {
      "creation": "Custom web UI for job dispatching",
      "type": "tool",
      "description": "Vibe-coded web UI that dispatches jobs to H100 or 5090 workers",
      "from": "gokuvonlange"
    },
    {
      "creation": "Judy Hopps LoRA",
      "type": "lora",
      "description": "T2V A14B LoRA for Wan 2.2",
      "from": "MisterMango"
    },
    {
      "creation": "Enhanced workflow with LoRA scheduling",
      "type": "workflow",
      "description": "Integration of Kijai workflow LoRA improvements",
      "from": "au"
    },
    {
      "creation": "Fixed Wan 2.2 Lightning LoRAs",
      "type": "lora",
      "description": "Corrected alpha values and converted to fp16 from original fp32 LoRAs",
      "from": "Kijai"
    },
    {
      "creation": "Multiple rank extractions of 720p distill I2V",
      "type": "lora",
      "description": "Extracted rank 32, 64, and 128 versions of the 720p distilled I2V model",
      "from": "Kijai"
    },
    {
      "creation": "Kijai's fixed Wan 2.2 Lightning LoRAs",
      "type": "lora",
      "description": "Fixed versions of official LoRAs that work at 1.0 strength with proper ComfyUI key naming",
      "from": "Kijai"
    },
    {
      "creation": "Judy Hopps LoRA for Wan 2.2",
      "type": "lora",
      "description": "Improved character LoRA for Judy Hopps, upcoming release on Civitai",
      "from": "MisterMango"
    },
    {
      "creation": "Aether punch face impact LoRA",
      "type": "lora",
      "description": "LoRA for Wan 2.2 5B I2V creating punch impact effects",
      "from": "Juampab12"
    },
    {
      "creation": "Claude prompting system",
      "type": "tool",
      "description": "Custom Claude project with Wan prompting guide for generating structured prompts",
      "from": "Juampab12"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "Kijai's wrapper with optimized VRAM management and compile optimizations",
      "from": "Kijai"
    },
    {
      "creation": "ClownsharkSampler",
      "type": "sampler",
      "description": "Alternative sampler that produces more fully sampled, less noisy results",
      "from": "Ablejones"
    },
    {
      "creation": "Qwen3 prompt enhancement node",
      "type": "node",
      "description": "Node that can take image or video to generate prompts, no auth key required",
      "from": "shockgun"
    },
    {
      "creation": "WanVideoWrapper improvements",
      "type": "node",
      "description": "Ongoing updates and fixes to the ComfyUI wrapper for Wan models",
      "from": "Kijai"
    },
    {
      "creation": "CFG scheduling node for native 2.2",
      "type": "node",
      "description": "Node that handles CFG lists for first step only CFG application in native workflows",
      "from": "Kijai"
    },
    {
      "creation": "Video prompter with API integration",
      "type": "tool",
      "description": "Tool for generating video descriptions from uploaded videos using API",
      "from": "thaakeno"
    },
    {
      "creation": "Image difference analysis CLI tool",
      "type": "tool",
      "description": "Python CLI for comparing VAE outputs with statistical analysis and amplified difference visualization",
      "from": "fredbliss"
    },
    {
      "creation": "WanVideo context windowing workflow",
      "type": "workflow",
      "description": "Advanced workflow for generating longer sequences using context sliding technique",
      "from": "Kijai"
    },
    {
      "creation": "Video batch splitter",
      "type": "node",
      "description": "Splits batch videos out for individual selection and downstream processing",
      "from": "Fill"
    },
    {
      "creation": "Video picker node (planned)",
      "type": "node",
      "description": "Select which video from batch to send downstream for upscaling",
      "from": "Fill"
    },
    {
      "creation": "WAN 2.2 14B Arcane Jinx LoRA",
      "type": "lora",
      "description": "High and low noise LoRA for character consistency",
      "from": "Cseti"
    },
    {
      "creation": "Collage side-by-side webapp",
      "type": "tool",
      "description": "Vibecoded webapp for creating side-by-side character references",
      "from": "Juampab12"
    },
    {
      "creation": "VACE module separation node",
      "type": "node",
      "description": "Saves disk space by storing VACE in separate file, similar to wrapper functionality",
      "from": "Kijai"
    },
    {
      "creation": "Subgraph workflow organization",
      "type": "workflow",
      "description": "Clean single node containing entire workflow, like Blender groups",
      "from": "Nekodificador"
    },
    {
      "creation": "FakeVace2.2",
      "type": "model",
      "description": "Experimental attempt to recreate VACE 2.2 functionality",
      "from": "JohnDopamine"
    },
    {
      "creation": "Latent injection nodes",
      "type": "node",
      "description": "Experimental nodes for latent encoder/injector with activation patching for better text-to-latent embedding",
      "from": "fredbliss"
    },
    {
      "creation": "VACE strength control node for Phantom embeds",
      "type": "node",
      "description": "Allows setting VACE strength of phantom embeds, can work with insane values like 1000",
      "from": "Ablejones"
    },
    {
      "creation": "Phantom+VACE merge model",
      "type": "model",
      "description": "Months of work to find good vace/phantom mix for character consistency and control",
      "from": "Piblarg"
    },
    {
      "creation": "FlF2V dreamy morphing workflow",
      "type": "workflow",
      "description": "Workflow using add noise setting with LoRAs to create smooth dreamy transitions",
      "from": "piscesbody"
    },
    {
      "creation": "VACE character swap workflow",
      "type": "workflow",
      "description": "Method for swapping characters using VACE module with Wan 2.2 and reference images",
      "from": "mdkb"
    },
    {
      "creation": "Scene teleportation I2V workflow",
      "type": "workflow",
      "description": "Beta workflow for teleporting characters between scenes with likeness preservation",
      "from": "Juampab12"
    },
    {
      "creation": "WanVideo I2I Expression/Perspective LoRA",
      "type": "lora",
      "description": "Helps maintain character similarity during scene changes and perspective shifts",
      "from": "Juampab12"
    },
    {
      "creation": "VACE/Phantom merge",
      "type": "workflow",
      "description": "Merging VACE and Phantom for identity preservation with control",
      "from": "Piblarg"
    },
    {
      "creation": "Native VACE node",
      "type": "node",
      "description": "Native ComfyUI node that works similar to loading VACE as a module",
      "from": "Kijai"
    },
    {
      "creation": "Custom LoRA training approach",
      "type": "workflow",
      "description": "Different training for high/low noise - removed static videos from high noise, used different resolutions and frame counts",
      "from": "Cseti"
    },
    {
      "creation": "RadialAttn implementation",
      "type": "tool",
      "description": "Works at 768x768 resolution, tested up to 81 frames",
      "from": "\u0414\u043c\u0438\u0442\u0440\u0438\u0439 \u041c\u0430\u0440\u043a\u043e\u0432"
    },
    {
      "creation": "VACE/Phantom merge workflow",
      "type": "workflow",
      "description": "Combines VACE control with Phantom consistency",
      "from": "Piblarg"
    },
    {
      "creation": "Gemini frame calculator",
      "type": "tool",
      "description": "Automatically calculates optimal frame counts for different resolutions",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "creation": "Sapiens pose detector (full version)",
      "type": "tool",
      "description": "Two-stage pose detection and drawing using full Sapiens model with torchscript, not lite version",
      "from": "fredbliss"
    },
    {
      "creation": "WAN 2.2 + VACE merge",
      "type": "model",
      "description": "Block merge combining WAN 2.2 with VACE capabilities",
      "from": "R."
    },
    {
      "creation": "Sapiens pose detector",
      "type": "tool",
      "description": "Updated pose detector with --max-people setting and input video removal",
      "from": "fredbliss"
    },
    {
      "creation": "Visual scheduler node concept",
      "type": "node",
      "description": "Proposed node to set schedules visually with sigma curve display for better understanding",
      "from": "Kijai"
    },
    {
      "creation": "VACE advanced nodes",
      "type": "node",
      "description": "Advanced VACE nodes on GitHub for enhanced control capabilities",
      "from": "Lodis"
    },
    {
      "creation": "Phantom merge with VACE",
      "type": "workflow",
      "description": "Combined VACE and phantom merge workflow for character consistency",
      "from": "Lodis"
    },
    {
      "creation": "WanVideo VACE Start to End Frame node",
      "type": "node",
      "description": "Kijai's node for single frame to video conversion with controlnet input",
      "from": "Lodis"
    },
    {
      "creation": "Sapiens wheels",
      "type": "tool",
      "description": "Pre-compiled wheels for easier Sapiens installation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "creation": "Sapiens flexible pipeline",
      "type": "tool",
      "description": "Comprehensive pipeline supporting multiple model formats, trackers, and optimization strategies with performance presets",
      "from": "fredbliss"
    },
    {
      "creation": "Sapiens ComfyUI node",
      "type": "node",
      "description": "ComfyUI integration for Sapiens pose detection with batch processing and memory optimization",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "creation": "Sapiens CLI multi-person tracking",
      "type": "tool",
      "description": "Complete pose detection pipeline with multi-person tracking, camera movement handling, and temporal smoothing",
      "from": "fredbliss"
    },
    {
      "creation": "2.2 VACE hybrid merge",
      "type": "workflow",
      "description": "Method to merge some early blocks + patch embed from 2.1 into 2.2 to increase VACE effect on high noise side",
      "from": "JohnDopamine"
    },
    {
      "creation": "Select Every Nth Frame node",
      "type": "node",
      "description": "Drops every nth frame, useful for 3-2 pulldown conversion from interpolated footage",
      "from": "Kijai"
    },
    {
      "creation": "Get Images From Batch Indexed node",
      "type": "node",
      "description": "Used for stripping out specific frames for fps conversion workflows",
      "from": "Kijai"
    },
    {
      "creation": "Custom sword fight LoRA",
      "type": "lora",
      "description": "LoRA trained on sword fights at 24fps, makes fighting look like 90s movie style",
      "from": "Ruairi Robinson"
    },
    {
      "creation": "Enhanced drag crop node",
      "type": "node",
      "description": "Added rectangle crop and magic image loading features to ComfyUI-Olm-DragCrop",
      "from": "Hoernchen"
    },
    {
      "creation": "Sapiens multi-person tracking",
      "type": "tool",
      "description": "CLI tool for tracking 10+ people with pose estimation, segmentation, and 300+ human features",
      "from": "fredbliss"
    },
    {
      "creation": "Enhanced WanVideoWrapper schedulers",
      "type": "node",
      "description": "Branch with additional schedulers and fancy looping node with dynamic prompt controls",
      "from": "Hoernchen"
    },
    {
      "creation": "Skyreels LoRA extraction",
      "type": "lora",
      "description": "Extracted from Skyreels model to help break 121 frame loop in 2.2",
      "from": "Kijai"
    },
    {
      "creation": "5B controlnet implementation",
      "type": "node",
      "description": "Traditional depth controlnet working with 5B model",
      "from": "TheDenk"
    },
    {
      "creation": "WanVideo wrapper updates",
      "type": "node",
      "description": "Continuous updates to wrapper with new model support",
      "from": "Kijai"
    },
    {
      "creation": "Sigma visualization nodes",
      "type": "node",
      "description": "Custom nodes for visualizing sigma splits with Fixed Fraction, Sigma Threshold, and SNR options",
      "from": "Alisson Pereira"
    },
    {
      "creation": "ComfyUI-WanMoeKSampler",
      "type": "node",
      "description": "KSampler modification for Wan 2.2 MoE",
      "from": "Tomber"
    },
    {
      "creation": "Block swap prefetch optimization",
      "type": "optimization",
      "description": "Added prefetch option to improve block swap transfer speeds",
      "from": "Kijai"
    },
    {
      "creation": "Skyreels LoRA extraction",
      "type": "lora",
      "description": "Extracted Skyreels LoRA for 121-frame generation capability",
      "from": "Kijai"
    },
    {
      "creation": "Skyreels LoRA conversions",
      "type": "lora",
      "description": "Converted old Skyreels I2V LoRA to work with T2V, breaks looping but enables longer generation",
      "from": "Kijai"
    },
    {
      "creation": "Stand-In ComfyUI implementation",
      "type": "node",
      "description": "ComfyUI wrapper for Stand-In LoRA face consistency system",
      "from": "Kijai"
    },
    {
      "creation": "VACE inpainting workflow",
      "type": "workflow",
      "description": "Workflow using Impact pack for face masking with VACE inpainting",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "creation": "Modified SeedVR GGUF nodes",
      "type": "node",
      "description": "Alisson Pereira created modified version for GGUF compatibility",
      "from": "Gill Bastar"
    },
    {
      "creation": "All-in-one Wan 2.2 workflow for ZLUDA",
      "type": "workflow",
      "description": "Specially targeting AMD GPU users with ZLUDA or TheRock torch",
      "from": "patientx"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "custom node",
      "description": "ComfyUI wrapper for Wan video models with example workflows",
      "from": "Kijai"
    },
    {
      "creation": "Skyreels v2 LoRA",
      "type": "lora",
      "description": "Speed LoRA that enables 121 frame generation for Wan",
      "from": "Kijai"
    },
    {
      "creation": "ComfyUI Wan 2.2 FFLF node",
      "type": "custom node",
      "description": "First Frame Last Frame implementation for Wan 2.2 5B",
      "from": "stduhpf"
    },
    {
      "creation": "LoRA extraction workflow",
      "type": "workflow",
      "description": "Method to extract LoRA from Wan 2.2 LN using Wan 2.1 as base model",
      "from": "Ablejones"
    },
    {
      "creation": "Fun Control workflow",
      "type": "workflow",
      "description": "Clean workflow for Fun Control bullet time style generations",
      "from": "T2 (RTX6000Pro)"
    },
    {
      "creation": "Enhanced Stand-In workflow",
      "type": "workflow",
      "description": "Uses PersonMaskUltra node to segment, crop and resize reference images for better quality output",
      "from": "BobbyD4AI"
    },
    {
      "creation": "Stand-In v2v workflow",
      "type": "workflow",
      "description": "Enables video-to-video with reference image using VACE integration",
      "from": "mdkb"
    },
    {
      "creation": "Stand-in workflow with face cropping",
      "type": "workflow",
      "description": "Uses MediaPipe and InspyreNet instead of official YOLO preprocessor, no additional packages needed beyond Essentials and controlnet-aux",
      "from": "Kijai"
    },
    {
      "creation": "Fantasy Portrait integration",
      "type": "feature",
      "description": "Face consistency system that works with MAGREF and other models for portrait generation",
      "from": "Kijai"
    },
    {
      "creation": "Context window workflow for long generation",
      "type": "workflow",
      "description": "Method for generating 177+ frame videos using context windows with MAGREF",
      "from": "Kijai"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "Kijai's wrapper implementation with different workflow approach",
      "from": "Juan Gea"
    },
    {
      "creation": "Fantasy Portrait visualization",
      "type": "node",
      "description": "Added bbox and landmark detection visualization with 222 keypoints",
      "from": "Kijai"
    },
    {
      "creation": "VACE RAM reduction node",
      "type": "node",
      "description": "Node for reduced RAM usage in wrapper workflows",
      "from": "Kijai"
    },
    {
      "creation": "Python script for Cascadour 3D scenes",
      "type": "workflow",
      "description": "Claude-generated Python script to create 3D controlnet input from Cascadour",
      "from": "mdkb"
    },
    {
      "creation": "Cached text encoder node",
      "type": "node",
      "description": "Fully unloads text encoder model after encoding, doesn't even leave it in RAM",
      "from": "Kijai"
    },
    {
      "creation": "WanVideo AddStandInLatent node",
      "type": "node",
      "description": "New node for debugging, connected to video combine",
      "from": "Kijai"
    },
    {
      "creation": "NAG node for Wan",
      "type": "node",
      "description": "Wan-specific node for negative prompting when not using CFG",
      "from": "Kijai"
    },
    {
      "creation": "Janky Memory Patcher",
      "type": "node",
      "description": "Overrides memory manager estimation, allows customizable VRAM buffer",
      "from": "Ablejones"
    },
    {
      "creation": "Advanced VACE control node",
      "type": "node",
      "description": "Allows separate reference and control strength in single node",
      "from": "Ablejones"
    },
    {
      "creation": "Experimental Phantom+VACE strength control",
      "type": "node",
      "description": "Controls VACE strength values for phantom embed latents",
      "from": "Ablejones"
    },
    {
      "creation": "Lazy switches for workflow switching",
      "type": "workflow technique",
      "description": "More reliable switches that don't need bypass/muting and work with API",
      "from": "Kijai"
    },
    {
      "creation": "VACE + Phantom merge workflow",
      "type": "workflow",
      "description": "Combined 1.3B VACE + Phantom workflow for style from prompt generation",
      "from": "Kijai"
    },
    {
      "creation": "WanVaceAdvanced nodes",
      "type": "node",
      "description": "Includes latent extraction node for video continuation experiments",
      "from": "Ablejones"
    },
    {
      "creation": "WanFM implementation",
      "type": "node",
      "description": "Frame Morphing sampling method for bidirectional generation",
      "from": "Kijai"
    },
    {
      "creation": "Phantom VACE merge",
      "type": "model",
      "description": "Community member Piblarg created and shared fp16 full weights version of Phantom VACE merge for GGUF conversion",
      "from": "Piblarg"
    },
    {
      "creation": "Wan LoRA trainer",
      "type": "node",
      "description": "ComfyUI node for training LoRAs on Wan models",
      "from": "shockgun"
    },
    {
      "creation": "Qwen-Image to WAN Bridge",
      "type": "node",
      "description": "Direct latent bridge from Qwen-image to WAN 2.2 without VAE decode",
      "from": "fredbliss"
    },
    {
      "creation": "Fantasy Portrait first/last frame workflow",
      "type": "workflow",
      "description": "Hack to enable video-to-video by extracting input video frames as start/end points",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "Modified frame extraction nodes",
      "type": "node",
      "description": "Custom nodes for grabbing first and last frames from video input",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "Gray frame padding node",
      "type": "node",
      "description": "Adds gray frames to end of batch to satisfy Wan requirements, use with get image batch range to chop additional frames",
      "from": "Flipping Sigmas"
    },
    {
      "creation": "Video grid creation node",
      "type": "node",
      "description": "Creates video grids from folder of videos, added to KJNodes, requires VHS nodes",
      "from": "Kijai"
    },
    {
      "creation": "Model loader refactor in wrapper",
      "type": "tool",
      "description": "Better handling for unmerged LoRAs, loads at sampler directly to blockswap for faster start and less RAM usage",
      "from": "Kijai"
    },
    {
      "creation": "MTV Crafter motion adapter extraction",
      "type": "tool",
      "description": "Extracted motion adapter from MTV Crafter model for use with MAGREF",
      "from": "Kijai"
    },
    {
      "creation": "Add Memory to Reserve node",
      "type": "node",
      "description": "Node to reserve additional memory for models beyond estimated requirements",
      "from": "Kosinkadink"
    },
    {
      "creation": "Layer extraction tool for PUSA",
      "type": "tool",
      "description": "Claude-coded tool for loading/injecting layer extractions with CLI script",
      "from": "JohnDopamine"
    },
    {
      "creation": "Modified InfiniteTalk workflow",
      "type": "workflow",
      "description": "Replaced MultiTalk with InfiniteTalk in existing Fantasy Portrait workflow for longer video generation",
      "from": "NC17z"
    },
    {
      "creation": "Fun Control multi-input workflow",
      "type": "workflow",
      "description": "Workflow with multiple control options including Flux generation, face landmarks, and various control inputs",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "WanMoeKSampler",
      "type": "node",
      "description": "Custom sampler for photo restoration experiments",
      "from": "cocktailprawn1212"
    },
    {
      "creation": "HiggsAudio ComfyUI Wrapper",
      "type": "wrapper",
      "description": "Alternative TTS wrapper with potential venv conflicts",
      "from": "NebSH"
    },
    {
      "creation": "MTVcrafter integration",
      "type": "workflow",
      "description": "Available in dev branch with test workflow",
      "from": "Kijai"
    },
    {
      "creation": "Dev branch updates",
      "type": "tool",
      "description": "Includes InfiniteTalk support and improved memory handling",
      "from": "Kijai"
    },
    {
      "creation": "Modified uni3c node",
      "type": "node",
      "description": "Automatically processes video input without manual latent connection when using multitalk sampling",
      "from": "Kijai"
    },
    {
      "creation": "Qwen to Wan bridge workflow",
      "type": "workflow",
      "description": "Integration between Qwen image generation and Wan video generation",
      "from": "fredbliss"
    },
    {
      "creation": "Automatic InfiniteTalk workflow",
      "type": "workflow",
      "description": "Modified Kijai's test v2v infinite talk workflow to run fully automatically",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "creation": "Tutorial video for InfiniteTalk",
      "type": "tutorial",
      "description": "Recording tutorial video for automatic InfiniteTalk workflow",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "creation": "Mask attention for Fantasy Portrait",
      "type": "modification",
      "description": "Hacky implementation of mask attention inside Fantasy Portrait extension",
      "from": "ManglerFTW"
    },
    {
      "creation": "Endless I2V workflow",
      "type": "workflow",
      "description": "Modified POM's endless workflow to use I2V instead of VACE",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "InfiniteTalk context method implementation",
      "type": "workflow",
      "description": "Working temporal mask implementation for Fun model",
      "from": "DawnII"
    },
    {
      "creation": "ComfyUI-QwenImageWanBridge",
      "type": "tool",
      "description": "Bridge for connecting Qwen image generation to Wan video generation with T2V method",
      "from": "fredbliss"
    },
    {
      "creation": "WanVideoWrapper InfiniteTalk V2V",
      "type": "node",
      "description": "Updated wrapper with streamlined workflow and faster sampling for video-to-video lip sync",
      "from": "Kijai"
    },
    {
      "creation": "Qwen Image to Wan latent bridge",
      "type": "workflow",
      "description": "Custom workflow for passing Qwen image latents directly to Wan generation",
      "from": "fredbliss"
    },
    {
      "creation": "Audio splitting custom node",
      "type": "node",
      "description": "Splits audio based on frame count and FPS for workflows with multiple samplers",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "Multiple character masking setup",
      "type": "workflow",
      "description": "Splits each character's face to 16:9 B&W mask for multi-character InfiniteTalk",
      "from": "mdkb"
    },
    {
      "creation": "Multiple people support for Fantasy Portrait",
      "type": "node",
      "description": "Pull request to add support for multiple people in Fantasy Portrait node",
      "from": "ManglerFTW"
    },
    {
      "creation": "TeaCache node",
      "type": "node",
      "description": "Caching node that caused user difficulties but provides memory optimization",
      "from": "Akumetsu971"
    },
    {
      "creation": "ComfyUI QwenImageWanBridge",
      "type": "tool",
      "description": "Bridge for using Qwen image processing with WAN, supports LoRAs and image editing",
      "from": "fredbliss"
    },
    {
      "creation": "ComfyUI-QwenImageWanBridge",
      "type": "tool",
      "description": "Nodes to convert Qwen image latents to WAN video, includes normalization and conversion nodes",
      "from": "fredbliss"
    },
    {
      "creation": "First person WAN 2.2 i2v LoRA",
      "type": "lora",
      "description": "Trained for first person perspective, works in both i2v and t2v",
      "from": "Drommer-Kille"
    },
    {
      "creation": "Blender control spline system",
      "type": "workflow",
      "description": "Using grease pencil outlines, curves and spheres for complex camera and object movement control",
      "from": "Blink"
    },
    {
      "creation": "EasyCache/LazyCache ComfyUI implementation",
      "type": "node",
      "description": "Native ComfyUI caching system supporting almost all models with configurable thresholds",
      "from": "Kosinkadink"
    },
    {
      "creation": "CineScale integration",
      "type": "node",
      "description": "RoPE frequency scaling support for resolution upscaling in WanVideoWrapper",
      "from": "Kijai"
    },
    {
      "creation": "Film grain LoRA",
      "type": "lora",
      "description": "Real film grain LoRA in training, Rank8, 73mb files",
      "from": "Drommer-Kille"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "Kijai's ComfyUI implementation for Wan models",
      "from": "Kijai"
    },
    {
      "creation": "CineScale LoRA quantized versions",
      "type": "lora",
      "description": "Kosinkadink quantized CineScale models for ComfyUI",
      "from": "Tony(5090)"
    },
    {
      "creation": "MelBandRoFormer wrapper",
      "type": "node",
      "description": "Vocal separation model wrapper for ComfyUI",
      "from": "Kijai"
    },
    {
      "creation": "Wan 2.2 FP8 E5M2 conversion",
      "type": "model",
      "description": "Converted 5B turbo model for older hardware compatibility",
      "from": "patientx"
    },
    {
      "creation": "Debug pass-through node",
      "type": "node",
      "description": "Outputs console info about image data, tensors, memory usage, and media properties for debugging",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "creation": "Uni3c workflow adaptation",
      "type": "workflow",
      "description": "Adapted Benji Future Thinkers workflow for Uni3c with controlnet support",
      "from": "mdkb"
    },
    {
      "creation": "Deepthroat LoRA",
      "type": "lora",
      "description": "Custom LoRA trained by community member, well received",
      "from": "Kenk"
    },
    {
      "creation": "Audio separation workflow",
      "type": "workflow",
      "description": "Extracts audio from every song, even from videos using voice isolation model",
      "from": "Kenk"
    },
    {
      "creation": "WanVideoAddControlEmbeds node",
      "type": "node",
      "description": "Alternative control node for Fun Control system",
      "from": "Kijai"
    },
    {
      "creation": "WanVacePhantomDualV2",
      "type": "node",
      "description": "Handles dual control inputs for VACE",
      "from": "Ablejones"
    },
    {
      "creation": "Helper node for mask operations",
      "type": "node",
      "description": "Originally for Fun InP, works for VACE by inverting mask",
      "from": "Kijai"
    },
    {
      "creation": "ComfyUI_WanVace-pipeline",
      "type": "node pack",
      "description": "Timeline tools and workflow management for Wan/VACE",
      "from": "tarkansarim"
    },
    {
      "creation": "Fakevace implementation",
      "type": "workflow",
      "description": "Alternative VACE that works with extend nodes",
      "from": "771193439399444490"
    },
    {
      "creation": "New anime style LoRA for Wan 2.2",
      "type": "lora",
      "description": "Community-created LoRA with detailed process writeup",
      "from": "crinklypaper"
    },
    {
      "creation": "WanVideoWrapper dev version",
      "type": "node",
      "description": "Refactored model loading system for ComfyUI",
      "from": "Kijai"
    },
    {
      "creation": "wav2vec2 safetensors conversion",
      "type": "model conversion",
      "description": "Converted wav2vec2 to safetensors with fp16 version for easier setup",
      "from": "Kijai"
    },
    {
      "creation": "VACE workflow zip package",
      "type": "workflow",
      "description": "Contains two VACE workflows including one for restyling",
      "from": "mdkb"
    },
    {
      "creation": "Kenk's Wan LoRAs",
      "type": "lora",
      "description": "Civitai Wan LoRA creator, fantastic work according to user feedback",
      "from": "Kenk"
    },
    {
      "creation": "CFG Skimming implementation",
      "type": "workflow",
      "description": "Alternative to 3-sampler setup with better prompt adherence",
      "from": "Mu5hr00m_oO"
    },
    {
      "creation": "WanVideo Scheduler Selector",
      "type": "node",
      "description": "Custom node for better scheduler control",
      "from": "Mu5hr00m_oO"
    },
    {
      "creation": "Cached CLIP encoding for KJNodes",
      "type": "node",
      "description": "Performance improvement for CLIP encoding",
      "from": "Mu5hr00m_oO"
    },
    {
      "creation": "CFG Skimming experiment",
      "type": "technique",
      "description": "Lightning 4 steps generation with LoRA on HIGH & LOW noise",
      "from": ": Not Really Human :"
    },
    {
      "creation": "Class extraction script",
      "type": "tool",
      "description": "Python script to split nodes.py into individual classes for LLM analysis of bugs",
      "from": "MysteryShack"
    },
    {
      "creation": "Character/pose sheet workflow",
      "type": "workflow",
      "description": "done in a past with flux/sdxl/cn. now with wan. char/anything sheet",
      "from": "N0NSens"
    },
    {
      "creation": "WanVideoWrapper native implementation",
      "type": "node",
      "description": "Native ComfyUI implementation providing raw model controls",
      "from": "comfy"
    },
    {
      "creation": "CFG Skimming port to WanVideoWrapper",
      "type": "node",
      "description": "Port of CFG skimming technique to reduce burn-in effects",
      "from": "Mu5hr00m_oO"
    },
    {
      "creation": "All additional quant sizes for S2V",
      "type": "model",
      "description": "Complete set of quantized model sizes uploaded",
      "from": "orabazes"
    },
    {
      "creation": "Custom Sapiens pose implementation",
      "type": "node",
      "description": "Improved pose detection with filtering for VACE, better than existing Sapiens node",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "creation": "S2V pose input node",
      "type": "node",
      "description": "New pose_input node available in S2V branch",
      "from": "Kijai"
    },
    {
      "creation": "PUSA flowmatch sampler",
      "type": "node",
      "description": "Sampler that expands timesteps for PUSA functionality",
      "from": "DawnII"
    },
    {
      "creation": "Iron Maiden Eddie tribute video",
      "type": "workflow",
      "description": "AI reimagined tribute using Wan 2.1 to animate iconic album covers and bring Eddie to life",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "creation": "fp8 Tensor subclassing for memory efficiency",
      "type": "tool",
      "description": "Custom torch.Tensor subclass to store frames in fp8 and convert on-the-fly for operations, enabling 1000+ 4K frames in ComfyUI",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "creation": "diffusion-pipe-TREAD",
      "type": "tool",
      "description": "Fork from Adamanthy that makes training faster than standard diffusion-pipe",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "creation": "Context window upscaler workflow",
      "type": "workflow",
      "description": "Uses new core context windows to lower VRAM needs for upscaling",
      "from": "ArtOfficial"
    },
    {
      "creation": "Sigma visualization node",
      "type": "node",
      "description": "Custom node to visualize sigma curves and timing information",
      "from": "Mu5hr00m_oO (5080 + 64)"
    },
    {
      "creation": "Custom VACE strength per frame",
      "type": "custom node",
      "description": "Allows setting different VACE strengths for each latent frame",
      "from": "Ablejones"
    },
    {
      "creation": "WanVaceAdvanced",
      "type": "custom node",
      "description": "Advanced VACE implementation with per-frame strength control",
      "from": "Ablejones"
    },
    {
      "creation": "Torch compile fix",
      "type": "tool",
      "description": "Fix for torch compile surviving prompt changes by padding embeddings",
      "from": "phazei"
    },
    {
      "creation": "Context window workflow",
      "type": "workflow",
      "description": "High noise + low noise two-pass system with different overlap/stride settings",
      "from": "Kijai"
    },
    {
      "creation": "Matrix recreation project",
      "type": "workflow",
      "description": "55 shots total Matrix scene recreation using gorilla character replacement, using VACE, DWPose, depth control, and various techniques per shot",
      "from": "Nekodificador"
    },
    {
      "creation": "Face refining workflow",
      "type": "workflow",
      "description": "Experimenting with face refining for better lip movement when the face is relatively small in the video",
      "from": "Ablejones"
    },
    {
      "creation": "WanFirstLastFrameToVideo",
      "type": "node",
      "description": "Custom node that replaces WanImageToVideo for first/last frame video generation",
      "from": "Juampab12"
    }
  ]
}