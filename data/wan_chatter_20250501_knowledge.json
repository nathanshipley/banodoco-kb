{
  "channel": "wan_chatter",
  "date_range": "2025-05-01 to 2025-06-01",
  "messages_processed": 19695,
  "chunks_processed": 50,
  "api_usage": {
    "input_tokens": 608667,
    "output_tokens": 137225,
    "estimated_cost": 3.884376
  },
  "extracted_at": "2026-02-03T03:51:46.111953Z",
  "discoveries": [
    {
      "finding": "Q8 GGUF version has higher quality than fp8",
      "details": "Despite being slower, Q8 GGUF produces better quality than fp8 version",
      "from": "MilesCorban"
    },
    {
      "finding": "TeaCache skips conditional and unconditional steps automatically",
      "details": "Shows skipped steps like [7, 9, 11, 13, 15, 17, 19, 21, 23] during generation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "SLG requires half the CFG value",
      "details": "If using 6 CFG without SLG, use 3 CFG with SLG",
      "from": "Miku"
    },
    {
      "finding": "Fun camera control has issues with extreme rotations",
      "details": "Model wasn't trained with major rotational changes, tends to push beyond limits",
      "from": "Kijai"
    },
    {
      "finding": "Fun camera control works better at 768p and higher resolution",
      "details": "Low resolutions like 656x384 cause bad results with camera movement",
      "from": "Kijai"
    },
    {
      "finding": "1.3B model is worse with extreme camera angles than 14B",
      "details": "14B can handle more extreme angles sometimes, 1.3B struggles more",
      "from": "Kijai"
    },
    {
      "finding": "Fun 1.1 models support reference image feature",
      "details": "New feature allows reference image input, different from start image",
      "from": "Kijai"
    },
    {
      "finding": "LoRA effects diminish dramatically after 3 steps in v2v workflows",
      "details": "Effects are strongest on the 2nd step, after the third step style changes into more generic AI look. Can observe this by watching preview as it samples",
      "from": "David Snow"
    },
    {
      "finding": "Two-pass v2v technique for better stylization",
      "details": "Stop first sampler after 3 steps, then feed to second sampler for refinement. Gets rock solid results at low step counts (6-7 steps total) with much stronger stylization",
      "from": "David Snow"
    },
    {
      "finding": "Video Depth Anything has bad banding issues that affect VACE and Fun Control",
      "details": "Creates hard lines/ridges in depth pass that cause wobbling artifacts in results, regular Depth Anything v2 performs better despite flickering",
      "from": "Nathan Shipley"
    },
    {
      "finding": "LoRA training timestep distribution affects performance",
      "details": "Sigmoid focuses on central steps for character, shift focuses on start/end for style. General consensus is shift for style, sigmoid for character",
      "from": "mamad8"
    },
    {
      "finding": "Video Depth Anything produces banding artifacts on depth maps",
      "details": "Creates ugly lazily-undulating banding lines on walls and gradients, where the model fixates on banding and turns them into edges in generations",
      "from": "Nathan Shipley"
    },
    {
      "finding": "Depth Anything V2 performs better than Video Depth Anything for VACE",
      "details": "The chattery/flickery nature of regular depth anything actually benefits generations because banding gets averaged out over generation versus being fixated on by the model",
      "from": "Nathan Shipley"
    },
    {
      "finding": "Multiplying inverted AnyLine pass on top of depth pass helps stabilize",
      "details": "Combining AnyLine with depth maps provides better stabilization for VACE control",
      "from": "Nathan Shipley"
    },
    {
      "finding": "Binary thresholding at 0.5 helps VACE line art control",
      "details": "For line art with VACE, thresholding values at 0.5 to make the map binary is really helpful",
      "from": "Rishi Pandey"
    },
    {
      "finding": "VACE struggles with multiple control types",
      "details": "There is so much specificity in what the control map needs to be, VACE has difficulty using multiple control types effectively",
      "from": "Rishi Pandey"
    },
    {
      "finding": "Optical flow can be used as VACE input",
      "details": "VACE paper shows optical flow data as input, could lead to interesting effects based on movement - no signal until something moves",
      "from": "Nathan Shipley"
    },
    {
      "finding": "Ex LoRA enables up to 225 frames comfortably",
      "details": "Using the extension LoRA, you can comfortably generate up to 225 frames",
      "from": "Rishi Pandey"
    },
    {
      "finding": "DepthCrafter produces more stable results than Video Depth Anything",
      "details": "Ranking: Depthcrafter > Video-depth-anything > depthanything V2",
      "from": "David Snow"
    },
    {
      "finding": "Torch compile optimization works well with wan models",
      "details": "Multiple users report successful compilation for speed improvements, though some encounter triton-related errors",
      "from": "Kijai"
    },
    {
      "finding": "DF models can be chained for longer video generation",
      "details": "Generate 5 second video, feed last frames to next section, drop duplicated frames and combine",
      "from": "MilesCorban"
    },
    {
      "finding": "Wan 720p and 480p have identical generation times",
      "details": "Testing shows no speed difference between resolutions, 720p has better prompt adherence",
      "from": "N0NSens"
    },
    {
      "finding": "Style transfer working with WAN using new method",
      "details": "Successfully implemented style transfer that doesn't transfer structure, works with both WAN and LTXV",
      "from": "Clownshark Batwing"
    },
    {
      "finding": "Normal maps produce better results than depth for some workflows",
      "details": "After extensive testing, normal maps provide additional information that improves generation quality",
      "from": "David Snow"
    },
    {
      "finding": "WanVideoSampler has teacache support that can be controlled with use_non_blocking parameter",
      "details": "Setting self.use_non_blocking = False in line 879 of model.py fixes CUDA runtime errors",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "finding": "Wan 2.1 has frame limits based on variant",
      "details": "Vanilla has 81 frame limit before looping, Skyreels v2 540p has 97 frames, 720p has 121 frames",
      "from": "N0NSens"
    },
    {
      "finding": "Skyreels runs at different fps than vanilla",
      "details": "Vanilla runs at 16fps, Skyreels at 24fps, but fps is abstract and can be adjusted during output",
      "from": "N0NSens"
    },
    {
      "finding": "Wan 2.1 context length varies by model",
      "details": "201 frames is the loop point for some variants, 81 is recommended chunk size",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Multiple LoRAs have merging issues in Wan",
      "details": "When using 2 LoRAs, one often doesn't get used properly, lowering strength doesn't help",
      "from": "Dream Making"
    },
    {
      "finding": "VACE can handle 720p resolution",
      "details": "VACE has no problem with 720p generation when properly configured",
      "from": "David Snow"
    },
    {
      "finding": "CausVid 14B works with 2-4 steps and cfg 1.0",
      "details": "Single step works but 4 steps seems optimal, no CFG needed",
      "from": "Kijai"
    },
    {
      "finding": "CausVid needs torch.compile to run",
      "details": "Out of memory without torch.compile even with small inputs",
      "from": "Kijai"
    },
    {
      "finding": "CausVid has flex attention support",
      "details": "Need to set frame per block to 3 for flex attention block mask",
      "from": "Kijai"
    },
    {
      "finding": "Setting Empty embed value to 97 fixes tensor size error",
      "details": "Changed from 53 to 97, other Empty embeds also at 97",
      "from": "Dream Making"
    },
    {
      "finding": "Format setting to 'Wan' auto-adjusts frames for model conditioning",
      "details": "Will add/subtract a frame to fit the model conditionings",
      "from": "Valle"
    },
    {
      "finding": "PyTorch 2.7.0+cu126 incompatible with xformers",
      "details": "Need xformers-0.0.31.dev1030 for compatibility",
      "from": "A.I.Warper"
    },
    {
      "finding": "CausVid inference is extremely fast",
      "details": "193 frames in 2 minutes on 4060ti, 4 steps gets 90% quality",
      "from": "V\u00e9role"
    },
    {
      "finding": "IPAdapter runs at 256x256 resolution",
      "details": "Makes it hard to get enough detail for consistent character generation",
      "from": "Draken"
    },
    {
      "finding": "Shift parameter doesn't affect CausVid",
      "details": "Set timestep schedule ignores shift values, can use shift 1.0",
      "from": "Kijai"
    },
    {
      "finding": "CausVid model has unusually high saturation in videos",
      "details": "Wan base model looks better than CausVid due to saturation issues",
      "from": "yi"
    },
    {
      "finding": "CausVid uses sliding window with kv_cache for generation",
      "details": "Uses 3 latents in sliding window, can generate videos endlessly and show video in real time while generating",
      "from": "Kijai"
    },
    {
      "finding": "Flex attention fixes latent flicker at beginning",
      "details": "Uses BlockMask to have attention only attend adjacent frames in sequence dimension",
      "from": "Kijai"
    },
    {
      "finding": "CausVid seed variation is very limited",
      "details": "Next to no variation of seed, only way to get big changes is to change the prompt",
      "from": "Cubey"
    },
    {
      "finding": "MoviiGen has better prompt adherence than base Wan",
      "details": "Prompt adherence seems to be a little better with MoviiGen finetune",
      "from": "yi"
    },
    {
      "finding": "MoviiGen LoRAs work but may need higher strength",
      "details": "LoRAs work with MoviiGen but lora strength needs to be a little higher",
      "from": "yi"
    },
    {
      "finding": "VACE 14B applies less frequently than 1.3B - every 5th block vs every other block",
      "details": "14B VACE is relatively faster than 1.3B VACE due to fewer block applications, though still slower overall due to larger size",
      "from": "Kijai"
    },
    {
      "finding": "14B VACE works better at higher resolutions, 1.3B works better at low res",
      "details": "720x720 and 1024x1024 work well with 14B, while 1.3B handles lower resolutions better",
      "from": "Kijai"
    },
    {
      "finding": "VACE can work with Causvid distilled models",
      "details": "Successfully tested 14B VACE with Causvid for faster inference",
      "from": "DawnII"
    },
    {
      "finding": "New VACE 1.3B final version produces sharper but less motion compared to preview",
      "details": "Final version shows improved sharpness but reduced movement, may need setting adjustments",
      "from": "multiple users"
    },
    {
      "finding": "VACE uses different concept than I2V - places images in latents rather than cross-attention embeds",
      "details": "VACE is technically an extension for T2V model, not traditional I2V",
      "from": "Kijai"
    },
    {
      "finding": "VACE can extend video infinitely using cyclical setup",
      "details": "Generate 81 frames at a time where each video drives the next one making it smooth",
      "from": "Juampab12"
    },
    {
      "finding": "CausVid has autoregressive capability for longer videos",
      "details": "CausVid has autoregressive mode for actual autoregression, not just VACE extension",
      "from": "DawnII"
    },
    {
      "finding": "VACE 14B needs higher resolution to work properly",
      "details": "Initial impression is that 14B VACE mostly needs higher resolution - comparison shows 640x640 vs 720x720 vs 1024x1024",
      "from": "Kijai"
    },
    {
      "finding": "MoviiGen outputs at 24fps",
      "details": "MoviiGen is trained on cinematic data and outputs 24fps, not 16fps as initially thought",
      "from": "DawnII"
    },
    {
      "finding": "CausVid has very low seed variation",
      "details": "CausVid produces nearly identical outputs across different seeds with only minor differences",
      "from": "TK_999"
    },
    {
      "finding": "CausVid can be extracted as a LoRA and works better with VACE at reduced strength",
      "details": "CausVid extracted as LoRA rank32, works at 0.2-0.4 strength, allows VACE to work better, sweet spot around 0.2-0.4 strength",
      "from": "Kijai"
    },
    {
      "finding": "CausVid LoRA works well at just 4 steps with CFG 1.0",
      "details": "4 steps appears to be sweet spot for quality vs speed, generates in ~41 seconds",
      "from": "Kijai"
    },
    {
      "finding": "Lineart for VACE needs to be inverted",
      "details": "When using VACE control with lineart, the input needs to be inverted",
      "from": "DawnII"
    },
    {
      "finding": "Reference images for VACE should have white background padding and not be full frame",
      "details": "Reference image should be background removed subject or image with white padding, not first frame",
      "from": "Kijai"
    },
    {
      "finding": "Using masks helps with VACE consistency",
      "details": "Masks can improve consistency issues with VACE",
      "from": "DawnII"
    },
    {
      "finding": "Flex attention is detrimental when using reduced CausVid LoRA strength",
      "details": "When using CausVid LoRA at reduced strength, flex attention shouldn't be used",
      "from": "Kijai"
    },
    {
      "finding": "CausVid LoRA reduces generation time by 5x with minimal quality loss",
      "details": "4 steps with CausVid LoRA takes ~1 minute vs 5 minutes with usual workflow on RTX 3090",
      "from": "seruva19"
    },
    {
      "finding": "CausVid LoRA works with both T2V and I2V models",
      "details": "LoRA can be applied to both model types, though effectiveness on I2V not fully tested",
      "from": "Kijai"
    },
    {
      "finding": "14B CausVid at 4 steps is faster than 1.3B at 20-30 steps",
      "details": "Speed comparison showing 14B model with distillation outperforms 1.3B base model",
      "from": "DawnII"
    },
    {
      "finding": "CausVid makes generations more deterministic",
      "details": "Same prompt tends to produce similar results despite seed changes",
      "from": "Ada"
    },
    {
      "finding": "LoRA strength affects noise quality but motion remains consistent",
      "details": "Different strength values change visual quality but preserve motion patterns",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Combining multiple LoRAs requires higher CausVid strength",
      "details": "When using UniAnimate + CausVid, CausVid needs 0.8-0.9 strength vs normal 0.4-0.6",
      "from": "Kijai"
    },
    {
      "finding": "CausVid 1.3B as LoRA doesn't have the latent flashing issue at all",
      "details": "Works better as a normal model for T2V generation compared to the full CausVid model",
      "from": "Kijai"
    },
    {
      "finding": "14B VACE is vastly superior for reference image consistency",
      "details": "Much better at keeping reference image fidelity compared to 1.3B version",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "FreSca works with CausVid at cfg 1.0",
      "details": "Only enhancement technique that functions with CausVid, but requires lowered values to avoid noise",
      "from": "Kijai"
    },
    {
      "finding": "VACE 14B better understands control video nuances",
      "details": "Better at handling complex movements like legs crossing over each other, character spinning around",
      "from": "A.I.Warper"
    },
    {
      "finding": "FP32 DWPose embeds crash ComfyUI with long UniAnimate clips",
      "details": "Changing DWPose embeds to model dtype fixes the crashes",
      "from": "Kijai"
    },
    {
      "finding": "VACE reference images work exceptionally well with Wan models",
      "details": "Using reference images with VACE produces high quality results that maintain character consistency and can handle wide shots with character and scenery",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "VACE can do effective outpainting",
      "details": "VACE outpainting works well and respects prompts intelligently, choosing 'union type' from inputs/prompt",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "CausVid LoRA should be placed after other LoRAs",
      "details": "Placing CausVid LoRA after other LoRAs in the chain fixes blurry results",
      "from": "yi"
    },
    {
      "finding": "Gray mask color (127,127,127 RGB or 0.5) is optimal for VACE inpainting/outpainting",
      "details": "0.5 gray is what VACE tries to fill, more important for inpainting and temporal masks",
      "from": "Kijai"
    },
    {
      "finding": "VACE remembers object details well in outpainting",
      "details": "VACE successfully remembers what objects like scooters look like when outpainting",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Context ref works in timing dimension",
      "details": "Context reference appears to work effectively in the timing dimension for VACE",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "CausVid LoRA works with both 1.3B and 14B models",
      "details": "Kijai extracted CausVid LoRA for both model sizes, available on HuggingFace",
      "from": "DiXiao"
    },
    {
      "finding": "CausVid 1.3B generates 1280x720x81 video very fast",
      "details": "26 seconds generation + 30 seconds VAE decoding on 4070Ti, total 56 seconds",
      "from": "DiXiao"
    },
    {
      "finding": "CausVid inference logic is not used as intended currently",
      "details": "The proper inference logic would be different but current implementation works, especially good combined with VACE",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "1.3B CausVid generates anime content particularly well",
      "details": "User noticed significantly better anime generation quality with CausVid compared to standard 1.3B",
      "from": "DiXiao"
    },
    {
      "finding": "Official 1.3B VACE full model available",
      "details": "6GB full model that doesn't require separate VACE model loading",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Shift parameter affects background quality but not motion",
      "details": "Shift 30 kills grass/road detail compared to shift 8, but motion remains the same",
      "from": "N0NSens"
    },
    {
      "finding": "Causvid Lora significantly reduces generation time",
      "details": "Cut wan time down by 30x compared to normal generation",
      "from": "The Punisher"
    },
    {
      "finding": "Lower Causvid strength with more steps improves quality",
      "details": "0.25 strength at 6 steps gives incredible quality while maintaining good motion",
      "from": "The Punisher"
    },
    {
      "finding": "Lower denoise improves motion with Causvid",
      "details": "For i2v with causvid lora, lowering denoise to 0.75 improves movement and prompt adherence significantly",
      "from": "Jonathan"
    },
    {
      "finding": "Inverted openpose prevents stick artifacts in VACE",
      "details": "Haven't seen the sticks appear since switching to inverted openpose with VACE 14B",
      "from": "ArtOfficial"
    },
    {
      "finding": "Q3_K_S GGUF VACE module works",
      "details": "Successfully generated 400x400x49f video with Q3_k_s GGUF VACE, 6 steps, causvid lora 0.25",
      "from": "artemonary"
    },
    {
      "finding": "GGUF VACE models work with native ComfyUI implementation",
      "details": "Q5_K_S GGUF provides significantly better quality than Q3_K_S, working well for video generation",
      "from": "The Punisher"
    },
    {
      "finding": "Model offloading to RAM doesn't significantly impact speed",
      "details": "Can offload CLIP to second GPU and model to RAM without major speed penalty",
      "from": "The Punisher"
    },
    {
      "finding": "Causvid LoRA works with GGUF models",
      "details": "Compatible with native VACE workflow using GGUF quantized models",
      "from": "The Punisher"
    },
    {
      "finding": "VACE reference encoding affects quality",
      "details": "Encoding reference image separately from control improves results",
      "from": "Piblarg"
    },
    {
      "finding": "Style transfer works better at lower resolution",
      "details": "VACE style transfer with HiDream transfers more closely at lower res",
      "from": "Clownshark Batwing"
    },
    {
      "finding": "Beta57 scheduler available for VACE",
      "details": "Alternative scheduler option for VACE generation",
      "from": "Clownshark Batwing"
    },
    {
      "finding": "Shift parameter of 2.5 works well for CausVid generations",
      "details": "User found shift values over 15 give blurry details, need to reduce below 5, with 2.5 being solid",
      "from": "Jemmo"
    },
    {
      "finding": "14 steps significantly improves quality over 8 steps with minimal time penalty",
      "details": "14 steps smooths out shimmer that appears in 8 step outputs, adds quality for small gen time increase",
      "from": "CaptHook"
    },
    {
      "finding": "CausVid LoRA with MoviiGen model produces better results than CausVid LoRA with CausVid model",
      "details": "Image quality much better, especially lighting and movie texture when using CausVid lora + MoviiGen model vs both CausVid components",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Reward LoRAs fix blurry and oversaturated look of CausVid",
      "details": "HPS and MPS reward loras improve generation quality 50-80%, both aim to improve motion and quality",
      "from": "yi"
    },
    {
      "finding": "MoviiGen may be best model for start/end frame generation",
      "details": "User reports it works particularly well for this use case",
      "from": "DawnII"
    },
    {
      "finding": "Style guide videos transfer motion information better than images",
      "details": "Using images for style guides results in stuttering, videos provide better motion transfer",
      "from": "Clownshark Batwing"
    },
    {
      "finding": "Normal I2V workflow supports end frame input without FLF or VACE",
      "details": "Can plug end image into normal I2V workflow and it works for first-frame-last-frame transitions",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "VACE with T2V model can replicate I2V functionality 4x faster",
      "details": "Using VACE workflow with start image, turning off end image, using T2V model + CausVid LoRA can achieve same I2V results but 4x faster (1min vs 4-5min generation time)",
      "from": "N0NSens"
    },
    {
      "finding": "V2V produces very clean results",
      "details": "Video-to-video using 0.6 denoise produces clean output quality",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "14B model timing performance on RTX 3090",
      "details": "238 seconds for 121 frames at 960x544 with 5 steps using sky14b720p+causvid, 146 seconds for 832x480 at 30 blocks",
      "from": "N0NSens"
    },
    {
      "finding": "Depth map provides better facial movement than MediaPipe",
      "details": "Depth preprocessing shows more facial movement transfer compared to MediaPipe face mesh",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Sapiens provides superior facial transfer precision",
      "details": "Sapiens facial preprocessing is much more precise than standard media pipe face for face control",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Reference image doesn't need to match first frame",
      "details": "VACE reference image can be completely unrelated to input video first frame, making it much more flexible than expected",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "VACE was trained with DWPose, not MediaPipe",
      "details": "VACE training used DWPose rather than MediaPipe for pose control",
      "from": "A.I.Warper"
    },
    {
      "finding": "CausVid and TeaCache don't work well together",
      "details": "Low-step CausVid workflows have issues when TeaCache is enabled",
      "from": "MilesCorban"
    },
    {
      "finding": "Depth preprocessor works better than pose/mediapipe for facial movement in VACE",
      "details": "Tested pose/mediapipe, facemesh, and depth - depth gets the facial movement best",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "CausVid Lora works with Fantasy Talking for fast generation",
      "details": "Sampling 81 frames at 720x720 with 5 steps, completed in 65.39 seconds with max VRAM usage of 21.382 GB",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Multiple VACE embeds can be combined using separate encoders",
      "details": "Using depth and pose together with separate VACE embeds, but using reference image in both can cause overcooking",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Face landmark nodes work with WAN VACE for facial movement",
      "details": "Face landmark detection can be used as control input, works both inverted (white on black) and non-inverted",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Iterative latent upscale works with VACE",
      "details": "Video restyle with triple sampler passes using latent upscale, all at denoise 1.0, with depth and pose blend as control",
      "from": "yo9o"
    },
    {
      "finding": "CausVid LoRA noise_aug_strength helps with movements",
      "details": "Using noise_aug_strength at about 0.5 or more helps with movements in CausVid, though it makes output slightly blurry requiring small upscale after",
      "from": "Ada"
    },
    {
      "finding": "CausVid shift parameter only affects sigma curve",
      "details": "Shift parameter affects sigma curve, not CFG, so has no effect with low CFG values like 1 used with CausVid",
      "from": "Kijai"
    },
    {
      "finding": "Inverted MediaPipe pose works for control",
      "details": "Blending normal pose with inverted MediaPipe pose creates interesting control effects",
      "from": "V\u00e9role"
    },
    {
      "finding": "FP16 model with FP8 quantization uses less VRAM than native FP8",
      "details": "FP16 model quantized to FP8 uses less VRAM than the native FP8 model (around 88% on 5090)",
      "from": "ingi // SYSTMS"
    },
    {
      "finding": "FP32 precision dramatically improves quality",
      "details": "Running FP16 model at FP32 precision gives insane quality, especially with T5 and VAE upcast improving prompt adherence",
      "from": "ingi // SYSTMS"
    },
    {
      "finding": "CausVid LoRA makes 14B model viable for practical use",
      "details": "Allows for high quality output at 4-9 steps with significant speed improvements",
      "from": "David Snow"
    },
    {
      "finding": "Multiple VACE embeds enhance output quality",
      "details": "Using separate embeds for pose and depth produces better results than blending them",
      "from": "David Snow"
    },
    {
      "finding": "Single reference image works better with multiple VACE embeds",
      "details": "When using multiple VACE encoders, using only one reference image produces better results",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "VACE can infer motion from masked RGB frames without control",
      "details": "VACE appears to analyze RGB information even with simple flat masks to understand motion",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "TeaCache requires higher threshold values with VACE 14B CausVid setup",
      "details": "Need threshold of 0.6+ instead of normal values, though quality may suffer",
      "from": "David Snow"
    },
    {
      "finding": "Every 5th block disabling improves VACE pose following",
      "details": "Disabling every 5th block when using CausVid with 14B reduces interference with VACE inputs",
      "from": "DawnII"
    },
    {
      "finding": "VACE 14B can run on 12GB VRAM with block swapping",
      "details": "81 frames possible on 12GB VRAM using block swapping technique",
      "from": "A.I.Warper"
    },
    {
      "finding": "VACE always adds 4 frames to the set frame count",
      "details": "When setting 97 frames, it generates 101 frames. This is because latents are batches of 4, plus one for init frame",
      "from": "N0NSens"
    },
    {
      "finding": "Normal maps work well with VACE for facial tracking",
      "details": "Using Sapiens normal maps or NormalCrafter gives good facial tracking results at 0.9 strength",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "CausVid LoRA works at different strengths for different use cases",
      "details": "0.3 strength with 12 steps or 0.6 strength with 4 steps work well. For motion with other LoRAs, 0.7 strength is better than 0.5",
      "from": "CFSStudios"
    },
    {
      "finding": "Double control (depth + pose) produces better VACE outputs",
      "details": "Using two VACE encode nodes with different control types (depth and pose) and compositing them together",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Pose control works better than face mesh for character animation",
      "details": "Testing showed pose-based control gives better results than face mesh for VACE workflows",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "CausVid dramatically speeds up generation",
      "details": "Using CausVid with 1280x780 reduced generation time from 30 minutes to 4 minutes",
      "from": "boorayjenkins"
    },
    {
      "finding": "Memory optimization fix for TeaCache",
      "details": "Fixed TeaCache using ~750MB unnecessary VRAM at 1280x780 by reversing .clone() and .to(device) operations - move to cpu first then clone",
      "from": "Kijai"
    },
    {
      "finding": "Single frame generation with WAN T2V as image generator",
      "details": "Can set WAN to one frame at very high resolution to use as image generator instead of loading separate models like Flux",
      "from": "Thom293"
    },
    {
      "finding": "CFG 1.5 with SLG shows improvement over CFG 1.0",
      "details": "5 steps with cfg 1.5 + SLG produces better results than cfg 1.0, worth the 2x inference time",
      "from": "Kijai"
    },
    {
      "finding": "VACE supports inpainting by using both gray area (RGB 127, 127, 127) and input mask together",
      "details": "Fill the inpaint area with gray (127, 127, 127) in the RGB input AND provide a black/white mask where black areas are kept as-is",
      "from": "Kijai"
    },
    {
      "finding": "VACE 14B works better with separated control and reference using two embed nodes",
      "details": "Using separate embed nodes for control and reference images works much better with 14B model than before",
      "from": "Kijai"
    },
    {
      "finding": "VACE takes actual composition of reference image into account",
      "details": "The model considers the full composition and layout of the reference image, not just the subject",
      "from": "Kijai"
    },
    {
      "finding": "CausVid LoRA dramatically improves inference speed",
      "details": "Makes inference time bearable for 14B model",
      "from": "Kijai"
    },
    {
      "finding": "240 frames possible on 4090 with CausVid LoRA",
      "details": "User successfully generated 240 frames on 4090 using 14B model with CausVid LoRA, no longer needs teacache",
      "from": "Cubey"
    },
    {
      "finding": "Character sheets work better for behind/back views",
      "details": "Using character sheet format helps generate views of subjects from behind that wouldn't be visible in single reference",
      "from": "Kijai"
    },
    {
      "finding": "CausVid supports very low step counts with good quality",
      "details": "CausVid can generate videos with as few as 4 steps, with good starting point being 0.5 strength and 4 steps",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "CausVid strength affects step requirements",
      "details": "Lower CausVid strength requires more steps. 0.4 with 8 steps or 0.5 with 4 steps work well",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "VACE inpainting requires both gray fill and mask",
      "details": "VACE inpainting needs the area filled with gray on the color image AND also a mask - using only mask doesn't replace anything, gray only oversaturates",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Raw input frames can be used directly with reduced strength",
      "details": "Plugging input video straight into input frames at 0.505 strength plus normal map encoder at 0.500 gives good results",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Normal map significantly improves VACE results",
      "details": "Having normal map makes a big difference compared to not using it when doing direct input frame processing",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Context options can interfere with motion tracking",
      "details": "Using context options with CausVid can make it not follow motion at all, removing context improved results",
      "from": "VK (5080 128gb)"
    },
    {
      "finding": "TeaCache properly skips diffusion steps",
      "details": "TeaCache literally skips the diffusion step and moves to the next one. You can see it working when progress jumps a step - if it doesn't jump, TeaCache isn't doing anything",
      "from": "Draken"
    },
    {
      "finding": "VACE reference frame alignment is critical",
      "details": "Key is setting up reference frame to be as aligned as possible. Usually you can do any 2 of: control video + reference frame + LoRA, but all three together ends up ignoring the reference frame",
      "from": "Piblarg"
    },
    {
      "finding": "First frame restyle technique",
      "details": "You don't need control frames in every frame - can use them to create a better starting frame that VACE will pick up on. Better to do a short run first and then use a frame from that for your longer run",
      "from": "Piblarg"
    },
    {
      "finding": "VACE mask behavior",
      "details": "The mask that goes into VACE works as: black = use input directly, white = controlnet mode. It's not like traditional CN masking",
      "from": "Draken"
    },
    {
      "finding": "WAN can generate high resolution single frames",
      "details": "WAN 14B can generate 10.48MP images (7280x1440) with good detail. Going much higher starts losing detail like the 1.3B model. Works best when fed Flux generations to guide it",
      "from": "ZombieMatrix"
    },
    {
      "finding": "Context window generation slows down significantly",
      "details": "81 frames takes 2 minutes, but 200 frames with context takes 20-30 minutes",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Context window frame math must be precise",
      "details": "Formula is (4*n)+1, so 101 frames works but 99 frames doesn't work with context window",
      "from": "A.I.Warper"
    },
    {
      "finding": "Input video strength 0.3 loses facial details",
      "details": "At 0.3 the input video is lost so the facial basically goes away for the most part",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Shift parameter affects image clarity",
      "details": "Having any shift at all made it blurry, removing shift entirely (set to 1) improved results",
      "from": "Ada"
    },
    {
      "finding": "Input video combined with normal map captures facial movement",
      "details": "Using input video at 0.4 and normal at 0.6 without latent sync captured mouth movement better than other methods",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "VACE reference image strength can be set above 1.0 with Causvid",
      "details": "Previously setting reference strength above 1.0 caused messed up outputs, but with Causvid it works okay and helps maintain character consistency when fighting against control inputs like depth",
      "from": "Johnjohn7855"
    },
    {
      "finding": "Separate VACE encode nodes for different control types",
      "details": "Using two separate VACE embed nodes - one for reference image and one for depth video - allows setting different strengths for each control input",
      "from": "Johnjohn7855"
    },
    {
      "finding": "Depth video effects start above 0.53 strength",
      "details": "In testing, depth video control begins to have noticeable effect above 0.53 strength, becomes very strong above 0.6",
      "from": "Johnjohn7855"
    },
    {
      "finding": "Adding depth around face helps preserve identity",
      "details": "Adding a tiny bit of depth information around the face area helps maintain character identity better, though eyes still need consideration",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "finding": "CFG=1 required for 4-step LoRA with standard WAN T2V",
      "details": "When using the 4-step LoRA with standard WAN T2V, CFG must be set to 1 for proper function",
      "from": "JohnDopamine"
    },
    {
      "finding": "Grid search for settings works better than manual tuning",
      "details": "Doing systematic grid searches for optimal settings produces better results than trying to set parameters manually",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "finding": "Spline editor can control VACE strength and CFG per step",
      "details": "Both CFG and VACE strength can be scheduled/animated using spline editor functionality on a per-step basis",
      "from": "Kijai"
    },
    {
      "finding": "Context windows loses character likeness but retains camera movement from pose data",
      "details": "161 frame test showed context windows implementation loses character identity but preserves camera movement even with only pose control data provided",
      "from": "A.I.Warper"
    },
    {
      "finding": "Batching method preserves character likeness but loses camera movement",
      "details": "Using 2x 81 frame batches keeps character identity but camera movement is lost and background gets sharper at stitching points",
      "from": "A.I.Warper"
    },
    {
      "finding": "VACE extension causes color discoloration",
      "details": "Video extensions using VACE show noticeable color shifts/discoloration at transition points between segments",
      "from": "pom"
    },
    {
      "finding": "Using original input video with masking produces higher quality results",
      "details": "Feeding original video while masking everything but the face maintains facial expressions better than other methods",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "DWPose/ControlNet model differs from official implementation",
      "details": "ComfyUI DWPose implementation produces different skeleton visualization compared to official HF demo, official version is cleaner with no dropped frames",
      "from": "A.I.Warper"
    },
    {
      "finding": "VACE can do seamless video extension with overlapping frames",
      "details": "Using overlapping frames from previous clip as input to next clip creates seamless transitions with minimal visible seams",
      "from": "seitanism"
    },
    {
      "finding": "VAE encoding/decoding causes color shifts in video extensions",
      "details": "Using extracted frames from compressed MP4 causes color shifts, but using lossless PNGs directly from VAE decode avoids this issue",
      "from": "seitanism"
    },
    {
      "finding": "CausVid LoRA at strength 1.0 with 4 steps gives very fast generation",
      "details": "85 frames at 1024x576 generated in 150 seconds on RTX 5090",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "System RAM can be used as shared VRAM for longer videos",
      "details": "A5000 24GB with 128GB RAM can generate 300-450 frames using shared virtual GPU memory",
      "from": "Guey.KhalaMari"
    },
    {
      "finding": "Wan 14B model significantly better than 1.3B for understanding complex objects like full length dresses",
      "details": "Same settings and seed produced fake-looking results on 1.3B but realistic results on 14B",
      "from": "traxxas25"
    },
    {
      "finding": "VACE reference image concatenates multiple images into single image automatically",
      "details": "If you feed VACE a batch of images as reference, it concatenates them under the hood into a single image",
      "from": "Kijai"
    },
    {
      "finding": "CausVid works with VACE and Fun Control modules",
      "details": "CausVid LoRA is compatible with VACE for T2V but not I2V, and works with Fun Control",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "finding": "Phantom model supports up to 3-4 reference images unlike VACE",
      "details": "Phantom can use multiple reference images, different from VACE's single image limitation",
      "from": "Kijai"
    },
    {
      "finding": "VACE minimum frame requirement for proper reference image usage",
      "details": "Single frame doesn't work well with VACE reference images, need at least 5 frames for proper results",
      "from": "Kijai"
    },
    {
      "finding": "VACE face change using pose transition",
      "details": "Modified the pose transition code to control the timing of the appearance of poses that do not exist in the preceding image",
      "from": "toyxyz"
    },
    {
      "finding": "VACE inpainting works with single encode",
      "details": "Successfully achieved inpainting by overlaying mid grey mask over video footage, but multi encode with dwpose degrades video quality",
      "from": "David Snow"
    },
    {
      "finding": "Seamless looping achievable with Wan",
      "details": "Generated video that loops seamlessly using separate workflow processing",
      "from": "The Shadow (NYC)"
    },
    {
      "finding": "Style transfer from reference image",
      "details": "Used cartoon frog in suit prompt with reference image, style came from the reference image automatically",
      "from": "David Snow"
    },
    {
      "finding": "Body correction in pose interpolation",
      "details": "Added body correction to interpolate poses while maintaining body shape for more natural transitions",
      "from": "toyxyz"
    },
    {
      "finding": "Long video generation with overlapping batches",
      "details": "Created system that breaks reference video into batches with overlaps for arbitrary length videos, but colors drift over time becoming crispier",
      "from": "notid"
    },
    {
      "finding": "CausVid LoRA works with Base VACE GGUF",
      "details": "Sample workflow available on HuggingFace",
      "from": "AshmoTV"
    },
    {
      "finding": "Context options node works for extending videos to thousands of frames",
      "details": "Can generate videos with thousands of frames using context options node connected to context options socket",
      "from": "David Snow"
    },
    {
      "finding": "Motion blur understanding in first frames",
      "details": "Model understands context so well that it preserves motion blur from reference images in generated output",
      "from": "David Snow"
    },
    {
      "finding": "CausVid LoRA stabilizes chaotic LoRAs but has tranquilizing effect",
      "details": "Makes chaotic LoRAs more stable but reduces expressiveness of well-working LoRAs, likely due to low CFG",
      "from": "hablaba"
    },
    {
      "finding": "ComfyUI has built-in batch processing for videos",
      "details": "Video load and combine nodes have a built-in batch feature that can process long videos in chunks (e.g., 80 frames at a time), restart with next batch, and combine at the end",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "VACE control video empty frames should be gray, not white",
      "details": "For VACE extension, control video empty frames should be gray value 0.5 (127, 127, 127), mask should be correct",
      "from": "Kijai"
    },
    {
      "finding": "CausVid LoRA strength affects VACE control",
      "details": "Too high CausVid LoRA strength reduces VACE control strength. Above 0.4 strength prevented mouth movement",
      "from": "Kijai"
    },
    {
      "finding": "AccVideo scheduler works as a standalone feature",
      "details": "Uses 10 actual steps when set to 50, works with cfg 1.0, timestamps: [1000.0000, 977.8983, 951.5095, 919.4528, 879.6821, 829.0320, 762.3315, 670.5145, 536.1040, 320.5109]",
      "from": "Kijai"
    },
    {
      "finding": "Color shift in VACE can be addressed with frame blending",
      "details": "Overlapping frames can be cross-faded to reduce seams, occurs over 16 frame overlap",
      "from": "Piblarg"
    },
    {
      "finding": "Wan can generate lip sync motion when prompted with 'talking'",
      "details": "Even in v2v without audio input, prompting talking makes mouth move",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "CausVid with Florence find and replace works well for v2v",
      "details": "User found this combination effective for video-to-video generation",
      "from": "hicho"
    },
    {
      "finding": "AccVid LoRA can be combined with CausVid LoRA",
      "details": "Users successfully combined CausVid 0.15 + AccVid 0.5, and another user merged CausVid/AccVid at 0.5 strength each",
      "from": "\u25b2, JohnDopamine"
    },
    {
      "finding": "VACE works with single encode using multiple control inputs",
      "details": "Expanded mask, depth, normals and pose can all be fed into a single VACE encode node and it works",
      "from": "David Snow"
    },
    {
      "finding": "Context options work with 14B but have issues with 1.3B",
      "details": "Context options for longer videos work fine with 14B model but don't work properly with 1.3B model",
      "from": "N0NSens"
    },
    {
      "finding": "AccVid LoRA works better at higher strengths",
      "details": "AccVid LoRA works well at 1.7 strength with 6 steps, and at 1.0 strength for regular use",
      "from": "Kijai, \ud83e\udd99rishappi"
    },
    {
      "finding": "Wan LoRAs trained on images work with video generation",
      "details": "LoRA trained with 60 images on CivitAI works effectively with Wan 14B T2V model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Phantom 14B model released",
      "details": "Available on HuggingFace, not quantized so download all safetensors files",
      "from": "DawnII"
    },
    {
      "finding": "VACE can work with Phantom models",
      "details": "Phantom gets the references and VACE module gets other preprocessors",
      "from": "DawnII"
    },
    {
      "finding": "AccVid LoRA speeds up rendering",
      "details": "Can be used with CausVid LoRA for faster inference",
      "from": "Johnjohn7855"
    },
    {
      "finding": "Mediapipe face mesh needs cropping for better face detection",
      "details": "Crop and paste faces, then process with mediapipe and crop back",
      "from": "Valle"
    },
    {
      "finding": "TeaCache skips specific steps during sampling",
      "details": "TeaCache skipped 16 conditional, unconditional, and prediction_2 steps at positions [7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37]",
      "from": "Kijai"
    },
    {
      "finding": "CausVid works better with more steps on Phantom",
      "details": "CausVid at 0.3 strength starts working much better with more steps, there's hope",
      "from": "Kijai"
    },
    {
      "finding": "Phantom can take up to 4 reference images",
      "details": "You can give it up to 4 images for reference",
      "from": "Kijai"
    },
    {
      "finding": "VACE input + Phantom ref preserves identity better",
      "details": "VACE input (no ref) + phantom ref works and identity is preserved much better than vace ref",
      "from": "Zuko"
    },
    {
      "finding": "480p works better than 720p for some cases",
      "details": "480p got basically every detail right with the hairs, seems we're supposed to use this in 480p not 720p",
      "from": "aikitoria"
    },
    {
      "finding": "Official specs suggest 121 frames at 24fps",
      "details": "Papers suggest 121 frames at 24fps instead of 81 frames",
      "from": "aikitoria"
    },
    {
      "finding": "Quality improves significantly over 81 frames",
      "details": "Quality definitely improved over 81 frames when using 121 frames",
      "from": "Kijai"
    },
    {
      "finding": "CausVid causes flicker at strength 1.0",
      "details": "1.0 CausVid causes that flicker issue like always",
      "from": "Kijai"
    },
    {
      "finding": "Phantom sometimes fails to render multiple characters",
      "details": "Phantom keeps failing and only rendering one of the 2 characters sometimes",
      "from": "aikitoria"
    },
    {
      "finding": "Reference latents appear in preview",
      "details": "References are in the latents, same with VACE, causing flashing in preview",
      "from": "Kijai"
    },
    {
      "finding": "Phantom works with reference latents for T2V",
      "details": "It's a T2V model that takes in reference latents, not pure T2V",
      "from": "Kijai"
    },
    {
      "finding": "SageAttention significantly speeds up Phantom",
      "details": "Makes Phantom about twice as fast when installed in same venv",
      "from": "aikitoria"
    },
    {
      "finding": "Phantom requires 121 frames and 24 fps",
      "details": "Old workflows work but need changing to 121 frames and 24 fps",
      "from": "aikitoria"
    },
    {
      "finding": "Multiple encoders allow individual strength control",
      "details": "Can use separate encoders for control video, ref image, and input video with individual strength settings",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "MMA Audio frame rate conversion workaround",
      "details": "Wan generates 16fps, MMAudio requires 24fps - custom setup needed to avoid generating sound for only 3 out of 5 seconds",
      "from": "Mngbg"
    },
    {
      "finding": "Using first 3 blocks at reduced strength eliminates CausVid flash",
      "details": "Kijai found that not applying CausVid LoRA to first 3 blocks gets rid of flash, even at full strength on remaining blocks",
      "from": "Kijai"
    },
    {
      "finding": "First block at 0.5 strength also eliminates flash",
      "details": "Reducing just the first block to 0.5 strength removes the CausVid flash issue",
      "from": "Kijai"
    },
    {
      "finding": "CausVid loses strength without first block",
      "details": "When first block is disabled, CausVid significantly loses its effectiveness and may need compensation with CFG or more steps",
      "from": "Kijai"
    },
    {
      "finding": "bf16 gives same results as fp16 but with more artifacts",
      "details": "Testing showed bf16 produces basically identical results to fp16 but introduces additional artifacts",
      "from": "aikitoria"
    },
    {
      "finding": "fp32 is 4x slower than fp16, not 2x as expected",
      "details": "Running at fp32 precision resulted in 4x performance hit instead of expected 2x",
      "from": "aikitoria"
    },
    {
      "finding": "Using euler scheduler gets rid of black noise",
      "details": "Euler scheduler eliminates black noise issues with Wan models",
      "from": "Kijai"
    },
    {
      "finding": "CausVid with all blocks but first works better",
      "details": "Applying CausVid on all blocks except the first gives better results than including first block",
      "from": "Kijai"
    },
    {
      "finding": "Phantom works with four images of same subject",
      "details": "Can use four reference images to cover different angles of a single subject (face, back of jacket, etc.)",
      "from": "David Snow"
    },
    {
      "finding": "32 frame overlap greatly improves context videos",
      "details": "Using 32 frame context overlap produces much better results than default settings",
      "from": "Nekodificador"
    },
    {
      "finding": "MoviiGen LoRA acts like a detailer",
      "details": "Adds minor details and helps clean up noise, making outputs cleaner, but can affect character likeness",
      "from": "Johnjohn7855"
    },
    {
      "finding": "Phantom 14B works better with 3+ subjects than 1.3B",
      "details": "14B model handles multiple subjects more effectively than the 1.3B variant",
      "from": "DawnII"
    },
    {
      "finding": "Phantom can be combined with VACE for inpainting",
      "details": "Phantom character consistency can be used together with VACE inpainting capabilities",
      "from": "David Snow"
    },
    {
      "finding": "Uni3C works with any video with camera motion",
      "details": "Can use any video with camera movement, not just depth-processed videos, as control input",
      "from": "N0NSens"
    },
    {
      "finding": "Phantom cfg at 1.0 doesn't capture likeness well",
      "details": "Using cfg 1.0 with phantom reduces likeness accuracy, especially with well-known faces",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "finding": "Adding original video input at low denoise improves face detection",
      "details": "Using wan video encode node with input video at 0.85 denoise helps model understand where character's face is located",
      "from": "David Snow"
    },
    {
      "finding": "VACE masks should be grayscale format",
      "details": "VACE expects mask videos in grayscale format for proper inpainting",
      "from": "David Snow"
    },
    {
      "finding": "Input video at 0.3 strength significantly helps mouth movement",
      "details": "Adding original video as input at 0.3 strength in separate encoder improves mouth animation compared to no input",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Overlaying pose on mask works better than separate VACE encodes",
      "details": "Combining pose control with inpaint mask in single encode gives better results than using multiple VACE encodes",
      "from": "David Snow"
    },
    {
      "finding": "Block swapping doesn't work with Uni3C controlnet",
      "details": "VRAM management works but block swapping causes issues with Uni3C",
      "from": "mamad8"
    },
    {
      "finding": "Inpainting works with the triple sample workflow",
      "details": "User confirmed inpainting functionality works well with the triple sample workflow",
      "from": "hau"
    },
    {
      "finding": "Uni3C can rotate subjects",
      "details": "User discovered Uni3C has rotation capabilities for subjects in video",
      "from": "hau"
    },
    {
      "finding": "Uni3C requires similar subject sizes between input image and control video",
      "details": "For Uni3C to work well, the subjects in the input image and control video need to be similar in size",
      "from": "toyxyz"
    },
    {
      "finding": "First Frame Last Frame model prefers Chinese prompts",
      "details": "Wan FLF2V model prefers prompts in Chinese because that's how it was trained",
      "from": "Guey.KhalaMari"
    },
    {
      "finding": "WanVideoWrapper supports skip layer guidance",
      "details": "The wrapper includes SLG arguments support",
      "from": "hau"
    },
    {
      "finding": "Phantom 14B is trained on more frames",
      "details": "Phantom 14B doesn't have the 81 frame limit that applies to other models",
      "from": "DawnII"
    },
    {
      "finding": "1.3B Phantom + VACE works fine vs 14B issues",
      "details": "1.3b phantom+vace works fine, suggesting issues with 14b vace may be related to lower block application",
      "from": "DawnII"
    },
    {
      "finding": "VACE + Phantom inpainting works by cutting VACE early and masking original input latents",
      "details": "Cut VACE at 0.8 strength and stop at 0.2, mask both the input latents (with blurred mask) and VACE input frames (with hard edges)",
      "from": "Zuko"
    },
    {
      "finding": "PerpNegGuider works well with Phantom using negative_img_text as empty negative",
      "details": "Feed normal negative and negative_img_text to PerpNegGuider, neg_scale controls how far from actual image embedding while retaining subject context",
      "from": "Ablejones"
    },
    {
      "finding": "Phantom works better with Chinese prompts",
      "details": "Model was trained mostly with Chinese, adheres better to Chinese prompts than English",
      "from": "DawnII"
    },
    {
      "finding": "Character LoRAs trained on T2V work only 30% with Phantom",
      "details": "T2V LoRAs have limited compatibility with Phantom model",
      "from": "ZeusZeus"
    },
    {
      "finding": "CausVid LoRA helps most when not applied to first block or applied at max 0.5 to first block",
      "details": "Rest of blocks can be 1.0 strength, but first block should be reduced or disabled",
      "from": "Kijai"
    },
    {
      "finding": "CausVid includes CFG distillation and should use CFG 1.0",
      "details": "Both CausVid and AccVid distillation includes cfg distillation so they'd use 1.0, which is same as disabled",
      "from": "Kijai"
    },
    {
      "finding": "CFG can be used for certain steps with distilled models",
      "details": "It's possible to use cfg only for certain steps, which works pretty well too",
      "from": "Kijai"
    },
    {
      "finding": "First few steps with CFG most impactful",
      "details": "CFG is most impactful at early steps",
      "from": "Kijai"
    },
    {
      "finding": "CausVid v2 works better with CFG",
      "details": "The new version works better with cfg indeed",
      "from": "Kijai"
    },
    {
      "finding": "Shift less than 1.0 has positive effect for 14B model",
      "details": "The main thing I learned is that shift less than 1 has a very positive effect. but only for model 14b",
      "from": "Mngbg"
    },
    {
      "finding": "Florence is very powerful with Wan",
      "details": "florence is very powerful with wan - works great for detailed prompts",
      "from": "hicho"
    },
    {
      "finding": "Character description text fixes consistency",
      "details": "Its this description text that fixes the character consistency - the model wants to have a description of each ref image every time",
      "from": "AJO"
    },
    {
      "finding": "CausVid v2 at strength above 1.0 with CFG works best",
      "details": "You could also try the v2 with strength above 1.0 tho, and with some cfg it's probably best",
      "from": "Kijai"
    },
    {
      "finding": "CausVid v2 can be used at full strength (1.0) without destroying motion in normal T2V and doesn't have the flash",
      "details": "Major improvement over previous versions that required lower strengths",
      "from": "Kijai"
    },
    {
      "finding": "CausVid v2 already has block 0 removed",
      "details": "No need for manual block removal node when using v2",
      "from": "Kijai"
    },
    {
      "finding": "ATI model released with trajectory control functionality",
      "details": "New ByteDance model supporting trajectory-based motion control with anchor points",
      "from": "JohnDopamine"
    },
    {
      "finding": "Phantom is faster than VACE for similar results",
      "details": "VACE: 738.22 seconds vs Phantom: 253.29 seconds",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "FP8 quantized Phantom gives half the generation time compared to FP16",
      "details": "Significant speed improvement with FP8 version",
      "from": "AJO"
    },
    {
      "finding": "Wan LoRAs work with Phantom model",
      "details": "LoRAs trained for Wan 14B T2V are compatible with Phantom model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "LoRAs can be merged at strength 1.0 successfully",
      "details": "Multiple LoRAs (CausVid V2, Phantom, detail enhancers) can be merged at full strength 1.0 to create custom models",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "CausVid V2 provides significant speed improvements",
      "details": "New CausVid V2 makes generation much faster compared to previous versions",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Multiple reference images supported in Phantom",
      "details": "You can bring in as many reference images as you want and they can all be of different things (person, background, objects)",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Reference image quality significantly affects output",
      "details": "The quality and type of reference image makes a huge difference on the quality of the ending result",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Merged models work without needing separate LoRAs",
      "details": "Successfully merged Phantom, CausVid V2, MoviiGen, and detail LoRAs into single model that produces same quality as stacked LoRAs",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "CausVid v2 was tuned on T2V and requires CFG >1 with scheduling for 1 step out of 8 total steps",
      "details": "Kijai mentioned that v2 needs different settings than v1, with cfg more than 1 and scheduled for 1 step",
      "from": "Johnjohn7855"
    },
    {
      "finding": "Film grain can reduce reactor jitter",
      "details": "Adding film grain after reactor processing helps reduce jitter artifacts, especially around eyes and mouth",
      "from": "Johnjohn7855"
    },
    {
      "finding": "VACE works with Normal Craft normal maps but not all normal map types",
      "details": "Normal Craft works well, Sapients one works but not as well. VACE not trained on normals but can work to some extent",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "Wan VAE should never require tiling with 24GB VRAM",
      "details": "Kijai states the Wan VAE literally never needs tiling",
      "from": "Kijai"
    },
    {
      "finding": "ATI requires 121 points even for 81 frames",
      "details": "Setting 121 points prevents cutting off the end, even when doing 81 frames",
      "from": "Kijai"
    },
    {
      "finding": "Single track in ATI usually does camera motion only",
      "details": "Need anchor points that don't move to avoid just camera motion",
      "from": "Kijai"
    }
  ],
  "troubleshooting": [
    {
      "problem": "Context windows change input image when frames exceed 81",
      "solution": "Each window needs an image or this happens when it doesn't get it",
      "from": "Kijai"
    },
    {
      "problem": "RuntimeError about tensor size mismatch",
      "solution": "Issue with expanded size of tensor (1024) vs existing size (1198)",
      "from": "Dream Making"
    },
    {
      "problem": "Steps value randomly changing to different numbers",
      "solution": "Mouse mishap or node updates shifting field values",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "Computer crashes when running on GPU2 vs GPU1",
      "solution": "Try switching power cables, avoid splitter cables that come in box",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "Skeleton lines appearing in VACE control renders",
      "solution": "Increase strength to 1.0, though may be situation specific",
      "from": "David Snow"
    },
    {
      "problem": "Strange colors and artifacts with Kijai's node vs native",
      "solution": "Use CFG 3 with SLG, keep denoise at 1.0 for T2V",
      "from": "Miku"
    },
    {
      "problem": "Grid pixel glitch patterns in videos",
      "solution": "Try Euler/beta scheduler instead of uni_pc, increase step count from too low values, test 81 frames without context loops to isolate issues",
      "from": "chrisd0073"
    },
    {
      "problem": "DG models prone to flashing",
      "solution": "Higher version DG models more flash prone, use stock versions as they're closest to base 1.3B. It's an ongoing issue with stronger models",
      "from": "David Snow"
    },
    {
      "problem": "VAE flashing and artifacts in v2v",
      "solution": "Usually cfg or step count issue, increasing shift strengthens style but also increases burn - eternal balancing act",
      "from": "David Snow"
    },
    {
      "problem": "Eyes don't follow/move properly in face videos",
      "solution": "Run through LivePortrait afterwards, or use Runway Act One for lip sync. Fun-control somewhat better for eye following but has other issues",
      "from": "David Snow"
    },
    {
      "problem": "VACE tries to recreate input video instead of inpainting",
      "solution": "Need to inpaint the first frame and pass it as guide - the inpainted first frame should have subject removed and area around inpainting should be gray",
      "from": "ArtOfficial"
    },
    {
      "problem": "Blurry or pixelated results with Wan Fun workflows",
      "solution": "Remove batched CFG - never gotten good results from that",
      "from": "Colin"
    },
    {
      "problem": "Flickering artifacts in long generations",
      "solution": "Change both schedulers to uni pc and remove all experimental args",
      "from": "Flipping Sigmas"
    },
    {
      "problem": "ComfyUI node disconnection glitch",
      "solution": "Happens when disconnecting nodes - they appear disconnected but still function as connected. More common in Opera browser, less in Chrome",
      "from": "Flipping Sigmas"
    },
    {
      "problem": "Kijai had problems with Blender depth maps",
      "solution": "Had to blur high quality Blender depth maps because VACE was likely trained on DepthAnything V2 outputs",
      "from": "David Snow"
    },
    {
      "problem": "FP8 not supported on RTX 3090",
      "solution": "Disable FP8 in model loader settings, change value to disable compilation",
      "from": "Juan Gea"
    },
    {
      "problem": "Triton bundler os.replace error on Windows",
      "solution": "Change line 268 in triton_bundler.py from os.replace to os.rename",
      "from": "Faust-SiN"
    },
    {
      "problem": "CUDA invalid argument error with LoRAs",
      "solution": "Disable torch compile, teacache, and SLG nodes - issue appears to be permissions/triton related",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "problem": "VHS node loading issues after ComfyUI update",
      "solution": "Delete VHS custom node folder and git pull fresh copy instead of using manager",
      "from": "TK_999"
    },
    {
      "problem": "Ubuntu 24.04 compatibility issues with CUDA",
      "solution": "Downgrade to Ubuntu 22.04 or properly install Nvidia CUDA toolkit",
      "from": "UsamaAhmedKhan"
    },
    {
      "problem": "GIMM-VFI node segfaults with torch compile",
      "solution": "Disable torch_compile in DownloadAndLoadGIMMVFIModel",
      "from": "gshawn"
    },
    {
      "problem": "Block swap causing CUDA errors",
      "solution": "Switch to VRAM Management node instead of Block swap node",
      "from": "Colin"
    },
    {
      "problem": "CUDA error: invalid argument with WanVideoWrapper",
      "solution": "Set self.use_non_blocking = False in line 879 of model.py",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "problem": "TeaCache causing CUDA runtime errors",
      "solution": "Issue is RAM-related, non_blocking reserves more RAM. Need more than 32GB RAM or use auto_cpu_offload",
      "from": "Kijai"
    },
    {
      "problem": "Low system memory preventing TeaCache use",
      "solution": "Use 'auto_cpu_offload' setting on wrapper, it's better for RAM but slower",
      "from": "Draken"
    },
    {
      "problem": "Slow generation times on 3090 Ti",
      "solution": "Use optimizations - native ComfyUI has fewer options than wrapper",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Depthcrafter resolution mismatch with NormalCrafter",
      "solution": "Make other preprocessors conform to Depthcrafter's multiple of 64 requirement",
      "from": "David Snow"
    },
    {
      "problem": "OOM errors with DepthAnything node",
      "solution": "Set depth anything node to fp16 instead of fp32",
      "from": "A.I.Warper"
    },
    {
      "problem": "VACE generating different character appearance",
      "solution": "Add start frame twice to beginning with no mask, or train a character LoRA",
      "from": "TimHannan"
    },
    {
      "problem": "Runpod disk allocation issues",
      "solution": "Check container disk limit - 20GB default is too small, increase to 80-100GB",
      "from": "chrisd0073"
    },
    {
      "problem": "RuntimeError: tensor size mismatch (14) vs (17)",
      "solution": "Set Empty embed value to 97 instead of 53",
      "from": "Dream Making"
    },
    {
      "problem": "xformers incompatibility with PyTorch 2.7.0",
      "solution": "pip install --pre xformers or use xformers-0.0.31.dev1030",
      "from": "A.I.Warper"
    },
    {
      "problem": "LBM model asking for SDXL base model download",
      "solution": "Check HF token setup in config file",
      "from": "slmonker"
    },
    {
      "problem": "CausVid FloatTensor/cuda.FloatTensor type mismatch",
      "solution": "Use native wrapper instead of WAN wrapper",
      "from": "MilesCorban"
    },
    {
      "problem": "Artifacts/glitch at beginning of CausVid videos",
      "solution": "Remove first 8 frames from generated output",
      "from": "V\u00e9role"
    },
    {
      "problem": "CFG 3.5 burns CausVid output",
      "solution": "Use CFG 1.0 or no CFG",
      "from": "Kijai"
    },
    {
      "problem": "TAEW preview bugging out after node update",
      "solution": "Change to latent2rgb preview",
      "from": "Kijai"
    },
    {
      "problem": "9 steps gives garbage results with CausVid",
      "solution": "Use 3 steps instead of 9 steps",
      "from": "Kijai"
    },
    {
      "problem": "CausVid has blocking artifacts",
      "solution": "No current fix available",
      "from": "yi"
    },
    {
      "problem": "Triton CUDA version error (12.8 < 10.0)",
      "solution": "May need to reinstall triton-windows, use without compile option",
      "from": "Juampab12"
    },
    {
      "problem": "Blurry results with MoviiGen using TEAcache",
      "solution": "Turn off teacache",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "problem": "14B VACE shows 'vace_blocks.8.modulation' error",
      "solution": "Need to update wrapper, 14B uses different blocks than 1.3B and won't work out of box",
      "from": "Kijai"
    },
    {
      "problem": "Static frames when using VACE for I2V",
      "solution": "Don't use I2V nodes with VACE, use VACE nodes directly and connect start frame only",
      "from": "multiple users"
    },
    {
      "problem": "Cannot load VACE module in model loader",
      "solution": "VACE module is not a full model - load base model in model loader, VACE module goes in VACE video select node",
      "from": "Kijai"
    },
    {
      "problem": "14B native nodes don't work yet",
      "solution": "14B uses different blocks, only works with wrapper currently, 1.3B works with native nodes",
      "from": "Kijai"
    },
    {
      "problem": "Flex attention doesn't work with VACE currently",
      "solution": "Use SDPA instead when running VACE",
      "from": "Kijai"
    },
    {
      "problem": "RTX 4080 16GB getting OOM errors even at 480x480 resolution",
      "solution": "Change vace_blocks_to_swap to 8 (not 35), select proper model quantization",
      "from": "DawnII"
    },
    {
      "problem": "Random noise output instead of video",
      "solution": "Disable enhance_a_video node or lower its value to 2",
      "from": "Hashu"
    },
    {
      "problem": "Video turns green and ControlNet pose visible in output with VACE 14B",
      "solution": "Must use T2V model, not I2V model with VACE",
      "from": "Kijai"
    },
    {
      "problem": "Workflow stuck at 0% on RTX 5090",
      "solution": "Use block swap node even with high-end GPUs, swap at least 1 VACE block",
      "from": "Kijai"
    },
    {
      "problem": "OpenPose visible in VACE output",
      "solution": "Lower strength/start/end percent values when using multiple VACE encodes",
      "from": "David Snow"
    },
    {
      "problem": "CausVid + VACE compatibility issues",
      "solution": "Update nodes to latest version for 14B VACE compatibility",
      "from": "Kijai"
    },
    {
      "problem": "VACE consistency issues with flickering between reference images",
      "solution": "Use white background padding on reference images, try turning off WanVideo TeaCache, use masks for better consistency",
      "from": "Kijai"
    },
    {
      "problem": "Canny always showing up as outlines in VACE output when blending with depth",
      "solution": "Blur the depth map to prevent VACE from going into colorization mode, blurring controlnet conditions helps in general",
      "from": "Kijai"
    },
    {
      "problem": "High contrast and plasticky output with CausVid",
      "solution": "Use CFG 1.0 and steps <9, softer colors require more steps with low-step methods",
      "from": "DawnII"
    },
    {
      "problem": "Block swap error appearing randomly",
      "solution": "Set non-blocking off when swapping blocks, though this slows things down",
      "from": "Stad"
    },
    {
      "problem": "VACE Module 14B not working",
      "solution": "Need clean install, git pull wasn't enough",
      "from": "ZRNR"
    },
    {
      "problem": "OOM with CausVid LoRA",
      "solution": "Use low VRAM load option on the LoRA loader",
      "from": "Kijai"
    },
    {
      "problem": "Messy results with CausVid",
      "solution": "Use CFG 1.0, disable everything related to CFG, disable teacache",
      "from": "Kijai"
    },
    {
      "problem": "Slow prompt changes taking 5+ minutes",
      "solution": "Use fp8 text encoder or increase system RAM (32GB not enough for 14B)",
      "from": "Kijai"
    },
    {
      "problem": "OOM on 3090 with 14B model",
      "solution": "Enable fp8 quantization in settings even if model is already fp8, use block swap to 30-40",
      "from": "Kijai"
    },
    {
      "problem": "Reference fading over time in DF",
      "solution": "Reference only fed to first sampler in DF, use prefix_samples input instead of VACE encode",
      "from": "DawnII"
    },
    {
      "problem": "VACE outpainting giving bogus results",
      "solution": "Outpaint area should be gray not white, only the mask should be white",
      "from": "Kijai"
    },
    {
      "problem": "Flickering with CausVid LoRA",
      "solution": "Apply block edit node to CausVid LoRA to skip VACE blocks",
      "from": "seruva19"
    },
    {
      "problem": "Artifacts at high CausVid strength",
      "solution": "Lower CausVid strength to 0.5-0.6 and increase steps to 6",
      "from": "Jonathan"
    },
    {
      "problem": "VACE 14B outpainting issues with color changes",
      "solution": "Use T2V model instead of I2V model, and use WAN text encoder instead of native encoder",
      "from": "Stad"
    },
    {
      "problem": "Hair strands not moving with CausVid LoRA",
      "solution": "Lower LoRA strength to 0.3, and disable LoRA for VACE blocks",
      "from": "Kijai"
    },
    {
      "problem": "Context node causing blurred frames at end of batches",
      "solution": "Copy entire node pack for next 81 frames using final frame from run 1 as ref for run 2",
      "from": "A.I.Warper"
    },
    {
      "problem": "Memory overflow with CausVid LoRA + I2V model setup",
      "solution": "Issue identified but no solution provided",
      "from": "N0NSens"
    },
    {
      "problem": "Getting static results with CausVid",
      "solution": "Turn off WanVideo Experimental Args nodes for 1.3B model",
      "from": "Mngbg"
    },
    {
      "problem": "OOM errors when running CausVid + VACE14B repeatedly",
      "solution": "Use 'Clear VRAM Used' node between sampler and VAE decode, restart ComfyUI between runs, or lower block swap values",
      "from": "The Punisher"
    },
    {
      "problem": "Artifacts and pose rigs appearing in output",
      "solution": "Disable all enhancements like TeaCache, SLG - use only torchcompile/model/lora",
      "from": "aipmaster"
    },
    {
      "problem": "Don't use TeaCache with CausVid",
      "solution": "TeaCache causes issues with CausVid, disable it",
      "from": "DawnII"
    },
    {
      "problem": "VACE doesn't work well with multiple combined controls",
      "solution": "Separate depth and pose into different VACE encoders, don't combine depth and pose controls",
      "from": "DawnII"
    },
    {
      "problem": "RAM errors with Wan 14B on 16GB VRAM",
      "solution": "Disable non-blocking transfer if using blockswap, use quantized text encoders, swap fewer blocks",
      "from": "Kijai"
    },
    {
      "problem": "CausVid giving blurry results",
      "solution": "Use 0.5-1 LoRA strength, 4-6 steps, place CausVid LoRA after other LoRAs",
      "from": "DawnII"
    },
    {
      "problem": "MediaPipe-FaceMeshPreprocessor error after ComfyUI update",
      "solution": "Downgrade mediapipe version or reinstall via pip",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "problem": "CausVid LoRA produces blurry/fogged output with base Wan14B",
      "solution": "Use CFG1, disable TeaCache, SLG, and other enhancements. Set shift to 5 instead of default",
      "from": "Fawks"
    },
    {
      "problem": "CausVid LoRA doesn't play well with other LoRAs",
      "solution": "Load CausVid last and use higher strength to compensate, or disable other LoRAs first",
      "from": "DawnII"
    },
    {
      "problem": "Marigold v2 nodes require diffusers>=0.28 error",
      "solution": "Check requirements file - error message contains incorrect required version",
      "from": "Mngbg"
    },
    {
      "problem": "CausVid 14B T2V produces bad results with latent hitching",
      "solution": "More common issue with Caus 14B, try different settings or use with VACE instead",
      "from": "DawnII"
    },
    {
      "problem": "ComfyUI nightly memory management issues",
      "solution": "Models not unloading correctly, block swapping not working. Roll back to earlier commit",
      "from": "The Punisher"
    },
    {
      "problem": "VACE reference image latent appearing at beginning of video",
      "solution": "Check that video length matches number of frames - issue was video shorter than frame count",
      "from": "ArtOfficial"
    },
    {
      "problem": "Running out of system RAM with blockswap",
      "solution": "Add 8GB swap memory - everything runs smoothly without noticeable slowdown",
      "from": "Captain of the Dishwasher"
    },
    {
      "problem": "VACE controls revealing preprocessor artifacts",
      "solution": "Don't combine multiple controls like depth + pose in single pass, use separate VACE embed nodes chained with prev_embeds input",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "V2V with ControlNet and Causvid 4 steps produces poor outputs",
      "solution": "Increase to 6 steps for better results",
      "from": "The Punisher"
    },
    {
      "problem": "Weird lines appearing in generated video",
      "solution": "Likely a settings issue, Q5_K_S GGUF resolved the problem",
      "from": "The Punisher"
    },
    {
      "problem": "LoRA key errors when using Fun LoRAs",
      "solution": "Error occurs when using Fun LoRAs on non-Fun models, but LoRAs still work",
      "from": "David Snow"
    },
    {
      "problem": "14B VACE crashes inconsistently on 4090",
      "solution": "Memory leak suspected, use lower max image size/frames or quantization",
      "from": "hablaba"
    },
    {
      "problem": "Reference image heavily ignored in VACE",
      "solution": "Reference image needs same resolution as video, use padding of 128 then resize",
      "from": "DennisM"
    },
    {
      "problem": "ComfyUI crash with 14B fp8",
      "solution": "Set quantization to fp8_e4 to match the model, use 45 block swap (40 base + 5 VACE)",
      "from": "DawnII"
    },
    {
      "problem": "Native VACE serious OOM issues even with offloading",
      "solution": "Use wrapper for better VRAM management, or switch to GGUF models",
      "from": "Draken"
    },
    {
      "problem": "OOM errors on 3060 12GB with 14B model",
      "solution": "Enable tiled VAE, use block swap to offload to RAM, reduce resolution",
      "from": "Kijai"
    },
    {
      "problem": "GGUF giving noised unfinished output with 4 steps",
      "solution": "Set CFG to 1, update ComfyUI to nightly, use correct LoRA (14B vs 1.3B)",
      "from": "The Punisher"
    },
    {
      "problem": "Control pose not working in VACE",
      "solution": "Use inverted lineart instead of control pose, because lineart alone doesn't work in VACE",
      "from": "Nokai"
    },
    {
      "problem": "Wrong connections in VACE workflow",
      "solution": "Control input should go to control_video, not reference. Both bones and start/end frames go in control input",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Sharp artificial edges in CausVid generations",
      "solution": "Lower denoising to 0.75 can add motion and reduce artificial look",
      "from": "JohnDopamine"
    },
    {
      "problem": "RuntimeError: No frames generated",
      "solution": "Check skip_first_frame parameter - if it's higher than video length it causes this error. Set to 0 or disconnect the input. Also set frame_load_cap to 0 or 81",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Mushy output with CausVid LoRA",
      "solution": "Ensure CFG is set to 1.0, turn off TeaCache and SLG enhancers, use CausVid LoRA strength 0.3-0.6",
      "from": "zelgo_"
    },
    {
      "problem": "Context option causing 'No frames generated' error",
      "solution": "Disable context option when using input video with fewer than 81 frames",
      "from": "DeZoomer"
    },
    {
      "problem": "Wrong text encoder error 'tensor size mismatch'",
      "solution": "Use correct text encoder - umt5-xxl for T2V workflows, or use ComfyUI native encoder with bridge node",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Slow LoRA loading (15+ minutes)",
      "solution": "Likely hardware issue with disk - check SMART status using CrystalDiskInfo, consider disk cleanup if 85%+ full",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "VACE patch not being recognized in custom node",
      "solution": "Model state dict may be modified correctly but sampler doesn't understand it's VACE - shows 'WAN21' instead of 'WAN21_vace'",
      "from": "The Punisher"
    },
    {
      "problem": "Yellow tinted video outputs in VACE",
      "solution": "Issue resolved by using proper depth preprocessing instead of DepthCrafter node which was too VRAM intensive for RTX 3090",
      "from": "BestWind"
    },
    {
      "problem": "Reference frame appearing at end/beginning of inpainting video",
      "solution": "Use frame trimming node to remove extra reference frames from output",
      "from": "JohnDopamine"
    },
    {
      "problem": "Poor quality results with CausVid",
      "solution": "Disable TeaCache when using CausVid workflows, especially low-step ones",
      "from": "MilesCorban"
    },
    {
      "problem": "Copy-paste in ComfyUI not preserving connections",
      "solution": "Known issue after recent ComfyUI update affecting multiple users",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Poor generation quality",
      "solution": "High CFG values (like 12) can cause poor results, use lower CFG",
      "from": "Dream Making"
    },
    {
      "problem": "Sapiens requiring unsafe checkpoint loading",
      "solution": "Need to use comfyui-unsafe-torch custom node to disable security for Sapiens models",
      "from": "MilesCorban"
    },
    {
      "problem": "'WanModel' object has no attribute 'vace_patch_embedding' error",
      "solution": "Use T2V base model instead of I2V - VACE only works with T2V models",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "ValueError: You are attempting to load a VACE module as a WanVideo model",
      "solution": "Load VACE module in the vace_model input, not the main model loader. Use matching T2V base model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "RuntimeError: tensor size mismatch (20250 vs 21060)",
      "solution": "Check input resizing - must be divisible by 16",
      "from": "Kijai"
    },
    {
      "problem": "Combined VACE processors causing oversaturated/cooked results",
      "solution": "Use blank/grey reference image for second embed, reduce strength on second encoder, don't use reference image in multiple encoders",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "HiDream cache clearing during WAN inference",
      "solution": "Delete ComfyUI-HiDream-Sampler folder from custom_nodes and restart ComfyUI",
      "from": "100a"
    },
    {
      "problem": "CausVid not following reference or control properly",
      "solution": "Use classic WAN T2V 14B model with CausVid lora, not the full CausVid model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "VACE model select node not updating",
      "solution": "Update wrapper nodes using git commands: cd to ComfyUI-WanVideoWrapper, git checkout main, git fetch, git pull",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Video getting dark at end with CausVid",
      "solution": "Problem was end_latent_strength set to 0, fixing this resolved the darkening issue",
      "from": "DiXiao"
    },
    {
      "problem": "OOM with depth crafter",
      "solution": "Downscale input to 768 longest side before running through depthcrafter",
      "from": "David Snow"
    },
    {
      "problem": "Xformers compatibility error after update",
      "solution": "Need correct xformers version 0.0.30 for pytorch 2.7.0+cu126, install with specific wheel or pip install xformers --index-url https://download.pytorch.org/whl/cu124",
      "from": "David Snow"
    },
    {
      "problem": "xformers installation failure",
      "solution": "Download xformers wheel file and install manually with pip: python -m pip install xformers-0.0.30-cp312-cp312-win_amd64.whl",
      "from": "David Snow"
    },
    {
      "problem": "VACE overlaying pose skeletons on output video",
      "solution": "Use two separate encoders - first with pose at strength 1, second with depth at strength 0.5, remove ref image from second encoder",
      "from": "Boop"
    },
    {
      "problem": "Video deformations with DOF scenes",
      "solution": "Remove diorama/DOF references from prompt or add explicit DOF description, or create images without DOF and add in post",
      "from": "DevouredBeef"
    },
    {
      "problem": "Context window artifacts",
      "solution": "User reports no luck with context_window but shows it working without window",
      "from": "N0NSens"
    },
    {
      "problem": "SapiensLoader weights loading error",
      "solution": "Check Ultralytics version in Git repo for compatibility issues",
      "from": "zelgo_"
    },
    {
      "problem": "VACE control failed at editing cuts while Fun was ok",
      "solution": "Switch to Fun Control for handling video cuts",
      "from": "N0NSens"
    },
    {
      "problem": "Sapiens nodes not loading into ComfyUI",
      "solution": "Check Github page about incompatibility with ultralytics 8.3.41",
      "from": "Valle"
    },
    {
      "problem": "VACE overloading memory with 14B + CausVid",
      "solution": "Reduce CausVid LoRA strength to at least half, or use specific blocks",
      "from": "Kijai"
    },
    {
      "problem": "Depth map appearing in output instead of reference image",
      "solution": "First encoder should be pose strength 1, second encoder depth strength 0.5, no reference image in depth encoder",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "UniPC creating visible noise with CausVid",
      "solution": "Use Euler sampler for cleaner results, though it has ghost-like effects",
      "from": "N0NSens"
    },
    {
      "problem": "VACE doesn't work with I2V models",
      "solution": "Don't use the VACE module at all if loading i2v model - VACE only works with T2V models",
      "from": "DawnII"
    },
    {
      "problem": "WAN wrapper nodes not loading/updating",
      "solution": "Uninstall bitsandbytes - it's the triton import that diffusers does when bitsandbytes is installed that causes loading failures",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI desktop version auto-updates causing crashes",
      "solution": "Don't use desktop version for production work - use separate instances with symlinks, one for careful updates and one for daily updates",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "VACE with CausVid on skyreels looks like noise",
      "solution": "Switch to T2V model instead of I2V when using VACE - skyreels I2V is incompatible with VACE",
      "from": "DawnII"
    },
    {
      "problem": "Points Editor limitations - can't remove first/last points",
      "solution": "Click 'new canvas' to reset to single point, or work within the constraint that you can't delete first or last points",
      "from": "Kijai"
    },
    {
      "problem": "VACE inpainting not working with just mask input",
      "solution": "Use both gray-filled RGB input (127, 127, 127 in inpaint area) AND input mask together",
      "from": "Kijai"
    },
    {
      "problem": "Blurred masks don't work for inpainting in VACE",
      "solution": "Keep mask sharp but expanded rather than blurred",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Over-saturation in VACE output",
      "solution": "Use input mask to avoid saturation issues",
      "from": "Kijai"
    },
    {
      "problem": "Can't get exact poses without controlnets in Hidream",
      "solution": "Use non-full denoise or switch back to Flux for specific pose control",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Third VACE embed has no effect",
      "solution": "Likely limitation in wrapper code rather than model",
      "from": "Kijai"
    },
    {
      "problem": "Block swap node causing errors",
      "solution": "Run off the bottom 'non_blocking' input of block swap node, or bypass if swapping zero blocks",
      "from": "ArtOfficial"
    },
    {
      "problem": "ComfyUI browser freezing",
      "solution": "Sometimes fake-frozen - minimize and bring browser back up to fix, or happens when adding too many video combine nodes",
      "from": "TK_999"
    },
    {
      "problem": "OOM at high resolutions",
      "solution": "Try 40 blocks to swap for high resolution generations like 1280x720",
      "from": "Valle"
    },
    {
      "problem": "Context options causing crashes",
      "solution": "Error with WanVideo Context Options causing ComfyUI crashes with IndexKernel assertion failed",
      "from": "MilesCorban"
    },
    {
      "problem": "TeaCache with CausVid low steps",
      "solution": "Turn off teacache when using CausVid since low steps mess up the video, though some report it working at 8 steps",
      "from": "MilesCorban"
    },
    {
      "problem": "RTX 5090 crashing at max resolution",
      "solution": "Setting enable_vae_tiling to True stopped crashes",
      "from": "AJO"
    },
    {
      "problem": "More than 2 VACEs not working properly",
      "solution": "Fixed bug where prev embeds were reset between nodes, so only the 2nd to last was being added",
      "from": "Kijai"
    },
    {
      "problem": "MultiGPU tensor device mismatch",
      "solution": "Expected all tensors to be on same device error - need to follow stack trace and cast tensor to right device",
      "from": "Kijai"
    },
    {
      "problem": "Noisy output",
      "solution": "Higher resolution, 2nd refine pass, or both can reduce noise",
      "from": "N0NSens"
    },
    {
      "problem": "Depth map masking issues",
      "solution": "Trying to mask depth map areas gives human with no head or literal depth image blended with normal human",
      "from": "Draken"
    },
    {
      "problem": "Memory allocation issues when changing resolution",
      "solution": "Enable block swapping with higher values (try 30 for 4090)",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Size mismatch errors when changing aspect ratio",
      "solution": "Ensure all num_frames values match everywhere and use frame_count output from load video node",
      "from": "A.I.Warper"
    },
    {
      "problem": "GGUF models not working correctly",
      "solution": "Hard-coded stuff prevents GGUF from working properly, but VACE GGUF versions on HuggingFace work fine in native",
      "from": "The Punisher"
    },
    {
      "problem": "Lora strength reduced with CausVid",
      "solution": "Use WanVideo CFG Schedule node to bring back intended lora look while still speeding up process",
      "from": "Zlikwid"
    },
    {
      "problem": "VACE node input missing in newest version",
      "solution": "Downgrade to older version of the node - newer version has input but no node to feed it",
      "from": "MilesCorban"
    },
    {
      "problem": "UniAnimate giving 'Got 5D input, but bilinear mode needs 4D input' error",
      "solution": "Use VACE embed node instead of encode node for proper functionality",
      "from": "Kijai"
    },
    {
      "problem": "Spline editor error when attaching to manage VACE strength",
      "solution": "Update to nightly version of nodes, set points to sample to number of steps being used",
      "from": "Kijai"
    },
    {
      "problem": "Depth control overpowering reference image",
      "solution": "Use separate VACE embed nodes with different strengths - reference image at higher strength, depth at lower strength",
      "from": "Johnjohn7855"
    },
    {
      "problem": "Reference image losing influence with higher steps/shift",
      "solution": "Lower shift value first (try shift 4 instead of 8), then adjust other parameters",
      "from": "Johnjohn7855"
    },
    {
      "problem": "UniAnimate pose detection failing",
      "solution": "Use 512x768 dimensions, generate neutral A-pose version of character, set score threshold to 0 if needed",
      "from": "Guey.KhalaMari"
    },
    {
      "problem": "MP4 output losing alpha channel",
      "solution": "Delete the alpha layer as MP4 format doesn't support alpha channels",
      "from": "Valle"
    },
    {
      "problem": "Pose control not working with spinning dress/complex clothing",
      "solution": "Manually draw pose in Photoshop or overlay pose on top of A-pose as workaround",
      "from": "Guey.KhalaMari"
    },
    {
      "problem": "CFG float error when starting at 0.7 percent",
      "solution": "IndexError: list index out of range in teacache_state processing",
      "from": "DawnII"
    },
    {
      "problem": "VACE FaceReference node struggles with identity at glancing angles",
      "solution": "Node tends to fail at non-frontal face angles, has limitations with pose variations",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "problem": "Pose processor failing to detect stylized characters",
      "solution": "Consider manually placing DW pose bones over reference image to get coordinates and remap",
      "from": "A.I.Warper"
    },
    {
      "problem": "Color degradation in video extensions",
      "solution": "Isolate problematic batch and color correct before doing 2nd pass cleanup",
      "from": "A.I.Warper"
    },
    {
      "problem": "Memory errors on previously working configurations",
      "solution": "Possible shadow updates to nodes causing OOM issues, restart ComfyUI",
      "from": "Cubey"
    },
    {
      "problem": "CUDA error: invalid argument on A100 80GB",
      "solution": "Issue was insufficient RAM - everything offloads by default eating lots of RAM. Can disable offloading by using main_device as load device and disabling force_offload",
      "from": "Kijai"
    },
    {
      "problem": "VHS_SelectImages error with 'Index '74None80' must be an integer'",
      "solution": "Update ComfyUI - this happens when frontend package version is behind the workflow version",
      "from": "JohnDopamine"
    },
    {
      "problem": "Double VACE Encode with Context Options fails without reference image",
      "solution": "Known issue that Kijai hasn't accounted for yet",
      "from": "Kijai"
    },
    {
      "problem": "CppCompileError with Chroma model",
      "solution": "Disable Torch Compile nodes, especially if using workflows with compile functionality",
      "from": "MilesCorban"
    },
    {
      "problem": "Halo edge ringing effect in Wan/MoviGen outputs",
      "solution": "Too high CFG can cause structured/HDR look with edge sharpening artifacts",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "VACE not following reference image",
      "solution": "Canny edge detection should be inverted for VACE, use Kijai's WanVideoWrapper instead of native nodes",
      "from": "Kijai"
    },
    {
      "problem": "CausVid + TeaCache compatibility issues",
      "solution": "TeaCache skips steps while CausVid optimizes steps - they don't pair well and TeaCache uses more VRAM with CausVid",
      "from": "Cubey"
    },
    {
      "problem": "WanVideoWrapper installation failing with cmake/sentencepiece errors",
      "solution": "Use VS 2017, check if using correct Python version (portable is 3.12 now), do git pull to update repository",
      "from": "Cubey"
    },
    {
      "problem": "Reference image losing face likeness with full body shots",
      "solution": "Split clothing and face reference images separately rather than using single full body reference",
      "from": "AJO"
    },
    {
      "problem": "CLIP text encode taking majority of generation time",
      "solution": "Force offload the main model using model_to_offload input, likely memory related issue",
      "from": "Kijai"
    },
    {
      "problem": "Node inputs greyed out",
      "solution": "GGUF nodes need updating, restart ComfyUI to resolve",
      "from": "Kijai"
    },
    {
      "problem": "Multi encode inpainting quality issues",
      "solution": "Avoid using multiple encodes simultaneously, stick to single encode for better results",
      "from": "David Snow"
    },
    {
      "problem": "RAM usage not clearing after model unload",
      "solution": "Python/ComfyUI memory management issue, Framepack can use 50GB RAM while model is only 16GB",
      "from": "Draken"
    },
    {
      "problem": "Context issues with 275 frames",
      "solution": "Tried with full input image and isolated, same problem persists",
      "from": "N0NSens"
    },
    {
      "problem": "Video combine node saving unwanted PNG",
      "solution": "PNG saved for lighter workflow storage, modify node code or request feature from maintainer",
      "from": "MilesCorban"
    },
    {
      "problem": "Flashing artifacts on first frames of videos",
      "solution": "Remove first few frames using TrimVideoLatent node or Get Image/Mask Range from Batch node",
      "from": "David Snow"
    },
    {
      "problem": "WSL2 CUDA out of memory errors when same workflow works in Windows",
      "solution": "Disable use_non_blocking on block swap node, use fp8 quantization, and configure .wslconfig properly",
      "from": "Kijai"
    },
    {
      "problem": "Color changes ruining transitions between video clips",
      "solution": "Use same 15 frames for end of one video and beginning of next for seamless transition",
      "from": "Piblarg"
    },
    {
      "problem": "VACE 1.3B not working with context options",
      "solution": "Context options works fine with 14B but may have issues with 1.3B models",
      "from": "N0NSens"
    },
    {
      "problem": "TypeError: only integer tensors of a single element can be converted to an index",
      "solution": "Roll back to previous version of WanVideoWrapper - latest update may have broken compatibility with certain workflows",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "CUDA out of memory with multi-key frames",
      "solution": "Total number of frames from Pad Image Batch Interleaved must match total frames being loaded (frame load cap), and disable block swap for small frame counts",
      "from": "jerms_ai_(4090_24g)"
    },
    {
      "problem": "Follow your emoji not working in VACE",
      "solution": "Try reducing CausVid LoRA strength to 0.4 or below, and increase VACE strength to 1.2 to balance it out",
      "from": "Kijai"
    },
    {
      "problem": "VACE 14B model loading error",
      "solution": "Load normal WAN model in loader, then hook the VACE module using the 'vace_model' output from the loader. Update wrapper nodes if getting compatibility errors",
      "from": "Draken"
    },
    {
      "problem": "ComfyUI crashing with memory issues",
      "solution": "Never use bf16 if you have memory issues - use fp8 models, set nodes to fp16, and use native text encoder that runs on fp8",
      "from": "hicho"
    },
    {
      "problem": "Ghost pose appearing in generations when using multiple control inputs",
      "solution": "Each control net needs its own encoder, can't blend them through single encoder",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "AccVideo scheduler gives step error with denoise under 1.0",
      "solution": "It won't work with denoise under 1.0 because it's taken from 50 steps",
      "from": "Kijai"
    },
    {
      "problem": "Color shift and degradation when using last frame as reference",
      "solution": "Image quality degrades each batch because taking from Wan output repeatedly lowers quality",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Teacache (KJNodes) gives error with AccVideo weight",
      "solution": "Known compatibility issue, no solution provided",
      "from": "yi"
    },
    {
      "problem": "OOM errors during sampling",
      "solution": "Lower resolution/length, increase block swapping (higher numbers), use tiled VAE. Set blocks so VRAM usage is 96-98%, not 99%",
      "from": "N0NSens, JohnDopamine"
    },
    {
      "problem": "AccVid disabled torch.compile for 10 days",
      "solution": "Re-enable torch.compile for better performance",
      "from": "Kijai"
    },
    {
      "problem": "CausVid artifacts in first frames",
      "solution": "Use fp8 and new AccVid model, CausVid artifacts happen because it uses different sampling method",
      "from": "hicho, Kijai"
    },
    {
      "problem": "Background noise in CausVid generations",
      "solution": "Try working workflows, adjust settings properly",
      "from": "MisterMango, VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Canny outlines appearing in longer generations",
      "solution": "Check control video matches ref image, lower controlnet strength from 0.8, avoid blending with other controlnets",
      "from": "enigmatic_e, VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Invalid credentials in Authorization header for HuggingFace",
      "solution": "Export HUGGING_FACE_HUB_TOKEN=\"your_token_here\" or use huggingface-cli login",
      "from": "jerms_ai_(4090_24g)"
    },
    {
      "problem": "Black background when inpainting",
      "solution": "Check mask setup - mask should be white box on black background for area to inpaint",
      "from": "Johnjohn7855"
    },
    {
      "problem": "Eyes being added to character wearing sunglasses",
      "solution": "Turn off 'detect face' in DWPose if you don't need facial movements",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "Wrong T5 encoder causing garbage VACE results",
      "solution": "Use umt5-xxl-enc-bf16.safetensors with wrapper, not umt5_xxl_fp16.safetensors from native",
      "from": "aikitoria"
    },
    {
      "problem": "TeaCache skipping generation on subsequent runs",
      "solution": "Issue acknowledged, first generation works but subsequent ones skip main sampler",
      "from": "thebaker"
    },
    {
      "problem": "RuntimeError: Input type (CUDABFloat16Type) and weight type (CPUBFloat16Type) mismatch",
      "solution": "T5 encoder must be run on GPU, not CPU",
      "from": "mamad8"
    },
    {
      "problem": "Chattery tiled noise all over foreground/background",
      "solution": "Up sampling steps, reduce TeaCache threshold, or use slower input if using VACE",
      "from": "zelgo_"
    },
    {
      "problem": "VACE default shift too low causing artifacts",
      "solution": "Use VACE default shift of 16 instead of lower values",
      "from": "Kijai"
    },
    {
      "problem": "Flashes of noise at beginning with Phantom + VACE",
      "solution": "Lower the strength of VACE embeds for the video input",
      "from": "Johnjohn7855"
    },
    {
      "problem": "VACE color shifts",
      "solution": "Use color match node as partial solution, or prompt for specific colors",
      "from": "David Snow"
    },
    {
      "problem": "Mask appearing at beginning of VACE inpainting",
      "solution": "Lower the strength of VACE embeds for the video input",
      "from": "Johnjohn7855"
    },
    {
      "problem": "Mouth movement continues after character stops talking",
      "solution": "Works better with close-up shots, far away shots make accurate lip sync difficult",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Video flicker in random frames",
      "solution": "Issue being investigated, may be related to reference image usage or sampler settings",
      "from": "humangirltotally"
    },
    {
      "problem": "Phantom only shows one character sometimes",
      "solution": "More clearly describe that there are two characters in prompt for better reliability",
      "from": "aikitoria"
    },
    {
      "problem": "SageAttention build failing",
      "solution": "Use --no-build-isolation flag and ensure building in same venv as torch",
      "from": "aikitoria"
    },
    {
      "problem": "CausVid flash at beginning of videos",
      "solution": "Use LoRA block selection to reduce first block strength to 0.5 or disable first 3 blocks entirely",
      "from": "Kijai"
    },
    {
      "problem": "RuntimeError: mat1 and mat2 shapes cannot be multiplied (512x768 and 4096x1536)",
      "solution": "Check text encoder model - likely have wrong T5 text encoder selected",
      "from": "Kijai"
    },
    {
      "problem": "Mask showing up as frame in phantom+vace inpainting",
      "solution": "Try using empty prompt or very short prompts, long prompts can cause flash issues",
      "from": "pom"
    },
    {
      "problem": "ValueError type fp8e4nv not supported with compile",
      "solution": "Change to e5m2 format - older nvidia GPUs don't support compile with e4m3fn",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI crashes when using Phantom + VACE",
      "solution": "Usually caused by running out of RAM, check RAM usage at crash moment",
      "from": "Kijai"
    },
    {
      "problem": "Context windows breaking at 81+ frames",
      "solution": "Disable context options for higher frame counts",
      "from": "Valle"
    },
    {
      "problem": "Flashing with Phantom setup",
      "solution": "Add LoRA block edit with block 0 turned off, or lower CausVid to 0.4",
      "from": "David Snow"
    },
    {
      "problem": "Characters constantly talking in Phantom",
      "solution": "Add 'talking' or 'conversation' to negative prompt, avoid 'not talking' in positive",
      "from": "Johnjohn7855"
    },
    {
      "problem": "Tensor dimension errors with Uni3C",
      "solution": "Check that input video and output resolution dimensions match",
      "from": "N0NSens"
    },
    {
      "problem": "MoviiGen LoRA causing flashes",
      "solution": "Flash comes from MoviiGen, turning off block 0 doesn't help, lower strength instead",
      "from": "N0NSens"
    },
    {
      "problem": "OOM error when encoding 1024x576 video with 33 frames on 5090",
      "solution": "VHS video loader node not properly resizing or capping frames - use KJ resize node v2 and cap at 33 frames",
      "from": "Kijai"
    },
    {
      "problem": "Skip first frames in VHS node unreliable",
      "solution": "Often gives wrong part of video, use alternative frame selection methods",
      "from": "David Snow"
    },
    {
      "problem": "Expected scalar type Half but found Float error with Uni3C",
      "solution": "Block swapping doesn't work with Uni3C, use VRAM management instead",
      "from": "mamad8"
    },
    {
      "problem": "Uni3C taking 277 s/it at 480p on 3090",
      "solution": "Performance issue confirmed but no specific solution provided",
      "from": "sneako1234"
    },
    {
      "problem": "WebP format doesn't work well with Phantom",
      "solution": "Convert to other image formats for better results",
      "from": "DawnII"
    },
    {
      "problem": "Tensor error with Uni3C: sizes must match except dimension 1",
      "solution": "May occur when using mixed control with 1.3B models, 14B models don't have this issue",
      "from": "N0NSens"
    },
    {
      "problem": "Wan sampler outputs slightly different resolution than specified",
      "solution": "Issue is likely due to VAE (/16) - specifying 480x980 results in 480x976 output",
      "from": "hau"
    },
    {
      "problem": "Block swapping causing half and float error with Uni3C",
      "solution": "Enable quantization to fix the block swapping issue",
      "from": "mamad8"
    },
    {
      "problem": "Wrong Uni3C controlnet model causing bias error",
      "solution": "Use Kijai's modified version: Wan21_Uni3C_controlnet_fp16.safetensors instead of the original ewrfcas version",
      "from": "Kijai"
    },
    {
      "problem": "14B depth controlnet stalling and not using VRAM",
      "solution": "Switch from offload device to main device - device mismatch was causing the stall",
      "from": "David Snow"
    },
    {
      "problem": "ComfyUI-IPAdapterWAN project completely ineffective",
      "solution": "Project is fundamentally broken and creator closed it after issue report",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "problem": "Phantom losing mouth movements in V2V",
      "solution": "Use crop&stitch node to maintain consistent face size, and lower denoising to 0.10-0.15 to preserve motion",
      "from": "DeZoomer"
    },
    {
      "problem": "FILM VFI doubling frames causing generation failure",
      "solution": "Use get image or masked range from batch to cut out every other frame",
      "from": "David Snow"
    },
    {
      "problem": "Grey flashes when using anchor/starting frames with VACE",
      "solution": "Appears to be related to morphing transformations, may need camera control or pose/depth map interpolation",
      "from": "pom"
    },
    {
      "problem": "SamplerCustomAdvanced doesn't behave same as ClownSharkSampler with PerpNegGuider",
      "solution": "Use SharkSampler instead of SamplerCustomAdvanced for consistent results",
      "from": "Ablejones"
    },
    {
      "problem": "Phantom inconsistent character and clothing consistency",
      "solution": "Use detailed prompts describing each reference image, crop head-only for face ref, crop below-neck for outfit ref",
      "from": "AJO"
    },
    {
      "problem": "GGUF Phantom has weird noise artifacts with CausVid",
      "solution": "Use fp8 instead of GGUF Q8 for better compatibility with CausVid",
      "from": "hicho"
    },
    {
      "problem": "Ladder reappearing in inpainting longer videos",
      "solution": "Remove ladder from first frame reference image, use pose control of original video back to VACE",
      "from": "Johnjohn7855"
    },
    {
      "problem": "Phantom CFG causes 3x slower generation",
      "solution": "Use main CFG 1.0 to disable phantom CFG",
      "from": "Kijai"
    },
    {
      "problem": "CausVid causes flash in first frames",
      "solution": "Use CausVid lora with first block disabled or the v2 version",
      "from": "Kijai"
    },
    {
      "problem": "CFG scheduling node errors with non-zero start percent",
      "solution": "Use manual list or spline editor for complex CFG scheduling",
      "from": "Kijai"
    },
    {
      "problem": "Gemini nodes broken by ComfyUI core update",
      "solution": "ComfyUI Core released paid Gemini version with same node names, custom node maker needs to change names",
      "from": "AJO"
    },
    {
      "problem": "Color saturation issues with VACE continuations",
      "solution": "Prompt for specific colors to offset saturation issues",
      "from": "pom"
    },
    {
      "problem": "WSL loading transformer parameters extremely slow",
      "solution": "WSL RAM allocation issues - need to edit config to give more RAM",
      "from": "MilesCorban"
    },
    {
      "problem": "CausVid v2 causing over-saturation when combined with high CFG",
      "solution": "Lower CFG from 5 to avoid burning/over-saturation, use CFG 5 only for first step",
      "from": "Kijai"
    },
    {
      "problem": "Reactor face swap causing flickery eyes and stuttery motion",
      "solution": "Changed to codeformer v1, but still produces less quality than original",
      "from": "AJO"
    },
    {
      "problem": "Permission error with triton backends on 5090",
      "solution": "Reboot recommended, though didn't solve in this case",
      "from": "JohnDopamine"
    },
    {
      "problem": "CausVid v2 not working well with AccVid",
      "solution": "User reverted to v1.5, may need different parameter combinations",
      "from": "Johnjohn7855"
    },
    {
      "problem": "OOM errors at higher resolutions",
      "solution": "Add block swapping - works for 1280x720 with 5 blocks swapped",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Getting storage sense warnings",
      "solution": "Delete unwanted merged models to manage disk space - video models consume lots of storage",
      "from": "Thom293"
    },
    {
      "problem": "Reactor causes jitter around face features",
      "solution": "Use film grain after reactor processing, or composite only the face part back using masks",
      "from": "Johnjohn7855"
    },
    {
      "problem": "ATI crashes with wrong dimensions",
      "solution": "Ensure dimensions are divisible by 16 (e.g., 608 not 600)",
      "from": "Kijai"
    },
    {
      "problem": "Blender depth maps too detailed for VACE",
      "solution": "Add 1 pixel blur to make them work as depth",
      "from": "Kijai"
    },
    {
      "problem": "Out of memory when using CausVid LoRa",
      "solution": "Disable non-blocking on block swap, use fp8 text encoder",
      "from": "Kijai"
    },
    {
      "problem": "CUDA error on 5090 with runpod template",
      "solution": "Triton may not support nvfp4 yet, try using unsupported model settings",
      "from": "TK_999"
    },
    {
      "problem": "Fp8_e4m3fn doesn't work with compile on RTX 3000 series causing triton/tensor errors",
      "solution": "Use fp8_e5m2 format instead or don't use compile",
      "from": "Kijai"
    },
    {
      "problem": "Tensor error with frame processing",
      "solution": "Check that image size is divisible by 16 and verify correct number of frames from source video",
      "from": "TK_999"
    },
    {
      "problem": "Significant facial changes in VACE animated videos",
      "solution": "Make reference image match start frame position, avoid using uncropped image as both character reference and background, or interpolate position. Also check depth map matches if using depth control",
      "from": "Piblarg"
    },
    {
      "problem": "Green artifacts appear when model fails to track movement",
      "solution": "Model goes green when it gets no trajectory points - ensure you have enough points for all frames",
      "from": "Kijai"
    }
  ],
  "comparisons": [
    {
      "comparison": "FantasyTalker vs Omnihuman lip sync",
      "verdict": "Omnihuman is much better for lip sync quality",
      "from": "Kytra"
    },
    {
      "comparison": "VACE vs Fun Control",
      "verdict": "VACE is better but more random, Fun Control bad when movement hard to detect",
      "from": "Dream Making"
    },
    {
      "comparison": "1.3B vs 14B Fun Control quality",
      "verdict": "14B is much better quality, notable jump from Fun 1.0 to 1.1 for 1.3B",
      "from": "Nathan Shipley"
    },
    {
      "comparison": "Q8 GGUF vs fp8 quality",
      "verdict": "Q8 GGUF higher quality despite slower speed",
      "from": "MilesCorban"
    },
    {
      "comparison": "LTX vs Fun camera control",
      "verdict": "LTX better for static scenes - 9sec/gen 1280x720 vs 1min+ for Fun 832x480",
      "from": "N0NSens"
    },
    {
      "comparison": "DPM++ vs UniPC with DG models",
      "verdict": "Outputs look almost identical, DPM++ has ever so slight edge on detail",
      "from": "David Snow"
    },
    {
      "comparison": "Higher vs lower DG model versions",
      "verdict": "Need stronger versions for more stylization, but stronger models introduce more flashes - exercise in frustration",
      "from": "David Snow"
    },
    {
      "comparison": "Video Depth Anything vs Depth Anything v2",
      "verdict": "Regular Depth Anything v2 performs better despite flickering, Video Depth Anything has bad banding that causes artifacts",
      "from": "Nathan Shipley"
    },
    {
      "comparison": "VACE 1.1 vs control video",
      "verdict": "1.1 is more forgiving than VACE, but control video has huge impact on style retention",
      "from": "Rishi Pandey"
    },
    {
      "comparison": "Fun vs VACE for multiple control types",
      "verdict": "Fun better for combining multiple control inputs, VACE struggles with specificity requirements",
      "from": "Rishi Pandey"
    },
    {
      "comparison": "FramePack vs Wan DiffusionForcing",
      "verdict": "FramePack faster than wan and handles scene shifts better than DF, but lacks control options",
      "from": "Draken"
    },
    {
      "comparison": "HunyuanVideo vs Wan for control",
      "verdict": "If HY had anything close to vace, no one would be talking about WAN - main issue with HY has always been control",
      "from": "Draken"
    },
    {
      "comparison": "Wan Fun vs AnimDiff",
      "verdict": "1.3B is miles better than animdiff - things don't morph over time, far more consistent especially for v2v. But for abstract stuff and creativity, animdiff is better",
      "from": "David Snow"
    },
    {
      "comparison": "Wan 720p vanilla vs Sky variants",
      "verdict": "720p Sky at 832x480 has better prompt following than vanilla, Sky 540p has worst quality",
      "from": "N0NSens"
    },
    {
      "comparison": "14B vs 1.3B LoRA compatibility",
      "verdict": "No compatibility between 1.3B and 14B models, but 1.3B LoRAs should work with 1.3DF",
      "from": "MilesCorban"
    },
    {
      "comparison": "Phantom vs Skyreels vs VACE vs Hunyuan Custom",
      "verdict": "Still prefer Phantom, Skyreels, or VACE over Hunyuan custom",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "14B vs 1.3B Wan models",
      "verdict": "14B is ungodly slow, 5B model more exciting for practical use",
      "from": "David Snow"
    },
    {
      "comparison": "VACE vs LTX 13B for I2V",
      "verdict": "LTX 13B gives great results in fraction of the time compared to VACE",
      "from": "David Snow"
    },
    {
      "comparison": "Higher parameter models for LoRA training",
      "verdict": "High param count makes LoRA training easier - Flux can learn in 60 steps unprecedented before",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "comparison": "Desktop vs Portable ComfyUI",
      "verdict": "Portable recommended - desktop version has fewer users for support and limited module installation options",
      "from": "David Snow"
    },
    {
      "comparison": "CausVid 14B vs regular Wan 14B",
      "verdict": "CausVid is much faster (3 steps vs standard) but has quality trade-offs",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 1.3B vs LTXV quality",
      "verdict": "LTXV is better quality than 1.3B Wan, but 14B Wan beats LTXV at 5x slower speed",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Skyreels i2v vs Wan2.1-Fun-V1.1-1.3B-InP",
      "verdict": "Fun-V1.1 was better, Skyreels was a flop",
      "from": "Ada"
    },
    {
      "comparison": "4 steps vs 9 steps CausVid",
      "verdict": "4 steps gets 90% of quality, 9 steps unnecessary",
      "from": "yi"
    },
    {
      "comparison": "CausVid vs Wan base",
      "verdict": "Wan base looks way better due to CausVid's high saturation issues",
      "from": "yi"
    },
    {
      "comparison": "3 steps vs 9 steps CausVid",
      "verdict": "Both look like 1.3b not 14b, 9 steps gives garbage results",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "VACE 1.3B vs 14B benchmarks",
      "verdict": "1.3B often performs better than 14B even for depth, unless you want 720p",
      "from": "Juampab12"
    },
    {
      "comparison": "Full finetunes vs LoRAs",
      "verdict": "Full finetunes can be higher quality but can also unlearn and fragment ecosystem",
      "from": "yi"
    },
    {
      "comparison": "VACE final vs preview 1.3B",
      "verdict": "Final is sharper but has less movement, preview may be better for some use cases",
      "from": "David Snow"
    },
    {
      "comparison": "1.3B vs 14B VACE",
      "verdict": "14B works better at higher resolutions, 1.3B sufficient for controlled generation",
      "from": "Juampab12"
    },
    {
      "comparison": "Causvid performance with VACE",
      "verdict": "Causvid working well with VACE, producing good results in fewer steps",
      "from": "DawnII"
    },
    {
      "comparison": "VACE 1.3B vs 14B quality",
      "verdict": "Not much difference except 14B works better at higher resolutions like 720p",
      "from": "Valle"
    },
    {
      "comparison": "MoviiGen vs CausVid",
      "verdict": "Different purposes - CausVid is lower step, MoviiGen is finetune on cinematographic shots",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "DG Wan boost vs base model",
      "verdict": "DG version is smoother with less obvious morphing, can do 6-8 steps instead of 20-30",
      "from": "David Snow"
    },
    {
      "comparison": "1024x576 vs 1280x720 with VACE 1.3B",
      "verdict": "1024x576 seems to have more charm for style",
      "from": "David Snow"
    },
    {
      "comparison": "CausVid as full model vs LoRA",
      "verdict": "LoRA version allows VACE compatibility and better control at reduced strength",
      "from": "Kijai"
    },
    {
      "comparison": "VACE vs FLF2V (First Last Frame To Video)",
      "verdict": "VACE seems to do a better job than FLF2V",
      "from": "DawnII"
    },
    {
      "comparison": "14B CausVid vs 1.3B VACE quality",
      "verdict": "14B with CausVid is close to 1.3B quality wise but much faster",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "CausVid vs normal workflow speed",
      "verdict": "5x speedup - 1 minute vs 5 minutes on RTX 3090 for same quality",
      "from": "seruva19"
    },
    {
      "comparison": "VACE DF vs VACE with context",
      "verdict": "Context windows more viable with CausVid due to speed, reference works across context windows",
      "from": "Kijai"
    },
    {
      "comparison": "4 step CausVid LoRA vs 20 steps normal",
      "verdict": "CausVid is order of magnitude faster - 94 seconds vs very long time",
      "from": "A.I.Warper"
    },
    {
      "comparison": "14B CausVid vs 1.3B speed",
      "verdict": "14B CausVid faster by a landslide, order of magnitude faster",
      "from": "A.I.Warper"
    },
    {
      "comparison": "4 step CausVid vs 8 step David's workflow",
      "verdict": "CausVid much faster: 10min vs 300s for same quality",
      "from": "DawnII"
    },
    {
      "comparison": "VACE 1.3B vs 14B for reference consistency",
      "verdict": "14B vastly superior for reference image fidelity",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "LTX vs WAN speed",
      "verdict": "LTX still much faster but WAN better quality for specific use cases",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Wan vs AnimateDiff replacement",
      "verdict": "AnimateDiff still has its own style which is good, Wan doesn't completely replace it",
      "from": "Piblarg"
    },
    {
      "comparison": "HiDream vs Flux for reference images",
      "verdict": "A.I.Warper personally prefers HiDream over Flux",
      "from": "A.I.Warper"
    },
    {
      "comparison": "CausVid 1.3B vs 14B",
      "verdict": "14B listens to prompts much better than 1.3B but has degradation over time",
      "from": "N0NSens"
    },
    {
      "comparison": "Bidirectional vs Causal CausVid",
      "verdict": "Bidirectional attends to past and future frames, causal only attends to past frames (LLM-like)",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "comparison": "1.3B vs 14B CausVid speed",
      "verdict": "1.3B: 45 seconds vs 14B: 1:30 for same content, but 14B much better quality for faces",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "1.3B face quality vs 14B",
      "verdict": "1.3B: 'looks like that guy', 14B: 'it is that guy!' - much better face fidelity",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "CausVid works better with DG models vs vanilla",
      "verdict": "DG Boost models show better results with CausVid LoRA than vanilla 1.3B",
      "from": "David Snow"
    },
    {
      "comparison": "CausVid vs regular Wan inference",
      "verdict": "CausVid much faster but more plastic/worse physics, quality trade-off for speed",
      "from": "DiXiao"
    },
    {
      "comparison": "1.3B vs 14B VACE quality",
      "verdict": "14B model is much better with references, 1.3B is quite bad with references",
      "from": "Stad"
    },
    {
      "comparison": "Causvid 0.25 vs 0.55 strength",
      "verdict": "0.25 strength looks much more natural, 0.55 looks a little baked",
      "from": "traxxas25"
    },
    {
      "comparison": "Q8_0 vs other quants",
      "verdict": "Q8_0 is basically nearly the same as f16 with half the size, wouldn't go lower than Q4_K_S for regular generating",
      "from": "The Punisher"
    },
    {
      "comparison": "Q5_K_S vs Q3_K_S GGUF",
      "verdict": "Q5_K_S is significantly better quality, Q3_K_S has noticeable patterning issues",
      "from": "The Punisher"
    },
    {
      "comparison": "14B vs 1.3B for fast motion",
      "verdict": "14BN significantly better for faster motion like Matrix fight scenes",
      "from": "A.I.Warper"
    },
    {
      "comparison": "VACE reference vs Phantom reference",
      "verdict": "VACE reference leaves something to be desired compared to Phantom",
      "from": "DawnII"
    },
    {
      "comparison": "8 steps vs 4 steps with Causvid",
      "verdict": "8 steps too contrasty, 4 steps lacks motion, 6 steps may be optimal",
      "from": "David Snow"
    },
    {
      "comparison": "Wrapper vs Native VACE",
      "verdict": "Wrapper has better support and VRAM management, Native limited but works fine",
      "from": "Draken"
    },
    {
      "comparison": "WAN vs Runway Gen4",
      "verdict": "Quality very close, WAN generates faster (2.5min vs 5min), but Runway has better lens flare interaction and color",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "CausVid LoRA + MoviiGen vs CausVid LoRA + CausVid model",
      "verdict": "MoviiGen combination much better for image quality, lighting and movie texture",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "8 steps vs 14 steps CausVid",
      "verdict": "14 steps removes shimmer present in 8 step outputs, significant quality improvement",
      "from": "CaptHook"
    },
    {
      "comparison": "4 steps vs 8 steps for wide shots",
      "verdict": "8 steps much better for wide shots, 4 steps may be enough for medium/close shots",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "T2V vs I2V without control",
      "verdict": "I2V is superior when not using control",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "30+ steps without CausVid vs CausVid LoRA",
      "verdict": "30+ steps without CausVid gives much better quality but is 5x longer, worth it for perfect shots",
      "from": "Yae"
    },
    {
      "comparison": "WAN vs Runway for video generation",
      "verdict": "WAN actually looks better than Runway, but Runway can do 20 seconds vs WAN's ~6 seconds due to OOM issues",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "Depth vs MediaPipe for facial control",
      "verdict": "Depth map provides better facial movement than MediaPipe",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "Sapiens vs standard MediaPipe",
      "verdict": "Sapiens much more precise for facial transfer but harder to setup",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "14B vs 1.3B model quality",
      "verdict": "14B much better quality, can generate long dresses and has almost no artifacting where 1.3B failed",
      "from": "traxxas25"
    },
    {
      "comparison": "Follow Your Emoji preprocessor vs aux preprocessor",
      "verdict": "Follow Your Emoji MediaPipe preprocessor is better than aux",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Depth vs other preprocessors for VACE",
      "verdict": "Depth preprocessor works better than pose/mediapipe/facemesh for facial movement",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "With vs without TeaCache for CausVid",
      "verdict": "30% slower without TeaCache (12min vs 8min), but quality appears similar",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Single vs combined VACE processors",
      "verdict": "Combined processors show more detail (wrinkles) but can look oversaturated/cooked",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "Depth vs Pose control",
      "verdict": "Depth alone can achieve good facial movement and is faster, pose adds some help with face movements but may not be necessary",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "MediaPipe vs DWPose",
      "verdict": "Both have positives and negatives, MediaPipe better at following original face expression, DWPose gives more natural settled look",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "DepthCrafter vs DepthAnything",
      "verdict": "DepthCrafter is the best depth model, DepthAnything V2 is fine too, depends on quality obsession level",
      "from": "David Snow"
    },
    {
      "comparison": "14B success rate vs other models",
      "verdict": "Success rate with 14B is extremely high, makes a nice change from other models",
      "from": "David Snow"
    },
    {
      "comparison": "SkyReels vs base WAN 2.1",
      "verdict": "SkyReels just different, trained at 24fps, LoRAs work between base and SkyReels but not with MoviiGen",
      "from": "JohnDopamine"
    },
    {
      "comparison": "VACE vs base WAN 14B for single reference animation",
      "verdict": "WAN 14B has better video quality, but VACE is more versatile for complex operations",
      "from": "zelgo_"
    },
    {
      "comparison": "14B success rate vs other models",
      "verdict": "14B has much higher success rate - two thirds of overnight batches are pretty good vs lower rates for other models",
      "from": "David Snow"
    },
    {
      "comparison": "TeaCache with/without coefficients on VACE 14B CausVid",
      "verdict": "382s vs 203s generation time, but quality differences noted",
      "from": "David Snow"
    },
    {
      "comparison": "MPS vs HPS reward LoRAs",
      "verdict": "For T2V both can be used, for I2V use MPS only. HPS changes faces drastically and overpowers other LoRAs",
      "from": "N0NSens"
    },
    {
      "comparison": "Euler vs UniPC samplers with CausVid",
      "verdict": "Euler produces cleaner results but has ghost-like effects, UniPC is fine but can create visible noise",
      "from": "N0NSens"
    },
    {
      "comparison": "VACE vs Fun Control for video cuts",
      "verdict": "Fun Control handles cuts better than VACE",
      "from": "N0NSens"
    },
    {
      "comparison": "MatAnyone vs BiRefNet for background removal",
      "verdict": "MatAnyone: 6.54s (short video), 22.02s (long video) vs BiRefNet: 9.60s (short), 18.22s (long). MatAnyone includes loose hairs but can cause NormalCrafter issues",
      "from": "MilesCorban"
    },
    {
      "comparison": "WAN T2V as image generator vs Flux",
      "verdict": "WAN can generate single frames but Flux likely produces better quality images. WAN approach helps with consistency across video clips",
      "from": "Nokai"
    },
    {
      "comparison": "WAN VACE vs Runway Vid to Vid",
      "verdict": "VACE works way better than Runway's Vid to Vid for video-to-video conversion",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "VACE 14B vs HY Custom",
      "verdict": "VACE 14B achieves more in 10 minutes than a week with HY Custom",
      "from": "AJO"
    },
    {
      "comparison": "VACE 14B vs 1.3B",
      "verdict": "14B is worlds better than 1.3B",
      "from": "Piblarg"
    },
    {
      "comparison": "Skyreels vs standard models",
      "verdict": "Skyreels has better motion for human subjects, native 24fps vs 16fps, and higher resolution (540p vs 480p)",
      "from": "DawnII"
    },
    {
      "comparison": "WAN vs Veo 3",
      "verdict": "Veo 3 shows significant advancement in prompt following and dialogue capabilities that open source hasn't matched yet",
      "from": "Draken"
    },
    {
      "comparison": "VACE vs Fun Control",
      "verdict": "VACE is far ahead of anything else for controlling video output",
      "from": "Ada"
    },
    {
      "comparison": "CausVid 14B vs 1.3B",
      "verdict": "14B is better quality but 1.3B is good for secondary v2v passes and super fast",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Different frame counts with NormalCrafter",
      "verdict": "49 frames vs 81 frames yields entirely different results even with fixed seed, making development difficult",
      "from": "Guey.KhalaMari"
    },
    {
      "comparison": "Wan vs LTX",
      "verdict": "Wan is the more powerful model, LTX's advantage is speed",
      "from": "David Snow"
    },
    {
      "comparison": "GGUF vs other quantization methods",
      "verdict": "GGUF is slower but nicer quality, whereas fp8/int8/fp4/int4 can be faster with size reduction",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "comparison": "Quality comparison BF16 vs GGUF Q8 vs FP8",
      "verdict": "bf16 > GGUF Q8 > fp8 in quality",
      "from": "MilesCorban"
    },
    {
      "comparison": "CausVid with and without CFG Schedule",
      "verdict": "No CausVid reflects lora intention better, CausVid with CFG Schedule brings back lora look while maintaining speed",
      "from": "Zlikwid"
    },
    {
      "comparison": "Hunyuan Custom vs VACE",
      "verdict": "VACE has better quality and support, Hunyuan Custom is 5-10 times slower to generate with shocking quality and no community traction",
      "from": "AJO"
    },
    {
      "comparison": "WAN vs HunyuanVideo",
      "verdict": "WAN is better quality due to 14B parameters, but HunyuanVideo has more LoRAs, been around longer, and is faster/smaller. HunyuanVideo doesn't start in same position - uses face and can put it anywhere",
      "from": "MilesCorban"
    },
    {
      "comparison": "Movii base vs WAN base for I2V",
      "verdict": "Movii as base model has better prompt adhesion than WAN base model when used with VACE for I2V",
      "from": "Johnjohn7855"
    },
    {
      "comparison": "Causvid distillation level",
      "verdict": "Causvid may have distilled slightly too much for some uses, similar to SDXL DMD2 going to 4 steps when 8 steps can be better balance",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "comparison": "UniAnimate vs standard pose for character consistency",
      "verdict": "UniAnimate has better consistency of the character compared to standard pose controls",
      "from": "Johnjohn7855"
    },
    {
      "comparison": "Context windows vs batching method",
      "verdict": "Batching followed by cleanup pass is superior method - preserves character likeness",
      "from": "A.I.Warper"
    },
    {
      "comparison": "Using own face vs latent sync for lip sync",
      "verdict": "Using own face input produces higher quality results than latent sync",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "Official DWPose vs ComfyUI implementation",
      "verdict": "Official implementation is cleaner, works on more content, no dropped frames",
      "from": "A.I.Warper"
    },
    {
      "comparison": "MediaPipe vs other face detection",
      "verdict": "MediaPipe breaks when more than 45 degrees or mouth too open",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "comparison": "VACE extension vs Skyreels Diffusion Forcing",
      "verdict": "VACE has better movement and quality but noticeable color shift, Skyreels doesn't have color shift",
      "from": "ArtOfficial"
    },
    {
      "comparison": "Chroma vs Base Flux speed",
      "verdict": "Chroma is slower than base Flux, needs higher step count and has CFG making it twice as slow",
      "from": "mamad8"
    },
    {
      "comparison": "GGUF vs FP8 on WAN models",
      "verdict": "GGUF more precise but quite a bit slower than fp8, fp8 doesn't need much offloading on 5090",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 14B vs 1.3B model quality",
      "verdict": "Night and day difference - 14B much better at understanding complex objects and producing realistic results",
      "from": "David Snow"
    },
    {
      "comparison": "Wan 1.3B vs 14B for dress understanding",
      "verdict": "1.3B kept producing fake results and never understood full length dress, 14B worked perfectly with same settings",
      "from": "traxxas25"
    },
    {
      "comparison": "CausVid speed vs standard inference",
      "verdict": "CausVid is extremely fast compared to normal inference",
      "from": "mamad8"
    },
    {
      "comparison": "TeaCache vs CausVid optimization approaches",
      "verdict": "TeaCache skips steps, CausVid optimizes every step to need fewer - different approaches that don't work well together",
      "from": "Cubey"
    },
    {
      "comparison": "14B vs 1.3B generation time",
      "verdict": "1.3B does 163 frames in 5 minutes vs 20 minutes for 14B",
      "from": "VK (5080 128gb)"
    },
    {
      "comparison": "Native vs GGUF loader performance",
      "verdict": "GGUF took 40 minutes, native loader took 17 minutes for same generation",
      "from": "The Shadow (NYC)"
    },
    {
      "comparison": "Normal Wan vs other models for VACE",
      "verdict": "Normal Wan has been best so far, skyreels for human centric videos, moviegen for realistic high res cinematic",
      "from": "Kijai"
    },
    {
      "comparison": "Flux vs SDXL for stylized content",
      "verdict": "SDXL better for heavily stylized stuff, Flux awful unless using LoRA, Flux controlnets much worse than SDXL",
      "from": "A.I.Warper"
    },
    {
      "comparison": "SDXL vs Flux controlnets",
      "verdict": "Xinsir + mistoline still peak controlnet, mistoline Flux version much worse, even VACE better in many things",
      "from": "Kijai"
    },
    {
      "comparison": "Vace 1.3B vs 14B",
      "verdict": "14B works fine with context options, 1.3B may not work properly with context options",
      "from": "N0NSens"
    },
    {
      "comparison": "David Snow's workflow vs vid2vid workflow",
      "verdict": "New workflow is complete character replacement vs stylization, can do things vid2vid workflow cannot",
      "from": "David Snow"
    },
    {
      "comparison": "SkyReels CausVid vs normal CausVid",
      "verdict": "SkyReels CausVid at 720p gives crisp results but not as prompt adherent as normal CausVid",
      "from": "hicho"
    },
    {
      "comparison": "VACE vs I2V model speed",
      "verdict": "VACE shouldn't be faster than I2V since it's T2V model + 8 more VACE blocks, but user reports faster performance with native nodes",
      "from": "aikitoria"
    },
    {
      "comparison": "RTX 5070 vs 5060 Ti",
      "verdict": "5070 is faster than 5060 Ti in specs, and for AI if models fit. 5060 Ti slightly slower than 3080",
      "from": "VK (5080 128gb)"
    },
    {
      "comparison": "Euler vs Euler/AccVideo scheduler",
      "verdict": "Very hard to spot difference, Euler might be marginally sharper",
      "from": "David Snow"
    },
    {
      "comparison": "AccVideo full model vs LoRA",
      "verdict": "Full model better, LoRA worse but maybe good enough - fur better on full model",
      "from": "Kijai"
    },
    {
      "comparison": "14B t2v + vace vs 14B vace standalone",
      "verdict": "t2v + vace handles v2v moves better than standalone vace",
      "from": "Nokai"
    },
    {
      "comparison": "UniPC vs Euler schedulers",
      "verdict": "UniPC timesteps: [999, 978, 952, 921, 882, 833, 769, 681, 555, 357]",
      "from": "Kijai"
    },
    {
      "comparison": "AccVid vs CausVid for T2V",
      "verdict": "AccVid seems worse than CausVid for unintended use, CausVid better for stylized content, AccVid better for realistic",
      "from": "Kijai"
    },
    {
      "comparison": "AccVid vs CausVid quality",
      "verdict": "AccVid output seems better than CausVid, but CausVid way better at 0.5 weight",
      "from": "\ud83e\udd99rishappi, Ada"
    },
    {
      "comparison": "AccVid vs CausVid prompt following",
      "verdict": "I2V with AccVid has decent quality but prompt following is gone, later seed did follow prompt but quality wasn't great",
      "from": "Kijai"
    },
    {
      "comparison": "Hunyuan AccVid vs Wan",
      "verdict": "Hunyuan AccVid makes everything realistic and is slower than Wan with bad V2V results at 0.50 denoiser",
      "from": "Kijai, hicho"
    },
    {
      "comparison": "BAGEL ComfyUI implementation",
      "verdict": "Not that good, takes lot of time, got 2 blurry outputs for image edit. To be frank, it SUX",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "DepthCrafter vs DepthAnything v2",
      "verdict": "DepthCrafter has less flicker and looks smoother but takes longer",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "comparison": "Phantom vs VACE for identity preservation",
      "verdict": "Phantom has better identity preservation for reference images and better multi-reference handling",
      "from": "Johnjohn7855"
    },
    {
      "comparison": "Phantom 14B vs regular models speed",
      "verdict": "Phantom 14B is significantly slower due to 3 noise predictions and CFG requirement",
      "from": "Kijai"
    },
    {
      "comparison": "CausVid vs no CausVid on Phantom",
      "verdict": "Without causvid the quality is ridiculously higher",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "comparison": "VACE ref vs Phantom ref for identity",
      "verdict": "Identity is preserved much better than vace ref with VACE input + phantom ref",
      "from": "Zuko"
    },
    {
      "comparison": "480p vs 720p generation quality",
      "verdict": "480p got basically every detail right, works better for some cases",
      "from": "aikitoria"
    },
    {
      "comparison": "Moviigen vs regular WAN",
      "verdict": "Model performs better on average at higher res, Moviigen can be pretty noisy",
      "from": "TK_999"
    },
    {
      "comparison": "Phantom vs Kling Elements",
      "verdict": "Kling Elements is better quality and higher res, but Phantom is closest local model has gotten to that level",
      "from": "aikitoria"
    },
    {
      "comparison": "Phantom vs other models",
      "verdict": "Slowest model used so far, but no other model can do what it does",
      "from": "aikitoria"
    },
    {
      "comparison": "Phantom facial consistency vs Kling",
      "verdict": "Phantom may have better facial consistency than Kling for single reference use cases",
      "from": "hau"
    },
    {
      "comparison": "causvid vs causvid+accvid",
      "verdict": "causvid+accvid combo provides better results, accvid helps with blur that causvid alone has",
      "from": "Ada"
    },
    {
      "comparison": "Phantom vs 14B for morphing",
      "verdict": "14B gave better morphing results than Phantom in limited testing",
      "from": "David Snow"
    },
    {
      "comparison": "1024x576 vs 864x480 vs 960x544",
      "verdict": "1024x576 produces WAY better output than 864x480 and much better than 960x544",
      "from": "CaptHook"
    },
    {
      "comparison": "6 steps vs 8 steps vs 10 steps",
      "verdict": "6 steps showed occasional morphing and less sharpness, 8 steps optimal, 10 steps too long with too much contrast",
      "from": "CaptHook"
    },
    {
      "comparison": "RTX 6000 Pro vs 4090 speed",
      "verdict": "RTX 6000 Pro about double the speed of 4090 on wan, maybe a little less",
      "from": "aikitoria"
    },
    {
      "comparison": "Phantom vs other models for motion transfer",
      "verdict": "Best model seen so far for motion transfer",
      "from": "Kijai"
    },
    {
      "comparison": "VACE vs Phantom for character consistency",
      "verdict": "Phantom is better for character consistency, VACE better for control",
      "from": "aikitoria"
    },
    {
      "comparison": "CausVid vs AccVid effectiveness",
      "verdict": "CausVid does most of the heavy lifting, didn't like what AccVid adds",
      "from": "Kijai"
    },
    {
      "comparison": "Phantom 14B vs 1.3B for multiple subjects",
      "verdict": "14B much better with 3+ subjects than 1.3B",
      "from": "DawnII"
    },
    {
      "comparison": "Phantom alone vs Phantom + VACE",
      "verdict": "VACE alone may be sufficient since it already provides subject consistency via ref image",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "VACE generation time vs Phantom + VACE",
      "verdict": "VACE alone: 177.56 seconds, Phantom + VACE: 261.39 seconds",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "Phantom with random AI images vs well-known celebrities",
      "verdict": "Very accurate on random AI images, obvious differences become apparent with well-known faces",
      "from": "David Snow"
    },
    {
      "comparison": "Phantom cfg between 1 and 10 with CausVid",
      "verdict": "Haven't seen much difference between cfg 1 and 10 when using CausVid",
      "from": "DawnII"
    },
    {
      "comparison": "Phantom alone vs Phantom + VACE",
      "verdict": "Phantom alone maintains good likeness, but coupling with VACE causes character consistency to go out the window and produces artifacts",
      "from": "David Snow"
    },
    {
      "comparison": "Topaz vs ComfyUI upscalers",
      "verdict": "All upscalers in ComfyUI are better than Topaz technically, but Topaz is plug-and-play while ComfyUI requires complex multi-pass workflows",
      "from": "chrisd0073"
    },
    {
      "comparison": "Different video generation speeds",
      "verdict": "CausVid is hard to top - killer performance",
      "from": "JohnDopamine"
    },
    {
      "comparison": "14B Phantom vs 1.3B Phantom",
      "verdict": "14B far better quality but really slow",
      "from": "Kijai"
    },
    {
      "comparison": "Phantom vs LoRA training",
      "verdict": "Phantom does hair as well or better than trained LoRAs right out of the box",
      "from": "Thom293"
    },
    {
      "comparison": "All-in-one model vs separate Phantom + CausVid",
      "verdict": "All-in-one suffers from LoRA integration, separate loading preferred",
      "from": "Kijai"
    },
    {
      "comparison": "CausVid with block 0 disabled vs other variants",
      "verdict": "CausVid w/ block 0 disabled is definitely the 'right' way for Phantom",
      "from": "JohnDopamine"
    },
    {
      "comparison": "Full Phantom model vs distilled versions",
      "verdict": "Full model was much better - difference was massive in quality and movement",
      "from": "TK_999"
    },
    {
      "comparison": "VACE vs I2V methods",
      "verdict": "I2V have very good consistency, different results but haven't tested which is actually better",
      "from": "wooden tank"
    },
    {
      "comparison": "Kontext vs other editing tools",
      "verdict": "Kontext very limited in practice - can't do perspective shifts like Runway, can't do major out of context edits well like 4o",
      "from": "pom"
    },
    {
      "comparison": "CausVid v2 vs v1.5",
      "verdict": "v2 preferred - can use full strength, less plasticy looking, has block 0 already removed",
      "from": "AJO"
    },
    {
      "comparison": "Phantom vs VACE with reference image",
      "verdict": "Phantom faster (253s vs 738s) and produces better results",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "FP8 vs FP16 Phantom",
      "verdict": "FP8 half the generation time with similar quality",
      "from": "AJO"
    },
    {
      "comparison": "Merged CausVid model vs base Phantom + v2 lora",
      "verdict": "Merged model seems better than separate lora application",
      "from": "Thom293"
    },
    {
      "comparison": "LoRAs vs no LoRAs",
      "verdict": "LoRAs made a HUGE difference in quality",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "Merged model vs stock model",
      "verdict": "You can't get this quality with stock model - looks really bad without the LoRAs",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "Merged model vs stacked LoRAs",
      "verdict": "Same quality but merged model generates faster",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "SkyReels A2 vs Phantom",
      "verdict": "Not nearly as accurate as phantom, and far less references can be given",
      "from": "DawnII"
    },
    {
      "comparison": "Film-like vs vibrant outputs",
      "verdict": "Film-like outputs (flatter, less saturated) preserve dynamic range better for post-processing, while vibrant outputs look good immediately but have blown highlights",
      "from": "Ruairi Robinson"
    },
    {
      "comparison": "CausVid v1 vs v1.5 vs v2",
      "verdict": "v1.5 similar settings to v1 but helps eliminate flashes, v2 needs >1 CFG",
      "from": "Johnjohn7855"
    },
    {
      "comparison": "Normal Craft vs Sapients normal maps",
      "verdict": "Normal Craft works great, Sapients works but not as well",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "comparison": "VACE vs FaceFusion for face swapping",
      "verdict": "VACE is incomparably better - handles lighting changes perfectly, works with partial face obscuration, doesn't lose tracking easily like FaceFusion which turns into flickery mess",
      "from": "Ruairi Robinson"
    }
  ],
  "tips": [
    {
      "tip": "Never use denoise other than 1.0 unless doing vid2vid",
      "context": "For T2V generation",
      "from": "Kijai"
    },
    {
      "tip": "Use block 8 with 1.3B model",
      "context": "Better results with 1.3B variant",
      "from": "Kijai"
    },
    {
      "tip": "Use reference image as start image in Fun 1.1",
      "context": "Works better than using actual start image, especially if start doesn't match source",
      "from": "Hashu"
    },
    {
      "tip": "Reverse video if reference object hidden in first frame",
      "context": "When trying to give reference to something not visible initially",
      "from": "ingi // SYSTMS"
    },
    {
      "tip": "Higher audio CFG helps with lip sync",
      "context": "Improving sync quality in FantasyTalker",
      "from": "Kijai"
    },
    {
      "tip": "Use conda environments for AI projects",
      "context": "Prevents breaking ComfyUI when new projects have special requirements",
      "from": "MilesCorban"
    },
    {
      "tip": "Use DG models for T2V with 1.3B",
      "context": "Can get usable results in 6 steps instead of 25-30, though still not good quality. Ares V3 is current favorite, V5 versions too cooked",
      "from": "David Snow"
    },
    {
      "tip": "Mask shadows when removing people with VACE",
      "context": "When trying to remove a person, need to mask their shadow as well or VACE will use shadow to add a person back",
      "from": "traxxas25"
    },
    {
      "tip": "Use bbox instead of person-shaped mask for VACE",
      "context": "Better results when removing objects/people from scenes",
      "from": "ArtOfficial"
    },
    {
      "tip": "Higher CFG values retain motion better",
      "context": "Head movements follow better with higher CFG values in v2v workflows",
      "from": "chrisd0073"
    },
    {
      "tip": "For edge control, do something between scribble and layout",
      "context": "Works kind of well with hard lines but outlining your overall motion",
      "from": "Rishi Pandey"
    },
    {
      "tip": "Consider using depth map layer where applicable",
      "context": "Usually gives better results than poses or just poses with scribble, can blur parts if faces get too detailed",
      "from": "Blink"
    },
    {
      "tip": "For long v2v animations, encode base video directly instead of using start frame",
      "context": "Superior in most cases - single start frame is limited when hands move off-screen or background changes",
      "from": "David Snow"
    },
    {
      "tip": "Kitbash different preprocessors for best results",
      "context": "Combining different control preprocessors as RGB data opens room for creativity",
      "from": "chrisd0073"
    },
    {
      "tip": "Add noise or grain to prevent gradient banding",
      "context": "Old trick to make gradients not have banding artifacts before compression",
      "from": "Nathan Shipley"
    },
    {
      "tip": "Use Skyreels v2 with shift over 10 for better movement",
      "context": "Otherwise no movement in generated videos",
      "from": "Colin"
    },
    {
      "tip": "Only overlap by 4 frames when chaining DF videos",
      "context": "For video extension workflows to minimize redundancy",
      "from": "MilesCorban"
    },
    {
      "tip": "Include color correction for extended videos",
      "context": "Needed for i2v workflows or videos get progressively brighter",
      "from": "MilesCorban"
    },
    {
      "tip": "Use normal maps with depth for better control",
      "context": "Additional information in normal maps produces better results than depth alone",
      "from": "David Snow"
    },
    {
      "tip": "Set frame load cap to anything divisible by 4 plus 1 frame",
      "context": "For proper frame handling in video workflows",
      "from": "Flipping Sigmas"
    },
    {
      "tip": "Use context node for videos longer than 81 frames",
      "context": "When generating long sequences to avoid looping artifacts",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Train character LoRAs for consistency",
      "context": "Use 10 images and 2-3 clips with diffusion pipe to solve character consistency issues",
      "from": "A.I.Warper"
    },
    {
      "tip": "Add start frame twice for better consistency",
      "context": "When using VACE, add start frame twice at beginning with no mask for morphing control",
      "from": "TimHannan"
    },
    {
      "tip": "Increase resolution to reduce noise in VACE output",
      "context": "When getting noisy results from VACE processing",
      "from": "N0NSens"
    },
    {
      "tip": "Process order: upscale then interpolate",
      "context": "For video post-processing workflow",
      "from": "TimHannan"
    },
    {
      "tip": "Turn off TeaCache when using LoRAs",
      "context": "For better LoRA compatibility with Wan 14B I2V",
      "from": "hablaba"
    },
    {
      "tip": "Use musubi-tuner for Wan LoRA training",
      "context": "For training LoRAs on Wan models",
      "from": "MisterMango"
    },
    {
      "tip": "Use Redux + ACE++ Fill + InstantID SDXL for consistent character workflows",
      "context": "For start/end frame generation",
      "from": "Draken"
    },
    {
      "tip": "Put LoRA trigger words at beginning of prompt",
      "context": "For Wan LoRAs",
      "from": "Cubey"
    },
    {
      "tip": "Remove first 8 frames from CausVid output",
      "context": "To eliminate artifacts at video start",
      "from": "V\u00e9role"
    },
    {
      "tip": "Use format 'Wan' to auto-adjust frame counts",
      "context": "When getting tensor size errors",
      "from": "Valle"
    },
    {
      "tip": "Use custom schedule with CausVid",
      "context": "CausVid had custom schedule available in the node",
      "from": "Kijai"
    },
    {
      "tip": "Use flex attention with CausVid for proper results",
      "context": "Eliminates start weirdness/latent flicker",
      "from": "Kijai"
    },
    {
      "tip": "MoviiGen likely wants 720p minimum resolution",
      "context": "Finetuned on high resolution content, lower resolutions may cause strangeness",
      "from": "Draken"
    },
    {
      "tip": "Extract full finetunes as LoRAs if needed",
      "context": "We can extract MoviiGen as a lora while keeping benefits of full finetune",
      "from": "yi"
    },
    {
      "tip": "VACE 14B max blocks to swap is 8",
      "context": "When configuring block swapping for memory management",
      "from": "DawnII"
    },
    {
      "tip": "Swapping even one block offloads VACE results and saves VRAM",
      "context": "For memory optimization with VACE wrapper",
      "from": "Kijai"
    },
    {
      "tip": "Separate preprocessor inputs rather than blend for better lip sync results",
      "context": "When using multiple control inputs",
      "from": "DawnII"
    },
    {
      "tip": "1.3B is good enough for controlled generation",
      "context": "When deciding between model sizes",
      "from": "Juampab12"
    },
    {
      "tip": "For VACE combined controls, use 2 VACE embeds nodes with prev_embeds input",
      "context": "When using multiple control inputs",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use bounding box with no feathering for better subject control",
      "context": "When trying to control subject movement along paths",
      "from": "Kijai"
    },
    {
      "tip": "Set CFG to 2 and use 6 steps for faster generation",
      "context": "For quick previews and seed hunting with base 1.3B model",
      "from": "David Snow"
    },
    {
      "tip": "Pose can be stronger but depth is better at lower strength",
      "context": "When setting VACE embed strength values",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Remove 'enhance-a-video' for lip sync as it adds unwanted details",
      "context": "When working with FantasyTalking lip sync",
      "from": "ingi // SYSTMS"
    },
    {
      "tip": "Use multiple VACE encoders for blending different control inputs",
      "context": "When you need to blend depth and canny without artifacts",
      "from": "Kijai"
    },
    {
      "tip": "Use gray or black images for additional VACE encoders when using multiple",
      "context": "When using multiple VACE encoders with reference images",
      "from": "Kijai"
    },
    {
      "tip": "e5m2 quantization recommended for 3090s and 30XX series",
      "context": "When choosing quantization for older GPUs",
      "from": "happy.j"
    },
    {
      "tip": "Start CausVid LoRA strength at 0.5, adjust based on needs",
      "context": "Lower strength requires more steps, higher strength can cause artifacts",
      "from": "Kijai"
    },
    {
      "tip": "Skip VACE blocks when using CausVid LoRA",
      "context": "Gives VACE breathing room and reduces conflicts",
      "from": "Kijai"
    },
    {
      "tip": "Use multiple reference images by composing them in single frame",
      "context": "Better than batch feeding which concatenates horizontally",
      "from": "Kijai"
    },
    {
      "tip": "Use context windows instead of long single generations",
      "context": "More efficient and reference works across context windows",
      "from": "Kijai"
    },
    {
      "tip": "Select fp8 quantization even for fp8 models",
      "context": "Prevents weight casting to base precision",
      "from": "Kijai"
    },
    {
      "tip": "Reduce LoRA strength to allow more motion transfer",
      "context": "When using CausVid LoRA with VACE for better movement",
      "from": "Kijai"
    },
    {
      "tip": "Use cfg 1.0 and 2-4 steps with CausVid LoRA",
      "context": "CausVid is distilled for both cfg and steps",
      "from": "Kijai"
    },
    {
      "tip": "Disable teacache, enhance-a-video, slg, and zero_star with CausVid",
      "context": "These don't work at cfg 1.0",
      "from": "Kijai"
    },
    {
      "tip": "Keep shift around 8 with CausVid LoRA",
      "context": "Don't move to shift 1 like with other speed optimizations",
      "from": "Kijai"
    },
    {
      "tip": "Turn off WanVideo Experimental Args for 1.3B model",
      "context": "Prevents issues with 1.3B model generation",
      "from": "Mngbg"
    },
    {
      "tip": "Use reference and start frame together for best results",
      "context": "When using the same image as both reference and start frame",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Train LoRAs for both image and video generation",
      "context": "For maintaining specific styles that aren't realism",
      "from": "Piblarg"
    },
    {
      "tip": "Use skip layer guidance for refining hands",
      "context": "Hands are Wan's weakness, but skip layer guidance can help refine them",
      "from": "Piblarg"
    },
    {
      "tip": "Filter out finger bones when using pose control",
      "context": "To avoid hand/paw confusion in character generation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Have only around 1GB VRAM free for optimal performance",
      "context": "When using block swapping with Wan models",
      "from": "DawnII"
    },
    {
      "tip": "Use offload device in the loader for memory management",
      "context": "When running into OOM issues with limited VRAM",
      "from": "Stad"
    },
    {
      "tip": "For CausVid LoRA, always use CFG 1.0 and disable all enhancements",
      "context": "TeaCache, SLG, enhance video should all be off",
      "from": "Kijai"
    },
    {
      "tip": "Use 2-4 steps with CausVid, nothing else",
      "context": "Keep it minimal for best results",
      "from": "Kijai"
    },
    {
      "tip": "1.3B cannot handle images with small heads",
      "context": "Use half body or medium shots instead of wide shots",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "tip": "VACE doesn't like combined controls",
      "context": "Combining pose and depth is unreliable, use one at a time",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use Color Match node for VACE results",
      "context": "Results come out darker than reference, color matching helps",
      "from": "N0NSens"
    },
    {
      "tip": "For multiple image references, batch node auto-stacks horizontally",
      "context": "Can manually pad/assemble for more precise control",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use closeup shots for better detail with 1.3B model",
      "context": "1.3B isn't great with details, so zoom in on subjects for better results",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Higher CFG with no prompt for better preprocessor following",
      "context": "In Fun Control, higher cfg with no prompt makes video follow preprocessor movement better",
      "from": "chrisd0073"
    },
    {
      "tip": "Fun Control is better than VACE for facial expressions",
      "context": "VACE is unstable for facial expression and hard to get right if person is completely different, Fun Control is way better but slower",
      "from": "chrisd0073"
    },
    {
      "tip": "Use symlinks or model path configuration to share models between ComfyUI installations",
      "context": "When having separate installations for different workflows",
      "from": "The Punisher"
    },
    {
      "tip": "Remove background and pad between subjects for multi-person VACE",
      "context": "When trying to use VACE with multiple people, though subject bleed may still occur",
      "from": "DawnII"
    },
    {
      "tip": "Use GetLatentRangeFromBatch to avoid decode/encode to pixel",
      "context": "For video stitching workflows to improve efficiency",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Lower preprocessing resolution for speed over quality",
      "context": "When pose preprocessing takes too long",
      "from": "DawnII"
    },
    {
      "tip": "Adjust LoRA strength significantly affects results",
      "context": "When using CausVid LoRA",
      "from": "DawnII"
    },
    {
      "tip": "Chain VACE embeds for multiple controls",
      "context": "Put depth on one embed, frames for inpainting on another, reference can feed to both",
      "from": "DawnII"
    },
    {
      "tip": "Use reward LoRAs to enhance CausVid generations",
      "context": "Fixes blurry and oversaturated look",
      "from": "yi"
    },
    {
      "tip": "Lower LoRA strength requires higher step count",
      "context": "When using CausVid LoRA",
      "from": "JohnDopamine"
    },
    {
      "tip": "Don't make VACE reference input too large",
      "context": "No point in oversized reference frames",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Reduce CausVid LoRA strength to 0.3, use 12 steps with UniPC scheduler",
      "context": "For better quality with less speed impact than 0.5 strength with 4 steps",
      "from": "Ada"
    },
    {
      "tip": "Use KJ set/get nodes instead of Anything Anywhere",
      "context": "Much less prone to breaking and more reliable",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Try lower version numbers for DG models",
      "context": "Higher version numbers tend to produce flashes and artifacts - too strong",
      "from": "David Snow"
    },
    {
      "tip": "Use first frame of video with FLUX ControlNet to create re-style as reference image",
      "context": "For better stylization in VACE workflows",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Understand workflows before running them",
      "context": "Don't just copy - try to understand why each component is there, especially when stacking multiple LoRAs",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Scale up reference image for better detail extraction",
      "context": "When using VACE reference images",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Crop reference image to head only unless you want t-shirt details",
      "context": "For better face focus in VACE",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Test with and without prompts for different results",
      "context": "Prompts make a big difference in VACE output quality",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use DepthAnything (DA) instead of DepthCrafter for lower VRAM usage",
      "context": "When depth preprocessing is needed but VRAM is limited",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Try skipping every 5th block when using CausVid with 14B VACE",
      "context": "For more efficient processing when combining CausVid and 14B VACE",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use cfg 1.0 and lora strength 0.3-0.6 with CausVid - lower values need more steps",
      "context": "CausVid allows 4-8 step inference",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Disable TeaCache and step-based optimizations for low step counts",
      "context": "When using 4-10 steps with CausVid",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use separate VACE embeds for multiple controls instead of blending",
      "context": "When combining depth and pose controls",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Reduce strength when using multiple VACE embeds on same timesteps",
      "context": "Prevents burning/overcooking with multiple controls",
      "from": "Kijai"
    },
    {
      "tip": "Use neutral grey image for secondary VACE embed reference",
      "context": "When combining multiple controls to avoid overcooking",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Colors matter for VACE controls - desaturate normal maps, grey areas get inpainted",
      "context": "For masks: white=change, black=ignore. Pose doesn't need color adjustment",
      "from": "DawnII"
    },
    {
      "tip": "CausVid works well for v2v with denoise 0.2",
      "context": "Video-to-video workflow with LoRAs",
      "from": "hicho"
    },
    {
      "tip": "Use end_percentage instead of strength for VACE control",
      "context": "Stops the control at certain point during process, better than just stopping at certain step",
      "from": "Draken"
    },
    {
      "tip": "Create reference image from first frame of input video",
      "context": "Helps with consistency when using VACE controls, optional but recommended",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use decay curve over time for control",
      "context": "Much better than just stopping control at certain step, gives more natural progression",
      "from": "Nekodificador"
    },
    {
      "tip": "Set depth strength to 0.5 when combining with pose",
      "context": "When depth is too strong it distorts the source image, combining at 0.5 with pose gives better balance",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Turn off Fresca enhancement when using 14B with CausVid LoRA",
      "context": "Other enhancements are fine but Fresca will screw up output",
      "from": "David Snow"
    },
    {
      "tip": "Use contrast node set to 0.5 to lighten baked control lines",
      "context": "When lines control bakes lines into output",
      "from": "David Snow"
    },
    {
      "tip": "For multi-reference setup: pose at strength 1 in first encoder, depth at 0.5 in second, no ref in second",
      "context": "When using multiple VACE encoders for better control",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use grow mask blur to match character body size",
      "context": "When mapping characters and needing size consistency",
      "from": "yo9o"
    },
    {
      "tip": "Simple backgrounds help reduce artifacts in long generations",
      "context": "For extended video generation and context handling",
      "from": "Draken"
    },
    {
      "tip": "Reduce CausVid LoRA strength to at least half when using with VACE",
      "context": "Memory management and better results",
      "from": "Kijai"
    },
    {
      "tip": "Use specific blocks application with VACE for better results",
      "context": "When using control inputs",
      "from": "Kijai"
    },
    {
      "tip": "Remove background from reference images for better VACE results",
      "context": "When VACE is using background instead of subject",
      "from": "ingi // SYSTMS"
    },
    {
      "tip": "Match reference image closely to first frame when using normal maps",
      "context": "At high strength (0.9), otherwise drop to 0.5 strength",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Prompt 'the animated character is talking' or 'the Disney character' for better character recognition",
      "context": "When model doesn't recognize specific characters like Shrek",
      "from": "Piblarg"
    },
    {
      "tip": "Use photo terminology instead of 'photorealistic' or 'realistic'",
      "context": "For better photorealistic outputs, use terms like '4k, uhd, 8k, high definition, detailed, raw photo, photoshoot, studio session, IMAX, National Geographic'",
      "from": "Thom293"
    },
    {
      "tip": "Start with 0.5 strength for NormalCrafter when face differs significantly",
      "context": "When the reference face is very different from the control video face, start at 0.5 and work up until it gets wonky",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Remove backgrounds or use black wall for better VACE results",
      "context": "Having a clean background (like a black wall) improves VACE performance for character animation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Lower depth effect strength to avoid edge limitations",
      "context": "When using double control (depth+pose), reduce depth strength and start/end percentages so depth doesn't limit the edges",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "tip": "Prompt for elements not in reference image",
      "context": "For VACE, need to prompt for items like skirts or accessories that aren't visible in the reference image, or it might generate different clothing",
      "from": "Kijai"
    },
    {
      "tip": "Use reference images with characters on white background for better character consistency",
      "context": "When creating multiple story scenes with same character",
      "from": "Piblarg"
    },
    {
      "tip": "Disconnect control inputs when not needed for reference-only generation",
      "context": "When using VACE with just reference images",
      "from": "Kijai"
    },
    {
      "tip": "Use DF (DeepFloyd) for better consistency through context windows",
      "context": "When extending videos beyond base frame limits",
      "from": "DawnII"
    },
    {
      "tip": "Minimum 5 frames needed for VACE with reference to work properly",
      "context": "When testing ideas with minimal frame counts",
      "from": "Kijai"
    },
    {
      "tip": "Use CausVid strength between 0.3 and 0.6",
      "context": "Lower requires more steps, 0.5 and 4 steps is good starting point",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "CFG 1.0 when using CausVid",
      "context": "CFG > 1.0 isn't recommended unless using SLG which doubles generation time",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use last N frames for video extension",
      "context": "Take the end frames of video you want to extend (like last 20) and put them in start images",
      "from": "Piblarg"
    },
    {
      "tip": "Reduce mask strength to avoid first frame jump",
      "context": "When using reference image, first frame shows the reference - need to stylize first frame or adjust approach",
      "from": "\u25b2"
    },
    {
      "tip": "Turn off Sage and TeaCache for VACE/CausVid",
      "context": "Recommended settings, though some report TeaCache working at 8 steps",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "tip": "Use aligned reference frames",
      "context": "When combining control video + reference frame + LoRA, align the reference frame as closely as possible to avoid it being ignored",
      "from": "Piblarg"
    },
    {
      "tip": "Keep only face over whole body",
      "context": "Found much better results from keeping just the face rather than whole body, and it picks up color cues from tiny remaining outline",
      "from": "AJO"
    },
    {
      "tip": "Use ChatGPT for first frame creation",
      "context": "ChatGPT is really good for first frame generation - gets close enough but isn't super locked in like classic controlnets",
      "from": "Draken"
    },
    {
      "tip": "Blend black layer on depth maps",
      "context": "Depth maps cause output to follow shapes too much, but blending in black layer on top makes it less visible, works like a strength control",
      "from": "\u25b2"
    },
    {
      "tip": "MultiGPU doesn't save VRAM for inference",
      "context": "Everything is always offloaded for inference anyway, only benefit is saving offload time. Makes sense for T5 but less so for VAE",
      "from": "Kijai"
    },
    {
      "tip": "Use multiple reference images as single combined image rather than chaining",
      "context": "When using multiple VACE encoders with different references",
      "from": "DawnII"
    },
    {
      "tip": "Use 1.3B model and/or CausVid LoRA for long context generation",
      "context": "When generating videos longer than 81 frames to manage slowness",
      "from": "Kijai"
    },
    {
      "tip": "Find 16:9 aspect ratio resolutions that are divisible by 16",
      "context": "When changing from default 512x512 resolution",
      "from": "A.I.Warper"
    },
    {
      "tip": "Blur depth mask for better results",
      "context": "When using depth control with VACE",
      "from": "Piblarg"
    },
    {
      "tip": "Use face masking to isolate facial details from input video",
      "context": "When trying to preserve facial movement while controlling other aspects",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use high quality for first video when doing 2-stage character replacement",
      "context": "When creating initial video to later replace characters, use high quality (14B) since second pass only replaces/refines character and leaves rest as-is",
      "from": "MilesCorban"
    },
    {
      "tip": "Don't use FlowMatch sampler with Causvid LoRA",
      "context": "UniPC is recommended scheduler instead of the FlowMatch sampler from Causvid repo",
      "from": "JohnDopamine"
    },
    {
      "tip": "Be careful with values above 1.0",
      "context": "Going over strength values of 1.0 is not always safe, though it can work with Causvid",
      "from": "Guey.KhalaMari"
    },
    {
      "tip": "Break down complex tasks into stages",
      "context": "Instead of trying to do everything in one generation, create strong still images first then use fewer controls for motion",
      "from": "Guey.KhalaMari"
    },
    {
      "tip": "Use black image as reference for depth-only control",
      "context": "When using only depth video control, put black image as reference for the depth node, though this may darken output",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Match reference frame to first pose frame",
      "context": "Have starting frame/ref frame as close to the pose first frame to prevent morphing the reference to work with control",
      "from": "Draken"
    },
    {
      "tip": "Use Sobol or Latin Hypercube sampling for parameter testing",
      "context": "These methods fill parameter space more evenly than random sampling, avoiding clumping issues",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "tip": "Use batching method followed by cleanup pass for long sequences",
      "context": "When generating sequences longer than 81 frames to maintain character consistency",
      "from": "A.I.Warper"
    },
    {
      "tip": "Start with high resolution base image for multi-angle generation",
      "context": "When extracting frames from animations for different viewpoints",
      "from": "David Snow"
    },
    {
      "tip": "Use gray (RGB 127 or #808080) for masked areas",
      "context": "When masking parts of input video for VACE processing",
      "from": "DawnII"
    },
    {
      "tip": "Use input video strength of at least 0.6 to maintain mouth movements",
      "context": "When using original video input with face masking",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Blend face detection methods at 0.5 strength as starting point",
      "context": "When combining DWPose and MediaPipe face detection",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "tip": "Use overlapping frames for seamless video extension",
      "context": "When extending videos with VACE, use last 6-10 frames as input for next generation",
      "from": "zelgo_"
    },
    {
      "tip": "Remove overlapping frames from first clip instead of second",
      "context": "First clip frames get changed slightly during generation, so remove those instead of the newly generated ones",
      "from": "seitanism"
    },
    {
      "tip": "Use different seeds for each extension stage",
      "context": "Same seeds for upscaling usually results in higher micro contrast and burnt look",
      "from": "zelgo_"
    },
    {
      "tip": "Increase step count when extending clips with CausVid",
      "context": "12 steps works well with dynamic movement for extensions",
      "from": "zelgo_"
    },
    {
      "tip": "Use higher resolution rendering for better small face quality",
      "context": "Render at 1280x720 on A6000 to help with small faces in wide shots",
      "from": "traxxas25"
    },
    {
      "tip": "Construct reference image manually for optimal VACE results",
      "context": "Don't rely on automatic concatenation, build your reference image yourself to avoid wasting space",
      "from": "Kijai"
    },
    {
      "tip": "Prompt for background motion with abstract reference images",
      "context": "When using abstract reference images, explicitly prompt for background movement to help VACE understand the scene",
      "from": "David Snow"
    },
    {
      "tip": "Use 5+ frames minimum with VACE reference images",
      "context": "Single frame sampling doesn't work well with reference images, duplicate pose image 5 times for better results",
      "from": "Kijai"
    },
    {
      "tip": "Match input video when using normal maps for facial movement",
      "context": "Normal maps work best for close-up shots and good facial movement but input video must match or results get wonky",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use simple prompts with pose interpolation",
      "context": "For better pose transition results",
      "from": "toyxyz"
    },
    {
      "tip": "Raise Confidence Threshold when body parts are obscured",
      "context": "For pose detection accuracy",
      "from": "toyxyz"
    },
    {
      "tip": "Use inspyrenet rembg for immaculate masks",
      "context": "Easiest way to get great masks for video processing",
      "from": "David Snow"
    },
    {
      "tip": "Use set latent noise mask for native workflows",
      "context": "When doing inpainting with native ComfyUI nodes",
      "from": "David Snow"
    },
    {
      "tip": "Use Euler sampler for most reliable results",
      "context": "UniPC gives subtle 'dancing haze'",
      "from": "David Snow"
    },
    {
      "tip": "CFG 1.0 is important for speed",
      "context": "Generations take significantly longer if CFG is raised",
      "from": "David Snow"
    },
    {
      "tip": "Enable quant unless you want your PC to die",
      "context": "When using 14B model",
      "from": "David Snow"
    },
    {
      "tip": "Use DG Boost models with Vace 1.3B",
      "context": "Base 1.3B model wasn't as good, DG models are better for inpainting workflows",
      "from": "David Snow"
    },
    {
      "tip": "Use face segmentation node for single faces",
      "context": "Gives accurate shape instead of bounding box for better face replacement",
      "from": "David Snow"
    },
    {
      "tip": "Use euler sampler for VACE",
      "context": "Most reliable sampler choice, though best is subjective",
      "from": "David Snow"
    },
    {
      "tip": "Use 1024x576 resolution to save time",
      "context": "Alternative to 1280x544 ultrawide when processing speed is important",
      "from": "David Snow"
    },
    {
      "tip": "Make block swap dynamic based on pixel counts",
      "context": "Use conditional nodes with thresholds for different block swap amounts",
      "from": "DevouredBeef"
    },
    {
      "tip": "For VACE inpainting, inpaint every frame in starting video or starting image for I2V, then feed output and mask to VACE encoder",
      "context": "When doing subject removal or area inpainting",
      "from": "Stad"
    },
    {
      "tip": "Use DG Boost models instead of base 1.3B model for better inpainting performance",
      "context": "When using VACE with 1.3B models",
      "from": "David Snow"
    },
    {
      "tip": "Reference images are notorious for causing flashes and artifacts - first 10 frames often unusable",
      "context": "When using reference images in VACE workflows",
      "from": "David Snow"
    },
    {
      "tip": "You can balance multiple LoRAs by adjusting VACE strength when they overpower style LoRAs",
      "context": "When using multiple LoRAs with VACE",
      "from": "David Snow"
    },
    {
      "tip": "For 4070 12GB with 32GB RAM, use VACE 1.3B with DG Boost models or continue with Q5 GGUF if that's only option",
      "context": "Hardware-specific model recommendations",
      "from": "David Snow"
    },
    {
      "tip": "Use separate encoders for each control net",
      "context": "When using multiple control inputs with VACE",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use built-in ComfyUI templates for learning",
      "context": "Select workflows -> browse templates -> video section for VACE workflows",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "tip": "Be selective about reference images with CausVid",
      "context": "It takes too much lighting from ref image, can't control lighting override",
      "from": "David Snow"
    },
    {
      "tip": "Avoid shiny disco balls until you get basics down",
      "context": "When learning new workflows, focus on fundamentals first",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "tip": "Training LoRA on high quality images makes output sharper",
      "context": "For improving Wan output quality",
      "from": "Thom293"
    },
    {
      "tip": "Use depth at 0.5 strength for similar camera movement",
      "context": "When trying to copy motion for background camera movement",
      "from": "Valle"
    },
    {
      "tip": "For longer videos, high frame count makes process really slow",
      "context": "When generating videos longer than 81 frames",
      "from": "Valle"
    },
    {
      "tip": "Remove depth and normals from multi-input VACE",
      "context": "Depth and normals constrain style too much, makes it too similar to source footage",
      "from": "David Snow"
    },
    {
      "tip": "Try normal pass for very hard movement",
      "context": "Can help with complex motion in VACE",
      "from": "N0NSens"
    },
    {
      "tip": "Use HED controlnet with lowered strength (0.5) for subtle control",
      "context": "When you want control but don't want it to overpower the generation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use pasteback with inpainting to avoid VAE compression",
      "context": "When doing inpainting workflows",
      "from": "A.I.Warper"
    },
    {
      "tip": "Reduce VACE encode strength and add detailed prompt if you want to change it a lot",
      "context": "When making significant changes with VACE",
      "from": "VK (5080 128gb)"
    },
    {
      "tip": "Reduce Vace_end_percent for better inpainting blending",
      "context": "Top is 1 and bottom is 0.80 for comparison",
      "from": "VK (5080 128gb)"
    },
    {
      "tip": "Lower CausVid LoRA to 3.5 range and adjust shift value in sampler",
      "context": "Too strong CausVid negates VACE controls",
      "from": "Guey.KhalaMari"
    },
    {
      "tip": "Switch off freenoise on context options",
      "context": "When using sliding context for longer videos",
      "from": "chrisd0073"
    },
    {
      "tip": "Use CausVid at 0.5 strength to avoid artifacts",
      "context": "When using CausVid with Phantom to prevent quality issues",
      "from": "Kijai"
    },
    {
      "tip": "Use CFG ending at 0.1 with CausVid",
      "context": "For better results with CausVid acceleration",
      "from": "Kijai"
    },
    {
      "tip": "Set TeaCache threshold to 0.2",
      "context": "For optimal performance balance",
      "from": "Kijai"
    },
    {
      "tip": "Use two VACE encode nodes for different strengths",
      "context": "One with depth and one with reference to play with strength values",
      "from": "Kijai"
    },
    {
      "tip": "Set Phantom CFG high (5) and sampler CFG to 1",
      "context": "For decent results when using caus+vace+phantom together",
      "from": "DawnII"
    },
    {
      "tip": "Don't mix pose and depth controlnets",
      "context": "Generally not good idea, depth and normals can be mixed",
      "from": "David Snow"
    },
    {
      "tip": "Prompt for specific colors to help with VACE color shifts",
      "context": "Prompting for colours actually helps quite a bit with color shifts",
      "from": "pom"
    },
    {
      "tip": "Don't pass ref image to VACE when combining with Phantom embeds",
      "context": "When using VACE encode with Phantom embeds",
      "from": "aikitoria"
    },
    {
      "tip": "Use encoder strength 0.4-0.5 for original video input",
      "context": "When using original video as control input in multi-encoder setup",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Keep source video at original frame rate",
      "context": "When doing v2v or using source video for controlnets, don't convert to 16fps",
      "from": "David Snow"
    },
    {
      "tip": "Use AccVid at 1.5 and CausVid at 0.5 for best results",
      "context": "When combining both LoRAs for speed and quality",
      "from": "David Snow"
    },
    {
      "tip": "Use cfg scheduling instead of fixed cfg",
      "context": "CFG doesn't do much after halfway through steps, so schedule it for best results",
      "from": "Kijai"
    },
    {
      "tip": "Add 16 more frames then cut first 16 to avoid flash",
      "context": "When flash appears in first second of output",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Use only 1 image with black image in other slot to avoid flash",
      "context": "When using Phantom with multiple image inputs",
      "from": "Thom293"
    },
    {
      "tip": "Use simple/short prompts to avoid flash issues",
      "context": "Long prompts can cause flash problems, especially with inpainting",
      "from": "pom"
    },
    {
      "tip": "Be precise with prompts for context windows",
      "context": "Don't give model too much freedom to change between windows, describe clothing, aspects, etc.",
      "from": "Kijai"
    },
    {
      "tip": "Use white background for reference images",
      "context": "Phantom requires white background for source images, especially if removing background",
      "from": "Kijai"
    },
    {
      "tip": "VACE reference image helps with consistency",
      "context": "Using VACE reference image helps a lot with maintaining consistency across context windows",
      "from": "Kijai"
    },
    {
      "tip": "Add 'mute' to positive prompt to prevent talking",
      "context": "Alternative to negative prompting for reducing unwanted speech",
      "from": "Ro"
    },
    {
      "tip": "Multiple LoRAs work well together",
      "context": "Combination of causvid 0.4 + accvid 1.5 + moviigen 0.4 + detailz 0.9 works well",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Use Shift+Ctrl+V to paste nodes while keeping connections",
      "context": "When copying groups of nodes in ComfyUI",
      "from": "David Snow"
    },
    {
      "tip": "Prompt 'walking towards the camera' for better inpainting results",
      "context": "When using phantom with VACE inpainting",
      "from": "David Snow"
    },
    {
      "tip": "Use separate VACE encodes to control each input strength individually",
      "context": "When combining multiple controls like ref image, depth, and lineart",
      "from": "hau"
    },
    {
      "tip": "Add 'face visible' to prompt when face isn't appearing",
      "context": "When using VACE reference images",
      "from": "hau"
    },
    {
      "tip": "Consider using original video input at low strength for better control",
      "context": "Helps with mouth movement and character positioning",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Use prompting for better Uni3C results",
      "context": "Better to use rough camera language to describe movements when using Uni3C",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "tip": "Lower shift value for better reference preservation",
      "context": "When using 1.3b phantom, lowering the shift preserves details from reference images better",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Use reactor face swap before video combine",
      "context": "To maintain face ID when using phantom + vace inpainting",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Smaller face = lower chances for face movements",
      "context": "Face size affects the ability to capture facial movements in generation",
      "from": "DeZoomer"
    },
    {
      "tip": "Blur Wan outputs before using Starlight upscaler",
      "context": "Starlight is trained on blurred images, so adding small blur removes blocks and improves temperature",
      "from": "chrisd0073"
    },
    {
      "tip": "Reduce block swap amount until 95% VRAM usage",
      "context": "For optimal performance when using block swapping",
      "from": "Kijai"
    },
    {
      "tip": "Lower denoising and reduce reference strength",
      "context": "Decreasing lora strength helps preserve original face movements but loses likeness, adding ref image with ~0.2 strength helps recover it",
      "from": "DeZoomer"
    },
    {
      "tip": "Use live preview to detect Phantom failures early",
      "context": "Can tell after 5-10 steps if generation will fail, restart with new seed",
      "from": "aikitoria"
    },
    {
      "tip": "Prompt Phantom with single blurb describing final image, use keywords relating to reference images",
      "context": "Don't describe reference images directly, describe what you want to keep from them",
      "from": "aikitoria"
    },
    {
      "tip": "Use simple prompts with Phantom and VACE",
      "context": "Extensive descriptions take away focus from reference image identity",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Send multiple reference images frame by frame rather than combined",
      "context": "Better results when feeding images separately to Phantom",
      "from": "hicho"
    },
    {
      "tip": "Use DocVQA mode in Florence for outfit descriptions",
      "context": "Gives simple one-sentence color descriptions perfect for Phantom outfit reference",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Use low CFG for few steps without losing much speed",
      "context": "When using distilled models to get some benefit from CFG",
      "from": "Kijai"
    },
    {
      "tip": "Describe reference images in prompts for character consistency",
      "context": "Model wants description of each ref image every time",
      "from": "AJO"
    },
    {
      "tip": "Use CausVid at 0.5 strength for better results",
      "context": "Found good balance with causvid 0.5 + accvid 0.5",
      "from": "Ada"
    },
    {
      "tip": "Expand mask area to avoid halos in VACE",
      "context": "When using VACE inpainting to prevent artifacts",
      "from": "MilesCorban"
    },
    {
      "tip": "Use CFG for first 2-4 steps then disable",
      "context": "With 6-8 step generations using CausVid",
      "from": "Kijai"
    },
    {
      "tip": "CausVid loras can be pushed quite far in strength",
      "context": "causvid 0.5 / accvideo 1.5 or causvid 1.0 / accvideo 2.0 work well",
      "from": "David Snow"
    },
    {
      "tip": "Use CFG scheduling with early high CFG only",
      "context": "CFG 5 for first step/few steps, then lower to 1 for distilled models",
      "from": "Johnjohn7855"
    },
    {
      "tip": "For distilled models, CFG 5 may be too much",
      "context": "If it burns/over-saturates, lower the CFG",
      "from": "Kijai"
    },
    {
      "tip": "Add unbatch video node between VAE and video combiner when looping",
      "context": "To get separate videos out instead of one combined video",
      "from": "AJO"
    },
    {
      "tip": "Stationary anchor points are important in ATI trajectory control",
      "context": "For proper motion control with trajectory system",
      "from": "Kijai"
    },
    {
      "tip": "Use close-up reference images for better results",
      "context": "Up close reference images work better than distant shots",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "HD LoRAs really amp up the models",
      "context": "Using high-definition LoRAs provides significant quality improvements",
      "from": "Thom293"
    },
    {
      "tip": "Merging is the way to greatness",
      "context": "Create entirely new models by merging multiple LoRAs, then add more LoRAs on top",
      "from": "Thom293"
    },
    {
      "tip": "Film-like outputs preserve dynamic range",
      "context": "Choose flatter, less saturated outputs for better post-processing flexibility",
      "from": "Ruairi Robinson"
    },
    {
      "tip": "Use film grain settings of 0.03 for all parameters",
      "context": "When fixing reactor jitter, don't go over 0.1 or effect is too much",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Use separate encoders for each control type",
      "context": "For VACE controls like depth and normal, allows individual strength adjustment",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Lower strength for lineart/canny controls",
      "context": "Otherwise the lines will appear in the outputs",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Use anchor points that don't move in ATI",
      "context": "To avoid only getting camera motion with single track",
      "from": "Kijai"
    },
    {
      "tip": "Shift click to add more points in spline editor",
      "context": "For creating complex motion paths in ATI",
      "from": "Kijai"
    },
    {
      "tip": "Few trajectory points are sufficient for good results",
      "context": "When using trajectory-based controls, similar to TORA",
      "from": "Kijai"
    },
    {
      "tip": "Eye correction can be done with VACE but requires advanced techniques",
      "context": "For precise facial animation control",
      "from": "chrisd0073"
    },
    {
      "tip": "Easy approach leaves you at mercy of seed, advanced approach gives full control but takes longer",
      "context": "When doing facial animation work",
      "from": "chrisd0073"
    },
    {
      "tip": "VACE works best with base AI case scenarios - people in controlled environments rather than extreme motion",
      "context": "For optimal face tracking and animation results",
      "from": "chrisd0073"
    }
  ],
  "news": [
    {
      "update": "Fun 1.1 models released with reference image support",
      "details": "New feature allowing reference image input, can be used with start image too",
      "from": "Kijai"
    },
    {
      "update": "CausVid model available - 4-step distilled version",
      "details": "Claims to distill 50-step diffusion into 4-step generator with 9.4 FPS streaming",
      "from": "David Snow"
    },
    {
      "update": "New Framepack model released",
      "details": "https://huggingface.co/lllyasviel/FramePack_F1_I2V_HY_20250503/tree/main",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "Judy Hopps LoRA updated for Wan2.1 14B",
      "details": "Released on Civitai, was previously for different model",
      "from": "MisterMango"
    },
    {
      "update": "14B VACE model development in progress",
      "details": "Updates visible on dev branch, expected to be game changer",
      "from": "JohnDopamine"
    },
    {
      "update": "LTXV 13B released",
      "details": "New model release discussed in LTXV channel",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "update": "CausVid model available for Wan 2.1",
      "details": "Generate videos in seconds, but no UI implementations yet",
      "from": "yi"
    },
    {
      "update": "VACE 14B mentioned in code",
      "details": "Code referencing VACE 14B was added yesterday, indicating potential release",
      "from": "Kijai"
    },
    {
      "update": "Distilled faster Wan model exists",
      "details": "They have a faster distilled model mentioned in the paper but haven't released it yet",
      "from": "Kijai"
    },
    {
      "update": "New no_ffn parameter appeared",
      "details": "no_ffn parameter was added about a week ago",
      "from": "Dream Making"
    },
    {
      "update": "VACE 14B released",
      "details": "May 14 release announced on Wan2.1 dev branch, model links not working yet",
      "from": "JohnDopamine"
    },
    {
      "update": "CausVid I2V code added",
      "details": "I2V implementation added to repository 2 hours ago",
      "from": "Kijai"
    },
    {
      "update": "CausVid distilled models available",
      "details": "Step and CFG distilled 14B model working with 2-4 steps",
      "from": "yi"
    },
    {
      "update": "Skyreels V2 roadmap includes CFG and step distillation",
      "details": "Also AccVideo author has it on roadmap",
      "from": "yi"
    },
    {
      "update": "VACE 14B and 1.3B official release",
      "details": "Released at 22:00 Beijing time 14/5, both 1.3B and 14B variants available",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "MoviiGen 1.1 released",
      "details": "New model based on WAN 2.1, can do 1080p, appears to be FP32 format",
      "from": "Njb"
    },
    {
      "update": "New sliding window Wan kernel on GitHub",
      "details": "Available for Hopper architecture only",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "update": "VACE 14B released",
      "details": "14B VACE module now available, uses different architecture than 1.3B",
      "from": "community"
    },
    {
      "update": "HuggingFace gate removed for VACE",
      "details": "Models now freely downloadable",
      "from": "Kijai"
    },
    {
      "update": "New VACE 1.3B final version released",
      "details": "Improved version with different characteristics than preview",
      "from": "community"
    },
    {
      "update": "New wrapper update available",
      "details": "Updates needed for 14B VACE compatibility",
      "from": "Dream Making"
    },
    {
      "update": "Index-anisora v2 model released",
      "details": "Wan 14B finetune for anime/multi-style generation, compatible with VACE",
      "from": "A.I.Warper"
    },
    {
      "update": "MoviiGen 1.1 released",
      "details": "Available in Q3 quantization, works with standard Wan workflows",
      "from": "The Punisher"
    },
    {
      "update": "Official Comfy repackaged VACE 14B model",
      "details": "35GB model that doesn't suffer from two tail problem",
      "from": "comfy"
    },
    {
      "update": "14B VACE model weights have been released",
      "details": "Available on HuggingFace, multiple variants including FP8",
      "from": "Vincent_luo"
    },
    {
      "update": "CausVid LoRA extracted and released",
      "details": "Wan21_CausVid_14B_T2V_lora_rank32.safetensors available on Kijai's HuggingFace",
      "from": "Kijai"
    },
    {
      "update": "New depth processor added to VACE annotators",
      "details": "They may have switched from midas in training",
      "from": "DawnII"
    },
    {
      "update": "CausVid converted to LoRA format for compatibility",
      "details": "Allows combining with VACE and other LoRAs, though not used as originally intended",
      "from": "Kijai"
    },
    {
      "update": "I2V model referenced in code but not openly shared yet",
      "details": "Alibaba has I2V model according to code updates but not publicly available",
      "from": "Kijai"
    },
    {
      "update": "VACE 14B module released",
      "details": "Can now use 14B VACE as module instead of full model",
      "from": "multiple users"
    },
    {
      "update": "CausVid LoRA versions available for both 1.3B and 14B",
      "details": "Kijai extracted CausVid models as LoRAs for better compatibility with VACE",
      "from": "Kijai"
    },
    {
      "update": "Index-anisora model announced",
      "details": "New anime model based on Wan14B, V2 version, files appearing on HuggingFace",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "Marigold 1.1 models released",
      "details": "Includes intrinsics model, new depth and normal model, released yesterday",
      "from": "Kijai"
    },
    {
      "update": "CausVid LoRA extracted for both 1.3B and 14B models",
      "details": "Available on Kijai's HuggingFace repo",
      "from": "DawnII"
    },
    {
      "update": "Official 1.3B VACE full model released",
      "details": "6GB model available at Wan-AI/Wan2.1-VACE-1.3B",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "VACE 14B GGUF quantizations available",
      "details": "Q3_K_S and Q8_0 versions tested and working, Q4_K_S and Q5_K_S in progress",
      "from": "The Punisher"
    },
    {
      "update": "Wrapper nodes updated for VACE compatibility",
      "details": "Need to update wrapper nodes to use new VACE modules, released yesterday",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "T2V LoRAs for 14B model released",
      "details": "Collection of T2V LoRAs specifically for Wan 14B model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "VACE 14B GGUF models released",
      "details": "Q3_K_S, Q5_K_S, Q6 variants available for native ComfyUI use",
      "from": "The Punisher"
    },
    {
      "update": "Hunyuan Image expected to release",
      "details": "Speculation about release timing, open source status unclear",
      "from": "The Punisher"
    },
    {
      "update": "CausVid repo updated with warping_4step model",
      "details": "New autoregressive_checkpoint_warp_4step_cfg2 model released 7 hours ago on HuggingFace",
      "from": "JohnDopamine"
    },
    {
      "update": "MoviiGen training and inference code released",
      "details": "ZulutionAI released training/inference code on GitHub, was private until May 17",
      "from": "JohnDopamine"
    },
    {
      "update": "MoviiGen 1.1 Prompt Rewriter uploaded to HF",
      "details": "New model for prompt enhancement uploaded by MoviiGen developers",
      "from": "JohnDopamine"
    },
    {
      "update": "ComfyUI native VACE support released",
      "details": "Official ComfyUI documentation and workflows now available",
      "from": "jerms_ai_(4090_24g)"
    },
    {
      "update": "Better CausVid version will remain closed source",
      "details": "Original CausVid team keeping improved version proprietary, trained on 400K videos vs 15K in open version",
      "from": "Ada"
    },
    {
      "update": "ComfyUI widgets now have inputs by default",
      "details": "Latest ComfyUI version automatically adds inputs to widgets, eliminating need to manually convert",
      "from": "traxxas25"
    },
    {
      "update": "New captioning model from ByteDance",
      "details": "Seed1.5-VL described as best new captioning model",
      "from": "Ada"
    },
    {
      "update": "New 4-step CausVid checkpoint released for 1.3B model",
      "details": "4 steps with cfg 2, available at tianweiy/CausVid autoregressive_checkpoint_warp_4step_cfg2",
      "from": "hicho"
    },
    {
      "update": "Tiling support added for I2V high resolution generation",
      "details": "Can now generate 1280x1280 I2V after Kijai added tiling support",
      "from": "DiXiao"
    },
    {
      "update": "New Origami LoRA available for WAN",
      "details": "shauray/Origami_WanLora on HuggingFace",
      "from": "hicho"
    },
    {
      "update": "WAN VACE 1.3B no longer preview version",
      "details": "New model released last week with more contrast and better control embeds",
      "from": "Blink"
    },
    {
      "update": "Native VACE module support in development",
      "details": "The Punisher successfully created native ComfyUI support for VACE modules, though still needs GGUF and Sage attention fixes",
      "from": "The Punisher"
    },
    {
      "update": "Wan model separation added to Civitai",
      "details": "Models are now organized separately on the platform",
      "from": "DiXiao"
    },
    {
      "update": "MTVCrafter released - WAN-based model",
      "details": "New 21B parameter model based on WAN architecture for video generation",
      "from": "yi"
    },
    {
      "update": "TeaCache memory optimization fixed in wrapper",
      "details": "Fixed unnecessary 750MB VRAM usage in TeaCache by optimizing clone/device operations",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI torch compile fix available",
      "details": "PR available to fix loras and other features not working with torch compile node",
      "from": "Kosinkadink"
    },
    {
      "update": "Veo 3 released with dialogue capabilities and advanced prompt following",
      "details": "Google's new model can generate videos with realistic dialogue and complex scene understanding, priced at $249/month",
      "from": "Draken"
    },
    {
      "update": "VACE 14B significantly outperforms previous versions",
      "details": "Major improvement over 1.3B and HY Custom models",
      "from": "AJO"
    },
    {
      "update": "RealisDance-DiT weights released",
      "details": "Controllable dance video generation model weights now available on HuggingFace",
      "from": "yi"
    },
    {
      "update": "Updated torch.compile wrapper",
      "details": "Added proper wrapper for torch.compile into ComfyUI core. Now works with LoRAs without special patching. Can target only transformer blocks for faster recompilation",
      "from": "Kijai"
    },
    {
      "update": "Added VACE strength control over timesteps",
      "details": "Added list support for VACE strength values - can control strength over timesteps with list of floats same length as steps",
      "from": "Kijai"
    },
    {
      "update": "VACE is supported in native ComfyUI",
      "details": "Has been supported for about a week",
      "from": "Kosinkadink"
    },
    {
      "update": "Lora masking implemented into core ComfyUI",
      "details": "Not just for AnimateDiff but implemented into core ComfyUI",
      "from": "Kosinkadink"
    },
    {
      "update": "Custom node for VACE with native being developed",
      "details": "Don't know if it's been released yet",
      "from": "David Snow"
    },
    {
      "update": "TheDirector workflow available on Civitai",
      "details": "Latest working version of automated movie generation workflow that won $800 in MimicPC AI Influencer competition",
      "from": "AJO"
    },
    {
      "update": "Minecraft LoRA created for WAN 2.1 T2V 14B",
      "details": "Custom LoRA for Minecraft content generation, coming soon",
      "from": "MisterMango"
    },
    {
      "update": "Minecraft LoRA available for Wan 2.1 14B",
      "details": "New Minecraft LoRA released for the 14B model, can be found in showcase channel",
      "from": "MisterMango"
    },
    {
      "update": "MoviiGen with VACE now available as GGUF",
      "details": "MoviiGen 1.1 with VACE control uploaded as GGUF quantizations to HuggingFace",
      "from": "The Punisher"
    },
    {
      "update": "FantasyTalking model input added",
      "details": "New fantasytalking_model input added to WanVideo model loader for lip sync, limited to 3 seconds",
      "from": "seitanism"
    },
    {
      "update": "Jenga inference pipeline released",
      "details": "Novel pipeline combining dynamic attention carving with progressive resolution generation for speedup",
      "from": "mamad8"
    },
    {
      "update": "Native ComfyUI now supports VACE",
      "details": "VACE support added to native ComfyUI workflows a few days ago",
      "from": "zelgo_"
    },
    {
      "update": "Phantom 14B model release planned",
      "details": "Phantom has 14B release on its roadmap",
      "from": "Kijai"
    },
    {
      "update": "New UltraSharpV2 upscaler released",
      "details": "DAT version of upscaler available on HuggingFace",
      "from": "DawnII"
    },
    {
      "update": "ComfyUI pose interpolation repo updated",
      "details": "Modified version with timing control for pose appearance uploaded 12 minutes ago",
      "from": "toyxyz"
    },
    {
      "update": "ComfyUI has Wan templates in official UI",
      "details": "Templates available in workflow browser, need latest version",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "update": "SkyReels CausVid model released",
      "details": "Merged model combining SkyReels and CausVid available on HuggingFace",
      "from": "hicho"
    },
    {
      "update": "RTX 6000 Blackwell 96GB available for testing",
      "details": "2x faster than RTX4090, free trial available through simplepod.ai",
      "from": "Tytanick"
    },
    {
      "update": "New Remade LoRAs released",
      "details": "Wan2.1 14B 480p I2V LoRAs collection available",
      "from": "DawnII"
    },
    {
      "update": "MisterMango war vehicles pack available",
      "details": "Wan2.1 T2V 14B war vehicles pack posted in loras channel",
      "from": "MisterMango"
    },
    {
      "update": "New AccVideo WanX T2V 14B model released",
      "details": "Distillation model similar to CausVid, available on HuggingFace",
      "from": "JohnDopamine"
    },
    {
      "update": "TheDenk released ControlNet models for Wan",
      "details": "HED ControlNet for both 1.3B and 14B T2V models on HuggingFace",
      "from": "hicho"
    },
    {
      "update": "Gateway releasing comprehensive workflow package",
      "details": "CN extraction workflow, Flux workflow with Redux/depth/PuLID, and Wan VACE/CausVid workflow",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "update": "AccVid LoRA released",
      "details": "Kijai released Wan21_AccVid_T2V_14B_lora_rank32_fp16.safetensors",
      "from": "Kijai"
    },
    {
      "update": "Uni3C camera control for Wan",
      "details": "Camera control controlnet available, works with new Wan HED controlnet",
      "from": "slmonker(5090D 32GB), hicho"
    },
    {
      "update": "OmniConsistency for first frame styling",
      "details": "Flux-related tool for first frame styling that can overlap with VACE functionality",
      "from": "happy.j"
    },
    {
      "update": "BAGEL ComfyUI implementation available",
      "details": "ComfyUI nodes for BAGEL released but performance is poor",
      "from": "fazeaction, slmonker(5090D 32GB)"
    },
    {
      "update": "Jenga research paper released",
      "details": "New research about faster generation or training, unclear if practical application",
      "from": "hicho"
    },
    {
      "update": "Phantom 14B model released",
      "details": "Available on HuggingFace, trained on 480P data but can work at 720P+",
      "from": "DawnII"
    },
    {
      "update": "HunyuanPortrait released by Tencent",
      "details": "Based on Stable Video Diffusion, not HunyuanVideo model",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "update": "HunyuanAvatar coming out this week",
      "details": "Will be based on HunyuanVideo model",
      "from": "Kijai"
    },
    {
      "update": "Torch compile bug fixed in WanVideoWrapper",
      "details": "Had silly mistake for 10 days that ignored torch compile arguments, fixed yesterday",
      "from": "Kijai"
    },
    {
      "update": "New WAN ControlNet depth model released",
      "details": "TheDenk/wan2.1-t2v-14b-controlnet-depth-v1 available on HuggingFace",
      "from": "hicho"
    },
    {
      "update": "Phantom CFG scheduling support added",
      "details": "Need to update nodes as CFG scheduling for Phantom was just added today",
      "from": "Kijai"
    },
    {
      "update": "TAEW (Tiny Auto Encoder WAN) experimental support",
      "details": "TAEW2_1.safetensors model for much higher quality preview, requires placement in comfyui/models/vae_approx folder",
      "from": "Kijai"
    },
    {
      "update": "Phantom model released",
      "details": "New T2V model that works with reference latents, requires 121 frames at 24fps",
      "from": "multiple users"
    },
    {
      "update": "Two new lipsync models released",
      "details": "Released in lipsync channel",
      "from": "TK_999"
    },
    {
      "update": "New Wan21_T2V_14B_MoviiGen_lora released",
      "details": "Kijai released untested MoviiGen LoRA for T2V model",
      "from": "Kijai"
    },
    {
      "update": "Controlnet support and partial uni3c support added",
      "details": "New controlnet support added with partial uni3c controlnet support (no camera embed stuff yet)",
      "from": "Kijai"
    },
    {
      "update": "Phantom cfg scheduling support added",
      "details": "Recent updates allow scheduling phantom cfg among other small changes",
      "from": "Kijai"
    },
    {
      "update": "Hakoniwa anime wan model released",
      "details": "svjack released hakoniwa_anime_wan2_1_models - anime style adjusted model",
      "from": "DawnII"
    },
    {
      "update": "HD/Pro version of Wan appearing on HuggingFace",
      "details": "aikitoria spotted HD version appearing on HF, not final form yet",
      "from": "aikitoria"
    },
    {
      "update": "Depth ControlNet released for Wan",
      "details": "New depth controlnet available that works with Phantom and T2V models",
      "from": "Kijai"
    },
    {
      "update": "MoviiGen LoRA released",
      "details": "New LoRA for detail enhancement and noise cleanup, affects character likeness",
      "from": "David Snow"
    },
    {
      "update": "Uni3C controlnet released",
      "details": "New controlnet for camera control using reference videos",
      "from": "David Snow"
    },
    {
      "update": "Native ComfyUI VACE implementation working",
      "details": "VACE can now patch other models natively in ComfyUI within the lazy eval patching system",
      "from": "Ablejones"
    },
    {
      "update": "John Dopamine created faster Phantom merge",
      "details": "Custom merged model that provides faster inference than standard Phantom",
      "from": "Thom293"
    },
    {
      "update": "Camera motion LoRAs updated",
      "details": "Some camera motion loras were updated 3 days ago and might work with VACE as alternative to Uni3C",
      "from": "Johnjohn7855"
    },
    {
      "update": "Jenga dev-wan branch available",
      "details": "New development branch for Jenga targeting Wan models appeared on GitHub",
      "from": "JohnDopamine"
    },
    {
      "update": "VORTA claims 14x speed up",
      "details": "New speed optimization claiming significant performance improvement, though with usual caveats about VRAM costs",
      "from": "Kijai"
    },
    {
      "update": "DiffPhy project improves physics",
      "details": "New project trained LoRA on top of Wan to hugely improve physics simulation",
      "from": "pom"
    },
    {
      "update": "Native Phantom support added to ComfyUI",
      "details": "New node WanPhantomSubjectToVideo added with two negative outputs",
      "from": "The Punisher"
    },
    {
      "update": "Flux 1 Kontext announced by Black Forest Labs",
      "details": "Official context multimodal editing model, not open-sourced yet",
      "from": "slmonker"
    },
    {
      "update": "GGUF quantizations of Phantom 14B released",
      "details": "Q8 and other quantizations available on HuggingFace",
      "from": "The Punisher"
    },
    {
      "update": "New Wan model from ByteDance ATI",
      "details": "Another new Wan model.. it's like daily occurance - trajectory model like TORA on steroids",
      "from": "Kijai"
    },
    {
      "update": "ByteDance released Dream-O ComfyUI implementation",
      "details": "Semi-Wan related - could be helpful if it works",
      "from": "JohnDopamine"
    },
    {
      "update": "New LayerAnimate tech released",
      "details": "More new tech: https://layeranimate.github.io",
      "from": "DawnII"
    },
    {
      "update": "Native ComfyUI now has VACE templates",
      "details": "In the latest Comfy there are even templates for VACE",
      "from": "zelgo_"
    },
    {
      "update": "CausVid v2 LoRA released",
      "details": "Can be used at strength 1.0, has block 0 already removed, no flash issues",
      "from": "David Snow"
    },
    {
      "update": "ATI model from ByteDance released",
      "details": "Trajectory-based video generation with motion control via anchor points",
      "from": "JohnDopamine"
    },
    {
      "update": "Direct3D-S2 model released",
      "details": "I-to-3D generation model",
      "from": "Mngbg"
    },
    {
      "update": "CausVid v1.5 no first block version released",
      "details": "Released 3 hours after v2, has block 0 removed",
      "from": "AJO"
    },
    {
      "update": "New Phantom 14B model available",
      "details": "Referred to as 'wondermus' model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "update": "CausVid V2 released with speed improvements",
      "details": "Significantly faster generation times compared to previous versions",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "update": "ATI (motion control) released for Wan",
      "details": "Allows spline-based motion control, needs better interface for drawing",
      "from": "Kijai"
    },
    {
      "update": "CausVid v2 available",
      "details": "Tuned differently than v1, requires CFG scheduling",
      "from": "Johnjohn7855"
    }
  ],
  "workflows": [
    {
      "workflow": "Two-stage video transition",
      "use_case": "Creating smooth transitions between aerial and ground views",
      "from": "notid"
    },
    {
      "workflow": "LTX Five sampler setup for batch generation",
      "use_case": "Generating 1000 videos overnight with multiple parallel generations to pick best results",
      "from": "David Snow"
    },
    {
      "workflow": "Fun Control with reference + start image",
      "use_case": "Better control results by using both reference and start images",
      "from": "Nathan Shipley"
    },
    {
      "workflow": "Fun 1.1 then VACE refinement",
      "use_case": "Run Fun 1.1 with reference, then use first frame for VACE to get better results",
      "from": "Hashu"
    },
    {
      "workflow": "Two-pass v2v for strong stylization",
      "use_case": "Getting better style transfer at low step counts (6-7 steps total). First sampler 2-3 steps, second sampler 4 steps for refinement",
      "from": "David Snow"
    },
    {
      "workflow": "V2V workflow shared for stylization",
      "use_case": "V2V tests with LoRAs, needs both sets of preprocessors despite slower speed. Different videos require different settings",
      "from": "David Snow"
    },
    {
      "workflow": "Double pass workflow with DepthCrafter",
      "use_case": "Long consistent video generation with control, uses depth crafter and ex lora with context options",
      "from": "David Snow"
    },
    {
      "workflow": "First-Frame-Last-Frame inpainting setup",
      "use_case": "VACE inpainting requiring inpainted first frame as guide with gray surrounding areas",
      "from": "ArtOfficial"
    },
    {
      "workflow": "Text-to-documentary maker using DF",
      "use_case": "Automated documentary creation workflow using DiffusionForcing",
      "from": "Colin"
    },
    {
      "workflow": "DF video extension with 3 sections",
      "use_case": "Creating longer videos by chaining multiple 5-second generations",
      "from": "MilesCorban"
    },
    {
      "workflow": "Style transfer with WAN",
      "use_case": "Transferring style without structure using new method",
      "from": "Clownshark Batwing"
    },
    {
      "workflow": "Multi-pass workflow with depth controlnet",
      "use_case": "Style transfer-like approach using depth control",
      "from": "Ablejones"
    },
    {
      "workflow": "NormalCrafter + DepthCrafter + Blender relighting",
      "use_case": "Video relighting using normal maps generated from video",
      "from": "David Snow"
    },
    {
      "workflow": "Redux+Fill+ControlNet+InstantID for start-end frames",
      "use_case": "Creating consistent character videos with FLF2V by generating controllable start and end frames",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "VACE + hires pass workflow",
      "use_case": "Improving VACE output quality with upscaling pass",
      "from": "Dream Making"
    },
    {
      "workflow": "Control 1.3 + ref image + mask + AnimateDiff + Ultimate SD upscale",
      "use_case": "720 to 1920 upscaling pipeline taking around 30 minutes",
      "from": "Gavmakes"
    },
    {
      "workflow": "Consistent character generation using Redux + ACE++ Fill + InstantID",
      "use_case": "Creating start/end frames for character animation",
      "from": "slmonker"
    },
    {
      "workflow": "CausVid basic inference with 4 steps, CFG 1.0",
      "use_case": "Fast video generation in 2-5 minutes",
      "from": "V\u00e9role"
    },
    {
      "workflow": "Real-time video generation with CausVid",
      "use_case": "Generate videos in sliding window with real-time preview using kv_cache",
      "from": "Kijai"
    },
    {
      "workflow": "UniAnimate with MoviiGen",
      "use_case": "UniAnimate could work with the MoviiGen model",
      "from": "Kijai"
    },
    {
      "workflow": "Combining VACE and Phantom",
      "use_case": "Possible to connect both for advantages of each, but results unpredictable",
      "from": "DawnII"
    },
    {
      "workflow": "Start/end frame morphing with VACE",
      "use_case": "Video transitions between two frames",
      "from": "multiple users"
    },
    {
      "workflow": "Face control with VACE using mediapipe/dwpose",
      "use_case": "Facial animation and lip sync",
      "from": "Kijai"
    },
    {
      "workflow": "VACE with Causvid for faster inference",
      "use_case": "Speed up generation while maintaining quality",
      "from": "DawnII"
    },
    {
      "workflow": "VACE outpainting",
      "use_case": "Extending video canvas spatially",
      "from": "Kijai"
    },
    {
      "workflow": "Start/end frame morphing with VACE",
      "use_case": "Creating smooth transitions between two unrelated images",
      "from": "David Snow"
    },
    {
      "workflow": "Context options for long videos",
      "use_case": "Splitting 810 frames into 10 batches with 81 frame contexts and overlap",
      "from": "Valle"
    },
    {
      "workflow": "Temporal extension with CausVid",
      "use_case": "Extending video length beyond base model capabilities",
      "from": "DawnII"
    },
    {
      "workflow": "Use CausVid LoRA with VACE at reduced strength",
      "use_case": "Fast 4-step generation with good quality, compatible with VACE",
      "from": "Kijai"
    },
    {
      "workflow": "Multiple VACE encoders for control blending",
      "use_case": "Blending different control inputs like depth and canny",
      "from": "Kijai"
    },
    {
      "workflow": "Use reference image instead of first frame for longer videos",
      "use_case": "Better consistency for context options and longer processing",
      "from": "Kijai"
    },
    {
      "workflow": "CausVid LoRA + VACE + 14B model for fast generation",
      "use_case": "High quality video generation in ~1 minute instead of 5+ minutes",
      "from": "seruva19"
    },
    {
      "workflow": "CausVid + UniAnimate for character animation",
      "use_case": "Character-driven video generation with distillation speedup",
      "from": "Kijai"
    },
    {
      "workflow": "Skyreels DF with CausVid for long videos",
      "use_case": "Chained generation for extended video lengths",
      "from": "DawnII"
    },
    {
      "workflow": "Video extension with 13 frame overlap using VACE 14B",
      "use_case": "Extending videos while maintaining continuity",
      "from": "Ablejones"
    },
    {
      "workflow": "Chaining 81-frame runs using last frame as reference",
      "use_case": "Creating longer videos (162+ frames) by connecting multiple generations",
      "from": "A.I.Warper"
    },
    {
      "workflow": "VACE with optical flow control input",
      "use_case": "Using optical flow as control method for video generation",
      "from": "DawnII"
    },
    {
      "workflow": "HiDream with RF_inversion for reference images",
      "use_case": "Creating reference-based generations without needing adapters",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Multiple VACE encoders for complex control",
      "use_case": "Separating depth, pose, and mediapipe face controls into different encoders",
      "from": "David Snow"
    },
    {
      "workflow": "Context options for long video generation",
      "use_case": "Generating 10+ second videos, up to 9 seconds at 24fps demonstrated",
      "from": "Valle"
    },
    {
      "workflow": "CausVid with VACE for fast I2V",
      "use_case": "Quick character animation with reference image and control",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "NormalCrafter + detail transfer for normal creation",
      "use_case": "Creating normal maps for relighting",
      "from": "David Snow"
    },
    {
      "workflow": "Multi-step resolution upscaling with CausVid",
      "use_case": "Generate low res then upscale latent for higher resolution output",
      "from": "DiXiao"
    },
    {
      "workflow": "Generate poses from video for VACE",
      "use_case": "Use Dwpose preprocessor then blend with depth or other controls using image blend node",
      "from": "cyncratic"
    },
    {
      "workflow": "Multiple VACE controls",
      "use_case": "Chain multiple VACE embed nodes using prev_embeds input instead of combining controls in single pass",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Multi-part video stitching with VACE",
      "use_case": "Creating longer videos by generating 41-frame segments and stitching with last frame as next start",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Two-pass VACE processing",
      "use_case": "Generate with 14B VACE then cleanup with 1.3B model for quality improvement",
      "from": "David Snow"
    },
    {
      "workflow": "Style transfer with HiDream + VACE",
      "use_case": "Combining HiDream preprocessing with VACE for style transfer applications",
      "from": "Clownshark Batwing"
    },
    {
      "workflow": "Will Smith video using 3 sequential 81-frame generations",
      "use_case": "Long video creation by forwarding 8 frames and adding new end frame each time",
      "from": "DawnII"
    },
    {
      "workflow": "Style guide video transfer",
      "use_case": "Encoding one video and using as style guide for another video generation",
      "from": "Clownshark Batwing"
    },
    {
      "workflow": "VACE multi-control setup",
      "use_case": "Chaining embeds for depth, inpainting, and reference controls",
      "from": "DawnII"
    },
    {
      "workflow": "V2V with pose control using VACE",
      "use_case": "Complex shot replication using begin frame, end frame, and pose control without depth or line controls",
      "from": "jerms_ai_(4090_24g)"
    },
    {
      "workflow": "VACE with T2V model for I2V simulation",
      "use_case": "Faster I2V generation by using VACE with start image only and T2V model + CausVid LoRA",
      "from": "N0NSens"
    },
    {
      "workflow": "Context-based long video generation",
      "use_case": "Generate videos up to 300+ frames by chunking into 81-frame segments, though context switches may cause jumps",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Flux + VACE automated pipeline",
      "use_case": "Auto-generates reference image from first frame using Flux T2I, then uses it with VACE for video generation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Two-pass depth + CausVid generation",
      "use_case": "First pass for motion, second pass for quality enhancement using depth control",
      "from": "BestWind"
    },
    {
      "workflow": "V2V with samples input",
      "use_case": "Encode input frames with WanVideoEncode and use samples input with denoise below 1.0 for video-to-video",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "VACE inpainting with pose control in masked regions",
      "use_case": "Control pose only in specific masked areas of video while preserving background",
      "from": "aipmaster"
    },
    {
      "workflow": "Face replacement using VACE with reference character",
      "use_case": "Replace face of character with any reference, works on 3090",
      "from": "BestWind"
    },
    {
      "workflow": "Iterative latent upscale with VACE triple sampler",
      "use_case": "Video restyle with progressive quality improvement using depth and pose blend",
      "from": "yo9o"
    },
    {
      "workflow": "CausVid + Fantasy Talking combination",
      "use_case": "Fast lip-sync generation at 81 frames, 720x720, 5 steps",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Combined depth and pose VACE processing",
      "use_case": "Better facial movement control by combining multiple preprocessors",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Multi-encoder VACE setup",
      "use_case": "Using separate encoders for pose (higher strength) and depth (lower strength) gives more control over output",
      "from": "MilesCorban"
    },
    {
      "workflow": "Context for long videos",
      "use_case": "Using context node with wrapper allows generation of 401+ frames, though may lose some info between chunks",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "VACE extension using last frames",
      "use_case": "Feed last 4 frames from first run into second VACE encode to extend videos",
      "from": "Draken"
    },
    {
      "workflow": "Dual VACE/WAN 14B workflow for versatility and quality",
      "use_case": "Combines VACE versatility with WAN 14B quality using both models together",
      "from": "zelgo_"
    },
    {
      "workflow": "Multiple VACE embeds with separate pose/depth encoders",
      "use_case": "Better control than blending - first encoder with pose, second with depth at different strengths",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Chaining last frames from one generation as start of next",
      "use_case": "Long video generation by using last frame as start frame in new encoders/samplers",
      "from": "A.I.Warper"
    },
    {
      "workflow": "VACE with normal maps for facial tracking",
      "use_case": "Capturing facial expressions and movements using normal map control",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "I2V with CausVid LoRA for speed",
      "use_case": "Faster video generation with reduced steps",
      "from": "DevouredBeef"
    },
    {
      "workflow": "Double VACE control with depth and pose",
      "use_case": "Creating more accurate character animations by combining two control types",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "VACE inpainting with masks",
      "use_case": "Removing or replacing subjects in videos while retaining background",
      "from": "MilesCorban"
    },
    {
      "workflow": "Single frame WAN generation for consistency",
      "use_case": "Using WAN T2V at one frame high resolution to generate consistent images for video clips instead of using separate models",
      "from": "Thom293"
    },
    {
      "workflow": "Video face replacement using reactor node",
      "use_case": "Fast and easy face swapping in videos",
      "from": "slmonker"
    },
    {
      "workflow": "Multi-keyframe VACE setup using pad nodes",
      "use_case": "Adding multiple control frames at specific intervals for complex camera movements",
      "from": "Kijai"
    },
    {
      "workflow": "Separate control and reference embed workflow",
      "use_case": "Better results with 14B model when control and reference are processed separately",
      "from": "Kijai"
    },
    {
      "workflow": "Context window extension for long videos",
      "use_case": "Generate videos longer than 81 frames using overlapping windows with blended seams",
      "from": "Kijai"
    },
    {
      "workflow": "Video extension using white frame padding",
      "use_case": "Extend DWPose output video by padding with white frames",
      "from": "MilesCorban"
    },
    {
      "workflow": "Character replacement with masking",
      "use_case": "Takes character, puts gray mask over them, composites with original to remove/replace character",
      "from": "MilesCorban"
    },
    {
      "workflow": "Direct input frame processing",
      "use_case": "Use input video directly in input frames with reduced strength plus normal map for style transfer effects",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Two-pass video refinement",
      "use_case": "Use first pass with VACE, then second pass with different model for refinement using encoded latents and denoise",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Video extension with gray frames",
      "use_case": "For extending video, use solid gray frames for areas to generate and black masks for frames to preserve",
      "from": "DawnII"
    },
    {
      "workflow": "Text + control to video",
      "use_case": "Use LoRA + WAN with DWPose control without reference image or input video, just text + control",
      "from": "Neex"
    },
    {
      "workflow": "Two-pass workflow using 1.3b for smoothing",
      "use_case": "Use promptless 1.3b v2v second pass at low denoise, steps and CFG to smooth out CausVid artifacting",
      "from": "Faust-SiN"
    },
    {
      "workflow": "Face and body separate encoding",
      "use_case": "Using one encoder for face and one for body to try to get better facial control",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Input video + normal map without latent sync",
      "use_case": "Getting facial movement by using input video at 0.4 strength and normal map at 0.6 strength",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Two-stage character replacement",
      "use_case": "Create initial video with rough character description, then use VACE with reference to replace character while preserving motion",
      "from": "AJO"
    },
    {
      "workflow": "Phantom + VACE + Causvid combination",
      "use_case": "Using Phantom embed nodes with VACE embed input for better identity preservation in character animation",
      "from": "Johnjohn7855"
    },
    {
      "workflow": "Dual VACE embed setup",
      "use_case": "Separate VACE embed nodes for reference image and control video to set different strengths for each input",
      "from": "Johnjohn7855"
    },
    {
      "workflow": "Video splitting for multiple processing",
      "use_case": "Load one video and extract different frame ranges to different samplers using split_index parameter",
      "from": "Boop"
    },
    {
      "workflow": "Video extension with VACE using first 20 frames plus gray frames",
      "use_case": "Extending video sequences by feeding initial frames and empty gray frames",
      "from": "Rishi Pandey"
    },
    {
      "workflow": "Multi-angle character generation using Wan i2v iterations",
      "use_case": "Generate consistent character at different angles by using Wan itself to create viewpoint variations",
      "from": "David Snow"
    },
    {
      "workflow": "Face-only masking with DWPose body control",
      "use_case": "Keep original face while changing everything else (hair, clothing, background)",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Start/end frame morphing with DWPose control",
      "use_case": "Character morphing using pose control with defined start and end states",
      "from": "David Snow"
    },
    {
      "workflow": "Video extension with overlapping frames using VACE",
      "use_case": "Creating longer videos by extending clips with seamless transitions",
      "from": "ArtOfficial"
    },
    {
      "workflow": "First-frame-last-frame morphing using VACE",
      "use_case": "Creating smooth transitions between two different subjects or styles",
      "from": "David Snow"
    },
    {
      "workflow": "Get image or mask from batch method for video extension",
      "use_case": "Simpler method for extending videos without reloading compressed video",
      "from": "zelgo_"
    },
    {
      "workflow": "Auto face masking for video processing",
      "use_case": "Automatically mask face in input video for pose-driven face swapping",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "SDXL first frame styling with VACE",
      "use_case": "Style first frame with SDXL/Flux then use as reference image for VACE video processing",
      "from": "Valle"
    },
    {
      "workflow": "Multi-pass video extension technique",
      "use_case": "Take last few frames from first pass and feed into another Wan instance, then combine passes for longer video",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "workflow": "Tiled video processing for higher resolution",
      "use_case": "Start big 720p video, stop at 20% steps, cut into 4 tiles and do vid2vid on each to finish",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "workflow": "VACE inpainting with mask overlay",
      "use_case": "Character replacement in video while maintaining scene context",
      "from": "David Snow"
    },
    {
      "workflow": "Overlapping batch processing for long videos",
      "use_case": "Creating arbitrary length videos with maintained consistency",
      "from": "notid"
    },
    {
      "workflow": "Seamless looping workflow",
      "use_case": "Creating perfectly looped video content",
      "from": "The Shadow (NYC)"
    },
    {
      "workflow": "SAM2 points editor for masking",
      "use_case": "Creating precise masks for video editing",
      "from": "David Snow"
    },
    {
      "workflow": "VACE inpainting workflow with context options",
      "use_case": "Character replacement and video extension up to thousands of frames",
      "from": "David Snow"
    },
    {
      "workflow": "Face replacement using face segmentation",
      "use_case": "Single character face swapping with accurate masks",
      "from": "David Snow"
    },
    {
      "workflow": "Video extension with consistent color transitions",
      "use_case": "Chaining videos using same 15 frames for seamless transitions",
      "from": "Piblarg"
    },
    {
      "workflow": "VACE inpainting workflow for subject removal/replacement",
      "use_case": "Removing subjects from video and replacing with new content using masks and prompts",
      "from": "David Snow"
    },
    {
      "workflow": "Multi-key frame interpolation with VACE",
      "use_case": "Using 4 image keys with pose control for video-to-video generation",
      "from": "jerms_ai_(4090_24g)"
    },
    {
      "workflow": "Batch video processing for long videos",
      "use_case": "Processing long videos in chunks (80 frames at a time) and automatically combining results",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "V2V with CausVid and Florence find and replace",
      "use_case": "Video style transfer and modification",
      "from": "hicho"
    },
    {
      "workflow": "Multi-batch video extension with last frame reference",
      "use_case": "Creating longer videos by using last frame as reference for next batch",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Motion extraction to VACE pipeline",
      "use_case": "Extract motion controls from video, process first frame through Flux, then generate with VACE",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "workflow": "Multiple VACE encodes for character replacement",
      "use_case": "Use RMBG to create mask, make mask gray, feed into VACE as control video. Second VACE node with openpose and reference image",
      "from": "MilesCorban"
    },
    {
      "workflow": "Chained VACE inputs for character insertion",
      "use_case": "Chain guide inputs in wrapper for adding character to background without character",
      "from": "TK_999"
    },
    {
      "workflow": "Loop workflow with video concatenation",
      "use_case": "For creating looping videos with Wan21 VACE",
      "from": "daking999"
    },
    {
      "workflow": "Inpainting with multiple control inputs",
      "use_case": "Use expanded mask, depth, normals and pose in single VACE encode for inpainting",
      "from": "David Snow"
    },
    {
      "workflow": "VACE inpainting with pasteback",
      "use_case": "Avoiding VAE compression when inpainting",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Mask inversion for background inpainting",
      "use_case": "Keeping character and replacing background instead of replacing character",
      "from": "voxJT"
    },
    {
      "workflow": "Multiple control inputs combined",
      "use_case": "Inpaint mask, pose, depth and normal all into one encode",
      "from": "David Snow"
    },
    {
      "workflow": "Phantom with VACE module",
      "use_case": "Using phantom example workflow from kijai and adding wan video vace encode node",
      "from": "Johnjohn7855"
    },
    {
      "workflow": "Phantom with dual reference images",
      "use_case": "Character consistency with multiple reference points",
      "from": "Kijai"
    },
    {
      "workflow": "VACE + Phantom combination",
      "use_case": "Better identity preservation than VACE ref alone",
      "from": "Zuko"
    },
    {
      "workflow": "CausVid with CFG scheduling",
      "use_case": "Faster inference with quality preservation using CFG ending at 0.1",
      "from": "Kijai"
    },
    {
      "workflow": "Two VACE encode nodes with different strengths",
      "use_case": "Separate control over depth and reference strength",
      "from": "Kijai"
    },
    {
      "workflow": "Multi-encoder VACE setup",
      "use_case": "Individual strength control for control video, ref image, and input video",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Phantom with VACE combination",
      "use_case": "Combining Phantom embeds with VACE encode for enhanced control",
      "from": "multiple users"
    },
    {
      "workflow": "Frame rate conversion for MMAudio",
      "use_case": "Converting Wan 16fps output to 24fps for MMAudio compatibility",
      "from": "Mngbg"
    },
    {
      "workflow": "VACE loop workflow",
      "use_case": "Creating looping videos, would be perfect except for Wan flash issue",
      "from": "David Snow"
    },
    {
      "workflow": "Phantom + VACE combination",
      "use_case": "Extended duration videos using both models in tandem",
      "from": "JohnDopamine"
    },
    {
      "workflow": "3D-to-wan workflow",
      "use_case": "Veo3 + Flux + Hunyuan3D + Wan with VACE pipeline",
      "from": "Yae"
    },
    {
      "workflow": "Uni3C controlnet for camera motion",
      "use_case": "Transfer motion from existing videos, works with simple motions better than extreme ones like orbit",
      "from": "Kijai"
    },
    {
      "workflow": "Phantom + VACE inpainting",
      "use_case": "Character replacement in existing videos with consistency",
      "from": "David Snow"
    },
    {
      "workflow": "Uni3C camera control",
      "use_case": "Transfer camera movements from any video or 3D rendered sequence",
      "from": "N0NSens"
    },
    {
      "workflow": "Multiple LoRA stack",
      "use_case": "Enhanced detail and speed with causvid + accvid + moviigen + detailz",
      "from": "Johnjohn7855"
    },
    {
      "workflow": "Four-image Phantom reference",
      "use_case": "Complete subject coverage using multiple angles of same subject",
      "from": "David Snow"
    },
    {
      "workflow": "SAM2 points editor for mask creation",
      "use_case": "Creating mask videos for VACE inpainting using subject tracking",
      "from": "David Snow"
    },
    {
      "workflow": "Face detailing with crop and stitch",
      "use_case": "Detailed face processing by cropping face region, processing, and stitching back",
      "from": "David Snow"
    },
    {
      "workflow": "Multiple VACE encodes for control combination",
      "use_case": "Using separate encodes for ref image, depth, and lineart with individual strength control",
      "from": "JohnDopamine"
    },
    {
      "workflow": "Phantom + VACE combination",
      "use_case": "Combining Phantom character consistency with VACE controls",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Triple sample workflow with inpainting",
      "use_case": "Combining inpainting functionality with triple sampling approach",
      "from": "hau"
    },
    {
      "workflow": "Moge to ComfyUI Load 3D workflow",
      "use_case": "Convert image to glb using Moge, upload to Load 3D node, and record screen for 3D control",
      "from": "toyxyz"
    },
    {
      "workflow": "Over-engineered face replacement workflow",
      "use_case": "Works for any video regardless of face size, potentially expandable to whole person replacement",
      "from": "DeZoomer"
    },
    {
      "workflow": "Wan i2v with Uni3C for FLF model",
      "use_case": "Using Chinese prompts with FLF2V model and Uni3C for camera control",
      "from": "Guey.KhalaMari"
    },
    {
      "workflow": "Video enhancement/upscaling using loras",
      "use_case": "Using existing low-res videos as input with LoRAs for enhancement, completed in 90.50 seconds with causVid",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "VACE + Phantom inpainting with dual masking",
      "use_case": "Character replacement while preserving background",
      "from": "Zuko"
    },
    {
      "workflow": "Phantom with LLM prompt generation",
      "use_case": "Auto-generate detailed character and outfit descriptions for consistent Phantom results",
      "from": "AJO"
    },
    {
      "workflow": "Video game footage \u2192 Florence \u2192 Wan CausVid T2V",
      "use_case": "Convert video game graphics with motion matching using low denoiser (0.10)",
      "from": "hicho"
    },
    {
      "workflow": "Automatic outfit designer workflow",
      "use_case": "Automatically designing outfits for characters in video generation",
      "from": "AJO"
    },
    {
      "workflow": "CFG scheduling with CausVid",
      "use_case": "4 steps with 4 cfg and 4 without using end percent 0.5",
      "from": "Ada"
    },
    {
      "workflow": "Multi-reference image processing",
      "use_case": "Using LLM to embed description text at start of every scene for character consistency",
      "from": "AJO"
    },
    {
      "workflow": "1st frame from vid > cosxl = init img, vace > depth from vid + init img = gen",
      "use_case": "Alternative approach to video generation",
      "from": "N0NSens"
    },
    {
      "workflow": "5 scene looping with CausVid v2 and Phantom",
      "use_case": "Creating multi-scene storyboards with consistent character",
      "from": "AJO"
    },
    {
      "workflow": "Batch prompt processing with phantom",
      "use_case": "Creating multiple videos automatically from folder of prompts",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Face detailing with crop and reprocess",
      "use_case": "Improving face quality in generated videos through cropping and re-generation",
      "from": "mamad8"
    },
    {
      "workflow": "ATI trajectory control",
      "use_case": "Directing specific motion paths in video generation using anchor points",
      "from": "Kijai"
    },
    {
      "workflow": "Multi-prompt looping workflow",
      "use_case": "Load prompts from text files, loop through them, encode and stitch together for music videos - 5 scenes in 254 seconds total",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Phantom + reference image + multiple LoRAs",
      "use_case": "Character consistency with style enhancement using reference image, Phantom, CausVid V2, and detail LoRAs",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Reactor -> RIFE -> Film Grain for face consistency",
      "use_case": "Reducing jitter from reactor face swapping across multiple scenes",
      "from": "AJO"
    },
    {
      "workflow": "One-shot story generation with costume consistency",
      "use_case": "Generate complete stories with 1 ref image, 5 scenes, consistent character across scenes",
      "from": "AJO"
    },
    {
      "workflow": "Using latent sync for multi-scene consistency",
      "use_case": "Creating consistent characters across multiple video scenes",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Two encoder setup for VACE controls",
      "use_case": "Using both depth and normal maps with separate strength controls",
      "from": "Johnjohn7855"
    },
    {
      "workflow": "Using VACE for music video post-production fixes",
      "use_case": "Fixing character consistency and doing edits on video generation outputs when direct editing is difficult",
      "from": "Ruairi Robinson"
    }
  ],
  "settings": [
    {
      "setting": "CFG with SLG",
      "value": "3 (half of normal CFG)",
      "reason": "SLG requires lower CFG values",
      "from": "Miku"
    },
    {
      "setting": "Denoise for T2V",
      "value": "1.0",
      "reason": "Never use other values unless doing vid2vid",
      "from": "Kijai"
    },
    {
      "setting": "Block setting for 1.3B",
      "value": "8",
      "reason": "Better results with 1.3B model",
      "from": "Kijai"
    },
    {
      "setting": "Context frames for FantasyTalker",
      "value": "169",
      "reason": "Works well with context windows, though math unclear",
      "from": "Kijai"
    },
    {
      "setting": "Shift skip for loop_args",
      "value": "Half of latents (e.g. 10 for 80 frames)",
      "reason": "4 frames per latent, so 80 frames = 20 latents, half = 10",
      "from": "BondoMan"
    },
    {
      "setting": "VACE strength for skeleton lines",
      "value": "1.0",
      "reason": "Counterintuitively helps avoid skeleton artifacts",
      "from": "David Snow"
    },
    {
      "setting": "First pass steps for two-pass v2v",
      "value": "2-3 steps",
      "reason": "LoRA effects strongest on 2nd step, diminish after 3rd step",
      "from": "David Snow"
    },
    {
      "setting": "Second pass steps for two-pass v2v",
      "value": "4 steps",
      "reason": "For refinement after first pass captures strong stylization",
      "from": "David Snow"
    },
    {
      "setting": "DG model steps for T2V",
      "value": "6 steps",
      "reason": "Usable results compared to 25-30 steps with base models",
      "from": "David Snow"
    },
    {
      "setting": "V2V denoise for lip sync results",
      "value": "0.6",
      "reason": "Good balance for following lips while maintaining style",
      "from": "Jas"
    },
    {
      "setting": "Ex LoRA context frames",
      "value": "161 frames with overlap of 24",
      "reason": "Enables long animation generation with proper blending",
      "from": "David Snow"
    },
    {
      "setting": "Double pass sampling",
      "value": "3 and 6 samples split between two samplers, max 10 total",
      "reason": "Fast generation while maintaining quality",
      "from": "David Snow"
    },
    {
      "setting": "Line art thresholding for VACE",
      "value": "0.5 binary threshold",
      "reason": "Makes control map more compatible with VACE training",
      "from": "Rishi Pandey"
    },
    {
      "setting": "TeaCache with low steps",
      "value": "Don't use TeaCache with 8 steps",
      "reason": "8 steps too low to benefit from TeaCache",
      "from": "Draken"
    },
    {
      "setting": "FramePack chunk size",
      "value": "33 frame chunks",
      "reason": "FP model was trained at 33 frame chunks",
      "from": "Draken"
    },
    {
      "setting": "FP8",
      "value": "disabled",
      "reason": "RTX 3090 doesn't support FP8, causes compilation errors",
      "from": "Juan Gea"
    },
    {
      "setting": "Frame overlap",
      "value": "4 frames",
      "reason": "Optimal for video extension without excessive redundancy",
      "from": "MilesCorban"
    },
    {
      "setting": "Shift parameter",
      "value": "over 10",
      "reason": "Required for movement in Skyreels v2",
      "from": "Colin"
    },
    {
      "setting": "Resolution comparison",
      "value": "720p preferred over 480p",
      "reason": "Same generation time but better prompt adherence",
      "from": "N0NSens"
    },
    {
      "setting": "Frame chunks",
      "value": "81 frames",
      "reason": "Recommended chunk size for context processing",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "RAM requirement",
      "value": "64GB minimum, 128GB preferred",
      "reason": "64GB fine for most use cases, 128GB needed for larger model conversions",
      "from": "Kijai"
    },
    {
      "setting": "Resolution for 16GB VRAM",
      "value": "832x480 for 720p/81fr",
      "reason": "Maximum resolution possible with 16GB VRAM",
      "from": "N0NSens"
    },
    {
      "setting": "DepthAnything precision",
      "value": "fp16",
      "reason": "Prevents OOM on 4090 compared to fp32",
      "from": "A.I.Warper"
    },
    {
      "setting": "CausVid steps",
      "value": "2-4 steps",
      "reason": "4 steps gets 90% quality, diminishing returns beyond",
      "from": "yi"
    },
    {
      "setting": "CausVid CFG",
      "value": "1.0",
      "reason": "CFG distilled model, higher values burn output",
      "from": "Kijai"
    },
    {
      "setting": "Empty embed value",
      "value": "97",
      "reason": "Fixes tensor size mismatch errors",
      "from": "Dream Making"
    },
    {
      "setting": "Flex attention frame per block",
      "value": "3",
      "reason": "Required for proper block mask operation",
      "from": "Kijai"
    },
    {
      "setting": "Shift parameter for CausVid",
      "value": "Any value",
      "reason": "Parameter is ignored due to set timestep schedule",
      "from": "Kijai"
    },
    {
      "setting": "CausVid steps",
      "value": "3 steps",
      "reason": "9 steps produces garbage results",
      "from": "Kijai"
    },
    {
      "setting": "CausVid sampling",
      "value": "one latent at a time in sliding window",
      "reason": "How it's meant to be sampled with kv_cache",
      "from": "Kijai"
    },
    {
      "setting": "CausVid window size",
      "value": "3 latents",
      "reason": "Standard window size for sliding generation",
      "from": "Kijai"
    },
    {
      "setting": "UniPC scheduler",
      "value": "Works with 9 steps",
      "reason": "9 steps works with unipc scheduler",
      "from": "aipmaster"
    },
    {
      "setting": "VACE block swapping",
      "value": "Max 8 blocks for 14B",
      "reason": "Hardware limitation and architecture design",
      "from": "DawnII"
    },
    {
      "setting": "Causvid with VACE",
      "value": "3-4 steps, CFG 1.0",
      "reason": "Optimal balance of speed and quality",
      "from": "Kijai"
    },
    {
      "setting": "14B VACE resolution",
      "value": "720x720 or 1024x1024",
      "reason": "Works better at higher resolutions",
      "from": "Kijai"
    },
    {
      "setting": "VACE empty frame color",
      "value": "0=black, 1=white, 0.5=neutral gray",
      "reason": "Controls background for masked areas",
      "from": "Kijai"
    },
    {
      "setting": "vace_blocks_to_swap",
      "value": "8 for 14B model",
      "reason": "Prevents OOM errors on RTX 4080 16GB",
      "from": "DawnII"
    },
    {
      "setting": "CFG",
      "value": "2.0",
      "reason": "Enables generation in 6 steps instead of 20-30",
      "from": "David Snow"
    },
    {
      "setting": "steps",
      "value": "2 steps with CausVid",
      "reason": "CausVid enables very low step generation",
      "from": "Kijai"
    },
    {
      "setting": "VACE strength",
      "value": "1.5",
      "reason": "Higher strength for better control",
      "from": "Kijai"
    },
    {
      "setting": "context_overlap",
      "value": "16",
      "reason": "Good default for blending batches in long videos",
      "from": "David Snow"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.2-0.4",
      "reason": "Sweet spot for quality, 0.2 loses quality, above 0.4 may be too strong",
      "from": "Kijai"
    },
    {
      "setting": "Steps with CausVid LoRA",
      "value": "4",
      "reason": "Appears to be sweet spot for quality vs speed",
      "from": "Kijai"
    },
    {
      "setting": "CFG with CausVid",
      "value": "1.0",
      "reason": "CausVid is CFG distilled, higher values don't work properly",
      "from": "Kijai"
    },
    {
      "setting": "Block swap for 14B",
      "value": "20 for general, 5-7 for VACE",
      "reason": "Memory management for large models",
      "from": "JohnDopamine"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.4-0.6 with VACE, 0.8-0.9 with other LoRAs",
      "reason": "Balance between quality and artifacts",
      "from": "Kijai"
    },
    {
      "setting": "CFG scale",
      "value": "1.0",
      "reason": "Required for CausVid distillation model",
      "from": "Kijai"
    },
    {
      "setting": "Steps",
      "value": "4 steps",
      "reason": "Optimal for CausVid distillation",
      "from": "Kijai"
    },
    {
      "setting": "Block swap",
      "value": "30-40 for 3090, 8 VACE blocks",
      "reason": "Manage VRAM usage on lower-end cards",
      "from": "Kijai"
    },
    {
      "setting": "VACE block application",
      "value": "Every 5th block for 14B, every 2nd for 1.3B",
      "reason": "Default VACE block patterns",
      "from": "Kijai"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.3-0.5",
      "reason": "0.3 allows proper hair movement, 0.5 gives cleaner results at 4 steps",
      "from": "A.I.Warper"
    },
    {
      "setting": "Steps with CausVid LoRA",
      "value": "2-4 steps",
      "reason": "CausVid is step-distilled, works well at very low step counts",
      "from": "Kijai"
    },
    {
      "setting": "CFG with CausVid",
      "value": "1.0",
      "reason": "CausVid is CFG-distilled, doubles compute speed",
      "from": "Draken"
    },
    {
      "setting": "Shift parameter with CausVid",
      "value": "8",
      "reason": "Don't reduce to 1 like other optimizations",
      "from": "Kijai"
    },
    {
      "setting": "Resolution",
      "value": "576x1024",
      "reason": "Working resolution for testing, 720p had issues with hair movement",
      "from": "A.I.Warper"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.5-1.0",
      "reason": "Optimal balance for quality without artifacts",
      "from": "DawnII"
    },
    {
      "setting": "CausVid steps",
      "value": "4-6 steps",
      "reason": "Good quality with reasonable generation time",
      "from": "DawnII"
    },
    {
      "setting": "Block swap",
      "value": "20",
      "reason": "Good balance for 16GB VRAM systems",
      "from": "N0NSens"
    },
    {
      "setting": "VACE mask color",
      "value": "127,127,127 RGB (0.5 gray)",
      "reason": "What VACE tries to fill for inpainting/outpainting",
      "from": "Kijai"
    },
    {
      "setting": "Max frames at 1024x576",
      "value": "81 frames",
      "reason": "VRAM limitation for single batch",
      "from": "A.I.Warper"
    },
    {
      "setting": "CFG for CausVid",
      "value": "1",
      "reason": "Standard setting for distilled models",
      "from": "Valle"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.2-0.6, commonly 0.5",
      "reason": "Optimal balance, 0.7 may be better for some cases",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Steps for CausVid",
      "value": "4-5 steps",
      "reason": "Less motion shots can use 4 or less, more motion needs 5+",
      "from": "N0NSens"
    },
    {
      "setting": "CFG for CausVid",
      "value": "1.0",
      "reason": "Always use CFG 1 with CausVid LoRA",
      "from": "Multiple users"
    },
    {
      "setting": "Shift parameter",
      "value": "8-10 for quality, avoid 30",
      "reason": "Higher shift values damage background details",
      "from": "N0NSens"
    },
    {
      "setting": "Block swap for 14B models",
      "value": "40 blocks for 14B + 5 for VACE",
      "reason": "VRAM optimization",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Causvid Lora strength",
      "value": "0.25 with 6 steps",
      "reason": "Incredible quality while maintaining good motion",
      "from": "The Punisher"
    },
    {
      "setting": "Causvid Lora for i2v",
      "value": "0.5 strength for 4 steps, 0.6 for 6 steps",
      "reason": "Tested multiple times for optimal results",
      "from": "Jonathan"
    },
    {
      "setting": "Denoise for i2v with Causvid",
      "value": "0.75",
      "reason": "Improves movement and prompt adherence significantly",
      "from": "Jonathan"
    },
    {
      "setting": "Base precision and quant",
      "value": "base_precision fp16, quant fp8_e5 for <4xxx cards or fp8_e4 for >4xxx cards",
      "reason": "Optimal performance based on GPU generation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Causvid LoRA strength",
      "value": "0.25 for normal I2V, 0.6-0.3 range generally",
      "reason": "Good balance of speed and quality",
      "from": "The Punisher"
    },
    {
      "setting": "Steps for Causvid",
      "value": "6 steps",
      "reason": "4 steps show issues, 6 steps resolve them while maintaining speed",
      "from": "The Punisher"
    },
    {
      "setting": "CFG for Causvid LoRA",
      "value": "1.0",
      "reason": "Required for distilled model, higher values double computation time",
      "from": "zelgo_"
    },
    {
      "setting": "VACE strength",
      "value": "80%",
      "reason": "100% pushes too far into parrot mode",
      "from": "A.I.Warper"
    },
    {
      "setting": "Denoise for second pass",
      "value": "80%",
      "reason": "Used in two-pass workflow for cleanup",
      "from": "A.I.Warper"
    },
    {
      "setting": "Block swap",
      "value": "45 (40 base model + 5 VACE)",
      "reason": "Prevents crashes with 14B VACE models",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Shift parameter",
      "value": "2.5",
      "reason": "Solid results, values over 15 cause blurry details",
      "from": "Jemmo"
    },
    {
      "setting": "CFG",
      "value": "1",
      "reason": "Required for CausVid LoRA to work properly",
      "from": "Davidodave"
    },
    {
      "setting": "Steps",
      "value": "14",
      "reason": "Removes shimmer from 8-step outputs with minimal time penalty",
      "from": "CaptHook"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.40",
      "reason": "Good balance for quality",
      "from": "CaptHook"
    },
    {
      "setting": "Denoising",
      "value": "0.75",
      "reason": "Can add motion and reduce artificial look",
      "from": "JohnDopamine"
    },
    {
      "setting": "Eta",
      "value": "0.5",
      "reason": "Good general value for noise addition in custom sampler",
      "from": "Clownshark Batwing"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.3-0.6",
      "reason": "Lower values need more steps, higher values work with fewer steps",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Steps with CausVid",
      "value": "4-12 steps",
      "reason": "Depends on LoRA strength - lower strength needs more steps",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "CFG with CausVid",
      "value": "1.0",
      "reason": "Required for proper CausVid LoRA function",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Denoise for V2V",
      "value": "0.6",
      "reason": "Good balance for video-to-video transformation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Block swap for 14B model",
      "value": "40 blocks",
      "reason": "For memory efficiency with 16GB VRAM",
      "from": "N0NSens"
    },
    {
      "setting": "CausVid steps",
      "value": "4-5 steps",
      "reason": "Good balance of speed and quality",
      "from": "Multiple users"
    },
    {
      "setting": "Block swap",
      "value": "35 transformer blocks",
      "reason": "Reduces VRAM usage significantly while maintaining performance",
      "from": "N0NSens"
    },
    {
      "setting": "Overlap frames for context",
      "value": "32 frames",
      "reason": "Recommended overlap setting",
      "from": "Flipping Sigmas"
    },
    {
      "setting": "Denoise for V2V",
      "value": "Below 1.0",
      "reason": "Required when using samples input for video-to-video",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "CausVid CFG",
      "value": "1.0",
      "reason": "Required for CausVid LoRA to work properly",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.3-0.6",
      "reason": "Lower values need more steps, 0.5 commonly used",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "CausVid steps",
      "value": "4-8",
      "reason": "Allows much lower step inference than standard",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Second VACE embed strength",
      "value": "0.2-0.5",
      "reason": "When combining multiple controls, lower strength prevents overcooking",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Video-to-video denoise with CausVid",
      "value": "0.2",
      "reason": "Works well with LoRAs for v2v workflow",
      "from": "hicho"
    },
    {
      "setting": "Video upload width",
      "value": "960 (can reduce to 512)",
      "reason": "Determined by Video Upload node, can reduce for VRAM constraints",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.3-0.5",
      "reason": "Base is 0.5 with 4 steps, 0.3 with 8 steps preferred, above 0.6 gives burned outputs",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "CausVid CFG",
      "value": "1",
      "reason": "Always use CFG=1 with CausVid LoRA",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Steps for quality",
      "value": "6 for standard, 4 for previews, 8 for final",
      "reason": "Balances speed and quality",
      "from": "David Snow"
    },
    {
      "setting": "VACE encode strength and end_percent",
      "value": "strength 0.9, end_percent 0.8",
      "reason": "Good starting values for VACE reference",
      "from": "Fawks"
    },
    {
      "setting": "Reserve VRAM",
      "value": "2-3",
      "reason": "Prevents OOM by limiting VRAM usage to max minus reserved amount",
      "from": "BestWind"
    },
    {
      "setting": "CausVid LoRA parameters",
      "value": "4-9 steps, CFG 1.0, LoRA strength 0.4-0.8, shift 8",
      "reason": "Optimal settings for speed and quality with CausVid",
      "from": "Johnjohn7855"
    },
    {
      "setting": "VACE block swapping for 12GB VRAM",
      "value": "Block swap 40 and 5 of VACE",
      "reason": "Enables running on lower VRAM systems",
      "from": "sneako1234"
    },
    {
      "setting": "TeaCache threshold with VACE 14B CausVid",
      "value": "0.6 or higher",
      "reason": "Lower values don't work properly with this setup",
      "from": "David Snow"
    },
    {
      "setting": "Multi-VACE encoder strengths",
      "value": "Pose: strength 1, Depth: strength 0.5",
      "reason": "Maintains reference image structure while adding control",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.3 with 12 steps or 0.6 with 4 steps",
      "reason": "Balance between speed and quality",
      "from": "CFSStudios"
    },
    {
      "setting": "VACE normal map strength",
      "value": "0.9",
      "reason": "Good facial tracking without artifacts",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "CFG and shift for UniPC with CausVid",
      "value": "CFG 1 and shift 8",
      "reason": "Optimal settings for UniPC sampler",
      "from": "Cubey"
    },
    {
      "setting": "VACE strength for pose control",
      "value": "0.6",
      "reason": "Good balance for control without artifacts",
      "from": "Kijai"
    },
    {
      "setting": "CFG scale",
      "value": "1.5 with SLG",
      "reason": "Better results than CFG 1.0, worth the 2x inference time",
      "from": "Kijai"
    },
    {
      "setting": "Steps for WAN",
      "value": "25-50 steps maximum",
      "reason": "75 steps is way too much, no benefit over 50 and even that is quite high for most use cases",
      "from": "Juampab12"
    },
    {
      "setting": "NormalCrafter resolution",
      "value": "1024",
      "reason": "Recommended resolution setting for NormalCrafter",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "NormalCrafter encoder strength",
      "value": "Start at 0.5",
      "reason": "When face differs significantly from reference, start at 0.5 and increase until it gets wonky",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Inpaint RGB fill",
      "value": "127, 127, 127 (gray)",
      "reason": "Required for VACE inpainting to work properly",
      "from": "David Snow"
    },
    {
      "setting": "Control strength for separated embeds",
      "value": "0.7",
      "reason": "Good balance for control influence when using separate control/reference",
      "from": "Kijai"
    },
    {
      "setting": "Denoise for latent upscale second pass",
      "value": "0.65",
      "reason": "Maintains consistency when upscaling",
      "from": "DawnII"
    },
    {
      "setting": "base_precision",
      "value": "fp16_fast",
      "reason": "Recommended for RTX 4090 performance",
      "from": "Cubey"
    },
    {
      "setting": "Context frames",
      "value": "81 (default)",
      "reason": "Use default 81 frame windows for memory efficiency in long generations",
      "from": "Kijai"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.3-0.6",
      "reason": "Lower requires more steps, 0.4 with 8 steps or 0.5 with 4 steps work well",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "CFG for CausVid",
      "value": "1.0",
      "reason": "Higher values not recommended unless using SLG which doubles gen time",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Steps with CausVid",
      "value": "4-8 steps",
      "reason": "4 steps with 0.5 strength or 8 steps with 0.4 strength",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Direct input strength",
      "value": "0.505",
      "reason": "For using input video directly in input frames",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Normal map strength",
      "value": "0.500",
      "reason": "When combining with direct input processing",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Block swap for high res",
      "value": "40 blocks",
      "reason": "For 1280x720 resolution generations to avoid OOM",
      "from": "Valle"
    },
    {
      "setting": "CausVid parameters",
      "value": "9 steps, 1.0 cfg, 0.7 weight, 1 shift, euler, beta scheduler",
      "reason": "Sweet spot for CausVid - follows prompts as good as with cfg, looks good at 9 steps, blurry without shift",
      "from": "Ada"
    },
    {
      "setting": "VACE strength over time",
      "value": "List of floats with same length as steps",
      "reason": "Allows controlling VACE strength at different timesteps for more refined control",
      "from": "Kijai"
    },
    {
      "setting": "Second pass denoise",
      "value": "Much lower denoise (around 0.75 or less)",
      "reason": "High denoise like 0.75 keeps too much structure from input when doing second pass refinement",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Block swapping",
      "value": "30 for 4090 24GB",
      "reason": "To avoid memory allocation issues when increasing resolution",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Input video strength sweet spot",
      "value": "0.4-0.5",
      "reason": "0.3 loses facial details, need to find balance",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Optimal settings for gun shot scene",
      "value": "0.7 weight, uni_pc, kl_optimal, 1.0 cfg, 15 steps, 1 shift",
      "reason": "Found ideal settings for image to video without cfg issues",
      "from": "Ada"
    },
    {
      "setting": "CFG Schedule for liquid art lora",
      "value": "Use WanVideo CFG Schedule node",
      "reason": "CausVid severely limits lora strength, CFG schedule helps restore it",
      "from": "Zlikwid"
    },
    {
      "setting": "CFG",
      "value": "1",
      "reason": "Required when using 4-step LoRA with standard WAN T2V",
      "from": "JohnDopamine"
    },
    {
      "setting": "Reference image strength",
      "value": "Above 1.0 (with Causvid)",
      "reason": "Helps maintain character consistency when fighting against control inputs",
      "from": "Johnjohn7855"
    },
    {
      "setting": "Depth video strength",
      "value": "0.53-0.65",
      "reason": "Effects start above 0.53, becomes very strong above 0.6",
      "from": "Johnjohn7855"
    },
    {
      "setting": "Steps/Shift combination",
      "value": "4 steps, shift 4",
      "reason": "Lower shift improves hair consistency from reference, prevents loss of reference influence",
      "from": "boorayjenkins"
    },
    {
      "setting": "UniAnimate resolution",
      "value": "512x768",
      "reason": "Better pose detection success rate at this resolution",
      "from": "Guey.KhalaMari"
    },
    {
      "setting": "Pose detection threshold",
      "value": "0",
      "reason": "Can help detect poses when standard thresholds fail",
      "from": "Guey.KhalaMari"
    },
    {
      "setting": "Sampler",
      "value": "DMP++",
      "reason": "Gives different variations but minimal effect on reference/depth control balance",
      "from": "boorayjenkins"
    },
    {
      "setting": "Points to sample in spline",
      "value": "Number of steps",
      "reason": "For CFG and VACE strength scheduling, must match step count",
      "from": "Kijai"
    },
    {
      "setting": "MoviiGen VACE steps",
      "value": "6 steps with 0.3 strength",
      "reason": "Provides decent results without artifacts",
      "from": "The Punisher"
    },
    {
      "setting": "Input video strength for mouth preservation",
      "value": "0.6 minimum",
      "reason": "Maintains mouth movement integrity from original video",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Face mesh strength for hair movement",
      "value": "0.4",
      "reason": "Helps with hair movement but reduces mouth expressiveness",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Mask color for VACE",
      "value": "Gray RGB 127 or #808080",
      "reason": "Standard masking color for VACE processing",
      "from": "DawnII"
    },
    {
      "setting": "CausVid strength",
      "value": "0.3 with 8-12 steps",
      "reason": "Good balance between speed and quality for video generation",
      "from": "seitanism"
    },
    {
      "setting": "CausVid high speed setting",
      "value": "strength 0.5 with 5 steps",
      "reason": "Very fast generation but may show visible patterns",
      "from": "patientx"
    },
    {
      "setting": "VACE overlap frames",
      "value": "6-10 frames",
      "reason": "Provides good motion consistency between clips",
      "from": "ArtOfficial"
    },
    {
      "setting": "Frame limit before VRAM issues",
      "value": "160 frames at 720p",
      "reason": "Typical limit before running out of VRAM",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "setting": "CausVid LoRA strength with Phantom",
      "value": "around 1.0",
      "reason": "Phantom requires higher strength for CausVid to work effectively",
      "from": "zelgo_"
    },
    {
      "setting": "Wan LoRA training images",
      "value": "50 images at 1024x1024 for 14B",
      "reason": "Standard good amount for training on 14B model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "TeaCache with low step counts",
      "value": "Don't use TeaCache with 10 steps",
      "reason": "Too low step count means too much changes, TeaCache can't effectively skip frames",
      "from": "Cubey"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.5",
      "reason": "Optimal balance for quality and adherence",
      "from": "David Snow"
    },
    {
      "setting": "CFG Scale",
      "value": "1.0",
      "reason": "Important for speed, higher values significantly slow generation",
      "from": "David Snow"
    },
    {
      "setting": "Steps",
      "value": "6",
      "reason": "Works well with CausVid LoRA",
      "from": "David Snow"
    },
    {
      "setting": "VACE steps",
      "value": "12",
      "reason": "For high quality 960x720 output",
      "from": "Neex"
    },
    {
      "setting": "CausVid LoRA for VACE",
      "value": "0.3",
      "reason": "Good balance for face animation control",
      "from": "Neex"
    },
    {
      "setting": "Context overlap",
      "value": "32",
      "reason": "For longer video generation with good blending",
      "from": "VK (5080 128gb)"
    },
    {
      "setting": "Resize divisible by",
      "value": "16",
      "reason": "Only hard requirement for Wan",
      "from": "Kijai"
    },
    {
      "setting": "CausVid LoRA steps",
      "value": "6 steps",
      "reason": "Allows nice animations in 6 steps instead of 20-30",
      "from": "David Snow"
    },
    {
      "setting": "Vid2vid with CausVid denoise",
      "value": "0.50",
      "reason": "Follows prompt well and takes camera motion",
      "from": "hicho"
    },
    {
      "setting": "Blocks to swap",
      "value": "40",
      "reason": "For higher resolution processing with sufficient VRAM",
      "from": "Ruairi Robinson"
    },
    {
      "setting": "VACE blocks to swap",
      "value": "15 (max)",
      "reason": "Maximum setting available for VACE blocks",
      "from": "Ruairi Robinson"
    },
    {
      "setting": "WSL2 memory allocation",
      "value": "64GB memory, 16GB swap",
      "reason": "For 128GB system RAM in WSL2 configuration",
      "from": "jerms_ai_(4090_24g)"
    },
    {
      "setting": "Base precision",
      "value": "fp16 instead of bf16",
      "reason": "Better memory efficiency, no point to upcast",
      "from": "Kijai"
    },
    {
      "setting": "Use_non_blocking",
      "value": "disabled",
      "reason": "Reserves less RAM, helps with memory issues",
      "from": "Kijai"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.3-0.4",
      "reason": "Above 0.4 reduces VACE control strength and prevents mouth movement",
      "from": "Kijai"
    },
    {
      "setting": "VACE strength",
      "value": "1.2",
      "reason": "Can balance out high CausVid LoRA strength",
      "from": "Kijai"
    },
    {
      "setting": "Steps for CausVid",
      "value": "8",
      "reason": "Used with 0.3-0.4 LoRA strength and shift of 1",
      "from": "A.I.Warper"
    },
    {
      "setting": "Resolution for human portraits",
      "value": "512x512",
      "reason": "Works best for human portrait and closeups",
      "from": "hicho"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.5 default, 0.3-0.6 range",
      "reason": "Higher values make generations look artificial",
      "from": "David Snow"
    },
    {
      "setting": "CFG with CausVid",
      "value": "1.0",
      "reason": "Other values dramatically increase generation times",
      "from": "David Snow"
    },
    {
      "setting": "AccVideo scheduler steps",
      "value": "50 (uses 10 actual)",
      "reason": "Required setting for AccVideo scheduler to work",
      "from": "David Snow"
    },
    {
      "setting": "VACE maximum frames",
      "value": "161 frames",
      "reason": "Can go this high without degradation on 5090 with 32GB",
      "from": "Piblarg"
    },
    {
      "setting": "Character LoRA training dataset",
      "value": "10-15 images",
      "reason": "2 front, 2 each side, 2 back, 2 3/4 turn, plus angles",
      "from": "Thom293"
    },
    {
      "setting": "AccVid LoRA strength",
      "value": "1.0 for regular use, 1.7 for 6 steps",
      "reason": "Optimal performance balance",
      "from": "\ud83e\udd99rishappi, Kijai"
    },
    {
      "setting": "AccVid steps and CFG",
      "value": "10 steps, CFG 1.0",
      "reason": "Standard AccVid configuration",
      "from": "Kijai"
    },
    {
      "setting": "AccVid T2V steps",
      "value": "4 steps at 1.0 strength, 5-10 steps generally",
      "reason": "Fast generation with good quality",
      "from": "Kijai, slmonker(5090D 32GB)"
    },
    {
      "setting": "CausVid configuration",
      "value": "CFG 1, 5 steps",
      "reason": "Standard CausVid setup",
      "from": "Boop"
    },
    {
      "setting": "VRAM optimization",
      "value": "Keep VRAM usage at 96-98%, not 99%",
      "reason": "99% usage makes generation very slow",
      "from": "N0NSens"
    },
    {
      "setting": "Context options default",
      "value": "81 frame contexts with 16 frame overlap",
      "reason": "Default settings work fine for longer videos",
      "from": "David Snow"
    },
    {
      "setting": "AccVid LoRA strength",
      "value": "1.5",
      "reason": "Used in combination with CausVid LoRA",
      "from": "David Snow"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "3.5 range",
      "reason": "Too strong negates VACE controls",
      "from": "Guey.KhalaMari"
    },
    {
      "setting": "Wan sampler",
      "value": "Euler",
      "reason": "Better overall than UniPC",
      "from": "David Snow"
    },
    {
      "setting": "Resolution for Phantom 14B",
      "value": "720p",
      "reason": "Original training resolution, 768 works but 720 is preferred",
      "from": "Kijai"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.5",
      "reason": "Prevents artifacts and flicker issues",
      "from": "Kijai"
    },
    {
      "setting": "CausVid LoRA strength for RTX 4090",
      "value": "1.1",
      "reason": "Works fine with 5-6 steps instead of 4",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "setting": "TeaCache threshold",
      "value": "0.2",
      "reason": "Optimal performance balance",
      "from": "Kijai"
    },
    {
      "setting": "VACE default shift",
      "value": "16",
      "reason": "Default recommended value, lower values can cause artifacts",
      "from": "Kijai"
    },
    {
      "setting": "Phantom + VACE + CausVid CFG",
      "value": "Phantom CFG: 5, Sampler CFG: 1",
      "reason": "Decent results for combined workflow",
      "from": "DawnII"
    },
    {
      "setting": "CausVid I2V configuration",
      "value": "0.5 strength, 12 steps, 2.5-3 CFG for first steps",
      "reason": "Works pretty good with I2V at 720p",
      "from": "\u30dc\u30b0\u30c0\u30f3\u304a\u3058\u3055\u3093"
    },
    {
      "setting": "Merged model configuration",
      "value": "CFG: 1, Steps: 6-7",
      "reason": "For merged CausVid + AccVid model, no LoRA needed",
      "from": "JohnDopamine"
    },
    {
      "setting": "Phantom CFG",
      "value": "1",
      "reason": "Full model requires CFG=1",
      "from": "JohnDopamine"
    },
    {
      "setting": "Phantom steps",
      "value": "40 minimum",
      "reason": "Below 40 steps is very artifacty, 40 still has some artifacts",
      "from": "aikitoria"
    },
    {
      "setting": "Merged model steps",
      "value": "6-7 steps",
      "reason": "JohnDopamine's CausVid/AccVid/Phantom merge works with fewer steps",
      "from": "JohnDopamine"
    },
    {
      "setting": "freenoise",
      "value": "false",
      "reason": "Better for videos longer than 81 frames and consistent results",
      "from": "chrisd0073"
    },
    {
      "setting": "Resolution",
      "value": "1024x576",
      "reason": "WAY better output vs 864x480 and much better vs 960x544",
      "from": "CaptHook"
    },
    {
      "setting": "Steps",
      "value": "8",
      "reason": "6 steps showed occasional morphing, 10 steps too long with too much contrast",
      "from": "CaptHook"
    },
    {
      "setting": "Shift",
      "value": "5",
      "reason": "1-3 too low causes morphing, 8-10 too high causes occasional morphing and too much contrast",
      "from": "CaptHook"
    },
    {
      "setting": "AccVid LoRA strength",
      "value": "1.0-1.5",
      "reason": "Little difference beyond 1.0, but 1.5 works well with CausVid 0.5",
      "from": "Johnjohn7855"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.4-0.5",
      "reason": "Higher than 0.5 makes flashes inevitable",
      "from": "Johnjohn7855"
    },
    {
      "setting": "CFG with AccVid+CausVid",
      "value": "3.5",
      "reason": "Better prompt following than CFG 1, CFG 6 burns it, CFG 2 kills prompt following",
      "from": "Ada"
    },
    {
      "setting": "Frames for Phantom",
      "value": "49 or 81",
      "reason": "Standard frame counts that work well",
      "from": "Johnjohn7855"
    },
    {
      "setting": "FPS",
      "value": "16",
      "reason": "Works well with 1024x576 resolution for 8.5min generation time",
      "from": "CaptHook"
    },
    {
      "setting": "MoviiGen LoRA strength",
      "value": "0.4-0.7",
      "reason": "Higher values lose ID consistency to reference image",
      "from": "Johnjohn7855"
    },
    {
      "setting": "AccVideo + CausVid",
      "value": "accvideo: 1.5, causvid: 0.5",
      "reason": "Recommended combination for speed improvement",
      "from": "David Snow"
    },
    {
      "setting": "Context overlap frames",
      "value": "32",
      "reason": "Much better results than default overlap",
      "from": "Nekodificador"
    },
    {
      "setting": "CFGDistill LoRA strength",
      "value": "0.1",
      "reason": "Way too strong at higher values, affects reference image details",
      "from": "Johnjohn7855"
    },
    {
      "setting": "CausVid strength when flashing occurs",
      "value": "0.4",
      "reason": "Helps reduce flashing issues",
      "from": "Johnjohn7855"
    },
    {
      "setting": "Block 0 disable",
      "value": "off",
      "reason": "Helps with flashes when using LoRA block edit",
      "from": "David Snow"
    },
    {
      "setting": "CausVid LoRA strength",
      "value": "0.5",
      "reason": "Standard strength for speed improvement",
      "from": "David Snow"
    },
    {
      "setting": "AccVid LoRA strength",
      "value": "1.5",
      "reason": "Recommended strength, though preferences vary",
      "from": "David Snow"
    },
    {
      "setting": "CFG with speed LoRAs",
      "value": "1.0",
      "reason": "When using CausVid and AccVid LoRAs",
      "from": "David Snow"
    },
    {
      "setting": "Denoise for input video bleed",
      "value": "0.85",
      "reason": "Allows some input video to help model understand face location",
      "from": "David Snow"
    },
    {
      "setting": "Input video strength",
      "value": "0.3-0.4",
      "reason": "Helps with mouth movement and character positioning",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "detail_wanz",
      "value": "0.4 and 1.0 tested",
      "reason": "Testing different detail levels",
      "from": "hau"
    },
    {
      "setting": "Skip Layer Guidance blocks",
      "value": "9,10 with 0.0 to 0.9",
      "reason": "SLG configuration for improved results",
      "from": "hau"
    },
    {
      "setting": "CFG for Phantom without causvid",
      "value": "7.5",
      "reason": "Default setting that works well",
      "from": "aikitoria"
    },
    {
      "setting": "Shift for Phantom",
      "value": "5",
      "reason": "Reduced from 10 for better performance",
      "from": "David Snow"
    },
    {
      "setting": "Phantom reference strength",
      "value": "~0.2",
      "reason": "Helps recover likeness when using reduced lora strength",
      "from": "DeZoomer"
    },
    {
      "setting": "Denoising for motion preservation",
      "value": "0.10-0.15",
      "reason": "Low denoising helps preserve motion in video-to-video",
      "from": "hicho"
    },
    {
      "setting": "Block swap optimization",
      "value": "Adjust until 95% VRAM used",
      "reason": "Optimal memory usage without slowdown",
      "from": "Kijai"
    },
    {
      "setting": "CausVid with merged model",
      "value": "7 steps, 1.0 CFG",
      "reason": "Fast generation with distillation built in",
      "from": "Thom293"
    },
    {
      "setting": "Phantom 14B without CausVid",
      "value": "30+ steps recommended",
      "reason": "16 steps not enough without distillation",
      "from": "Kijai"
    },
    {
      "setting": "VACE + Phantom inpainting",
      "value": "0.8 VACE strength, cut at 0.2",
      "reason": "Allows Phantom superior reference fidelity to come through",
      "from": "Zuko"
    },
    {
      "setting": "PerpNegGuider neg_scale",
      "value": "0.25-0.5 recommended",
      "reason": "Controls divergence from input image while retaining subject context",
      "from": "Ablejones"
    },
    {
      "setting": "CausVid LoRA block application",
      "value": "First block 0.5 max, rest 1.0",
      "reason": "Most important part is reducing first block application",
      "from": "Kijai"
    },
    {
      "setting": "CFG with CausVid distilled",
      "value": "1.0",
      "reason": "Distillation includes CFG distillation",
      "from": "Kijai"
    },
    {
      "setting": "Phantom CFG",
      "value": "5.0",
      "reason": "Original paper setting",
      "from": "Guey.KhalaMari"
    },
    {
      "setting": "CausVid v2 CFG scheduling",
      "value": "5.0 start, end percent 0.5",
      "reason": "Prevents flash while maintaining speed",
      "from": "Ada"
    },
    {
      "setting": "Wan21-I2V-14B-480P with CausVid",
      "value": "LORA: 1.0, steps: 4, cfg: 1, shift 0.30, scheduler: euler/beta",
      "reason": "Good parameters found through testing",
      "from": "Mngbg"
    },
    {
      "setting": "CausVid strength with AccVid",
      "value": "CausVid 0.5 + AccVid 0.5",
      "reason": "Good balance found",
      "from": "\u25b2"
    },
    {
      "setting": "CFG with distilled for early steps",
      "value": "3-4 CFG for 2-4 steps with 6-8 total steps",
      "reason": "Most impactful at early steps",
      "from": "Kijai"
    },
    {
      "setting": "CausVid v2 strength",
      "value": "1.0",
      "reason": "Can handle full strength without motion destruction",
      "from": "Kijai"
    },
    {
      "setting": "CFG scheduling for CausVid v2",
      "value": "Start 5, end 1, stop at 0.3, 6 steps",
      "reason": "Avoid over-saturation while maintaining quality",
      "from": "Johnjohn7855"
    },
    {
      "setting": "Steps for fast generation",
      "value": "8 steps",
      "reason": "Good balance of speed and quality",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "ATI generation",
      "value": "CFG 1",
      "reason": "Works well with trajectory control",
      "from": "Kijai"
    },
    {
      "setting": "MoviiGen LoRA",
      "value": "0.3 + 0.6 detailz lora",
      "reason": "Improves quality without over-processing",
      "from": "mamad8"
    },
    {
      "setting": "Steps",
      "value": "8 steps",
      "reason": "Works well with Phantom and CausVid V2",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "LoRA merge strength",
      "value": "1.0",
      "reason": "Full strength merging works effectively for multiple LoRAs",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Block swapping",
      "value": "5 blocks",
      "reason": "Enables 1280x720 generation without OOM",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "CFG with CausVid",
      "value": "1",
      "reason": "CausVid is tuned for CFG 1",
      "from": "MilesCorban"
    },
    {
      "setting": "Shift",
      "value": "8",
      "reason": "Standard setting that works well",
      "from": "MilesCorban"
    },
    {
      "setting": "Film grain parameters",
      "value": "0.03 for all three parameters",
      "reason": "Higher values create too much effect",
      "from": "Johnjohn7855"
    },
    {
      "setting": "CausVid v2 CFG",
      "value": ">1 with scheduling",
      "reason": "Tuned on T2V, needs different approach than v1",
      "from": "Johnjohn7855"
    },
    {
      "setting": "ATI points",
      "value": "121 points for 81 frames",
      "reason": "Prevents cutting off the end",
      "from": "Kijai"
    },
    {
      "setting": "RIFE multiplier minimum",
      "value": "2",
      "reason": "Setting to 1 doesn't add frames",
      "from": "Johnjohn7855"
    },
    {
      "setting": "Normal Craft strength",
      "value": "0.8 on encoder",
      "reason": "Follows expressions well",
      "from": "HeadOfOliver"
    }
  ],
  "concepts": [
    {
      "term": "TeaCache",
      "explanation": "Optimization that skips certain conditional and unconditional steps during generation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "term": "Context windows",
      "explanation": "Window size for processing longer videos, should be something model can do (81 default)",
      "from": "Kijai"
    },
    {
      "term": "Latent frame ratio",
      "explanation": "Each latent contains 4 frames in WanVideo",
      "from": "Draken"
    },
    {
      "term": "SLG",
      "explanation": "Setting that requires lower CFG values (half of normal) for proper results",
      "from": "Miku"
    },
    {
      "term": "DG Wan models",
      "explanation": "Distilled versions of 1.3B with extra tweaks, requires less steps than base models. Multiple versions available with Ares V3 being current favorite",
      "from": "David Snow"
    },
    {
      "term": "LoRA timestep distribution",
      "explanation": "Different training approaches - sigmoid (central steps for character), linear, shift (start/end for style). Affects which timesteps the LoRA focuses on",
      "from": "mamad8"
    },
    {
      "term": "4+1 latent rule",
      "explanation": "For frame to latent conversion: latentCount = (frameCount - 1) / 4, always need the +1, so 33 frames = 8 latents",
      "from": "Kijai"
    },
    {
      "term": "Context frames",
      "explanation": "Number of frames per batch when doing longer animations - think of it as doing longer animations in batches, blends based on frame overlap setting",
      "from": "David Snow"
    },
    {
      "term": "VACE control map specificity",
      "explanation": "VACE requires very specific control maps that match training data - if control map isn't nearly exactly how it appeared during training, doesn't work well",
      "from": "Rishi Pandey"
    },
    {
      "term": "VACE module",
      "explanation": "Separate module that works with any 1.3B model, not a full model itself",
      "from": "Kijai"
    },
    {
      "term": "DF (Diffusion Forcing)",
      "explanation": "Method for extending videos beyond 5 seconds by chaining generations",
      "from": "MilesCorban"
    },
    {
      "term": "Style transfer without structure",
      "explanation": "New method that transfers visual style while preserving original structure/movement",
      "from": "Clownshark Batwing"
    },
    {
      "term": "Frame looping in Wan",
      "explanation": "Wan models loop back at certain frame counts - 81 for vanilla, 97/121 for Skyreels variants",
      "from": "N0NSens"
    },
    {
      "term": "TeaCache non_blocking",
      "explanation": "Parameter that controls memory allocation method - non_blocking reserves more RAM",
      "from": "Kijai"
    },
    {
      "term": "FLF2V",
      "explanation": "First-Frame-Last-Frame morphing technique, described as best start-end method",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "term": "CausVid",
      "explanation": "Distilled version of Wan 14B with step and CFG distillation for faster inference",
      "from": "yi"
    },
    {
      "term": "Flex Attention",
      "explanation": "Memory optimization technique that requires torch.compile and specific frame per block settings",
      "from": "Kijai"
    },
    {
      "term": "ACE++ Fill",
      "explanation": "Inpainting technique better than standard Fill for consistent character workflows",
      "from": "Draken"
    },
    {
      "term": "Flex attention",
      "explanation": "Relatively new fully customizable attention in pytorch, uses BlockMask for adjacent frame attention",
      "from": "Kijai"
    },
    {
      "term": "kv_cache in CausVid",
      "explanation": "How sliding windows work, enables endless generation and real-time preview",
      "from": "Kijai"
    },
    {
      "term": "Block mask",
      "explanation": "Used with flex attention to limit attention to adjacent frames in sequence dimension",
      "from": "Kijai"
    },
    {
      "term": "R2V",
      "explanation": "Reference to video - input person without forcing first frame",
      "from": "Juampab12"
    },
    {
      "term": "VACE reference vs start image",
      "explanation": "Two separate concepts - reference image for style/subject, start image for temporal beginning",
      "from": "Kijai"
    },
    {
      "term": "VACE latent placement",
      "explanation": "VACE places images in latents rather than using cross-attention embeds like traditional I2V",
      "from": "Kijai"
    },
    {
      "term": "VACE temporal inpainting",
      "explanation": "Can inpaint spatially and temporally, meaning I2V would be inpainting every other frame besides first",
      "from": "Kijai"
    },
    {
      "term": "VACE as ControlNet",
      "explanation": "Should think of VACE as a controlnet for T2V model rather than standalone",
      "from": "Kijai"
    },
    {
      "term": "Block swapping",
      "explanation": "Technique to reduce VRAM usage by swapping model blocks to system memory",
      "from": "Kijai"
    },
    {
      "term": "Context batches",
      "explanation": "Each context acts as a batch (81 frames default) that blends with others based on overlap",
      "from": "David Snow"
    },
    {
      "term": "CausVid LoRA",
      "explanation": "Extracted version of CausVid model that can be used as LoRA with base T2V 14B model for faster inference",
      "from": "Kijai"
    },
    {
      "term": "VACE reference vs first frame",
      "explanation": "Reference image is NOT first frame - if you have first frame to animate, put it in control images with input mask",
      "from": "Kijai"
    },
    {
      "term": "FLF2V",
      "explanation": "First Last Frame To Video - start/end frame model with weird naming, 720p only",
      "from": "Kijai"
    },
    {
      "term": "Block masking training",
      "explanation": "CausVid trained with 3 latent windows, causing quality loss when used differently",
      "from": "Kijai"
    },
    {
      "term": "VACE block skipping",
      "explanation": "Technique to avoid applying certain LoRAs to blocks used by VACE control",
      "from": "Kijai"
    },
    {
      "term": "Clip vision embeds",
      "explanation": "Used in I2V models for image conditioning, affects all frames throughout generation",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Method for generating long videos by processing overlapping segments",
      "from": "Kijai"
    },
    {
      "term": "CausVid",
      "explanation": "Distilled model for both CFG and steps, allows cfg 1.0 and low steps (2-4) for much faster generation",
      "from": "Draken"
    },
    {
      "term": "VACE blocks",
      "explanation": "Can be disabled for LoRA to allow more motion transfer while reducing adverse effects",
      "from": "Kijai"
    },
    {
      "term": "Model dtype for DWPose",
      "explanation": "DWPose embeds should match model dtype instead of fp32 to prevent crashes with long clips",
      "from": "Kijai"
    },
    {
      "term": "VACE",
      "explanation": "Video control system that is essentially inpainting a panel, works with style transfer, inpainting, subject-driven, outpainting",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "term": "CausVid",
      "explanation": "Converts bidirectional attention models into causal attention models, then distills them using DMD2 method",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "term": "Causal vs Bidirectional attention",
      "explanation": "Causal attention only attends to past frames (like ChatGPT), bidirectional attends to both past and future frames",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "term": "Diffusion Forcing",
      "explanation": "Hybrid between diffusion and autoregressive methods",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "term": "Shift parameter",
      "explanation": "Has to do with frame interpolation - more dynamic or more subtle, need to find balance with CFG",
      "from": "chrisd0073"
    },
    {
      "term": "Distorch",
      "explanation": "Like block swap, offloads nearly entire model with only 10% speed loss, gives more space for latent/video so higher res or frames possible",
      "from": "The Punisher"
    },
    {
      "term": "K quants vs 0 quants",
      "explanation": "K quants are generally better/more efficient than Q4_0, should always use K quants when available",
      "from": "The Punisher"
    },
    {
      "term": "CFG distillation",
      "explanation": "Models trained to work at CFG 1.0, doubling speed by eliminating negative prompt computation",
      "from": "Draken"
    },
    {
      "term": "Subject bleed",
      "explanation": "When using multiple subjects in VACE reference, characteristics may blend between subjects",
      "from": "DawnII"
    },
    {
      "term": "Fun LoRAs vs regular models",
      "explanation": "LoRAs trained on Fun models may cause key errors on regular models but still function",
      "from": "David Snow"
    },
    {
      "term": "Shift parameter",
      "explanation": "Controls detail quality in CausVid, values too high cause blur",
      "from": "Jemmo"
    },
    {
      "term": "Reward LoRAs",
      "explanation": "LoRAs trained to improve motion and quality, HPS and MPS variants available",
      "from": "yi"
    },
    {
      "term": "Eta in custom sampler",
      "explanation": "Amount of noise added after each step, 0.0 is ODE like dpmpp_2m, >0.0 is SDE like dpmpp_2s_a",
      "from": "Clownshark Batwing"
    },
    {
      "term": "MoviiGen",
      "explanation": "Fine-tuned WAN T2V model for enhancing film and television styles, like WAN with integrated LoRA",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "term": "Inverted lineart",
      "explanation": "Alternative to control pose that works with VACE when regular lineart alone doesn't work",
      "from": "Nokai"
    },
    {
      "term": "Block swap memory",
      "explanation": "Memory management technique - shows transformer blocks on CPU vs GPU, enables running larger models with limited VRAM",
      "from": "N0NSens"
    },
    {
      "term": "Context switching in video generation",
      "explanation": "When generating long videos, the model processes in chunks (e.g. 81 frames), causing potential jumps between segments",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "Samples input",
      "explanation": "Input method for encoded video frames in V2V workflows, requires denoise below 1.0",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "Block swap",
      "explanation": "Memory management technique that keeps some transformer blocks on CPU to reduce VRAM usage",
      "from": "N0NSens"
    },
    {
      "term": "Safe checkpoint loading",
      "explanation": "Security feature that validates model checkpoints, some models like Sapiens require disabling this",
      "from": "MilesCorban"
    },
    {
      "term": "MoviiGen",
      "explanation": "A cinematic finetune of WAN",
      "from": "yo9o"
    },
    {
      "term": "VACE module vs full model",
      "explanation": "Module only works with KJ wrapper, full model compatible with both native and wrapper",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "Inverted vs non-inverted face landmarks",
      "explanation": "White on black rather than black on white landmark detection",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "end_percentage in VACE",
      "explanation": "Parameter that stops the control at a certain point during the whole generation process",
      "from": "Draken"
    },
    {
      "term": "Context in video generation",
      "explanation": "Feature allowing longer video generation by chunking, may lose some information between chunks",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "Block swapping",
      "explanation": "Technique to manage VRAM by swapping model blocks to system RAM, slower but allows larger models",
      "from": "BestWind"
    },
    {
      "term": "Every 5th block disabling",
      "explanation": "VACE applies to every 5th block on 14B, so disabling reduces interference with VACE inputs",
      "from": "DawnII"
    },
    {
      "term": "SLG and Zero Star optimizations",
      "explanation": "These optimizations do nothing with CFG 1.0 as their code is never executed",
      "from": "Kijai"
    },
    {
      "term": "VACE embed chaining",
      "explanation": "Chain multiple VACE encoders together like ControlNets for cumulative effects",
      "from": "David Snow"
    },
    {
      "term": "CausVid LoRA distillation",
      "explanation": "They are using brute force distillation rather than the causal aspect, not using proper causal sampling method",
      "from": "Kijai"
    },
    {
      "term": "VACE frame batching",
      "explanation": "Latents are processed in batches of 4, plus one for init frame, which is why it adds extra frames",
      "from": "Cubey"
    },
    {
      "term": "Double VACE encode",
      "explanation": "Using two separate VACE encode nodes with different control types (like depth and pose) and compositing the results for better output quality",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "term": "WAN FPS behavior",
      "explanation": "WAN models are trained at 16fps (24fps for Skyreels). Changing FPS in video output just changes playback speed, not generation framerate. 16 frames always equals 1 second of model time",
      "from": "Juampab12"
    },
    {
      "term": "TeaCache memory issue",
      "explanation": "When doing .clone() then .to(device), it reserves VRAM unnecessarily. Moving to CPU first then cloning frees the memory properly",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Method to generate videos longer than base limit by running multiple 81-frame generations and blending seams, uses same memory as single 81-frame gen",
      "from": "Kijai"
    },
    {
      "term": "VACE input mask",
      "explanation": "Black and white mask where black areas are kept as-is, white areas are regenerated",
      "from": "Kijai"
    },
    {
      "term": "CausVid LoRA",
      "explanation": "Speed optimization LoRA that makes inference time bearable for larger models",
      "from": "Kijai"
    },
    {
      "term": "VACE recolorizing mode",
      "explanation": "When giving VACE plain input footage at 1.0 strength with no control frame, it will 'recolorize' the video - one of the tasks VACE was trained on",
      "from": "Piblarg"
    },
    {
      "term": "Video extension with last frames",
      "explanation": "Take the last N frames (like 20) of a video you want to extend and put them in start images to create seamless extension",
      "from": "Piblarg"
    },
    {
      "term": "Context adapter blocks",
      "explanation": "Alternative to full model fine-tuning, uses Res-Tuning scheme to inject tokens into transformer blocks for plug-and-play editing",
      "from": "AJO"
    },
    {
      "term": "VACE mask behavior",
      "explanation": "Black areas use input directly, white areas use controlnet mode. Different from traditional controlnet masking",
      "from": "Draken"
    },
    {
      "term": "First frame restyle",
      "explanation": "Technique of using controlnets to create better aligned starting frames, then removing them for longer runs",
      "from": "Draken"
    },
    {
      "term": "Relative head motion",
      "explanation": "Custom technique that normalizes reference head and transforms it to target head, with relative movement calculated as diff from first frame to current frame",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "term": "Context window frame math",
      "explanation": "Formula is (4*n)+1 for proper frame count with context window",
      "from": "A.I.Warper"
    },
    {
      "term": "VACE reference image processing",
      "explanation": "Multiple references are better combined into single image or batched horizontally rather than chained",
      "from": "DawnII"
    },
    {
      "term": "Block swapping",
      "explanation": "Helps with memory management but can be adjusted - start high and lower until first step generates",
      "from": "MilesCorban"
    },
    {
      "term": "VACE embed vs encode nodes",
      "explanation": "Use VACE embed node instead of encode node for proper functionality, especially with control inputs",
      "from": "Kijai"
    },
    {
      "term": "Regional LoRAs",
      "explanation": "Using masks to drive LoRAs for selective application to different parts of the image/video",
      "from": "sneako1234"
    },
    {
      "term": "Control input battle",
      "explanation": "When multiple control inputs (reference image, depth, pose) compete for influence, requiring careful strength balancing",
      "from": "Johnjohn7855"
    },
    {
      "term": "Pose retargeting",
      "explanation": "Process of mapping pose from one character to another, often causes issues like extended arms due to proportion differences",
      "from": "Kijai"
    },
    {
      "term": "VACE start/end frame morphing",
      "explanation": "Method using start and end frame images with control data to create smooth transitions between different characters or states",
      "from": "David Snow"
    },
    {
      "term": "Context windows vs batching",
      "explanation": "Two different approaches to handling long sequences - context windows process full length but lose consistency, batching maintains consistency but requires stitching",
      "from": "A.I.Warper"
    },
    {
      "term": "Face blending with pose control",
      "explanation": "Combining DWPose body detection with MediaPipe face detection at blended strengths for better facial animation",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "term": "Block swapping",
      "explanation": "Technique for managing VRAM by offloading model parts to system RAM, handled automatically in native ComfyUI",
      "from": "zelgo_"
    },
    {
      "term": "Context sliding",
      "explanation": "Method for generating longer videos by using overlapping context from previous frames",
      "from": "Draken"
    },
    {
      "term": "Digi-doubles",
      "explanation": "VFX term for digital replicas of humans or products without training custom LoRAs",
      "from": "humangirltotally"
    },
    {
      "term": "VACE reference image handling",
      "explanation": "VACE can only use single reference image which gets inserted as first latent, multiple images get concatenated automatically",
      "from": "Kijai"
    },
    {
      "term": "Causal model KV caching",
      "explanation": "Causal models get access to additional speedup called KV caching for faster inference",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "term": "VACE mask values",
      "explanation": "For input mask: white is active, black is inactive. For input frames: gray is empty value to inpaint",
      "from": "Kijai"
    },
    {
      "term": "CausVid dual functionality",
      "explanation": "CausVid does both distillation and conversion to causal, but Wan CausVid in ComfyUI only has distillation so far",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "term": "Block swap",
      "explanation": "Slows things down, goal is to use as few blocks as possible, not for speed improvement",
      "from": "David Snow"
    },
    {
      "term": "Differential diffusion",
      "explanation": "Method for compositing masked changes back into original video",
      "from": "David Snow"
    },
    {
      "term": "Set latent noise mask",
      "explanation": "Node required for inpainting when using native ComfyUI workflows",
      "from": "David Snow"
    },
    {
      "term": "Context overlap",
      "explanation": "Blend between batches when generating longer animations in segments",
      "from": "David Snow"
    },
    {
      "term": "CausVid LoRA distillation",
      "explanation": "Distills step and cfg, culling anything that isn't 'solid' in network knowledge and making network converge in fewer steps",
      "from": "Draken"
    },
    {
      "term": "Flow edit",
      "explanation": "Vid2vid with LoRA and flow edit for changing only specific parts like faces",
      "from": "chrisd0073"
    },
    {
      "term": "Block swap",
      "explanation": "Memory management technique to swap model blocks between VRAM and RAM",
      "from": "Kijai"
    },
    {
      "term": "VACE mask colors",
      "explanation": "White = active area, black = inactive area (kept as is). For inpainting, black area is preserved",
      "from": "Kijai"
    },
    {
      "term": "Control video frame colors",
      "explanation": "Empty frames should be gray (0.5 or 127,127,127), keyframes should be black, new frames white on mask",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Puts most of model into regular RAM and swaps when needed during sampling. Don't need until hitting allocation errors",
      "from": "Kijai"
    },
    {
      "term": "AccVideo",
      "explanation": "Distillation technique similar to CausVid that allows faster inference",
      "from": "Draken"
    },
    {
      "term": "VACE separate layers",
      "explanation": "VACE applies itself over WAN as separate layers, so T2V LoRA training should work fine",
      "from": "Piblarg"
    },
    {
      "term": "Frame overlap blending",
      "explanation": "Cross-fading overlapping frames to reduce color shifts and seams in extended videos",
      "from": "Piblarg"
    },
    {
      "term": "Stride in context",
      "explanation": "How many frames the model skips when picking frames. Stride = 1 uses every frame (smooth, detailed, slower), Stride = 4 uses every 4th frame (faster, less smooth)",
      "from": "chrisd0073"
    },
    {
      "term": "RAM Tag color scheme",
      "explanation": "Color scheme from recognize anything annotator that VACE is trained with, used for layout control inputs",
      "from": "Kijai"
    },
    {
      "term": "Block swapping",
      "explanation": "Higher blocks = more RAM used, less VRAM used. Blocks swap data from VRAM to RAM. More swaps = longer generation but helps with VRAM limitations",
      "from": "N0NSens"
    },
    {
      "term": "Pasteback",
      "explanation": "Image composite technique using mask to avoid VAE compression in inpainting",
      "from": "A.I.Warper"
    },
    {
      "term": "Vace_end_percent",
      "explanation": "Parameter that controls inpainting blending, lower values like 0.80 blend better than 1.0",
      "from": "VK (5080 128gb)"
    },
    {
      "term": "Sliding context windows",
      "explanation": "Method to generate longer videos without increasing memory use by keeping constant sampled frames",
      "from": "Kijai"
    },
    {
      "term": "Riflex",
      "explanation": "Breaks looping of positional embeds for frames beyond 81, doesn't affect memory use",
      "from": "Kijai"
    },
    {
      "term": "TeaCache",
      "explanation": "Caching system that skips certain steps during sampling to improve performance",
      "from": "Kijai"
    },
    {
      "term": "TAEW",
      "explanation": "Tiny Auto Encoder WAN - provides much higher quality preview but slightly slower",
      "from": "Kijai"
    },
    {
      "term": "Reference latents in preview",
      "explanation": "References are embedded in the latents, causing flashing between reference images and generated content in preview",
      "from": "Kijai"
    },
    {
      "term": "freenoise",
      "explanation": "Controls noise addition - true uses random noise for natural motion, false reuses same noise for consistency and looping",
      "from": "chrisd0073"
    },
    {
      "term": "Phantom reference latents",
      "explanation": "T2V model that takes reference latents as input rather than pure text-to-video",
      "from": "Kijai"
    },
    {
      "term": "Wan flash",
      "explanation": "Predictable flash/contrast shift that appears at beginning of Wan videos, especially with CausVid LoRA",
      "from": "David Snow"
    },
    {
      "term": "LoRA block selection",
      "explanation": "Technique to apply LoRA to specific blocks only, can eliminate flash by avoiding first blocks",
      "from": "Kijai"
    },
    {
      "term": "Uni3C controlnet",
      "explanation": "Camera control system that can transfer motion from existing videos even without camera embeds",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Method for extending video duration, potentially works with Phantom",
      "from": "Kijai"
    },
    {
      "term": "Empty_frame_level",
      "explanation": "Parameter in context processing, specific function unclear from discussion",
      "from": "Yae"
    },
    {
      "term": "Uni3C",
      "explanation": "Controlnet that transfers camera movements from reference videos to generated content",
      "from": "N0NSens"
    },
    {
      "term": "Phantom latents",
      "explanation": "Character embedding system that provides high consistency with reference images",
      "from": "David Snow"
    },
    {
      "term": "VACE mask format",
      "explanation": "VACE expects mask videos in grayscale format for proper inpainting",
      "from": "David Snow"
    },
    {
      "term": "Uni3C architecture limitation",
      "explanation": "Uni3C constructs latent from I2V input (36 channels), making it incompatible with T2V models currently",
      "from": "Kijai"
    },
    {
      "term": "VACE reference latent addition",
      "explanation": "VACE adds extra latent when using reference image, which can cause frame count mismatches",
      "from": "Kijai"
    },
    {
      "term": "Skip Layer Guidance (SLG)",
      "explanation": "Feature supported by WanVideoWrapper with configurable arguments",
      "from": "hau"
    },
    {
      "term": "Temporal mask in WanVideo ImageToVideo Encode",
      "explanation": "Not implemented yet, would allow masking active/inactive frames for temporal inpainting, only works with fun inp models",
      "from": "Kijai"
    },
    {
      "term": "Block swapping",
      "explanation": "Technique that offloads some GPU data to system RAM, slowing down the process but allowing larger models to run",
      "from": "Thom293"
    },
    {
      "term": "Crop&stitch",
      "explanation": "Method to normalize face sizes by cropping and scaling faces then pasting back, similar to old deep face lab scripts",
      "from": "DeZoomer"
    },
    {
      "term": "Perpendicular guidance",
      "explanation": "Uses normal negative and empty negative to get negative component more unrelated to positive embedding",
      "from": "Ablejones"
    },
    {
      "term": "negative_img_text",
      "explanation": "Negative without the image component, acts like empty negative for PerpNegGuider",
      "from": "Ablejones"
    },
    {
      "term": "Diffdiff latent masking",
      "explanation": "Needs blurred mask for encoding latents, different from VACE input frames which need hard edges",
      "from": "Zuko"
    },
    {
      "term": "CFG distillation",
      "explanation": "Process included in CausVid and AccVid that allows using CFG 1.0 (effectively disabled)",
      "from": "Kijai"
    },
    {
      "term": "First block effect",
      "explanation": "First block of CausVid LoRA causes flash artifacts, removing it retains motion without flash",
      "from": "Kijai"
    },
    {
      "term": "Phantom CFG",
      "explanation": "Additional CFG parameter specific to Phantom model that causes 3x slower generation",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Technique with 40 blocks (0-39) for training longer videos",
      "from": "Kijai"
    },
    {
      "term": "Block 0 removal",
      "explanation": "Removing first block from CausVid LoRA to improve results, now built into v2",
      "from": "Kijai"
    },
    {
      "term": "ATI trajectory control",
      "explanation": "System using anchor points (stationary and moving) to direct motion in video generation",
      "from": "Kijai"
    },
    {
      "term": "Differential diffusion",
      "explanation": "Method to apply denoising only to masked areas of video frames",
      "from": "David Snow"
    },
    {
      "term": "CFG scheduling",
      "explanation": "Using different CFG values at different steps, higher at start then lower",
      "from": "Johnjohn7855"
    },
    {
      "term": "Film vs video color response",
      "explanation": "Video content has blown out oversaturated highlights while film/cinema has non-linear highlight rolloff that preserves dynamic range for post-processing",
      "from": "Ruairi Robinson"
    },
    {
      "term": "Log images",
      "explanation": "Professional cameras output log images with very low contrast and smooth highlight curves to preserve full dynamic range, similar to RAW images",
      "from": "Ruairi Robinson"
    },
    {
      "term": "CFG scheduling",
      "explanation": "Setting CFG to apply only for specific steps out of total, like 1 step out of 8",
      "from": "Johnjohn7855"
    },
    {
      "term": "Block swap",
      "explanation": "Primary cause of slowdown on consumer GPUs, more VRAM eliminates need for it",
      "from": "Kijai"
    },
    {
      "term": "ATI",
      "explanation": "Motion control system for Wan using spline-based trajectory editing",
      "from": "Kijai"
    },
    {
      "term": "Trajectory points",
      "explanation": "Control points used to guide model movement - model fills gaps naturally when you suggest how it should move, similar to TORA system",
      "from": "Kijai"
    }
  ],
  "resources": [
    {
      "resource": "CausVid Wan 2.1 14B model",
      "url": "https://huggingface.co/lightx2v/Wan2.1-T2V-14B-CausVid",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "DG Custom Node for Wan",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI",
      "type": "workflow",
      "from": "Dream Making"
    },
    {
      "resource": "IPAdapter WAN (experimental)",
      "url": "https://github.com/SirLatore/ComfyUI-IPAdapterWAN",
      "type": "repo",
      "from": "Gavmakes"
    },
    {
      "resource": "Camera control blender addon",
      "url": "Referenced but no direct link provided",
      "type": "tool",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "CCP6 dsnow LoRA",
      "url": "https://huggingface.co/CCP6/dsnow/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "CyberPop LoRA",
      "url": "https://huggingface.co/CCP6/dsnow/tree/main",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "DG Wan models collection",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2_1_v1_3b_t2v_Boost_Final",
      "type": "models",
      "from": "David Snow"
    },
    {
      "resource": "Judy Hopps 14B LoRA",
      "url": "https://civitai.com/models/1133220?modelVersionId=1743913",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "Claymation LoRA",
      "url": "https://civitai.com/models/1439375/claydoh",
      "type": "lora",
      "from": "Jas"
    },
    {
      "resource": "New Framepack model",
      "url": "https://huggingface.co/lllyasviel/FramePack_F1_I2V_HY_20250503/tree/main",
      "type": "model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "Ex video LoRA for increased animation length",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "ComfyUI Hot Reload Hack",
      "url": "https://github.com/logtd/ComfyUI-HotReloadHack",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "FramePack Studio standalone",
      "url": "https://github.com/colinurbs/FramePack-Studio/",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "TTP ComfyUI FramePack",
      "url": "https://github.com/TTPlanetPig/TTP_Comfyui_FramePack_SE",
      "type": "node",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Kijai FramePack Wrapper",
      "url": "https://github.com/kijai/ComfyUI-FramePackWrapper",
      "type": "node",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "GeometryCrafter - upgraded DepthCrafter",
      "url": "https://github.com/TencentARC/GeometryCrafter",
      "type": "model",
      "from": "Soutlkf"
    },
    {
      "resource": "ParaAttention optimization",
      "url": "https://github.com/chengzeyi/ParaAttention/commit/42e9bb7a5bf273f24854c13c08dbfb6209da8a49",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "Triton Windows compilation",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "DF extension workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_skyreels_diffusion_forcing_extension_example_01.json",
      "type": "workflow",
      "from": "MilesCorban"
    },
    {
      "resource": "CausVid model",
      "url": "https://huggingface.co/lightx2v/Wan2.1-T2V-14B-CausVid",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "CausVid paper",
      "url": "https://causvid.github.io/",
      "type": "paper",
      "from": "V\u00e9role"
    },
    {
      "resource": "Fun v1.1 models collection",
      "url": "https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17",
      "type": "model",
      "from": "DeZoomer"
    },
    {
      "resource": "Sonic lip sync repo",
      "url": "https://github.com/jixiaozhong/Sonic",
      "type": "repo",
      "from": "Flipping Sigmas"
    },
    {
      "resource": "14B VACE development branch",
      "url": "https://github.com/Wan-Video/Wan2.1/commits/dev/vace/",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "NormalCrafter",
      "url": "https://normalcrafter.github.io/",
      "type": "tool",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "resource": "Hunyuan Custom",
      "url": "https://github.com/Tencent/HunyuanCustom?tab=readme-ov-file",
      "type": "repo",
      "from": "Soutlkf"
    },
    {
      "resource": "NormalCrafter ComfyUI wrapper",
      "url": "https://github.com/AIWarper/ComfyUI-NormalCrafterWrapper",
      "type": "node",
      "from": "A.I.Warper"
    },
    {
      "resource": "Tiger Tank LoRA for Wan 2.1",
      "url": "https://civitai.com/models/1158489?modelVersionId=1767754",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "T34 Tank LoRA for Wan 2.1",
      "url": "https://civitai.com/models/1562203/wan21-t2v-soviet-tank-t34",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "Sherman Tank LoRA for Wan 2.1",
      "url": "https://civitai.com/models/1564089?modelVersionId=1769923",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "Mi-24 Helicopter LoRA",
      "url": "https://civitai.com/models/1568410?modelVersionId=1774838",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "AH-64 Helicopter LoRA",
      "url": "https://civitai.com/models/1568429?modelVersionId=1774865",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "KA-52 Helicopter LoRA",
      "url": "https://civitai.com/models/1569158?modelVersionId=1775683",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "FLF2V workflow on RunningHub",
      "url": "https://www.runninghub.ai/post/1921875986818744321",
      "type": "workflow",
      "from": "David Snow"
    },
    {
      "resource": "ComfyUI context tutorial",
      "url": "https://www.youtube.com/watch?v=PcrwJ5C3zGM",
      "type": "tutorial",
      "from": "pom"
    },
    {
      "resource": "Beeble AI relighting tool",
      "url": "https://beeble.ai",
      "type": "tool",
      "from": "Valle"
    },
    {
      "resource": "CausVid 14B fp8 model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-T2V-14B_CausVid_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "CausVid project page",
      "url": "https://causvid.github.io/",
      "type": "project",
      "from": "slmonker"
    },
    {
      "resource": "CausVid paper and info",
      "url": "https://news.mit.edu/2025/causevid-hybrid-ai-model-crafts-smooth-high-quality-videos-in-seconds-0506",
      "type": "research",
      "from": "yi"
    },
    {
      "resource": "Xformers compatibility wheel",
      "url": "https://download.pytorch.org/whl/xformers/",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "German Panther tank LoRA",
      "url": "https://civitai.com/models/1574908/wan21-t2v-14b-german-panther-ga-tank",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "Pz.IV H tank LoRA",
      "url": "https://civitai.com/models/1574943/wan21-t2v-14b-german-pziv-h-tank-panzer-4",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "M18 Hellcat Tank LoRA",
      "url": "https://civitai.com/models/1578601/wan21-t2v-14b-us-army-m18-gmc-hellcat-tank",
      "type": "lora",
      "from": "MisterMango"
    },
    {
      "resource": "VACE 14B dev branch",
      "url": "https://github.com/Wan-Video/Wan2.1/tree/dev/vace?tab=readme-ov-file#model-download",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "CausVid I2V branch",
      "url": "https://github.com/tianweiy/CausVid/tree/i2v",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "CausVid GGUF quantized model",
      "url": "https://huggingface.co/Njbx/Wan2.1-T2V-14B-CausVid-GGUF",
      "type": "model",
      "from": "Njb"
    },
    {
      "resource": "MoviiGen 1.1 FP8",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-MoviiGen1_1_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "VACE 1.3B module",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-VACE_1_3B_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "MoviiGen original",
      "url": "https://huggingface.co/ZuluVision/MoviiGen1.1",
      "type": "model",
      "from": "Njb"
    },
    {
      "resource": "VACE 14B official",
      "url": "https://huggingface.co/Wan-AI/Wan2.1-VACE-14B",
      "type": "model",
      "from": "ingi // SYSTMS"
    },
    {
      "resource": "VACE 1.3B official",
      "url": "https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "LightX2V config",
      "url": "https://github.com/ModelTC/lightx2v/blob/main/configs/wan_t2v_causvid.json",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "TaylorSeer caching discussion",
      "url": "https://github.com/Shenyi-Z/TaylorSeer/issues/5#issuecomment-2862664196",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "Wan livestream",
      "url": "https://x.com/i/broadcasts/1eaKbWYRBEYGX",
      "type": "tool",
      "from": "Juampab12"
    },
    {
      "resource": "VACE 14B fp8 module",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-VACE_module_14B_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "VACE 14B bf16 module",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-VACE_module_14B_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Causvid 14B fp8",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-T2V-14B_CausVid_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Colin"
    },
    {
      "resource": "Modelscope download command",
      "url": "pip install modelscope; modelscope download --model Wan-AI/Wan2.1-VACE-14B",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "Model merger tool",
      "url": "https://drive.google.com/file/d/1iJ4T54opP5-fyEaKY4DrTkrSJVZZuvXL/view?usp=sharing",
      "type": "tool",
      "from": "The Punisher"
    },
    {
      "resource": "MoviiGen 1.1 GGUF",
      "url": "https://huggingface.co/wsbagnsv1/MoviiGen1.1-GGUF",
      "type": "model",
      "from": "The Punisher"
    },
    {
      "resource": "Index-anisora model",
      "url": "https://huggingface.co/IndexTeam/Index-anisora",
      "type": "model",
      "from": "A.I.Warper"
    },
    {
      "resource": "MoviiGen 1.1",
      "url": "https://huggingface.co/ZuluVision/MoviiGen1.1",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "DG Wan boost models",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main/DG_Model_Wan2_1_v1_3b_t2v_Boost_Final",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "Comfy repackaged VACE 14B",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_vace_14B_fp16.safetensors",
      "type": "model",
      "from": "comfy"
    },
    {
      "resource": "VACE 1.3B module",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-VACE_module_1_3B_bf16.safetensors",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "CausVid LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "14B VACE FP8 module",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-VACE_module_14B_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "happy.j"
    },
    {
      "resource": "14B VACE full model",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_vace_14B_fp16.safetensors",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "NormalCrafter for depth processing",
      "url": "https://github.com/AIWarper/ComfyUI-NormalCrafterWrapper",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "IC-Light for detail transfer",
      "url": "https://github.com/kijai/ComfyUI-IC-Light",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "VACE mask usage guide",
      "url": "https://github.com/ali-vilab/VACE/issues/45",
      "type": "documentation",
      "from": "DawnII"
    },
    {
      "resource": "DistillT5 for faster text encoding",
      "url": "https://github.com/LifuWang-66/DistillT5",
      "type": "repo",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "resource": "MoviiGen GGUFs",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1kmuccc/new_moviigen11ggufs/",
      "type": "model",
      "from": "The Punisher"
    },
    {
      "resource": "CausVid LoRA workflow",
      "url": "",
      "type": "workflow",
      "from": "seruva19"
    },
    {
      "resource": "Wan VACE workflows on Civitai",
      "url": "https://civitai.com/models",
      "type": "workflow",
      "from": "Nokai"
    },
    {
      "resource": "Context options documentation",
      "url": "https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved/tree/main/documentation/nodes",
      "type": "documentation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "CausVid 1.3B LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_bidirect2_T2V_1_3B_lora_rank32.safetensors",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "CausVid 14B LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
      "type": "lora",
      "from": "N0NSens"
    },
    {
      "resource": "VACE 1.3B example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_1_3B_VACE_examples_03.json",
      "type": "workflow",
      "from": "Nokai"
    },
    {
      "resource": "Optical flow ComfyUI nodes",
      "url": "https://github.com/seanlynch/comfyui-optical-flow/tree/main",
      "type": "tool",
      "from": "DawnII"
    },
    {
      "resource": "Triton Windows build discussion",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1kmcddj/updated_triton_v320_updated_v330_py310_updated/",
      "type": "tool",
      "from": "TK_999"
    },
    {
      "resource": "Kijai WanVideo models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main",
      "type": "model",
      "from": "VK (5080 128gb)"
    },
    {
      "resource": "Index-anisora model",
      "url": "https://huggingface.co/IndexTeam/Index-anisora",
      "type": "model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "ComfyUI-WanVideoWrapper",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper.git",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "ImagePadKJ node for outpainting",
      "url": "",
      "type": "node",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "CausVid 1.3B LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_bidirect2_T2V_1_3B_lora_rank32.safetensors",
      "type": "model",
      "from": "DiXiao"
    },
    {
      "resource": "Official 1.3B VACE full model",
      "url": "https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B/tree/main",
      "type": "model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "Tiny VAE for VRAM reduction",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/taew2_1.safetensors",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Kijai's WanVideo ComfyUI models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main",
      "type": "repo",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "14B VACE GGUF models",
      "url": "https://huggingface.co/QuantStack/Wan2.1-VACE-14B-GGUF",
      "type": "model",
      "from": "The Punisher"
    },
    {
      "resource": "Base Model 14B T2V fp8",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-T2V-14B_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "VACE Module 14B fp8",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-VACE_module_14B_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Causvid Lora 14B",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Causvid Lora 1.3B",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_bidirect2_T2V_1_3B_lora_rank32.safetensors",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "ComfyUI MultiGPU fork",
      "url": "https://github.com/pollockjj/ComfyUI-MultiGPU",
      "type": "repo",
      "from": "The Punisher"
    },
    {
      "resource": "Sapiens Pose",
      "url": "https://github.com/smthemex/ComfyUI_Sapiens",
      "type": "repo",
      "from": "Nokai"
    },
    {
      "resource": "LTX Video VAE",
      "url": "https://huggingface.co/Lightricks/LTX-Video-0.9.7-distilled/blob/main/vae/diffusion_pytorch_model.safetensors",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Native VACE documentation",
      "url": "https://docs.comfy.org/tutorials/video/wan/vace",
      "type": "documentation",
      "from": "The Punisher"
    },
    {
      "resource": "City96 GGUF nodes",
      "url": "https://github.com/city96/ComfyUI-GGUF",
      "type": "repo",
      "from": "artemonary"
    },
    {
      "resource": "T2V LoRAs for 14B",
      "url": "https://huggingface.co/collections/Remade-AI/wan21-14b-t2v-loras-67dc73d82f66cfac2b4eb253",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "ControlFlowUtils",
      "url": "https://github.com/VykosX/ControlFlowUtils/tree/main",
      "type": "repo",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "VACE GitHub repo",
      "url": "https://github.com/ali-vilab/VACE",
      "type": "repo",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "VACE 14B GGUF Reddit post",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1koefcg/new_wan21vace14bggufs/",
      "type": "post",
      "from": "The Punisher"
    },
    {
      "resource": "Wan2.1-Fun-Reward-LoRAs-comfy",
      "url": "https://huggingface.co/Kijai/Wan2.1-Fun-Reward-LoRAs-comfy/tree/main",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "CausVid paper",
      "url": "https://arxiv.org/abs/2412.07772",
      "type": "research",
      "from": "DawnII"
    },
    {
      "resource": "MoviiGen1.1 training code",
      "url": "https://github.com/ZulutionAI/MoviiGen1.1",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "WAN LoRAs collection",
      "url": "https://huggingface.co/collections/Remade-AI/wan21-14b-t2v-loras-67dc73d82f66cfac2b4eb253",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "ComfyUI VACE documentation",
      "url": "https://docs.comfy.org/tutorials/video/wan/vace",
      "type": "documentation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "CausVid quantized model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-T2V-14B_CausVid_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "quase"
    },
    {
      "resource": "Ghibli WAN LoRA",
      "url": "https://civitai.com/models/1474964/ghibli-wan-13b",
      "type": "model",
      "from": "BestWind"
    },
    {
      "resource": "DG_Model_Wan2_1_v1_3b_t2v_Boost_Final",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main/DG_Model_Wan2_1_v1_3b_t2v_Boost_Final",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "CrystalDiskInfo",
      "url": "https://crystalmark.info/en/software/crystaldiskinfo/",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "WAN upscaling tutorial",
      "url": "https://www.youtube.com/watch?v=xY56o8wxQu0&ab_channel=Benji%E2%80%99sAIPlayground",
      "type": "tutorial",
      "from": "CaptHook"
    },
    {
      "resource": "Sapiens ComfyUI node",
      "url": "https://github.com/smthemex/ComfyUI_Sapiens",
      "type": "node",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "Follow Your Emoji Wrapper",
      "url": "https://github.com/kijai/ComfyUI-FollowYourEmojiWrapper",
      "type": "node",
      "from": "A.I.Warper"
    },
    {
      "resource": "Unsafe torch loading node",
      "url": "https://github.com/ltdrdata/comfyui-unsafe-torch",
      "type": "node",
      "from": "MilesCorban"
    },
    {
      "resource": "CausVid-Plus repository",
      "url": "https://github.com/GoatWu/CausVid-Plus",
      "type": "repo",
      "from": "aipmaster"
    },
    {
      "resource": "Seed1.5-VL captioning model",
      "url": "https://huggingface.co/spaces/ByteDance-Seed/Seed1.5-VL",
      "type": "model",
      "from": "Ada"
    },
    {
      "resource": "Inspire Pack nodes",
      "url": "https://github.com/ltdrdata/ComfyUI-Inspire-Pack",
      "type": "node",
      "from": "MilesCorban"
    },
    {
      "resource": "PySceneDetect",
      "url": "https://github.com/Breakthrough/PySceneDetect",
      "type": "tool",
      "from": "Piblarg"
    },
    {
      "resource": "CausVid 14B LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
      "type": "lora",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "All WAN ComfyUI models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main",
      "type": "repo",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "VACE 14B GGUF models",
      "url": "https://huggingface.co/QuantStack/Wan2.1-VACE-14B-GGUF/tree/main",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Origami WAN LoRA",
      "url": "https://huggingface.co/shauray/Origami_WanLora",
      "type": "lora",
      "from": "hicho"
    },
    {
      "resource": "4-step CausVid checkpoint",
      "url": "https://huggingface.co/tianweiy/CausVid/tree/main/autoregressive_checkpoint_warp_4step_cfg2",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "HiDream Sampler removal fix",
      "url": "https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler/",
      "type": "repo",
      "from": "100a"
    },
    {
      "resource": "KeySync lip-sync tool",
      "url": "https://antonibigata.github.io/KeySync/",
      "type": "tool",
      "from": "Zuko"
    },
    {
      "resource": "WanVideo VACE workflows",
      "url": "https://ptb.discord.com/channels/1076117621407223829/1342763350815277067/1373677243917926401",
      "type": "workflow",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "CausVid 1.3B LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_bidirect2_T2V_1_3B_lora_rank32.safetensors",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "MoviiGen1.1",
      "url": "https://github.com/ZulutionAI/MoviiGen1.1",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "WAN VACE 14B bf16 model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-VACE_module_14B_bf16.safetensors",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "SAM2 points editor for masking",
      "url": "https://github.com/kijai/ComfyUI-segment-anything-2",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "DetailZ WAN detail enhancer LoRA",
      "url": "https://civitai.com/models/1385506/detailz-wan-detail-enhancer-for-wan-videos?modelVersionId=1565668",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "WAN prompt generator for LLMs",
      "url": "https://pastebin.com/WiuEUFQp",
      "type": "tool",
      "from": "aipmaster"
    },
    {
      "resource": "ComfyUI Sapiens nodes",
      "url": "https://github.com/smthemex/ComfyUI_Sapiens/tree/main",
      "type": "node",
      "from": "hablaba"
    },
    {
      "resource": "NormalCrafterWrapper",
      "url": "https://github.com/AIWarper/ComfyUI-NormalCrafterWrapper",
      "type": "node",
      "from": "ArtOfficial"
    },
    {
      "resource": "ComfyUI_Sapiens",
      "url": "https://github.com/smthemex/ComfyUI_Sapiens",
      "type": "node",
      "from": "Boop"
    },
    {
      "resource": "Wan2.1-Fun-Reward-LoRAs",
      "url": "https://huggingface.co/alibaba-pai/Wan2.1-Fun-Reward-LoRAs",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "WanVideo_comfy models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "MediaPipe Iris tracking",
      "url": "https://research.google/blog/mediapipe-iris-real-time-iris-tracking-depth-estimation/",
      "type": "tool",
      "from": "DawnII"
    },
    {
      "resource": "NormalCrafter Wrapper",
      "url": "https://github.com/AIWarper/ComfyUI-NormalCrafterWrapper",
      "type": "repo",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "MTVCrafter",
      "url": "https://github.com/DINGYANB/MTVCrafter",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "WAN documentation",
      "url": "https://docs.comfy.org/tutorials/video/wan/wan-video",
      "type": "documentation",
      "from": "zelgo_"
    },
    {
      "resource": "WAN Japanese guide",
      "url": "https://scrapbox.io/work4ai/%F0%9F%A6%8AWan2.1_VACE",
      "type": "guide",
      "from": "Guey.KhalaMari"
    },
    {
      "resource": "ComfyUI torch compile fix PR",
      "url": "https://github.com/comfyanonymous/ComfyUI/pull/8213",
      "type": "repo",
      "from": "Kosinkadink"
    },
    {
      "resource": "MatAnyone node",
      "url": "https://github.com/KytraScript/ComfyUI_MatAnyone_Kytra",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "WanVideoWrapper example workflows",
      "url": "",
      "type": "workflow",
      "from": "Kijai"
    },
    {
      "resource": "VACE 14B model",
      "url": "",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "CausVid LoRA",
      "url": "",
      "type": "lora",
      "from": "Colin"
    },
    {
      "resource": "Skyreels base model",
      "url": "",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "RealisDance-DiT",
      "url": "https://huggingface.co/theFoxofSky/RealisDance-DiT",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "RealisDance-DiT project page",
      "url": "https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index",
      "type": "documentation",
      "from": "yi"
    },
    {
      "resource": "Video extension workflow",
      "url": "https://pastebin.com/sY0zSHce",
      "type": "workflow",
      "from": "MilesCorban"
    },
    {
      "resource": "Custom VACE GPT guide",
      "url": "https://chatgpt.com/g/g-682d7773290c819188afd2c5e09a0811-wan2-1-vace-video-guide",
      "type": "tool",
      "from": "AJO"
    },
    {
      "resource": "GRAT speed optimization",
      "url": "https://oliverrensu.github.io/project/GRAT/",
      "type": "research",
      "from": "Ada"
    },
    {
      "resource": "ComfyUI OIIO plugin",
      "url": "https://github.com/melMass/comfy_oiio",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Headless Wan2GP fork",
      "url": "https://github.com/peteromallet/Headless-Wan2GP",
      "type": "repo",
      "from": "pom"
    },
    {
      "resource": "Wan2.1-VACE-14B model",
      "url": "https://huggingface.co/Wan-AI/Wan2.1-VACE-14B/tree/main",
      "type": "model",
      "from": "Valle"
    },
    {
      "resource": "Wan2.1 control LoRAs",
      "url": "https://huggingface.co/spacepxl/Wan2.1-control-loras",
      "type": "lora",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "SkyReels-A1",
      "url": "https://github.com/SkyworkAI/SkyReels-A1",
      "type": "repo",
      "from": "chrisd0073"
    },
    {
      "resource": "GRAT attention",
      "url": "https://github.com/OliverRensu/GRAT",
      "type": "repo",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "resource": "Pepe Lora",
      "url": "https://civitai.com/models/1518710?modelVersionId=1718279",
      "type": "lora",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Live Wallpaper Style",
      "url": "https://civitai.com/models/1264662/live-wallpaper-style",
      "type": "model",
      "from": "Mngbg"
    },
    {
      "resource": "RollingDepth",
      "url": "https://github.com/prs-eth/rollingdepth",
      "type": "repo",
      "from": "yo9o"
    },
    {
      "resource": "RollingDepth demo",
      "url": "https://rollingdepth.github.io/",
      "type": "demo",
      "from": "yo9o"
    },
    {
      "resource": "WanVideoWrapper",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper",
      "type": "repo",
      "from": "yo9o"
    },
    {
      "resource": "Kijai's WanVideo models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main",
      "type": "models",
      "from": "jerms_ai_(4090_24g)"
    },
    {
      "resource": "Frame Interpolation schedule",
      "url": "https://github.com/Fannovel16/ComfyUI-Frame-Interpolation/blob/main/interpolation_schedule.png",
      "type": "reference",
      "from": "JohnDopamine"
    },
    {
      "resource": "Aspect ratio calculator",
      "url": "https://www.aspectratiocalculator.com/16-9.html",
      "type": "tool",
      "from": "A.I.Warper"
    },
    {
      "resource": "TheDirector workflow",
      "url": "https://civitai.com/models/1476469/thedirector",
      "type": "workflow",
      "from": "AJO"
    },
    {
      "resource": "ComfyUI Browser for XY plotting",
      "url": "https://github.com/talesofai/comfyui-browser",
      "type": "tool",
      "from": "Johnjohn7855"
    },
    {
      "resource": "Kijai IC-Light with spline editor",
      "url": "https://github.com/kijai/ComfyUI-IC-Light",
      "type": "repo",
      "from": "Johnjohn7855"
    },
    {
      "resource": "UniAnimate-W project",
      "url": "https://github.com/Isi-dev/ComfyUI-UniAnimate-W",
      "type": "repo",
      "from": "Guey.KhalaMari"
    },
    {
      "resource": "MoviiGen 1.1 VACE GGUF",
      "url": "https://huggingface.co/QuantStack/MoviiGen1.1-VACE-GGUF",
      "type": "model",
      "from": "The Punisher"
    },
    {
      "resource": "360 degree rotation LoRA",
      "url": "https://civitai.com/models/1346623/360-degree-rotation-microwave-rotation-wan21-i2v-lora",
      "type": "lora",
      "from": "Yae"
    },
    {
      "resource": "Bullet time camera LoRA",
      "url": "https://civitai.com/models/1475368",
      "type": "lora",
      "from": "N0NSens"
    },
    {
      "resource": "ControlNet MediaPipe Face",
      "url": "https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace",
      "type": "model",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "resource": "VACE workflow collection",
      "url": "https://www.patreon.com/posts/comfyui-workflow-129211762",
      "type": "workflow",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "resource": "COCO WholeBody pose dataset",
      "url": "https://github.com/jin-s13/COCO-WholeBody/",
      "type": "repo",
      "from": "A.I.Warper"
    },
    {
      "resource": "RTMPose implementation",
      "url": "https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose",
      "type": "repo",
      "from": "A.I.Warper"
    },
    {
      "resource": "Samaritan 3D Cartoon SDXL LoRA",
      "url": "https://civitai.com/models/121932/samaritan-3d-cartoon-sdxl",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "VACE Benchmark examples",
      "url": "https://huggingface.co/datasets/ali-vilab/VACE-Benchmark/tree/main/assets/examples",
      "type": "dataset",
      "from": "fazeaction"
    },
    {
      "resource": "VACE User Guide",
      "url": "https://huggingface.co/datasets/ali-vilab/VACE-Benchmark/blob/main/UserGuide.md",
      "type": "documentation",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "WAN 2.1 ComfyUI models",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models",
      "type": "model",
      "from": "ArtOfficial"
    },
    {
      "resource": "Jenga GitHub repository",
      "url": "https://github.com/dvlab-research/Jenga",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "TimeUi ComfyUI Timeline Node",
      "url": "https://github.com/jimmm-ai/TimeUi-a-ComfyUi-Timeline-Node",
      "type": "node",
      "from": "chrisd0073"
    },
    {
      "resource": "ComfyUI-MotionDiff",
      "url": "https://github.com/Fannovel16/ComfyUI-MotionDiff",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "WAN Video ComfyUI Documentation",
      "url": "https://docs.comfy.org/tutorials/video/wan/wan-video",
      "type": "documentation",
      "from": "zelgo_"
    },
    {
      "resource": "Wan 2.1 LoRA collection for 14B T2V",
      "url": "https://huggingface.co/collections/Remade-AI/wan21-14b-t2v-loras-67dc73d82f66cfac2b4eb253",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "AziibPixelMix XL checkpoint",
      "url": "https://civitai.com/models/451177/aziibpixelmix-xl",
      "type": "model",
      "from": "A.I.Warper"
    },
    {
      "resource": "ComfyUI Pose Interpolation node",
      "url": "https://github.com/toyxyz/ComfyUI_pose_inter",
      "type": "tool",
      "from": "toyxyz"
    },
    {
      "resource": "UltraSharpV2 upscaler",
      "url": "https://huggingface.co/Kim2091/UltraSharpV2",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "DiffSynth Studio LoRA collection",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main/DG_Model_Wan2_1_v1_3b_t2v_Boost_Final",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "ComfyUI pose interpolation",
      "url": "https://github.com/toyxyz/ComfyUI_pose_inter",
      "type": "repo",
      "from": "toyxyz"
    },
    {
      "resource": "Clarity upscale workflows",
      "url": "https://github.com/roblaughter/comfyui-workflows",
      "type": "workflow",
      "from": "David Snow"
    },
    {
      "resource": "InspyreNet RemBG",
      "url": "https://github.com/john-mnz/ComfyUI-Inspyrenet-Rembg",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "SAM2 segment anything",
      "url": "https://github.com/kijai/ComfyUI-segment-anything-2",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "SkyReels CausVid merged model",
      "url": "https://huggingface.co/Zuntan/Wan-SkyReels-CausVid/tree/main",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "MatAnyone for video matting",
      "url": "https://github.com/FuouM/ComfyUI-MatAnyone",
      "type": "repo",
      "from": "chrisd0073"
    },
    {
      "resource": "RES4LYF node pack",
      "url": "https://github.com/ClownsharkBatwing/RES4LYF",
      "type": "repo",
      "from": "A.I.Warper"
    },
    {
      "resource": "ComfyUI_FaceAnalysis",
      "url": "https://github.com/cubiq/ComfyUI_FaceAnalysis",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "CausVid LoRA for Wan21 14B T2V",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_14B_T2V_lora_rank32.safetensors",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "Remade AI Wan2.1 LoRAs collection",
      "url": "https://huggingface.co/collections/Remade-AI/wan21-14b-480p-i2v-loras-67d0e26f08092436b585919b",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "ComfyUI_pose_inter",
      "url": "https://github.com/toyxyz/ComfyUI_pose_inter",
      "type": "repo",
      "from": "toyxyz"
    },
    {
      "resource": "Bjornulf custom nodes",
      "url": "https://github.com/justUmen/Bjornulf_custom_nodes",
      "type": "repo",
      "from": "UsamaAhmedKhan"
    },
    {
      "resource": "VACE GGUF workflow",
      "url": "https://huggingface.co/QuantStack/Wan2.1-VACE-14B-GGUF/tree/main",
      "type": "workflow",
      "from": "AshmoTV"
    },
    {
      "resource": "String formatting node",
      "url": "",
      "type": "node",
      "from": "Guey.KhalaMari"
    },
    {
      "resource": "SkyReels CausVid LoRA",
      "url": "https://huggingface.co/Zuntan/Wan-SkyReels-CausVid/tree/main",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "SkyReels V2 GGUF with CausVid LoRA",
      "url": "https://huggingface.co/QuantStack/SkyReels-V2-T2V-14B-720P-VACE-GGUF/tree/main",
      "type": "model",
      "from": "V\u00e9role"
    },
    {
      "resource": "Wan2.1 Fun Reward LoRAs",
      "url": "https://huggingface.co/alibaba-pai/Wan2.1-Fun-Reward-LoRAs/tree/main",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "DiffSynth Studio LoRA for Wan2.1",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Wan2.1 VACE 14B GGUF Q5",
      "url": "https://huggingface.co/QuantStack/Wan2.1-VACE-14B-GGUF/blob/main/Wan2.1-VACE-14B-Q5_0.gguf",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "VACE inpainting workflow",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1376124067261714434",
      "type": "workflow",
      "from": "David Snow"
    },
    {
      "resource": "Bagel AI - picture movement and rotation tool",
      "url": "https://bagel-ai.org/",
      "type": "tool",
      "from": "Mngbg"
    },
    {
      "resource": "TheDenk Wan ControlNet HED 1.3B",
      "url": "https://huggingface.co/TheDenk/wan2.1-t2v-1.3b-controlnet-hed-v1",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "TheDenk Wan ControlNet HED 14B",
      "url": "https://huggingface.co/TheDenk/wan2.1-t2v-14b-controlnet-hed-v1",
      "type": "model",
      "from": "pom"
    },
    {
      "resource": "AccVideo WanX T2V 14B",
      "url": "https://huggingface.co/aejion/AccVideo-WanX-T2V-14B/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Spacepxl Wan Control LoRAs",
      "url": "https://huggingface.co/spacepxl/Wan2.1-control-loras",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "DG Boost Wan models",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main/DG_Model_Wan2_1_v1_3b_t2v_Boost_Final",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "Uni3C",
      "url": "https://github.com/ewrfcas/Uni3C",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "RealisDance",
      "url": "https://github.com/damo-cv/RealisDance",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "Jenga",
      "url": "https://github.com/dvlab-research/Jenga",
      "type": "repo",
      "from": "MisterMango"
    },
    {
      "resource": "Wan21_AccVid_T2V_14B_lora_rank32_fp16.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_AccVid_T2V_14B_lora_rank32_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Uni3C camera control",
      "url": "https://github.com/ewrfcas/Uni3C, https://huggingface.co/ewrfcas/Uni3C/tree/main",
      "type": "repo",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "OmniConsistency",
      "url": "https://huggingface.co/showlab/OmniConsistency",
      "type": "model",
      "from": "happy.j"
    },
    {
      "resource": "ComfyUI-BAGEL",
      "url": "https://github.com/neverbiasu/ComfyUI-BAGEL",
      "type": "repo",
      "from": "hicho, fazeaction"
    },
    {
      "resource": "BAGEL online demo",
      "url": "https://huggingface.co/spaces/ByteDance-Seed/BAGEL",
      "type": "tool",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "ComfyUI-UNO",
      "url": "https://github.com/jax-explorer/ComfyUI-UNO",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Jenga research",
      "url": "https://github.com/dvlab-research/Jenga",
      "type": "repo",
      "from": "hicho"
    },
    {
      "resource": "Loop workflow",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1ktljys/loop_anything_with_wan21_vace/",
      "type": "workflow",
      "from": "daking999"
    },
    {
      "resource": "Phantom 14B model",
      "url": "https://huggingface.co/bytedance-research/Phantom",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "AccVid LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_AccVid_T2V_14B_lora_rank32_fp16.safetensors",
      "type": "lora",
      "from": "Johnjohn7855"
    },
    {
      "resource": "HunyuanPortrait",
      "url": "https://huggingface.co/tencent/HunyuanPortrait",
      "type": "model",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "resource": "Mat Anyone ComfyUI node",
      "url": "https://github.com/KytraScript/ComfyUI_MatAnyone_Kytra",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "Wan 2.1 Knowledge Base guide",
      "url": "https://nathanshipley.notion.site/Wan-2-1-Knowledge-Base-1d691e115364814fa9d4e27694e9468f",
      "type": "guide",
      "from": "JohnDopamine"
    },
    {
      "resource": "ComfyUI QwenVL for prompting",
      "url": "https://github.com/alexcong/ComfyUI_QwenVL",
      "type": "tool",
      "from": "Johnjohn7855"
    },
    {
      "resource": "TAEW2_1.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/taew2_1.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "WAN ControlNet Depth v1",
      "url": "https://huggingface.co/TheDenk/wan2.1-t2v-14b-controlnet-depth-v1",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Ultimate OpenPose Editor",
      "url": "https://github.com/toyxyz/ComfyUI-ultimate-openpose-editor",
      "type": "tool",
      "from": "toyxyz"
    },
    {
      "resource": "MoviiGen Prompt Rewriter",
      "url": "https://huggingface.co/ZuluVision/MoviiGen1.1_Prompt_Rewriter/tree/main",
      "type": "tool",
      "from": "sneako1234"
    },
    {
      "resource": "Merged CausVid + AccVid Model",
      "url": "https://huggingface.co/CCP6/blahblah/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "JohnDopamine's merged model",
      "url": "https://huggingface.co/CCP6/blahblah/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "SageAttention",
      "url": "https://github.com/thu-ml/SageAttention",
      "type": "repo",
      "from": "aikitoria"
    },
    {
      "resource": "SageAttention compiled wheel",
      "url": "https://huggingface.co/Alissonerdx/sageattention-2.1.0-cu128torch270-cp312-cp312-linux_x86_64.whl/tree/main",
      "type": "tool",
      "from": "MaQue"
    },
    {
      "resource": "WarperNodes",
      "url": "https://github.com/AIWarper/ComfyUI-WarperNodes",
      "type": "node",
      "from": "A.I.Warper"
    },
    {
      "resource": "All Wan models collection",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main",
      "type": "model",
      "from": "Nekodificador"
    },
    {
      "resource": "Wan21_T2V_14B_MoviiGen_lora",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_T2V_14B_MoviiGen_lora_rank32_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Hakoniwa anime wan models",
      "url": "https://huggingface.co/svjack/hakoniwa_anime_wan2_1_models",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "VACE loop workflow",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1375946880793710703",
      "type": "workflow",
      "from": "David Snow"
    },
    {
      "resource": "Phantom model merge",
      "url": "https://discord.com/channels/1076117621407223829/1344057524935983125/1377153370950729809",
      "type": "model",
      "from": "Thom293"
    },
    {
      "resource": "Wan 2.1 prompting paper",
      "url": "https://arxiv.org/pdf/2502.11079v2",
      "type": "paper",
      "from": "aikitoria"
    },
    {
      "resource": "Normalized Attention Guidance paper",
      "url": "https://arxiv.org/pdf/2505.21179",
      "type": "paper",
      "from": "Ada"
    },
    {
      "resource": "SageAttention++ paper",
      "url": "https://arxiv.org/pdf/2505.21136",
      "type": "paper",
      "from": "Ada"
    },
    {
      "resource": "MoviiGen LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/e79b038af91ca315fdb638bc4f9fbb543258856f/Wan21_T2V_14B_MoviiGen_lora_rank32_fp16.safetensors",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "Uni3C ControlNet",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/e79b038af91ca315fdb638bc4f9fbb543258856f/Wan21_Uni3C_controlnet_fp16.safetensors",
      "type": "controlnet",
      "from": "David Snow"
    },
    {
      "resource": "Depth ControlNet",
      "url": "https://huggingface.co/TheDenk/wan2.1-t2v-14b-controlnet-depth-v1",
      "type": "controlnet",
      "from": "Kijai"
    },
    {
      "resource": "DetailZ LoRA",
      "url": "https://civitai.com/models/1385506/detailz-wan-detail-enhancer-for-wan-videos?modelVersionId=1565668",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "ComfyUI OpenPose Editor",
      "url": "https://github.com/toyxyz/ComfyUI-ultimate-openpose-editor",
      "type": "tool",
      "from": "toyxyz"
    },
    {
      "resource": "SkyReels V2",
      "url": "https://github.com/SkyworkAI/SkyReels-V2",
      "type": "model",
      "from": "N0NSens"
    },
    {
      "resource": "CFGDistill LoRAs",
      "url": "https://huggingface.co/spacepxl/wan-cfgdistill-loras",
      "type": "lora",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "Custom Phantom merge workflow",
      "url": "https://discord.com/channels/1076117621407223829/1344057524935983125/1377153370950729809",
      "type": "workflow",
      "from": "Thom293"
    },
    {
      "resource": "SAM2 nodes and workflows",
      "url": "https://github.com/kijai/ComfyUI-segment-anything-2",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "Inpaint Crop and Stitch nodes",
      "url": "https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "Multiple VACE encodes workflow",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1373697534773563573",
      "type": "workflow",
      "from": "JohnDopamine"
    },
    {
      "resource": "Uni3C controlnet model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_Uni3C_controlnet_fp16.safetensors",
      "type": "model",
      "from": "manwanggege"
    },
    {
      "resource": "Wan controlnet HED model",
      "url": "https://huggingface.co/TheDenk/wan2.1-t2v-1.3b-controlnet-hed-v1/discussions",
      "type": "model",
      "from": "manwanggege"
    },
    {
      "resource": "Wan21_Uni3C_controlnet_fp16.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_Uni3C_controlnet_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI-MoGe",
      "url": "https://github.com/kijai/ComfyUI-MoGe",
      "type": "repo",
      "from": "toyxyz"
    },
    {
      "resource": "Remade-AI Wan21 14B LoRAs collection",
      "url": "https://huggingface.co/collections/Remade-AI/wan21-14b-480p-i2v-loras-67d0e26f08092436b585919b",
      "type": "model",
      "from": "Johnjohn7855"
    },
    {
      "resource": "ComfyUI-Inpaint-CropAndStitch",
      "url": "https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch",
      "type": "repo",
      "from": "DeZoomer"
    },
    {
      "resource": "DiffPhy project",
      "url": "https://bwgzk-keke.github.io/DiffPhy/",
      "type": "model",
      "from": "pom"
    },
    {
      "resource": "Jenga dev-wan branch",
      "url": "https://github.com/dvlab-research/Jenga/tree/dev-wan",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "DualParal WAN-based project",
      "url": "https://github.com/DualParal-Project/DualParal",
      "type": "repo",
      "from": "Cseti"
    },
    {
      "resource": "LayerAnimate-DiT",
      "url": "https://huggingface.co/Yuppie1204/LayerAnimate-DiT",
      "type": "model",
      "from": "YatharthSharma"
    },
    {
      "resource": "CameraBench dataset",
      "url": "https://huggingface.co/datasets/syCen/CameraBench",
      "type": "dataset",
      "from": "mamad8"
    },
    {
      "resource": "Phantom 14B GGUF quantizations",
      "url": "https://huggingface.co/QuantStack/Phantom_Wan_14B-GGUF",
      "type": "model",
      "from": "The Punisher"
    },
    {
      "resource": "Merged Phantom + CausVid + AccVid model",
      "url": "https://huggingface.co/CCP6/blahblah/tree/main",
      "type": "model",
      "from": "MilesCorban"
    },
    {
      "resource": "PerpNeg paper",
      "url": "https://perp-neg.github.io",
      "type": "paper",
      "from": "zelgo_"
    },
    {
      "resource": "seed-vc voice swap",
      "url": "https://github.com/Plachtaa/seed-vc",
      "type": "tool",
      "from": "DeZoomer"
    },
    {
      "resource": "LatentSync",
      "url": "https://github.com/ShmuelRonen/ComfyUI-LatentSyncWrapper",
      "type": "tool",
      "from": "DeZoomer"
    },
    {
      "resource": "TheDirector workflow",
      "url": "https://civitai.green/models/1476469/thedirector",
      "type": "workflow",
      "from": "AJO"
    },
    {
      "resource": "CausVid v2 LoRA - attention layers only",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_14B_T2V_lora_rank32_v2.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "CausVid v1.5 no first block",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_14B_T2V_lora_rank32_v1_5_no_first_block.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "OpenPose to Mixamo converter",
      "url": "https://github.com/Astropulse/mixamotoopenpose",
      "type": "tool",
      "from": "Relven 96gb"
    },
    {
      "resource": "OpenPose rig for Blender",
      "url": "https://toyxyz.gumroad.com/l/ciojz?layout=profile",
      "type": "tool",
      "from": "toyxyz"
    },
    {
      "resource": "Dream-O ComfyUI implementation",
      "url": "https://github.com/ToTheBeginning/ComfyUI-DreamO",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "LayerAnimate",
      "url": "https://layeranimate.github.io",
      "type": "tool",
      "from": "DawnII"
    },
    {
      "resource": "CausVid v2 LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan21_CausVid_14B_T2V_lora_rank32_v2.safetensors",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "ATI trajectory model",
      "url": "https://huggingface.co/bytedance-research/ATI/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "ATI GitHub repo",
      "url": "https://github.com/bytedance/ATI",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Ani Wan model",
      "url": "https://civitai.com/models/1626197?modelVersionId=1840868",
      "type": "model",
      "from": "Cubey"
    },
    {
      "resource": "Phantom GGUF version",
      "url": "https://huggingface.co/QuantStack/Phantom_Wan_14B-GGUF",
      "type": "model",
      "from": "The Punisher"
    },
    {
      "resource": "Direct3D-S2",
      "url": "https://huggingface.co/wushuang98/Direct3D-S2/tree/main",
      "type": "model",
      "from": "Mngbg"
    },
    {
      "resource": "Instagram Women LoRA",
      "url": "https://civitai.green/models/1539088/instagram-women",
      "type": "model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "Realism Boost LoRA",
      "url": "https://civitai.green/models/1626063",
      "type": "model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "Wan14B_Detailer/Enhancer_T2V LoRA",
      "url": "https://civitai.green/models/1626063",
      "type": "lora",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "Detailz Wan Detail Enhancer LoRA",
      "url": "https://civitai.green/models/1385506/detailz-wan-detail-enhancer-for-wan-videos",
      "type": "lora",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "2D Animation Effects LoRA and workflow",
      "url": "https://discordapp.com/channels/1076117621407223829/1344309523187368046/1377548588804083722",
      "type": "lora",
      "from": "852\u8a71 (hakoniwa)"
    },
    {
      "resource": "ComfyUI_RopeWrapper",
      "url": "https://github.com/fssorc/ComfyUI_RopeWrapper",
      "type": "tool",
      "from": "AJO"
    },
    {
      "resource": "ComfyUI-AdvancedLivePortrait",
      "url": "https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait",
      "type": "tool",
      "from": "toyxyz"
    },
    {
      "resource": "Better Film Grain node",
      "url": "https://civitai.com/models/1476469?modelVersionId=1670194",
      "type": "node",
      "from": "AJO"
    },
    {
      "resource": "DeZoomer's VACE workflow",
      "url": "https://discord.com/channels/1076117621407223829/1377706250376319106/1377706250376319106",
      "type": "workflow",
      "from": "DeZoomer"
    },
    {
      "resource": "Normal maps example",
      "url": "https://discord.com/channels/1076117621407223829/1373520070596231251/1374398950924357802",
      "type": "workflow",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "Wan2_1-Wan-I2V-ATI-14B_fp8_e5m2.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-Wan-I2V-ATI-14B_fp8_e5m2.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Music video example using FaceFusion",
      "url": "https://www.youtube.com/watch?v=KVoiooE8C0c&lc=UgxYYcSbDpI5grL_BVJ4AaABAg",
      "type": "workflow",
      "from": "Ruairi Robinson"
    }
  ],
  "limitations": [
    {
      "limitation": "FantasyTalker lip sync quality",
      "details": "Sync is always a little off, even in official demos",
      "from": "amli"
    },
    {
      "limitation": "VACE face issues at low resolution",
      "details": "Badly screws faces at 640x360 when face is mid-distance",
      "from": "xwsswww"
    },
    {
      "limitation": "Fun camera control rotation limits",
      "details": "180 rotation around subject potentially too far within 81 frames",
      "from": "DevouredBeef"
    },
    {
      "limitation": "Context windows beyond 81 frames",
      "details": "Breaks hard after 81 frames, changes input image",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "Fun camera creates 2D parallax effect",
      "details": "Movement feels fake, like 2D parallax rather than true 3D rotation",
      "from": "N0NSens"
    },
    {
      "limitation": "Eyes don't move/follow properly in 1.3B models",
      "details": "Biggest problem with face videos, eyeballs don't track motion. Only way to deal with this is post-processing with LivePortrait",
      "from": "David Snow"
    },
    {
      "limitation": "DG models introduce flashing with stronger versions",
      "details": "Higher versions more likely to flash, need stronger versions for stylization but they're flash prone",
      "from": "David Snow"
    },
    {
      "limitation": "Video Depth Anything has banding issues",
      "details": "Creates hard lines and ridges that cause wobbling artifacts in VACE and Fun Control results",
      "from": "Nathan Shipley"
    },
    {
      "limitation": "VACE always adds people when trying to remove objects",
      "details": "Difficult to remove people/objects from scenes, tends to re-add them",
      "from": "PirateWolf"
    },
    {
      "limitation": "Only about 5 useful style LoRAs for 1.3B",
      "details": "Main usable ones are flat color, studio ghibli, and cyberpop. Much fewer good 1.3B LoRAs compared to checkpoints",
      "from": "David Snow"
    },
    {
      "limitation": "VACE struggles with multiple control types",
      "details": "Due to specificity requirements in control maps matching training data",
      "from": "Rishi Pandey"
    },
    {
      "limitation": "Base 1.3B can't use pose control",
      "details": "Can only use depth, lineart or normals - pose requires VACE",
      "from": "David Snow"
    },
    {
      "limitation": "HunyuanVideo FramePack lacks control",
      "details": "Only has prompt travel which is far from real control, no equivalent to VACE",
      "from": "Draken"
    },
    {
      "limitation": "HunyuanVideo starts looping at 201 frames",
      "details": "Model begins to create loops when generating beyond 201 frames",
      "from": "Draken"
    },
    {
      "limitation": "Single start frame is limited for complex scenes",
      "details": "Problems when hands move off-screen, backgrounds change, or other elements not visible in start frame",
      "from": "David Snow"
    },
    {
      "limitation": "FP8 not supported on RTX 3090 and older GPUs",
      "details": "Causes compilation errors with triton, must be disabled",
      "from": "Juan Gea"
    },
    {
      "limitation": "VACE only works with 1.3B models",
      "details": "Cannot use VACE with 14B models, limited to smaller model size",
      "from": "Christian Sandor"
    },
    {
      "limitation": "No LoRA compatibility between 1.3B and 14B",
      "details": "LoRAs trained for one size don't work with the other",
      "from": "MilesCorban"
    },
    {
      "limitation": "Ubuntu 24.04 has CUDA compatibility issues",
      "details": "Causes various problems, 22.04 recommended instead",
      "from": "UsamaAhmedKhan"
    },
    {
      "limitation": "CausVid has no UI implementations yet",
      "details": "Model available but no ComfyUI or other UI support",
      "from": "yi"
    },
    {
      "limitation": "Frame count limits cause looping",
      "details": "Generating more than 81 frames (vanilla) causes videos to loop backwards or show artifacts",
      "from": "Nokai"
    },
    {
      "limitation": "LoRA merging issues",
      "details": "Using multiple LoRAs often results in one not being applied properly",
      "from": "Dream Making"
    },
    {
      "limitation": "1.3B model quality limitations",
      "details": "Lacks concept understanding compared to 14B, harder to train LoRAs on",
      "from": "Piblarg"
    },
    {
      "limitation": "VACE character consistency",
      "details": "Generates slightly different images than input, character appearance can drift",
      "from": "Valle"
    },
    {
      "limitation": "CausVid has artifacts at video start",
      "details": "First 8 frames often have glitches that need removal",
      "from": "V\u00e9role"
    },
    {
      "limitation": "CausVid can't use TeaCache or FETA",
      "details": "Optimization techniques don't work with distilled model",
      "from": "Kijai"
    },
    {
      "limitation": "CausVid motion doesn't work properly beyond 81 frames",
      "details": "Needs additional code implementation for longer videos",
      "from": "Kijai"
    },
    {
      "limitation": "Consistent character workflow still has issues",
      "details": "Hair changes, clothing color changes even with full pipeline",
      "from": "David Snow"
    },
    {
      "limitation": "IPAdapter only 256x256 input resolution",
      "details": "Not enough detail for precise character consistency",
      "from": "Draken"
    },
    {
      "limitation": "CausVid not properly usable yet",
      "details": "Current implementation doesn't match intended sampling method with sliding window and kv_cache",
      "from": "Kijai"
    },
    {
      "limitation": "MoviiGen is T2V only",
      "details": "Cannot do I2V, just T2V finetune",
      "from": "Kijai"
    },
    {
      "limitation": "FP8 quality degradation",
      "details": "FP8 has quality hit and won't stack with fp16_fast for significant benefit",
      "from": "Kijai"
    },
    {
      "limitation": "TEAcache quality loss",
      "details": "TEAcache can lower quality a fair bit",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "limitation": "MoviiGen huge file size",
      "details": "Model is very large, probably in FP32 format",
      "from": "Dream Making"
    },
    {
      "limitation": "14B VACE doesn't work with native ComfyUI nodes yet",
      "details": "Uses different blocks, only wrapper implementation available",
      "from": "Kijai"
    },
    {
      "limitation": "VACE modules are only available in bf16",
      "details": "No fp8 versions for VACE modules",
      "from": "Kijai"
    },
    {
      "limitation": "14B works poorly at low resolutions where 1.3B excels",
      "details": "Resolution-dependent performance differences",
      "from": "Kijai"
    },
    {
      "limitation": "Fast motion causes quality degradation",
      "details": "Motion speed affects output quality",
      "from": "Kijai"
    },
    {
      "limitation": "VACE doesn't work with Fun models",
      "details": "Only works with original T2V based models, Fun models are incompatible",
      "from": "Kijai"
    },
    {
      "limitation": "CausVid reduces reference strength with VACE",
      "details": "When used together, CausVid can't do precise control and weakens reference",
      "from": "Kijai"
    },
    {
      "limitation": "MoviiGen preview still shows teacache artifacts",
      "details": "Quality issues persist in preview generations",
      "from": "DawnII"
    },
    {
      "limitation": "VACE 14B requires significant VRAM",
      "details": "35GB model, needs block swapping even on high-end GPUs",
      "from": "Kijai"
    },
    {
      "limitation": "CausVid not fully implemented yet",
      "details": "KJ mentioned it's not done at all and proper use wouldn't support VACE",
      "from": "Kijai"
    },
    {
      "limitation": "TeaCache and SLG don't work with CausVid",
      "details": "Because CausVid doesn't use CFG, these nodes won't work",
      "from": "Kijai"
    },
    {
      "limitation": "No proper way to use multiple first frames at different starting points",
      "details": "Context options always take first frame as reference, works better with reference instead",
      "from": "Kijai"
    },
    {
      "limitation": "Flex attention very strict about dimensions",
      "details": "Most dimensions tried just errored out, 14B at 480p gives poor results",
      "from": "Kijai"
    },
    {
      "limitation": "Difficult to blend control inputs with VACE",
      "details": "Fundamentally difficult due to how VACE works, only working way is using multiple VACEs",
      "from": "Kijai"
    },
    {
      "limitation": "CausVid seems deterministic",
      "details": "Same results for a prompt regardless of seed",
      "from": "Ada"
    },
    {
      "limitation": "CausVid not used as originally intended",
      "details": "Model meant for 3 latent windows with specific sampling code, current usage is hacky",
      "from": "Kijai"
    },
    {
      "limitation": "Reference fading in long generations",
      "details": "VACE reference only fed to first sampler, loses strength over time in DF",
      "from": "DawnII"
    },
    {
      "limitation": "Prompt delay in CausVid generations",
      "details": "Prompt doesn't seem to take effect until ~2 seconds into video",
      "from": "Jonathan"
    },
    {
      "limitation": "UniAnimate needs I2V model",
      "details": "Cannot combine UniAnimate with CausVid on T2V models",
      "from": "Kijai"
    },
    {
      "limitation": "CausVid has adverse effects when not used as intended",
      "details": "Model not trained to be used as LoRA, but control methods like VACE mitigate most issues",
      "from": "Kijai"
    },
    {
      "limitation": "Detail loss with distilled models",
      "details": "Small details are lost, happens to some extent with these models generally",
      "from": "Kijai"
    },
    {
      "limitation": "1.3B model insufficient for understanding specifics",
      "details": "Cannot understand specific details as well as 14B model",
      "from": "Piblarg"
    },
    {
      "limitation": "VACE breaks at high speed complex scenes",
      "details": "Expected to fail with complex martial arts scenes or high-speed movement",
      "from": "cyncratic"
    },
    {
      "limitation": "Blurry hands remain an issue",
      "details": "Hands still come out blurry even with VACE 14B",
      "from": "Piblarg"
    },
    {
      "limitation": "Wan's weakness is hands",
      "details": "Hand generation is problematic, though skip layer guidance can help",
      "from": "Piblarg"
    },
    {
      "limitation": "CausVid has degradation over time",
      "details": "Especially apparent with CausVid, each subsequent iteration is slower with quality degradation",
      "from": "DawnII"
    },
    {
      "limitation": "VACE doesn't work well with combined controls",
      "details": "Combining depth and pose controls is unreliable and often reveals pose bones",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "SkyDF 1.3B doesn't understand input images",
      "details": "While extremely fast, SkyDF 1.3B is extremely useless because it doesn't understand what to do with input images",
      "from": "N0NSens"
    },
    {
      "limitation": "1.3B poor face quality in wide shots",
      "details": "Cannot handle images with small heads, needs medium/close shots",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "CausVid produces plastic look and broken physics",
      "details": "Quality trade-off for speed, worse than regular models",
      "from": "DiXiao"
    },
    {
      "limitation": "CausVid not properly implemented",
      "details": "Current inference logic different from intended use, but works anyway",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "VACE unreliable with combined controls",
      "details": "Combining pose and depth often produces artifacts",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "1.3B has low-level noise and poor detail",
      "details": "Noticeable quality degradation compared to 14B",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "1.3B model poor with references",
      "details": "1.3B model is quite bad with references compared to 14B, not great with details",
      "from": "Stad"
    },
    {
      "limitation": "VACE unstable for facial expressions",
      "details": "VACE is unstable for facial expression and hard to get right if the person is completely different from reference",
      "from": "chrisd0073"
    },
    {
      "limitation": "Native VACE doesn't support addons",
      "details": "Native workflow doesn't allow VACE addons for now, unlike wrapper implementation",
      "from": "The Punisher"
    },
    {
      "limitation": "Wrapper doesn't support GGUF",
      "details": "Kijai's wrapper doesn't support GGUF models, need to use native workflow for GGUF",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Cannot use two GPUs in parallel for generation",
      "details": "Can offload CLIP to second GPU but not run models simultaneously",
      "from": "The Punisher"
    },
    {
      "limitation": "GGUF models don't work with Kijai wrapper",
      "details": "Only compatible with native ComfyUI implementation",
      "from": "The Punisher"
    },
    {
      "limitation": "Causvid LoRA unpredictable results",
      "details": "Works well but results can be inconsistent",
      "from": "artemonary"
    },
    {
      "limitation": "VACE doesn't support multiple control types well",
      "details": "Combination of several controls causes issues",
      "from": "artemonary"
    },
    {
      "limitation": "Native VACE limited compared to wrapper",
      "details": "Cannot load as addon to another model, wrapper has better support",
      "from": "Draken"
    },
    {
      "limitation": "Distilled models lose motion",
      "details": "Speed optimization comes at cost of motion detail",
      "from": "Draken"
    },
    {
      "limitation": "GGUF models don't work with Kijai's wrapper",
      "details": "Quantized GGUF versions only work with native ComfyUI workflows",
      "from": "Davidodave"
    },
    {
      "limitation": "CausVid kills quality with some FX LoRAs",
      "details": "Very drastic quality hit when combined with certain effect LoRAs",
      "from": "JohnDopamine"
    },
    {
      "limitation": "MoviiGen only supports T2V",
      "details": "No I2V capability, only text-to-video generation",
      "from": "JohnDopamine"
    },
    {
      "limitation": "MoviiGen + VACE + CausVid LoRA I2V produces poor results",
      "details": "Combination doesn't work well for image-to-video",
      "from": "N0NSens"
    },
    {
      "limitation": "VACE removes 16fps restriction but adds complexity",
      "details": "While solving framerate issues, requires more complex workflow setup",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Long video generation limited by VRAM",
      "details": "OOM issues when going over 121 frames on RTX 5090, maximum ~6 seconds vs Runway's 20 seconds",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "Context switching creates jumps in long videos",
      "details": "When generating 300+ frames in chunks, visible jumps occur at context boundaries (background characters appearing/disappearing)",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "H.265 video compatibility",
      "details": "H.265 encoded videos don't play in some browsers/Discord clients, though they save disk space",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "14B model memory requirements",
      "details": "Even with 96GB RAM, extensive offloading is needed for 14B models",
      "from": "David Snow"
    },
    {
      "limitation": "Animal pose detection is poor quality",
      "details": "ControlNet_aux animal pose detection produces very bad results, advised against using",
      "from": "David Snow"
    },
    {
      "limitation": "MediaPipe doesn't capture mouth movement well",
      "details": "Standard MediaPipe face mesh fails to capture detailed mouth movements",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "Sapiens has color-based errors for body detection",
      "details": "Sapiens can misassign human parts when colors cause confusion",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Original CausVid only supports 1.3B model",
      "details": "Original CausVid implementation doesn't support 14B models",
      "from": "aipmaster"
    },
    {
      "limitation": "VACE only works with T2V models, not I2V",
      "details": "Attempting to use I2V models with VACE causes attribute errors",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Multiple VACE embeds can cause oversaturation",
      "details": "Using reference images in multiple encoders leads to cooked/oversaturated results",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "Input resolution must be divisible by 16",
      "details": "Tensor size mismatches occur if input dimensions aren't properly divisible",
      "from": "Kijai"
    },
    {
      "limitation": "TeaCache may interfere with low step generation",
      "details": "Step-based optimizations can have more weight impact on 4-8 step generation",
      "from": "JohnDopamine"
    },
    {
      "limitation": "Limited 1.3B model LoRAs available",
      "details": "Most LoRAs are for 14B models, only 1 LoRA available for 1.3B",
      "from": "Mngbg"
    },
    {
      "limitation": "VACE context loses information between chunks",
      "details": "When using context for long videos, some information can be lost between chunks, like clothing changes",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "CausVid 1.3B quality is poor",
      "details": "Quality of 1.3B CausVid is really bad, not same team behind it and needs special sampling code",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "First few frames of VACE Reference output are noisy",
      "details": "VACE Reference produces noisy first frames, may be related to reference not matching video being generated",
      "from": "Fawks"
    },
    {
      "limitation": "LoRA compatibility between models",
      "details": "LoRAs trained on base WAN work with SkyReels but not with MoviiGen",
      "from": "JohnDopamine"
    },
    {
      "limitation": "SLG and Zero Star don't work with CFG 1.0",
      "details": "Code never executes with CFG 1.0, so these optimizations have no effect",
      "from": "Kijai"
    },
    {
      "limitation": "TeaCache coefficients not calculated for CausVid setup",
      "details": "Using TeaCache with distillation models may not work properly as coefficients are off",
      "from": "Kijai"
    },
    {
      "limitation": "VACE doesn't separate ref from input frames/mask",
      "details": "Everything is processed as one unit, need multiple VACEs for separate control",
      "from": "Kijai"
    },
    {
      "limitation": "Lines control bakes lines into output",
      "details": "Even with semi-transparent lines, they get baked into the final video",
      "from": "Blink"
    },
    {
      "limitation": "Face preprocessors output points that VACE can't interpret",
      "details": "Point-based face controls don't work well with VACE",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "VACE struggles with less realistic content",
      "details": "Has difficulty with animated or stylized characters",
      "from": "Piblarg"
    },
    {
      "limitation": "CausVid makes generations feel stiff and lose dynamics",
      "details": "Characters get frozen facial expressions and jerky movement, similar to VEO2",
      "from": "amli"
    },
    {
      "limitation": "VACE doesn't capture eye movement well",
      "details": "Eye darting movements are often lost in generation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "LivePortrait is very picky about head position",
      "details": "Moving head too much causes bobble head effect",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "Model doesn't know specific characters without descriptive prompts",
      "details": "Needs explicit description like 'animated character' or 'Disney character' to recognize characters like Shrek",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "VACE incompatible with I2V models",
      "details": "VACE module only works with T2V versions of WAN, cannot be used with I2V models",
      "from": "DawnII"
    },
    {
      "limitation": "CausVid doesn't work with I2V in wrapper currently",
      "details": "CausVid LoRA cannot be used with I2V models in the wrapper workflow currently",
      "from": "Kijai"
    },
    {
      "limitation": "Points Editor can't delete first or last points",
      "details": "JavaScript limitations prevent deleting the first or last point in the points editor - always maintains a point pair",
      "from": "Kijai"
    },
    {
      "limitation": "540p Skyreels quality is poor",
      "details": "540p version of Skyreels produces terrible results in testing",
      "from": "Ada"
    },
    {
      "limitation": "Native ComfyUI VACE lacks module support",
      "details": "Native ComfyUI only supports full VACE model, no module version to use with other WAN models",
      "from": "zelgo_"
    },
    {
      "limitation": "VACE doesn't support blurred masks for inpainting",
      "details": "Sharp masks work but blurred masks cause issues",
      "from": "Kijai"
    },
    {
      "limitation": "Model limited to 81 frames properly, longer videos start to break down",
      "details": "Requires context windows or other techniques for longer generations",
      "from": "Kijai"
    },
    {
      "limitation": "Third VACE embed appears to have no effect",
      "details": "Likely code limitation in wrapper",
      "from": "Kijai"
    },
    {
      "limitation": "Under 5 frames with reference doesn't work reliably",
      "details": "Need minimum 5 frames for reference-based generation",
      "from": "Kijai"
    },
    {
      "limitation": "Context options break motion tracking",
      "details": "Using context options with CausVid can make it completely not follow motion",
      "from": "VK (5080 128gb)"
    },
    {
      "limitation": "NormalCrafter inconsistent with different frame counts",
      "details": "49 vs 81 frames yields entirely different results even with fixed seed",
      "from": "Guey.KhalaMari"
    },
    {
      "limitation": "Direct input only works with similar structure",
      "details": "Input frames method only works if reference image has same structure as first frame of video",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "High resolution memory requirements",
      "details": "1280x720 at 85 frames crashes at decode stage even with 40 block swap on RTX 5090",
      "from": "AJO"
    },
    {
      "limitation": "Video models trained on 8-bit color",
      "details": "All image/video models and VAEs are trained on 8-bit, tensors are 32-bit but represent 8-bit colors",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "VACE control combinations",
      "details": "Difficult to use control video + reference frame + LoRA all together - usually can only do 2 of 3 successfully",
      "from": "Piblarg"
    },
    {
      "limitation": "VACE masking doesn't work like traditional controlnets",
      "details": "Can't properly mask VACE effect like traditional controlnets - trying to mask depth maps results in no head or literal depth image artifacts",
      "from": "Draken"
    },
    {
      "limitation": "High resolution limits",
      "details": "Going much higher than 7280x1440 causes the model to start losing detail like the 1.3B model",
      "from": "ZombieMatrix"
    },
    {
      "limitation": "MPS LoRA compatibility",
      "details": "MPS LoRA only works with Fun model, not compatible with normal WAN models",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "VACE cannot target loras to specific masks",
      "details": "Currently not possible to apply different loras to different parts of the image using masks",
      "from": "David Snow"
    },
    {
      "limitation": "DWPose doesn't work well for distant shots",
      "details": "DW pose does not work well for far away shots for mouth movement, also normal and depth don't grab facial unless super close",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "Context window generation is very slow",
      "details": "Best to use with 1.3B model and/or CausVid LoRA due to extreme slowness",
      "from": "Kijai"
    },
    {
      "limitation": "81 frames not enough for scene transformation",
      "details": "81 frames is not enough for a scene transformation seemingly",
      "from": "AJO"
    },
    {
      "limitation": "Latent sync requires 25 FPS",
      "details": "Latent sync requires 25 FPS which causes sync issues with other frame rates",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "UniAnimate pose retargeting fails with arm extension",
      "details": "Almost always extends arms incorrectly when retargeting poses, barely works except with their example inputs",
      "from": "Kijai"
    },
    {
      "limitation": "Depth control overpowers reference image for body shape",
      "details": "When trying to change body proportions, depth control is too strong and loses reference image details",
      "from": "boorayjenkins"
    },
    {
      "limitation": "VACE redraws characters even with pose-only control",
      "details": "VACE tends to redraw the input character even when only using pose control without other inputs like depth or normals",
      "from": "N0NSens"
    },
    {
      "limitation": "Pose detection fails with complex clothing",
      "details": "Spinning dresses and complex clothing items cause pose detection to fail completely",
      "from": "A.I.Warper"
    },
    {
      "limitation": "I2V with Causvid has poor prompt adherence",
      "details": "Model listens to prompts very poorly when using I2V with Causvid LoRA",
      "from": "N0NSens"
    },
    {
      "limitation": "OpenPose preprocessing is unreliable",
      "details": "OpenPose preprocessing is the biggest limitation holding back open source video generation",
      "from": "Draken"
    },
    {
      "limitation": "FantasyTalking model only supports 3 seconds",
      "details": "New lip sync model is limited to very short durations and is extremely slow",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "VACE FaceReference fails at glancing angles",
      "details": "Face reference node struggles with identity preservation when face is not frontal",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "limitation": "MediaPipe face detection breaks beyond 45 degrees",
      "details": "MediaPipe fails when face angle exceeds 45 degrees or mouth is too open",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "limitation": "Context windows lose character likeness in long sequences",
      "details": "Processing 161+ frames with context windows results in character identity drift",
      "from": "A.I.Warper"
    },
    {
      "limitation": "CausVid LoRA compatibility unclear",
      "details": "Uncertain if CausVid LoRA works with i2v, VACE models or only t2v",
      "from": "patientx"
    },
    {
      "limitation": "VACE can't perform miracles with unrelated poses",
      "details": "Images need to have some relation to the desired pose/movement for VACE to work effectively",
      "from": "David Snow"
    },
    {
      "limitation": "Color shifts occur when extending videos",
      "details": "Noticeable color changes and quality degradation when using multiple VACE extensions, likely due to VAE encoding/decoding",
      "from": "ArtOfficial"
    },
    {
      "limitation": "Native ComfyUI VACE is not modular",
      "details": "Native VACE support lacks the flexibility of Kijai's wrapper implementation",
      "from": "David Snow"
    },
    {
      "limitation": "LoRAs trained on 1.3B model don't work with 14B model",
      "details": "LoRAs are not compatible between different model sizes",
      "from": "MilesCorban"
    },
    {
      "limitation": "Wan struggles with small faces in wide shots",
      "details": "Even with 14B model, small faces in wide shots remain challenging",
      "from": "traxxas25"
    },
    {
      "limitation": "VACE doesn't work well with blended control net inputs",
      "details": "Can't blend multiple control net outputs like depth + pose, they need separate encoders",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "CausVid doesn't work with I2V models",
      "details": "CausVid LoRA incompatible with image-to-video models, only works with T2V",
      "from": "Cubey"
    },
    {
      "limitation": "Frame interpolation quality issues",
      "details": "Current frame interpolation technology produces undesirable results, need better morphing tech",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "limitation": "Wan prompt weighting not working",
      "details": "Weighted prompts like (text:0.5) appear to be treated as full 1.0 weight",
      "from": "the_darkwatarus_museum"
    },
    {
      "limitation": "Multi encode VACE degrades quality",
      "details": "Adding second encode with dwpose makes video quality go to shit",
      "from": "David Snow"
    },
    {
      "limitation": "FollowYourEmoji threshold unforgiving",
      "details": "Only works with 'big' faces, would need face crop -> analyze -> repaste workflow",
      "from": "Valle"
    },
    {
      "limitation": "Color drift in long videos",
      "details": "Colors drift over time becoming crispier and crispier with overlapping batch method",
      "from": "notid"
    },
    {
      "limitation": "VACE not as sharp on higher resolutions",
      "details": "Friend reported VACE not as sharp as normal Wan on higher resolutions",
      "from": "The Punisher"
    },
    {
      "limitation": "Context issues persist",
      "details": "275 frame context problems occur with both full input image and isolated input",
      "from": "N0NSens"
    },
    {
      "limitation": "CausVid LoRA reduces expressiveness",
      "details": "Makes well-working LoRAs less expressive due to tranquilizing effect",
      "from": "hablaba"
    },
    {
      "limitation": "VACE model visual quality issues",
      "details": "Visual quality is garbage compared to original i2v 720p at full samples",
      "from": "aikitoria"
    },
    {
      "limitation": "Vid2vid with CausVid setup difficulty",
      "details": "Step counts and denoise values are tricky to setup for vid2vid with CausVid",
      "from": "Kijai"
    },
    {
      "limitation": "Impossible character fitting",
      "details": "Cannot fit characters with very different shapes (e.g., Spongebob into human silhouette)",
      "from": "David Snow"
    },
    {
      "limitation": "VACE extension works poorly with masks and has motion/color shifts",
      "details": "Extension causes different motion and colors to shift, and phases to slightly wrong video rather than seamless continuation",
      "from": "Kijai"
    },
    {
      "limitation": "FantaTalker not compatible with VACE",
      "details": "FantaTalker is for I2V model and I2V conditioning is not compatible with VACE",
      "from": "Kijai"
    },
    {
      "limitation": "VACE extension unusable for video extension on 14B",
      "details": "Giving initial image + last 16 frames results in visible break and low quality, unusable for extension",
      "from": "aikitoria"
    },
    {
      "limitation": "DiffSynth only available for 1.3B model",
      "details": "No DiffSynth support for 14B model currently",
      "from": "hicho"
    },
    {
      "limitation": "Wan can't hear audio",
      "details": "Not multimodal, lip sync is from prompting 'talking' not audio input",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "AccVideo scheduler requires denoise 1.0",
      "details": "Won't work with denoise settings under 1.0",
      "from": "Kijai"
    },
    {
      "limitation": "Quality degradation in extended videos",
      "details": "Using last frame as reference repeatedly degrades quality each batch",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "VACE lighting override",
      "details": "Takes too much lighting from reference image, overrides scene lighting, can't control this",
      "from": "Ruairi Robinson"
    },
    {
      "limitation": "Wan frame count restrictions",
      "details": "Can't do 44 frames, can do 41 or 45. Formula: divide by 4 plus 1",
      "from": "David Snow"
    },
    {
      "limitation": "AccVid seems worse for stylized content",
      "details": "AccVid good for realistic content but useless for stylized, definitely seems worse than CausVid for unintended use",
      "from": "Kijai"
    },
    {
      "limitation": "AccVid I2V loses prompt following",
      "details": "I2V with AccVid has decent quality but prompt following is gone, though some seeds do follow prompt",
      "from": "Kijai"
    },
    {
      "limitation": "TeaCache not good with AccVid",
      "details": "TeaCache generates messy results with AccVid and only saves 10 seconds",
      "from": "TK_999"
    },
    {
      "limitation": "Context options don't work with 1.3B",
      "details": "Context options for longer videos work with 14B but not with 1.3B model",
      "from": "N0NSens"
    },
    {
      "limitation": "VACE GGUF support unlikely",
      "details": "Would be a lot of work for something developer wouldn't use",
      "from": "Kijai"
    },
    {
      "limitation": "Phantom 14B doesn't work well with CFG distillation",
      "details": "Relies on CFG and has 3 noise predictions making it slow",
      "from": "Kijai"
    },
    {
      "limitation": "Phantom 14B trained on 480P data",
      "details": "Can work at 720P+ but results may be less stable",
      "from": "Guey.KhalaMari"
    },
    {
      "limitation": "CausVid LoRA doesn't work well with Phantom",
      "details": "Phantom needs CFG so acceleration methods are limited",
      "from": "Kijai"
    },
    {
      "limitation": "Sliding context windows have visible seams",
      "details": "Without tight control like VACE, windows don't blend well and seams are visible",
      "from": "Kijai"
    },
    {
      "limitation": "Mediapipe face detection is unreliable",
      "details": "Very hit or miss, half the time doesn't detect face even when prominent",
      "from": "David Snow"
    },
    {
      "limitation": "Phantom extremely slow generation",
      "details": "1375.69 seconds on RTX Pro 6000, described as 'unbelievably slow'",
      "from": "aikitoria"
    },
    {
      "limitation": "CausVid quality degradation",
      "details": "Quality drop is pretty immense with CausVid, causes flicker at 1.0 strength",
      "from": "Kijai"
    },
    {
      "limitation": "Phantom character rendering failures",
      "details": "Sometimes fails and only renders one of multiple characters",
      "from": "aikitoria"
    },
    {
      "limitation": "VACE color shifts",
      "details": "VACE causes color shifts that need correction",
      "from": "pom"
    },
    {
      "limitation": "Video artifacts with fast movement",
      "details": "Fast movements cause chattery tiled noise, sometimes only solution is slower input",
      "from": "Kijai"
    },
    {
      "limitation": "Preview flashing with references",
      "details": "Preview flashes between reference images and generated content due to latent structure",
      "from": "aikitoria"
    },
    {
      "limitation": "Phantom is very slow",
      "details": "~55 seconds/step at 480p, slower than other models",
      "from": "hau"
    },
    {
      "limitation": "CausVid works very badly with new Phantom",
      "details": "Will technically produce result but quality is poor",
      "from": "aikitoria"
    },
    {
      "limitation": "Lip sync difficult on wide shots",
      "details": "Only really works with close-up shots, far away shots make mouth movement inaccurate",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "Phantom file size is huge",
      "details": "Large model size causes storage issues",
      "from": "multiple users"
    },
    {
      "limitation": "Prompt adherence is really bad",
      "details": "Difficulty getting range of emotions from faces, struggles with complex prompts",
      "from": "hau"
    },
    {
      "limitation": "Quality looks more plasticky with merged models",
      "details": "Speed improvement comes at cost of more plastic-looking output",
      "from": "hau"
    },
    {
      "limitation": "Adding controls to Phantom reduces quality",
      "details": "Any additional controls added to phantom reduce quality and take away ID preserving functionality",
      "from": "Johnjohn7855"
    },
    {
      "limitation": "1280x720 causes OOM",
      "details": "Resolution limit on 4080 16gb system",
      "from": "CaptHook"
    },
    {
      "limitation": "Black noise prevalent in fp8",
      "details": "fp8 quantization introduces black noise artifacts",
      "from": "Kijai"
    },
    {
      "limitation": "Uni3C extreme motions don't work well",
      "details": "Orbit motion is too extreme, works better with simpler motions",
      "from": "Kijai"
    },
    {
      "limitation": "VACE + Phantom compatibility issues",
      "details": "Haven't had much success using VACE with Phantom, causes wicked flashes",
      "from": "Kijai"
    },
    {
      "limitation": "CausVid affects motion with I2V",
      "details": "Works but affects motion quite a bit when used with I2V models",
      "from": "Kijai"
    },
    {
      "limitation": "AccVid affects character consistency",
      "details": "AccVideo changes character consistency too much when combined with Phantom",
      "from": "David Snow"
    },
    {
      "limitation": "Frame limitations with certain models",
      "details": "Model doesn't like using more than 121 frames",
      "from": "aikitoria"
    },
    {
      "limitation": "DWPose failures on stylized characters",
      "details": "ComfyUI DWPose fails frequently on stylized characters",
      "from": "A.I.Warper"
    },
    {
      "limitation": "Phantom extraction as LoRA unsuccessful",
      "details": "Tried to extract Phantom as LoRA but it didn't work at all",
      "from": "Kijai"
    },
    {
      "limitation": "Uni3C only works with I2V models",
      "details": "Cannot currently work with T2V models due to latent architecture differences",
      "from": "Kijai"
    },
    {
      "limitation": "VACE doesn't work with I2V models",
      "details": "Architecture incompatibility prevents VACE from working with I2V",
      "from": "Kijai"
    },
    {
      "limitation": "Uni3C + VACE combination not possible",
      "details": "Since Uni3C requires I2V and VACE works with T2V, they cannot be combined",
      "from": "Kijai"
    },
    {
      "limitation": "4K processing challenging in ComfyUI",
      "details": "Pipeline issues and VRAM limitations make 4K processing difficult",
      "from": "chrisd0073"
    },
    {
      "limitation": "Phantom accuracy degrades with multiple controls",
      "details": "Adding more controls to Phantom reduces its ability to maintain consistency",
      "from": "Thom293"
    },
    {
      "limitation": "Uni3C doesn't work with VACE",
      "details": "Uni3C is I2V model so incompatible with VACE, though paper suggests it should be compatible",
      "from": "Guey.KhalaMari"
    },
    {
      "limitation": "Phantom loses referencing when combined with other tools",
      "details": "Phantom is good alone but loses referencing functionality when combined with anything else like VACE or controlnets",
      "from": "Johnjohn7855"
    },
    {
      "limitation": "14B depth controlnet has limited effectiveness",
      "details": "Works fine or great with some inputs, then not at all with others, similar to Flux controlnets",
      "from": "Kijai"
    },
    {
      "limitation": "DollyZoom doesn't work with Uni3C",
      "details": "Attempting to use DollyZoom in/out with Uni3C produces no working results",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "Can't fully capture likeness with speed LoRAs",
      "details": "Have to switch off causvid and accvideo loras to get proper character likeness, making testing prohibitive due to long generation times",
      "from": "David Snow"
    },
    {
      "limitation": "Phantom can't be used with Uni3C",
      "details": "Since Phantom is T2V and Uni3C requires I2V, they are incompatible",
      "from": "Johnjohn7855"
    },
    {
      "limitation": "AccVid doesn't work well on its own",
      "details": "From personal testing, AccVid needs to be combined with CausVid + Phantom, doesn't work well with VACE",
      "from": "Johnjohn7855"
    },
    {
      "limitation": "Uni3c doesn't work well with very short generations",
      "details": "Really short generations probably just don't work well",
      "from": "Kijai"
    },
    {
      "limitation": "Some images don't want to animate properly with Uni3c",
      "details": "Success depends on input image, complex camera movements can cause failure",
      "from": "N0NSens"
    },
    {
      "limitation": "Characters try to adjust to camera rather than follow prompt with Uni3c",
      "details": "Feeling that characters adapt to camera movement instead of prompt instructions",
      "from": "N0NSens"
    },
    {
      "limitation": "T2V character LoRAs don't work with VACE",
      "details": "Only I2V character LoRAs would work, but none found",
      "from": "hicho"
    },
    {
      "limitation": "Phantom extracting as LoRA doesn't work",
      "details": "Kijai tried extracting Phantom as LoRA but it didn't work",
      "from": "Johnjohn7855"
    },
    {
      "limitation": "All-in-one models suffer from LoRA integration",
      "details": "Merged models lose ability to adjust LoRA settings and suffer quality loss",
      "from": "Kijai"
    },
    {
      "limitation": "VACE can't blur masked input area",
      "details": "You can't blur the masked area in the input, the one that's supposed to be gray. Because then it's no longer gray for the blurred area",
      "from": "Kijai"
    },
    {
      "limitation": "Kontext limited for major edits",
      "details": "Can't do perspective shifts like Runway, can't do major out of context edits well like 4o",
      "from": "pom"
    },
    {
      "limitation": "CausVid v2 poor prompt following",
      "details": "v2 sucks for the prompt following",
      "from": "hicho"
    },
    {
      "limitation": "Full power Phantom too slow",
      "details": "It's just too slow for me to bother with - referring to full Phantom without distillation",
      "from": "Kijai"
    },
    {
      "limitation": "Character LoRA not viable option",
      "details": "Character lora will never be an option unfortunately",
      "from": "AJO"
    },
    {
      "limitation": "ATI missing HTML trajectory editor",
      "details": "Model released without the web interface for creating trajectories",
      "from": "Mngbg"
    },
    {
      "limitation": "ATI may not work properly at small resolutions",
      "details": "Potentially issues with 640x480 resolution",
      "from": "Mngbg"
    },
    {
      "limitation": "ATI is I2V only, doesn't work with VACE",
      "details": "Limited to image-to-video, not compatible with VACE system",
      "from": "Kijai"
    },
    {
      "limitation": "Uni3c kills prompt following",
      "details": "When using reference video for camera motion, prompt adherence suffers",
      "from": "mamad8"
    },
    {
      "limitation": "Phantom doesn't work well when paired with VACE",
      "details": "Good by itself but compatibility issues with VACE",
      "from": "David Snow"
    },
    {
      "limitation": "VACE face variance",
      "details": "There will be some variance in faces - close but not exact, good for generic face swaps but bad for specific exact face swaps",
      "from": "David Snow"
    },
    {
      "limitation": "Text/lettering issues",
      "details": "WAN has problems with generating accurate lettering",
      "from": "TimHannan"
    },
    {
      "limitation": "Prompt following with CausVid",
      "details": "CFG stuck at 1 means prompt following isn't fantastic",
      "from": "MilesCorban"
    },
    {
      "limitation": "Inpainting replaces rather than removes",
      "details": "Inpainting tends to replace objects with other things rather than cleanly removing them",
      "from": "Johnjohn7855"
    },
    {
      "limitation": "VACE not trained on normal maps",
      "details": "Can work to some extent but doesn't understand them as proper normal maps",
      "from": "Kijai"
    },
    {
      "limitation": "Reactor causes jitter around facial features",
      "details": "Particularly around eyes and mouth, affects ornaments around face",
      "from": "Johnjohn7855"
    },
    {
      "limitation": "ATI spline editor can't handle very fast movements",
      "details": "Too fast movements cause the system to stop working properly",
      "from": "Mngbg"
    },
    {
      "limitation": "Model primarily trained on realistic images",
      "details": "Works less well with stylized or cartoon content",
      "from": "Mngbg"
    },
    {
      "limitation": "VACE requires controlled scenarios for optimal results",
      "details": "Works best with people in stable environments rather than extreme motion scenarios at high speeds",
      "from": "chrisd0073"
    },
    {
      "limitation": "FaceFusion licensing restrictions",
      "details": "Mostly non-commercial licensing limits commercial use",
      "from": "chrisd0073"
    },
    {
      "limitation": "Model produces green artifacts when trajectory tracking fails",
      "details": "When insufficient trajectory points provided or tracking is lost, model fills gaps with green coloring",
      "from": "Kijai"
    }
  ],
  "hardware": [
    {
      "requirement": "RTX 5090 performance",
      "details": "5-7 minutes for 81 frames at 768x768 with 30 steps, max 21-24GB VRAM usage",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "RTX 4090 vs 5090 speed",
      "details": "5090 about 40% faster or more if you can utilize memory over offloading",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 4090 performance",
      "details": "4:08 for 640x480 @ 81 frames with Q8 GGUF, under 5 min for 1024x512",
      "from": "MilesCorban"
    },
    {
      "requirement": "Fun Control 14B timing",
      "details": "1.3B takes 02:30, 14B takes 15:30 on RTX 6000 Ada",
      "from": "Nathan Shipley"
    },
    {
      "requirement": "5090 power issues",
      "details": "Can spike over 600w, avoid splitter cables that come in box",
      "from": "ingi // SYSTMS"
    },
    {
      "requirement": "720p Fun Control on RTX 4080",
      "details": "Very slow, no OOM with 40 blocks at 720p but takes too much time",
      "from": "N0NSens"
    },
    {
      "requirement": "LoRA training time on 4090",
      "details": "Takes about 1 hour for 1.3B LoRA training using ai-toolkit",
      "from": "Jas"
    },
    {
      "requirement": "1001 frame generation possible with ex LoRA",
      "details": "Long generation achievable using extension LoRA with context windowing",
      "from": "Flipping Sigmas"
    },
    {
      "requirement": "VRAM for 4080",
      "details": "3:50 generation time for video, can handle various Wan models",
      "from": "N0NSens"
    },
    {
      "requirement": "RTX 3090 FP8 limitation",
      "details": "FP8 not supported, must use fp8_e5m2 or disable FP8 entirely",
      "from": "Kijai"
    },
    {
      "requirement": "4070 VRAM",
      "details": "12GB VRAM recognized correctly, sufficient for basic operations",
      "from": "Boop"
    },
    {
      "requirement": "Ubuntu version compatibility",
      "details": "22.04 recommended over 24.04 for CUDA toolkit compatibility",
      "from": "UsamaAhmedKhan"
    },
    {
      "requirement": "VRAM for 81 frames at various resolutions",
      "details": "24GB VRAM handles 1024x576, 16GB can do 832x480 at 720p/81fr",
      "from": "N0NSens"
    },
    {
      "requirement": "RAM for TeaCache",
      "details": "32GB RAM minimum, issues occur with non_blocking=True. 128GB recommended for large models",
      "from": "Kijai"
    },
    {
      "requirement": "Generation speed reference",
      "details": "3090: 3 minutes for 81 frames with VACE, 3090 Ti taking 14+ minutes indicates optimization issues",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "Runpod container disk space",
      "details": "Default 20GB too small, recommend 80-100GB container disk for ComfyUI",
      "from": "chrisd0073"
    },
    {
      "requirement": "CausVid 14B VRAM",
      "details": "4060ti can run 193 frames in 2-5 minutes at 832x480",
      "from": "V\u00e9role"
    },
    {
      "requirement": "CausVid requires torch.compile",
      "details": "Out of memory without it even on small inputs",
      "from": "Kijai"
    },
    {
      "requirement": "Regular Wan 14B",
      "details": "1001 frames takes 41 minutes on 4090",
      "from": "Flipping Sigmas"
    },
    {
      "requirement": "MoviiGen VRAM",
      "details": "Same as normal 14B base model, just a finetune",
      "from": "Kijai"
    },
    {
      "requirement": "MoviiGen system RAM",
      "details": "Need lots of system RAM to hold and convert the large model",
      "from": "Draken"
    },
    {
      "requirement": "LTX 13B",
      "details": "Can do 97 frames FHD on 24GB GPU using all VRAM",
      "from": "David Snow"
    },
    {
      "requirement": "Kijai's setup",
      "details": "Has 128GB RAM and 1gbps internet",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "requirement": "14B + VACE VRAM usage",
      "details": "Challenging to run on 16GB VRAM, successfully run on 12GB 4070 with optimizations",
      "from": "DawnII"
    },
    {
      "requirement": "VACE 14B performance",
      "details": "10-20% slower than base 14B when using TeaCache",
      "from": "DawnII"
    },
    {
      "requirement": "Causvid VACE timing",
      "details": "4 steps in 2:46 on unspecified hardware",
      "from": "DawnII"
    },
    {
      "requirement": "VACE 14B VRAM usage",
      "details": "Max 17.738 GB allocated with 20 blocks swapped on RTX 4090, 85 frames at 720x720",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 4080 16GB limitations",
      "details": "Needs 8 blocks swapped for VACE 14B, VRAM at 99% usage",
      "from": "Nokai"
    },
    {
      "requirement": "Generation speed",
      "details": "RTX 4090: ~300 seconds, RTX 4070: ~500 seconds per step",
      "from": "David Snow"
    },
    {
      "requirement": "VRAM for 14B VACE",
      "details": "User getting OOM even with 48GB, 24GB 3090Ti should work with proper block swap settings",
      "from": "Shubhooooo"
    },
    {
      "requirement": "CausVid LoRA generation time",
      "details": "1280x720, 69 frames, 4 steps takes ~2 minutes, max allocated 14.763GB VRAM",
      "from": "Kijai"
    },
    {
      "requirement": "Fast generation with CausVid LoRA",
      "details": "4 steps, 41 seconds generation time, 5 second videos in under a minute total",
      "from": "Nokai"
    },
    {
      "requirement": "RTX 3090 performance",
      "details": "1mn30 for 832x480x81 frames with CausVid, needs fp8 quantization and block swap",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "System RAM for 14B model",
      "details": "32GB RAM insufficient for smooth prompt changes, causes 5-minute freezes",
      "from": "Draken"
    },
    {
      "requirement": "VRAM for 14B model",
      "details": "Manageable on 4090 at 480p, incredibly slow at 720p",
      "from": "Kijai"
    },
    {
      "requirement": "5090 performance",
      "details": "40 minutes for VACE processing, much more capable for standard workflows",
      "from": "Piblarg"
    },
    {
      "requirement": "VRAM usage",
      "details": "2 seconds to generate 81 frames at 1.3B with CausVid, 17 seconds for different test",
      "from": "Kijai"
    },
    {
      "requirement": "Memory management",
      "details": "VAE decoding takes longer than generation time",
      "from": "Kijai"
    },
    {
      "requirement": "Performance timing",
      "details": "162 frames in 233 seconds, 129.86 seconds for 81 frames at 576x1024",
      "from": "A.I.Warper"
    },
    {
      "requirement": "VRAM for VACE 14B + CausVid",
      "details": "Can run on 16GB VRAM with proper block swapping (20 blocks), 4060Ti 16GB works with fp8 quants and block swap 30",
      "from": "N0NSens"
    },
    {
      "requirement": "RAM requirements",
      "details": "32GB RAM may not be sufficient, 64GB recommended for comfortable operation, especially with 24GB VRAM systems",
      "from": "Captain of the Dishwasher"
    },
    {
      "requirement": "Performance at 832x480",
      "details": "97 frames at 832x480 with 4 steps takes ~1:18 on 16GB VRAM system, 121 frames takes ~17:47",
      "from": "N0NSens"
    },
    {
      "requirement": "Max batch size",
      "details": "81 frames at 1024x576 max for single batch, 41 frames at 720p also works",
      "from": "A.I.Warper"
    },
    {
      "requirement": "CausVid 1.3B performance",
      "details": "4070Ti: 56 seconds for 1280x720x81 frames",
      "from": "DiXiao"
    },
    {
      "requirement": "CausVid speed comparison",
      "details": "3090: 45sec for 1.3B vs 1:30 for 14B, same content",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "14B VRAM with block swap",
      "details": "16GB should work with 40 block swap + 5 for VACE",
      "from": "DiXiao"
    },
    {
      "requirement": "5090D performance",
      "details": "10-35 seconds for various resolutions with CausVid",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "requirement": "VRAM usage with optimizations",
      "details": "720x720x81f with Q8_0 uses max 8.7GB VRAM on 4070ti with distorch, completes in 115-164 seconds",
      "from": "The Punisher"
    },
    {
      "requirement": "RAM requirements for VACE 14B",
      "details": "24GB VRAM + 32GB RAM user had issues, fixed by adding 8GB swap memory",
      "from": "Captain of the Dishwasher"
    },
    {
      "requirement": "1.3B generation speed",
      "details": "40 seconds for 81 frames at ~960x420 on RTX 3090",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "VRAM usage with GGUF Q5_K_S",
      "details": "10.272GB VRAM for 720x720x81f with distorch optimization",
      "from": "The Punisher"
    },
    {
      "requirement": "Q5_K_S works on 12GB VRAM",
      "details": "Confirmed working configuration for mid-range GPUs",
      "from": "The Punisher"
    },
    {
      "requirement": "14B model issues on 4090",
      "details": "Consistent problems running 14B VACE on RTX 4090, memory management issues",
      "from": "David Snow"
    },
    {
      "requirement": "System RAM important for model loading",
      "details": "Models shuttled to system RAM before VRAM, insufficient RAM causes disk paging",
      "from": "MilesCorban"
    },
    {
      "requirement": "4080 16GB performance",
      "details": "832x464 24fps 73frames 8step: 267.57s total, 29.41s/it, 62GB RAM usage",
      "from": "CaptHook"
    },
    {
      "requirement": "Native VACE OOM issues",
      "details": "24GB VRAM + 64GB RAM still limited to 320x320x49, offloading not effective",
      "from": "TK_999"
    },
    {
      "requirement": "14B model VRAM needs",
      "details": "12GB can work with tiled VAE and RAM offloading, 16GB+ recommended, needs lots of RAM",
      "from": "Kijai"
    },
    {
      "requirement": "162 frames generation",
      "details": "Takes 8 minutes on RTX 3090, uses max 17.6GB VRAM, 20.3GB reserved",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "81 frames at 1280x704",
      "details": "Uses 26.6GB max allocated memory on RTX 5090",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "CausVid memory usage",
      "details": "Only uses memory for 3 latents worth plus KV cache",
      "from": "Kijai"
    },
    {
      "requirement": "MoviiGen generation time",
      "details": "22 minutes for 1280x720 generation",
      "from": "ArtOfficial"
    },
    {
      "requirement": "14B model performance on RTX 3090 24GB",
      "details": "238 seconds for 121 frames at 960x544, 146 seconds for 832x480, requires 40 block swap",
      "from": "N0NSens"
    },
    {
      "requirement": "LoRA memory usage",
      "details": "Each LoRA adds approximately 200MB of VRAM",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "Recommended specs for workflows",
      "details": "RTX 3090 with 64GB RAM works well for 14B models with proper block swapping",
      "from": "BestWind"
    },
    {
      "requirement": "RTX 3090 VRAM usage",
      "details": "21.455 GB max allocated for 125 frames at 1024x576 with 5 steps, 268 seconds generation time",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "RTX 3090 with CausVid",
      "details": "8.278 GB max allocated, 10.969 GB reserved for 85 frames at 480x832 with 4 steps, 183 seconds",
      "from": "Jemmo"
    },
    {
      "requirement": "Block swap memory usage",
      "details": "13484.13MB on CPU, 1926.30MB on GPU, 15410.43MB total for transformer blocks",
      "from": "N0NSens"
    },
    {
      "requirement": "DepthCrafter VRAM requirements",
      "details": "Too much VRAM for RTX 3090, use DepthAnything instead",
      "from": "BestWind"
    },
    {
      "requirement": "VACE fits in 16GB VRAM",
      "details": "Can run on 12GB, 16GB confirmed working",
      "from": "Boop"
    },
    {
      "requirement": "CausVid + Fantasy Talking VRAM usage",
      "details": "Max allocated: 21.382 GB, Max reserved: 23.031 GB for 81 frames at 720x720",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "3090 compatibility confirmed",
      "details": "Face replacement and VACE workflows work on RTX 3090",
      "from": "BestWind"
    },
    {
      "requirement": "Long video generation RAM usage",
      "details": "401 frames generation used 105GB RAM with context",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "VACE 14B VRAM with block swapping",
      "details": "14B model runs on RTX 3090 24GB with 128GB RAM using block swapping (40 base blocks, 5 VACE blocks)",
      "from": "sneako1234"
    },
    {
      "requirement": "FP16 model on RTX 5090",
      "details": "FP16 14B model with FP8 quantization uses ~88% VRAM on RTX 5090 with no block swapping",
      "from": "ingi // SYSTMS"
    },
    {
      "requirement": "FP32 precision performance",
      "details": "FP32 precision took nearly an hour for 49 frames at 1280x720 but gave insane quality",
      "from": "ingi // SYSTMS"
    },
    {
      "requirement": "VACE 14B VRAM usage",
      "details": "Can run on 12GB VRAM with block swapping (40 and 5 blocks)",
      "from": "sneako1234"
    },
    {
      "requirement": "Generation times with optimizations",
      "details": "With SLG: 8-12 minutes, without optimizations: 16 minutes on unspecified hardware",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "5090 generation time",
      "details": "15 minutes for full generation with default context settings",
      "from": "852\u8a71 (hakoniwa)"
    },
    {
      "requirement": "TeaCache speed improvement",
      "details": "382s vs 203s (47% faster) with coefficient-disabled TeaCache on VACE 14B CausVid",
      "from": "David Snow"
    },
    {
      "requirement": "VRAM usage with VACE + 14B + CausVid",
      "details": "Can overload 16GB VRAM and 64GB RAM even with 20-30 blocks",
      "from": "N0NSens"
    },
    {
      "requirement": "Memory usage with TeaCache",
      "details": "TeaCache adds VRAM usage as trade-off for speed, not recommended with CausVid",
      "from": "Cubey"
    },
    {
      "requirement": "High memory usage with VACE",
      "details": "128GB RAM and 24GB VRAM both nearly maxed out during generation",
      "from": "Valle"
    },
    {
      "requirement": "VRAM usage for TeaCache",
      "details": "TeaCache was using ~750MB unnecessary VRAM at 1280x780 resolution before optimization fix",
      "from": "Kijai"
    },
    {
      "requirement": "CausVid speed improvement",
      "details": "CausVid at 1280x780 reduced generation time from 30 minutes to 4 minutes",
      "from": "boorayjenkins"
    },
    {
      "requirement": "RTX 4090 can handle 240 frames with CausVid LoRA",
      "details": "Using 14B model, significant improvement over previous capabilities",
      "from": "Cubey"
    },
    {
      "requirement": "Context windows don't increase VRAM usage",
      "details": "Uses same memory as single 81-frame generation regardless of total length",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 5090 performance",
      "details": "640x960 81 frames in 57 seconds, 544x832 501 frames possible with fp8 and no block swapping",
      "from": "seitanism"
    },
    {
      "requirement": "RTX 3090 performance",
      "details": "960x544 takes 20 minutes, much slower than newer cards",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "RTX 5080 performance",
      "details": "832x480 VACE 81 frames, 4 steps, no CausVid takes 13 minutes, with CausVid only 6 minutes",
      "from": "VK (5080 128gb)"
    },
    {
      "requirement": "RTX 4080 capacity",
      "details": "864x480 121f or 960x544 73f (97f OOM) with blocks 40/7",
      "from": "CaptHook"
    },
    {
      "requirement": "Memory usage at high resolution",
      "details": "1280x720 85 frames uses max 18.745 GB allocated, 21.250 GB reserved",
      "from": "AJO"
    },
    {
      "requirement": "RTX 5090 VRAM usage",
      "details": "Requires VAE tiling at max resolution to prevent crashes",
      "from": "AJO"
    },
    {
      "requirement": "4070 performance",
      "details": "50 s/it at 768x512 resolution with CausVid, around 3s/it for second pass refinement",
      "from": "Boop"
    },
    {
      "requirement": "H100 capability",
      "details": "User with H100s interested in running 14B VACE + WAN models",
      "from": "Christian Sandor"
    },
    {
      "requirement": "VRAM for higher resolutions",
      "details": "576x1024 at 85 frames with 4 steps takes 2:39, max allocated 10.090 GB on 14B model",
      "from": "A.I.Warper"
    },
    {
      "requirement": "Block swapping for 4090",
      "details": "Need block swapping value of 30 for 4090 24GB to avoid OOM with higher resolutions",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "GGUF loading with limited VRAM",
      "details": "Can load Q8 16GB wan with 12GB VRAM without issues using distorch for offloading",
      "from": "The Punisher"
    },
    {
      "requirement": "Performance at reduced resolution",
      "details": "864x480 121f 24fps 6steps = 8.27m on RTX 4080 16gb",
      "from": "CaptHook"
    },
    {
      "requirement": "Memory limits on cloud platforms",
      "details": "Users hitting device and disk limits on RunPod with 41GB VRAM RTX 4090, 100GB volume when running 14B fp16 model",
      "from": "stas"
    },
    {
      "requirement": "VRAM management for ControlNet preprocessing",
      "details": "ControlNet preprocessors consume significant VRAM and don't release it properly, causing OOM issues",
      "from": "MilesCorban"
    },
    {
      "requirement": "1024x1200 resolution processing time",
      "details": "Can process 1024x1200 resolution in under 3 minutes on appropriate hardware",
      "from": "Thom293"
    },
    {
      "requirement": "VRAM for 720p generation",
      "details": "160 frames at 720p before hitting VRAM limits on high-end cards",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "requirement": "System RAM for longer videos",
      "details": "128GB system RAM allows 300-450 frames with shared virtual GPU memory on A5000 24GB",
      "from": "Guey.KhalaMari"
    },
    {
      "requirement": "A100 80GB VRAM usage",
      "details": "May run out of system RAM due to offloading, needs sufficient RAM allocation",
      "from": "sneako1234"
    },
    {
      "requirement": "5090 performance with CausVid",
      "details": "85 frames at 1024x576 in 150 seconds with strength 1.0 and 4 steps",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "Higher resolution rendering for quality",
      "details": "Used A6000 on RunPod to render 1280x720 for better results with wide shots",
      "from": "traxxas25"
    },
    {
      "requirement": "VRAM usage with TeaCache and CausVid",
      "details": "TeaCache takes up more VRAM and slows inference when used with CausVid",
      "from": "Cubey"
    },
    {
      "requirement": "14B model VRAM usage",
      "details": "Kills PC without quantization, 96GB RAM user still hitting limits",
      "from": "David Snow"
    },
    {
      "requirement": "RAM usage with Wan",
      "details": "Framepack can use 50GB RAM while model is only 16GB, 128GB gets full",
      "from": "AR"
    },
    {
      "requirement": "2060 with 64GB RAM",
      "details": "Can run 14B Wan using ComfyUI offloading to RAM",
      "from": "hicho"
    },
    {
      "requirement": "4090 performance",
      "details": "163 frames in 20 minutes for 14B vs 5 minutes for 1.3B",
      "from": "VK (5080 128gb)"
    },
    {
      "requirement": "RTX 6000 Blackwell",
      "details": "96GB VRAM, 2x faster than RTX4090",
      "from": "Tytanick"
    },
    {
      "requirement": "4090 with 128GB RAM",
      "details": "Confirmed working setup for VACE 14B workflows",
      "from": "Ruairi Robinson"
    },
    {
      "requirement": "Higher resolution memory usage",
      "details": "1280x720 resolution causes system to struggle even with 4090/128GB",
      "from": "Ruairi Robinson"
    },
    {
      "requirement": "VRAM management for lower-end GPUs",
      "details": "2060 6GB can offload to 64GB RAM using vram_management setting of 1",
      "from": "hicho"
    },
    {
      "requirement": "WSL2 memory allocation",
      "details": "Need proper .wslconfig setup for WSL2 to avoid out of memory errors",
      "from": "jerms_ai_(4090_24g)"
    },
    {
      "requirement": "14B model with quantization",
      "details": "For 4090 24GB, should enable quantization and block swap for 14B models. Can do 1280x720x81 on fp16 without quant when swapping all blocks with 128GB RAM",
      "from": "Kijai"
    },
    {
      "requirement": "4070 12GB recommendations",
      "details": "Should use 1.3B VACE models or Q5 GGUF. 14B models may work with native but will test system limits. 32GB RAM helpful",
      "from": "David Snow"
    },
    {
      "requirement": "Memory management",
      "details": "Use fp8 models with fp16 node settings, avoid bf16 for memory issues. Block swap not needed until allocation errors occur",
      "from": "hicho"
    },
    {
      "requirement": "RTX 5070 vs 5070 Ti performance",
      "details": "5070 gets Flux image in 9 seconds, 5070 Ti in 7 seconds. 5070 essentially 3090 with 12GB VRAM",
      "from": "VK (5080 128gb)"
    },
    {
      "requirement": "VACE 161 frames",
      "details": "Possible on RTX 5090 with 32GB RAM without degradation",
      "from": "Piblarg"
    },
    {
      "requirement": "Wan I2V vs VACE memory",
      "details": "User can run Wan I2V 14B 480p Q8 but not VACE 14B Q8",
      "from": "Boop"
    },
    {
      "requirement": "Multi-GPU setup",
      "details": "User has 48GB VRAM across two RTX 3090 cards",
      "from": "SegmentationFault"
    },
    {
      "requirement": "VRAM management",
      "details": "Keep VRAM usage at 96-98% for optimal performance. 99% usage makes generation very slow. Use block swapping for OOM issues",
      "from": "N0NSens"
    },
    {
      "requirement": "System RAM for block swapping",
      "details": "64GB+ RAM essential if swapping all blocks (40 and 15) to avoid VRAM OOM",
      "from": "JohnDopamine"
    },
    {
      "requirement": "Performance examples",
      "details": "RTX 5090: 321 frames at 960x576, 5 steps in 616 seconds. RTX 3090: 81 frames, 4 steps in 500 seconds. RTX 4090: 720p 81 frames in 229 seconds",
      "from": "VRGameDevGirl84(RTX 5090), Valle, slmonker(5090D 32GB)"
    },
    {
      "requirement": "Phantom 14B performance",
      "details": "Very slow, estimated 30+ minutes on 4090, 2/40 steps taking 89.10s each",
      "from": "Kijai"
    },
    {
      "requirement": "RTX Pro 6000 performance",
      "details": "96GB VRAM, torch.compile doesn't work, not 2x faster than 5090",
      "from": "aikitoria"
    },
    {
      "requirement": "TeaCache optimization",
      "details": "Can be enabled from step 6 to speed up generation",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM usage Phantom 89 frames 1280x768",
      "details": "Max allocated: 13.303 GB, Max reserved: 19.250 GB",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM usage CausVid 89 frames 1280x720",
      "details": "Max allocated: 21.446 GB, Max reserved: 27.188 GB",
      "from": "Kijai"
    },
    {
      "requirement": "Performance RTX 4090",
      "details": "Phantom 1024x576 89 frames 6 steps: 119.92 seconds",
      "from": "JohnDopamine"
    },
    {
      "requirement": "Performance comparison 480p vs 720p",
      "details": "480p: 413.80 seconds vs 1375.69 seconds for higher resolution on RTX Pro 6000",
      "from": "aikitoria"
    },
    {
      "requirement": "Phantom VRAM usage",
      "details": "Max allocated 12.836 GB, max reserved 13.875 GB for generation",
      "from": "JohnDopamine"
    },
    {
      "requirement": "Phantom speed",
      "details": "~55 seconds/step at 832x480 with 40 steps on fp16 14B with flash_attn2",
      "from": "hau"
    },
    {
      "requirement": "Merged model performance",
      "details": "133 seconds total for generation without SageAttention",
      "from": "Thom293"
    },
    {
      "requirement": "4080 16GB performance",
      "details": "1024x576 16fps 8steps = 8.5min generation time, 1280x720 causes OOM",
      "from": "CaptHook"
    },
    {
      "requirement": "4070ti performance",
      "details": "480p 81fps generation in just two minutes",
      "from": "MaQue"
    },
    {
      "requirement": "RTX 6000 Pro vs 4090",
      "details": "RTX 6000 Pro about double the speed of 4090 on wan, also has 96GB VRAM",
      "from": "aikitoria"
    },
    {
      "requirement": "Generation speed with 14B",
      "details": "~16s/step at 7 steps reported",
      "from": "hau"
    },
    {
      "requirement": "Phantom generation time",
      "details": "1280x720 completed in 262 seconds",
      "from": "Thom293"
    },
    {
      "requirement": "VRAM for 1024x576",
      "details": "24GB VRAM user experiencing OOM at 1024x576 with combo model",
      "from": "The Shadow (NYC)"
    },
    {
      "requirement": "VACE processing time",
      "details": "189 frame shots at 1280x720 take 26-30 minutes on RTX 4090",
      "from": "Ruairi Robinson"
    },
    {
      "requirement": "Temperature monitoring",
      "details": "76 degrees on RTX 3090ti should be fine, not causing crashes",
      "from": "Kijai"
    },
    {
      "requirement": "Phantom 14B VRAM usage",
      "details": "89 frames at 1024x576 with 8 steps: Max allocated 22.296 GB, Max reserved 24.500 GB, completed in 188.08 seconds",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "Uni3C performance on 3090",
      "details": "Taking 277 s/it at 480p, 25 frames - significantly slow",
      "from": "sneako1234"
    },
    {
      "requirement": "VRAM usage optimization",
      "details": "Should adjust block swap until reaching 95% VRAM usage for optimal performance",
      "from": "Kijai"
    },
    {
      "requirement": "Video enhancement speed",
      "details": "1280x720 video enhancement with LoRA completed in 90.50 seconds using CausVid on RTX 5090",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "Frame limitations for RTX 2060",
      "details": "81 frames at 720p causes issues on RTX 2060, requiring restart and reduction of resolution and frames",
      "from": "hicho"
    },
    {
      "requirement": "Windows VRAM monitoring limitations",
      "details": "nvtop not available for Windows, nvitop limited to total VRAM usage, can't show process-specific usage",
      "from": "Kijai"
    },
    {
      "requirement": "Phantom 14B generation time",
      "details": "25 minutes for 1x832x480 at 40 steps on RTX 4090",
      "from": "Janosch Simon"
    },
    {
      "requirement": "Character LoRA training on Wan T2V 14B",
      "details": "20 images dataset, ~40 minutes training time, requires 32GB VRAM",
      "from": "DeZoomer"
    },
    {
      "requirement": "WSL RAM allocation issues",
      "details": "WSL would only use a percentage of system RAM and would start swapping way before it needed to",
      "from": "MilesCorban"
    },
    {
      "requirement": "Loading transformer parameters slow on WSL",
      "details": "Loading transformer parameters to cpu taking 8+ minutes on WSL vs instant on other systems",
      "from": "mamad8"
    },
    {
      "requirement": "VRAM for batch processing",
      "details": "768x480 resolution, 8 steps, batch prompts took 335.75 seconds on RTX 5090",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "Speed comparison",
      "details": "5 scenes, 81 frames, 8 steps took 300 seconds",
      "from": "AJO"
    },
    {
      "requirement": "FP8 performance boost",
      "details": "FP8 quantization provides roughly half the generation time vs FP16",
      "from": "AJO"
    },
    {
      "requirement": "VRAM for 1024x576, 45 frames",
      "details": "Max allocated: 18.113 GB, Max reserved: 18.750 GB, 48.60 seconds",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "VRAM for 1024x576, 85 frames",
      "details": "Max allocated: 21.914 GB, Max reserved: 23.062 GB, 103.29 seconds",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "VRAM for 1280x720, 85 frames with block swapping",
      "details": "Max allocated: 25.309 GB, Max reserved: 26.969 GB, 217.20 seconds with 5 blocks swapped",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "VRAM for Wan VAE",
      "details": "24GB sufficient, never requires tiling",
      "from": "Kijai"
    },
    {
      "requirement": "RAM for model offloading",
      "details": "Insufficient system RAM causes OOM when using CausVid LoRa",
      "from": "TK_999"
    },
    {
      "requirement": "nvfp4 support",
      "details": "Only available on Blackwell (5090) and up, not useful for 4xxx series",
      "from": "aikitoria"
    },
    {
      "requirement": "Film grain node VRAM usage",
      "details": "Takes up significant VRAM for unknown reasons",
      "from": "Johnjohn7855"
    },
    {
      "requirement": "RTX 3000 series compatibility issue",
      "details": "Fp8_e4m3fn format doesn't work with compile on RTX 3000 series GPUs, need to use fp8_e5m2 instead",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 8000 compatibility question",
      "details": "User asking about running Wan 2.1 and VACE on RTX 8000 with 48GB each in dual GPU NVLinked setup",
      "from": "Ruairi Robinson"
    }
  ],
  "community_creations": [
    {
      "creation": "Python 3D camera control app",
      "type": "tool",
      "description": "Mouse movement through 3D space with WASD keys like FPS game to create camera movements",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "CbrPnK style LoRA",
      "type": "lora",
      "description": "Custom style LoRA trained for 14000 steps with trigger 'cbrpnk style'",
      "from": "JohnDopamine"
    },
    {
      "creation": "Two-pass v2v workflow",
      "type": "workflow",
      "description": "Workflow for strong stylization using two samplers with 3+4 step split for better LoRA effects",
      "from": "David Snow"
    },
    {
      "creation": "CyberPop LoRA",
      "type": "lora",
      "description": "Style LoRA based on latentpop mixed with other LoRAs, trigger 'cbrpnk style', better as v2v LoRA due to 1.3B limitations",
      "from": "David Snow"
    },
    {
      "creation": "FP8 quantized Fun v1.1 InP",
      "type": "model",
      "description": "Community member created FP8 version of 47GB Fun v1.1 model",
      "from": "JmySff"
    },
    {
      "creation": "NormalCrafter ComfyUI node",
      "type": "node",
      "description": "Node implementation for NormalCrafter normal map generation",
      "from": "A.I.Warper"
    },
    {
      "creation": "Long form video workflow",
      "type": "workflow",
      "description": "Corrected workflow for extended video generation",
      "from": "Flipping Sigmas"
    },
    {
      "creation": "NormalCrafter ComfyUI Wrapper",
      "type": "node",
      "description": "Wrapper for NormalCrafter to generate normal maps from video",
      "from": "A.I.Warper"
    },
    {
      "creation": "Multiple military vehicle LoRAs",
      "type": "lora",
      "description": "Tiger Tank, T34, Sherman, Mi-24, AH-64, KA-52 LoRAs for Wan 2.1",
      "from": "MisterMango"
    },
    {
      "creation": "FLF2V workflow with character consistency",
      "type": "workflow",
      "description": "Uses Redux+Fill+ControlNet+InstantID for consistent character generation",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "creation": "Custom frame removal node",
      "type": "node",
      "description": "Removes first N frames from video output",
      "from": "V\u00e9role"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "wrapper",
      "description": "ComfyUI wrapper for Wan models by Kijai",
      "from": "Kijai"
    },
    {
      "creation": "Tank LoRAs series",
      "type": "lora",
      "description": "German Panther, Pz.IV H, M18 Hellcat tank models",
      "from": "MisterMango"
    },
    {
      "creation": "Fruitiger LoRA",
      "type": "lora",
      "description": "Works with MoviiGen model",
      "from": "Dream Making"
    },
    {
      "creation": "Model conversion script",
      "type": "tool",
      "description": "Combines multipart models into single safetensors and saves to fp16",
      "from": "Juampab12"
    },
    {
      "creation": "CausVid LoRA extraction",
      "type": "lora",
      "description": "Extracted CausVid as LoRA for use with base T2V 14B model, took 30 minutes to extract",
      "from": "Kijai"
    },
    {
      "creation": "Blender relighting setup",
      "type": "workflow",
      "description": "Simple relighting using normals test with NormalCrafter and IC-Light",
      "from": "David Snow"
    },
    {
      "creation": "CausVid LoRA conversion",
      "type": "lora",
      "description": "Converted CausVid distillation model to LoRA format for compatibility",
      "from": "Kijai"
    },
    {
      "creation": "Block edit node for CausVid",
      "type": "node",
      "description": "Node to skip specific blocks when applying CausVid LoRA",
      "from": "seruva19"
    },
    {
      "creation": "WanVideoWrapper updates",
      "type": "node",
      "description": "Updated example workflows and outpainting fixes",
      "from": "Kijai"
    },
    {
      "creation": "CausVid LoRA extraction",
      "type": "lora",
      "description": "Extracted CausVid models as LoRAs for better VACE compatibility",
      "from": "Kijai"
    },
    {
      "creation": "WanVideoWrapper updates",
      "type": "node",
      "description": "Removed blocks option from VACE loader node, improved compatibility",
      "from": "Kijai"
    },
    {
      "creation": "Juxtapoz fluxtapoz nodes fork",
      "type": "node",
      "description": "Forked nodes for use with HiDream and RF_inversion",
      "from": "A.I.Warper"
    },
    {
      "creation": "Clean VRAM Used node",
      "type": "node",
      "description": "From comfyui easy used, works for preventing OOMs between generations",
      "from": "The Punisher"
    },
    {
      "creation": "VACE 14B GGUF quantizations",
      "type": "model",
      "description": "Q3_K_S, Q8_0, Q4_K_S, Q5_K_S quantized versions of VACE 14B module",
      "from": "The Punisher"
    },
    {
      "creation": "Installation scripts for Windows portable",
      "type": "tool",
      "description": "One-click installers for CUDA 12.8, Triton, Sage attention, xformers, and torch 2.8 dev",
      "from": "The Punisher"
    },
    {
      "creation": "VACE 14B GGUF models",
      "type": "model",
      "description": "Q3_K_S, Q5_K_S, Q6 quantized versions of VACE 14B for native ComfyUI",
      "from": "The Punisher"
    },
    {
      "creation": "Second pass cleanup workflow",
      "type": "workflow",
      "description": "Two-stage process using 14B for generation and 1.3B for cleanup",
      "from": "David Snow"
    },
    {
      "creation": "Custom sampler with advanced options",
      "type": "node",
      "description": "Sampler with eta, bongmath, steps control, and scheduler options",
      "from": "Clownshark Batwing"
    },
    {
      "creation": "VACE workflow for low VRAM",
      "type": "workflow",
      "description": "Working VACE setup for GGUF models on low VRAM systems",
      "from": "The Punisher"
    },
    {
      "creation": "Style guide video workflow",
      "type": "workflow",
      "description": "System for using one video as style reference for another",
      "from": "Clownshark Batwing"
    },
    {
      "creation": "Flux + VACE workflow",
      "type": "workflow",
      "description": "Automated pipeline that generates reference image from video first frame using Flux, then processes with VACE",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "CausVid 14B support",
      "type": "lora",
      "description": "Extended CausVid to support Wan2.1-14B model with 9-step distillation",
      "from": "aipmaster"
    },
    {
      "creation": "Face replacement workflow",
      "type": "workflow",
      "description": "Replace face of character with any reference using VACE",
      "from": "BestWind"
    },
    {
      "creation": "Combined depth+pose VACE workflow",
      "type": "workflow",
      "description": "Uses separate VACE embeds for depth and pose with blend controls",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "Fantasy Talking + CausVid workflow",
      "type": "workflow",
      "description": "Fast lip-sync generation using CausVid LoRA",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "Character LoRAs for 14B model",
      "type": "lora",
      "description": "Multiple LoRAs trained on David Snow's image set for 14B model, various epochs available",
      "from": "JohnDopamine"
    },
    {
      "creation": "VACE model patching code",
      "type": "tool",
      "description": "Code to add VACE patch to standard WAN models, though still missing 30 layers and config issues",
      "from": "The Punisher"
    },
    {
      "creation": "Native VACE module support",
      "type": "node",
      "description": "Native ComfyUI implementation of VACE modules, works but needs GGUF and Sage attention fixes",
      "from": "The Punisher"
    },
    {
      "creation": "WAN prompt generator",
      "type": "tool",
      "description": "Gemini-generated prompt templates for WAN models, works with various LLMs",
      "from": "aipmaster"
    },
    {
      "creation": "DetailZ LoRA for WAN",
      "type": "lora",
      "description": "Detail enhancer specifically trained for WAN models, adds additional detail to outputs",
      "from": "David Snow"
    },
    {
      "creation": "Custom GGUF DistORCH loader",
      "type": "node",
      "description": "Custom implementation for GGUF compatibility with VACE",
      "from": "The Punisher"
    },
    {
      "creation": "Custom safetensors node for WAN",
      "type": "node",
      "description": "Works with safetensors (1.3b version) but not GGUFs yet",
      "from": "The Punisher"
    },
    {
      "creation": "Double VACE workflow",
      "type": "workflow",
      "description": "Workflow using two VACE encode nodes with depth and pose controls for better character animation",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "creation": "Video inpainting workflow",
      "type": "workflow",
      "description": "500+ frame video face replacement workflow being developed",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "creation": "Custom VACE GPT",
      "type": "tool",
      "description": "GPT trained on VACE/WAN documentation to guide users through setup, prompts, and troubleshooting",
      "from": "AJO"
    },
    {
      "creation": "Color consistency LoRA",
      "type": "lora",
      "description": "LoRA trained for color and contrast consistency across multiple generations",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "creation": "Headless Wan2GP",
      "type": "tool",
      "description": "Headless fork of deepmeepbeeps VRAM efficient Wan repo - one command setup",
      "from": "pom"
    },
    {
      "creation": "Relative head motion node",
      "type": "node",
      "description": "Custom node for stabilized view with head rotations - combines pose of heads with relative movement from first frame",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "creation": "Updated torch.compile node",
      "type": "node",
      "description": "Updated compile node to use ComfyUI core wrapper, works with LoRAs without special patching",
      "from": "Kijai"
    },
    {
      "creation": "VACE custom node for native",
      "type": "node",
      "description": "Custom node to make VACE work with native ComfyUI",
      "from": "The Punisher"
    },
    {
      "creation": "WanVideo CFG Schedule node",
      "type": "node",
      "description": "Node for dynamically changing CFG during generation",
      "from": "Kijai"
    },
    {
      "creation": "VACE GGUF models",
      "type": "model",
      "description": "GGUF versions of VACE models that work in native without custom nodes",
      "from": "The Punisher"
    },
    {
      "creation": "Boolean invert node for automated stacking",
      "type": "node",
      "description": "Simple custom node that automatically determines stacking based on whether image is vertical or horizontal",
      "from": "Valle"
    },
    {
      "creation": "Scipy-based parameter optimization node",
      "type": "node",
      "description": "Uses Scipy methods for more efficient parameter testing than grid search or random search",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "creation": "FaceReference node",
      "type": "node",
      "description": "Custom node attempting to replace LivePortrait functionality for face reference, currently in development with NC licensing issues",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "creation": "ControlNet + Masking workflow",
      "type": "workflow",
      "description": "Workflow that processes video input to generate pose, depth, canny, MediaPipe face controls and subject masks using Segment Anything v2",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "creation": "Video extension workflow with color matching",
      "type": "workflow",
      "description": "Complex workflow for extending videos with overlapping frames and color correction",
      "from": "ArtOfficial"
    },
    {
      "creation": "Get image or mask from batch method",
      "type": "workflow",
      "description": "Simplified approach to video extension without reloading compressed video",
      "from": "zelgo_"
    },
    {
      "creation": "ComfyUI Pose Interpolation",
      "type": "node",
      "description": "Node for creating smooth pose transitions between different poses",
      "from": "toyxyz"
    },
    {
      "creation": "Flux Redux with PuLID workflow",
      "type": "workflow",
      "description": "Comprehensive Flux workflow including Redux, PuLID, face detailer and upscaler for character concept creation",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "creation": "Auto face masking workflow nodes",
      "type": "workflow",
      "description": "Embedded workflow nodes for automatic face masking in video processing",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "Modified pose interpolation",
      "type": "tool",
      "description": "Controls timing of pose appearance with body shape correction",
      "from": "toyxyz"
    },
    {
      "creation": "Overlapping batch video system",
      "type": "workflow",
      "description": "Breaks reference video into batches with overlaps for arbitrary length videos",
      "from": "notid"
    },
    {
      "creation": "Seamless looping workflow",
      "type": "workflow",
      "description": "Processes generated video to create perfect loops",
      "from": "The Shadow (NYC)"
    },
    {
      "creation": "TrimVideoLatent node",
      "type": "node",
      "description": "Node for removing frames from video latents",
      "from": "Guey.KhalaMari"
    },
    {
      "creation": "Dynamic block swap system",
      "type": "workflow",
      "description": "Conditional nodes based on pixel counts with thresholds for different block swap amounts",
      "from": "DevouredBeef"
    },
    {
      "creation": "Custom node for last frame reference",
      "type": "node",
      "description": "Grabs most recent PNG from output folder and saves last frame from output video for next batch reference",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "AccVideo LoRA extraction",
      "type": "lora",
      "description": "Kijai working on extracting LoRA from AccVideo model",
      "from": "Kijai"
    },
    {
      "creation": "Comprehensive workflow package",
      "type": "workflow",
      "description": "CN extraction, Flux with Redux/depth/PuLID, and Wan VACE/CausVid workflows",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "creation": "Toon style LoRA for Wan 14B",
      "type": "lora",
      "description": "Trained with 60 images on CivitAI, works effectively with Wan 14B T2V",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "AccVid LoRA extraction",
      "type": "lora",
      "description": "Extracted AccVid LoRA from the model for use in workflows",
      "from": "Kijai"
    },
    {
      "creation": "VACE workflow with multiple inputs",
      "type": "workflow",
      "description": "Single VACE encode using expanded mask, depth, normals and pose together",
      "from": "David Snow"
    },
    {
      "creation": "Merged CausVid + AccVid Model",
      "type": "model",
      "description": "Combined LoRA merge using Claude-assisted tool, fp16 format, no separate LoRA needed",
      "from": "JohnDopamine"
    },
    {
      "creation": "OpenPose Body Editor",
      "type": "tool",
      "description": "Tool for scaling skeleton proportions for character adaptation",
      "from": "toyxyz"
    },
    {
      "creation": "CausVid/AccVid/Phantom merge",
      "type": "model",
      "description": "Merged model combining CausVid, AccVid and Phantom for faster inference",
      "from": "JohnDopamine"
    },
    {
      "creation": "Cinematic LoRA",
      "type": "lora",
      "description": "Acts more as a detailer, works with VACE at strengths 1.5-2.0",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "DWPose scaler node",
      "type": "node",
      "description": "Single node for scaling DWPose output",
      "from": "A.I.Warper"
    },
    {
      "creation": "Phantom model merge",
      "type": "model",
      "description": "Merge of fp16 phantom model with accvid and causvid LoRAs integrated",
      "from": "JohnDopamine"
    },
    {
      "creation": "Cinematic model LoRA",
      "type": "lora",
      "description": "Cinematic model converted to LoRA format, can help reduce Phantom's animated tendency",
      "from": "Juampab12"
    },
    {
      "creation": "Hakoniwa anime model",
      "type": "model",
      "description": "Adjusted model to give stronger anime style output",
      "from": "852\u8a71 (hakoniwa)"
    },
    {
      "creation": "Ultimate OpenPose Editor",
      "type": "node",
      "description": "Tool for editing OpenPose with automatic scaling based on target keypoints",
      "from": "toyxyz"
    },
    {
      "creation": "Phantom custom merge",
      "type": "model",
      "description": "Faster Phantom model merged with CausVid and AccVid",
      "from": "Johnjohn7855"
    },
    {
      "creation": "Native ComfyUI VACE implementation",
      "type": "node",
      "description": "Allows VACE to patch other models natively in ComfyUI lazy eval system",
      "from": "Ablejones"
    },
    {
      "creation": "John Dopamine's Phantom merge",
      "type": "model",
      "description": "Faster version of Phantom model with improved inference speed",
      "from": "JohnDopamine"
    },
    {
      "creation": "Arnold LoRA for Wan2.1",
      "type": "lora",
      "description": "Character LoRA trained for Arnold Schwarzenegger",
      "from": "JohnDopamine"
    },
    {
      "creation": "Over-engineered face replacement workflow",
      "type": "workflow",
      "description": "Month-long project creating organized workflow for face replacement that works with any video regardless of face size",
      "from": "DeZoomer"
    },
    {
      "creation": "GGUF quantizations of Phantom 14B",
      "type": "model",
      "description": "Q8 and other quantized versions of Phantom model",
      "from": "The Punisher"
    },
    {
      "creation": "Merged Phantom + CausVid + AccVid model",
      "type": "model",
      "description": "All-in-one model with distillation built in for faster generation",
      "from": "JohnDopamine"
    },
    {
      "creation": "Character LoRA training workflow",
      "type": "workflow",
      "description": "Local training with ai-toolkit using 20 images on Wan T2V 14B",
      "from": "DeZoomer"
    },
    {
      "creation": "Open Pose Interpolation node update",
      "type": "node",
      "description": "Updated to allow for Pose Sequence input, can change body shape naturally",
      "from": "toyxyz"
    },
    {
      "creation": "CFG Schedule node in WanVideoWrapper",
      "type": "node",
      "description": "Node for CFG scheduling with CausVid",
      "from": "Ada"
    },
    {
      "creation": "CausVid LoRA variants",
      "type": "lora",
      "description": "Multiple pruned versions including attention layers only and first block disabled versions",
      "from": "Kijai"
    },
    {
      "creation": "Face detailer node for videos",
      "type": "node",
      "description": "Detects faces, crops across all frames, reprocesses for detail enhancement",
      "from": "mamad8"
    },
    {
      "creation": "LoRA merger workflow",
      "type": "workflow",
      "description": "ComfyUI nodes for merging multiple LoRAs into single model",
      "from": "Thom293"
    },
    {
      "creation": "GGUF Phantom workflow",
      "type": "workflow",
      "description": "Native ComfyUI workflow for GPU-poor users to run high quality generations",
      "from": "The Punisher"
    },
    {
      "creation": "2D Animation Effects LoRA",
      "type": "lora",
      "description": "Outputs 2D animation effects based on simple instruction videos made in After Effects, controllable drawing amount, can create water, waves, smoke, spray, flames, explosions through prompts",
      "from": "852\u8a71 (hakoniwa)"
    },
    {
      "creation": "Merged Phantom + CausVid V2 + detail LoRAs model",
      "type": "model",
      "description": "Combined model with Phantom, CausVid V2, MoviiGen, and detail enhancement LoRAs merged at strength 1.0",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "Custom face cropping node",
      "type": "node",
      "description": "Crops faces in video so all faces have same size, created with LLM help",
      "from": "mamad8"
    },
    {
      "creation": "ComfyUI-trained GPT",
      "type": "tool",
      "description": "GPT model trained on ComfyUI to help create custom nodes",
      "from": "VRGameDevGirl84(RTX 5090)"
    }
  ]
}