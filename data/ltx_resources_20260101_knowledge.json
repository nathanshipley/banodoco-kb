{
  "channel": "ltx_resources",
  "date_range": "2026-01-01 to 2026-02-01",
  "messages_processed": 2891,
  "chunks_processed": 8,
  "api_usage": {
    "input_tokens": 113124,
    "output_tokens": 24749,
    "estimated_cost": 0.710607
  },
  "extracted_at": "2026-02-02T00:31:50.040816Z",
  "discoveries": [
    {
      "finding": "LTX Video 2 can be fine-tuned on a subject or concept in 1-2 hours",
      "details": "Much faster than previous models like Wan 2.2 which took significant resources and time",
      "from": "oumoumad"
    },
    {
      "finding": "Lower steps and lower LoRA strength brings back normal audio functionality",
      "details": "Audio was a challenge in initial tests but this fixed sound working normally",
      "from": "oumoumad"
    },
    {
      "finding": "CFG scheduling can be done with clownsampler using 15/5 split",
      "details": "User testing 15/5 CFG split, though time between samplers almost negates the benefit",
      "from": "TK_999"
    },
    {
      "finding": "I2V workflow fixed with noise masks issue resolved",
      "details": "Problem was with noise masks and how they are carried on the latent state info",
      "from": "Ablejones"
    },
    {
      "finding": "Preview issues fixed by Kijai, unrelated to PR",
      "details": "Previews breaking when steps and steps to run == 1, disabling previews node fixed everything",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "ComfyUI now has native LoRA training capability",
      "details": "Comfy mentioned you can train a lora in ComfyUI, works on any supported model",
      "from": "TK_999"
    },
    {
      "finding": "Spatial upscaler doesn't need CFG, causes memory spikes",
      "details": "Not much benefit to using CFG on upscaler pass, memory use spikes significantly",
      "from": "TK_999"
    },
    {
      "finding": "Using LTXV Spatio Temporal Tiled VAE Decode instead of standard tile prevents freezing and improves speed by 2.5x",
      "details": "Also helps with stability on RTX 4090",
      "from": "avataraim"
    },
    {
      "finding": "Disabling distilled LoRA, upscale temporary, and using 25fps instead of 50fps improves performance",
      "details": "Multiple optimizations for better speed",
      "from": "avataraim"
    },
    {
      "finding": "Dev model requires distilled model or distilled LoRA for proper artifact removal and upscaling",
      "details": "Dev model alone produces noise artifacts, distilled component is required even when using dev model",
      "from": "Ablejones"
    },
    {
      "finding": "Latent normalization works better when implemented inside the sampler rather than at outer sampler level",
      "details": "Functions with complex chained sampling and unsampling, using values like 1,1,0.5,1 per step",
      "from": "Ablejones"
    },
    {
      "finding": "Running a step or two with CFG helps improve results",
      "details": "Similar approach as used with WAN testing",
      "from": "Ablejones"
    },
    {
      "finding": "LTX-2 has intentionally sparse latent space so advanced sampling may not be as useful",
      "details": "Model limitations mean best sampling can only get most accurate output from what model can produce",
      "from": "Ablejones"
    },
    {
      "finding": "Context length should ideally be around less than 15k, but 20k is probably fine for LTX-2",
      "details": "This is the context limit of audio and video dimension and length. Going bigger/longer may result in more artifacts",
      "from": "TK_999"
    },
    {
      "finding": "New ComfyUI commit drastically reduces LTX-2 VAE VRAM use",
      "details": "Recent update to ComfyUI that significantly improves VRAM efficiency for LTX-2 VAE",
      "from": "TK_999"
    },
    {
      "finding": "Dev model at full CFG for first stage may help quality",
      "details": "Using pure dev model with full CFG for first stage potentially improves results",
      "from": "Ablejones"
    },
    {
      "finding": "LTX-2 preprocess node applies h264 video compression at level 33",
      "details": "The LTXVpreprocess node compresses images using h264 at CRF 33 to better match training data and fight static video issues",
      "from": "TK_999"
    },
    {
      "finding": "GGUF quantized models require shorter prompts for good adherence",
      "details": "When using quantized GGUF models, prompts should be under 200 words for good adherence",
      "from": "The Shadow (NYC)"
    },
    {
      "finding": "Negative prompts are powerful for excluding specific styles in LTX",
      "details": "Using negative prompts to exclude unwanted visual styles and aesthetics works effectively",
      "from": "Ablejones"
    },
    {
      "finding": "LTX model tends to randomly add nudity unprompted",
      "details": "Model sometimes generates nude content even when not requested, requires adding nudity to negative prompts",
      "from": "hudson223"
    },
    {
      "finding": "Four finger generations are common issue",
      "details": "Model frequently generates characters with four fingers instead of five",
      "from": "hudson223"
    },
    {
      "finding": "LTX works well for Western style/Fox animation aesthetic",
      "details": "Model performs particularly well for this specific animation style",
      "from": "hudson223"
    },
    {
      "finding": "LTX-2 i2v LoRA adapter significantly improves image-to-video generation",
      "details": "High-rank LoRA trained on 30,000 generated videos that eliminates need for complex workflows, image preprocessing, or ControlNet stacking for i2v",
      "from": "Fill"
    },
    {
      "finding": "HuMo can be used as detailer for LTX-2 outputs with context windows",
      "details": "HuMo maintains reference for higher denoising while preserving scene and lip sync, allowing stronger correction of motion artifacts",
      "from": "Ablejones"
    },
    {
      "finding": "Using 32fps for LTX-2 then downsampling to 16fps helps with edge artifacts",
      "details": "Generate at 32fps, select every 2nd frame to get 16fps, then RIFE back to 32fps reduces artifacting on edges",
      "from": "The Shadow (NYC)"
    },
    {
      "finding": "First frame of LTX-2 i2v is often blurry with compression artifacts",
      "details": "Using 2nd or 3rd frame as HuMo reference instead of first frame may improve results",
      "from": "Ablejones"
    },
    {
      "finding": "Diagimplicit samplers work well with motion blur effects",
      "details": "Particularly effective for achieving squash stretch feel in animations",
      "from": "hudson223"
    },
    {
      "finding": "Memory override node with 0.01 value fixes OOM issues on 4090 with 121 frames at 1024x1024",
      "details": "Resolved out of memory errors on second stage using v3 workflow",
      "from": "scf"
    },
    {
      "finding": "Shift value in ModelSamplingSD3 can be cranked up to 40-50 for more denoise changes",
      "details": "Equates to about 0.5 denoise value and is safe to use",
      "from": "Ablejones"
    },
    {
      "finding": "LoRA + reference images gives best character consistency",
      "details": "Combination works better than either method alone",
      "from": "Eiw / Dennis"
    },
    {
      "finding": "60 steps makes huge difference compared to lower steps in LTX2",
      "details": "First pass workflow with higher steps significantly improves results",
      "from": "NC17z"
    },
    {
      "finding": "Using dev model with distilled lora at 0.4-0.6 strength gives better results than full distilled",
      "details": "Works better for I2V than using distilled model alone",
      "from": "N0NSens"
    },
    {
      "finding": "30 FPS gives much better results than 24/25 FPS with less artifacts",
      "details": "Works even with 8 steps, not much slower than lower framerates",
      "from": "Elvaxorn"
    },
    {
      "finding": "LTX2 beats WAN 2.2 on video leaderboard",
      "details": "Can do variable frame rates, 10-20+ seconds, I2V/V2V/T2V, audio to video in one model",
      "from": "Phr00t"
    },
    {
      "finding": "LTX-2 multiwhatever guide node works with sharksamplers",
      "details": "Works out of the box without modification",
      "from": "Ablejones"
    },
    {
      "finding": "LTX reacts to all audio types",
      "details": "Speech to lips, music to bodies - reacts differently to different audio elements",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "finding": "Model prompts well with animation LoRAs",
      "details": "BTNB animation LoRA works at various weights",
      "from": "hudson223"
    }
  ],
  "troubleshooting": [
    {
      "problem": "Expected all tensors to be on same device error",
      "solution": "Issue occurred on 2nd stage 1st sampler, related to OOM and CFG hitting VRAM hard",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "Preview breaking on 1 step samplers",
      "solution": "Disable previews node when steps and steps to run == 1",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "I2V workflow not working",
      "solution": "Use the RES4LYF PR branch, problem was with noise masks on latent state",
      "from": "Ablejones"
    },
    {
      "problem": "Temporal upscaler audio mismatch",
      "solution": "Temporal doubles fps (25->50), need to account for this in audio latent processing",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "OOM issues on second pass",
      "solution": "Apply 16 VRAM measurements, disable temporal upscaling, lower CFG, or use fewer steps",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "Random no movement or lipsync",
      "solution": "Avoid words implying stillness: static, still, frozen, locked, hold, pause. Use 'music video' in prompt or describe specific singing actions",
      "from": "EnragedAntelope"
    },
    {
      "problem": "Tensor size mismatch in I2V with guides",
      "solution": "Don't upscale latent when using spatial resize, crop images instead of resizing, reapply guide conditioning on both stages",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "Nested tensor clone error when using RES4LYF",
      "solution": "Update to latest RES4LYF main version, remove old RES4LYF folders to avoid conflicts",
      "from": "Ablejones"
    },
    {
      "problem": "VAE decoding crashes at high resolutions",
      "solution": "Lower tile size, increase system RAM or swap space - 64GB RAM still needs swap for these large models",
      "from": "Ablejones"
    },
    {
      "problem": "Audio and video length mismatch causing errors",
      "solution": "Ensure audio duration matches frame count exactly - e.g., 121 frames = 4.85 sec for 25fps",
      "from": "avataraim"
    },
    {
      "problem": "Clownshark sampler errors with custom audio in first pass",
      "solution": "Compare workflow to working examples, often caused by mismatch between audio and video parameters",
      "from": "Arts Bro"
    },
    {
      "problem": "Quick shift from start image in first few frames",
      "solution": "Issue acknowledged but no specific solution provided yet",
      "from": "Ablejones"
    },
    {
      "problem": "Ghosting effects in output",
      "solution": "Appears to be timeline/temporal coherence issue, no specific fix mentioned",
      "from": "The Shadow (NYC)"
    },
    {
      "problem": "Second VAE decode sometimes hangs",
      "solution": "Customize vae decode tiled inputs to match system capabilities for particular video",
      "from": "Ablejones"
    },
    {
      "problem": "VAE decoding tiled artifacts visible in sky",
      "solution": "Use more overlap and larger tiles, or try better decoder nodes",
      "from": "Ablejones"
    },
    {
      "problem": "Out of sync audio issues",
      "solution": "Check if input audio is getting into sampler, ensure audio input toggle is enabled",
      "from": "neofuturo"
    },
    {
      "problem": "Poor mouth movement/lip sync",
      "solution": "Prompt correctly for audio synchronization - model doesn't do it automatically, requires specific prompting",
      "from": "Ablejones"
    },
    {
      "problem": "ClownsharKSampler error with LTXV Set Audio Video Mask By Time",
      "solution": "Use SamplerCustomAdvanced instead, or use Ablejones' dev fork which has a fix",
      "from": "Ablejones"
    },
    {
      "problem": "Page file/swap disk space size issues in containers",
      "solution": "Increase page file size for container - not specific to workflow, occurs when decoding video at large sizes with VAE",
      "from": "Ablejones"
    },
    {
      "problem": "EasyFilePaths breaks core nodes due to PyTorch version conflict",
      "solution": "Don't use EasyFilePaths as it has ultralytics dependency incompatible with PyTorch 2.10.0",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "ClownShark workflow randomly adding 16 frames when generating",
      "solution": "Issue may be related to addguidemulti node, chainsampler might also need it",
      "from": "JUSTSWEATERS"
    },
    {
      "problem": "Cannot encode larger latent with image to video node",
      "solution": "Use WanEx I2VCustomEmbeds node and encode latents before passing in using tiled VAE encoding node",
      "from": "Ablejones"
    },
    {
      "problem": "Spatial upscale in chainsampler causing tensor unpacking error",
      "solution": "Most functions don't preserve state info for continuation - need to grab and re-apply state info using separate nodes",
      "from": "Ablejones"
    },
    {
      "problem": "Chunk override mode incrementing filenames instead of overwriting",
      "solution": "Use backup mode instead, as it works as expected",
      "from": "mightynice"
    },
    {
      "problem": "RESplain debug line causing NameError in RES4LYF",
      "solution": "Remove debug line from latents.py line 953",
      "from": "Xor"
    },
    {
      "problem": "Mask size tensor mismatch in RES4LYF",
      "solution": "Fixed in dev fork with major changes for LTX-2 compatibility",
      "from": "Ablejones"
    },
    {
      "problem": "FL_ImageNotes node causing Mac compatibility issues",
      "solution": "Bypass the FL_ImageNotes nodes, likely font-related issue",
      "from": "Fill"
    },
    {
      "problem": "GGUF Gemma model architecture errors",
      "solution": "Use alternative model as provided GGUF has unsupported 'gemma3' architecture",
      "from": "army"
    },
    {
      "problem": "Second chunk video generation becomes very slow",
      "solution": "Speed degradation from 4min to 13min likely due to VRAM management, use clean VRAM nodes",
      "from": "army"
    },
    {
      "problem": "OOM on second stage with 121 frames at 1024x1024 on 4090",
      "solution": "Use memory override node with 0.01 value",
      "from": "scf"
    },
    {
      "problem": "Audio normalizing node crashes with shark sampler nodes",
      "solution": "Disconnect audio normalizing node from nodes using multiple samplers",
      "from": "Ablejones"
    },
    {
      "problem": "No sample_sigmas matched current timestep error",
      "solution": "Use v1 workflow instead of v2, or switch to Ablejones fork of res4lyf",
      "from": "N0NSens"
    },
    {
      "problem": "Wrong T5 clip causing errors",
      "solution": "Make sure T5 clip doesn't have 'enc' in the name",
      "from": "Ablejones"
    },
    {
      "problem": "Multistep samplers failing with low step counts",
      "solution": "Steps too far apart, use fallback sampler or increase step count",
      "from": "Ablejones"
    },
    {
      "problem": "Bug in workflow with text encoder connector",
      "solution": "Change text encoder connector from dev to distill",
      "from": "Phr00t"
    },
    {
      "problem": "Cycles resampler error",
      "solution": "Can do cycles manually by chaining unsample/resample nodes instead of using cycles rebounds",
      "from": "Ablejones"
    },
    {
      "problem": "Error from LTX2 Attention Tuner",
      "solution": "Try bypassing the LTX2 Attention Tuner node",
      "from": "Ablejones"
    },
    {
      "problem": "Memory efficient sageattn error",
      "solution": "Restart ComfyUI after updating everything",
      "from": "Gleb Tretyak"
    }
  ],
  "comparisons": [
    {
      "comparison": "LQ vs HQ output quality",
      "verdict": "LQ version had more vivid and natural facial expressions and gestures, though less accurate lip sync",
      "from": "Rusch Meyer"
    },
    {
      "comparison": "Dev model vs Distilled model for first pass",
      "verdict": "Dev model alone cannot generate coherent video by itself, needs distilled component",
      "from": "TK_999"
    },
    {
      "comparison": "First pass only vs second pass with distill LoRA",
      "verdict": "Second pass with dev+distill LoRA for 3 steps shows significant improvement over first pass alone",
      "from": "TK_999"
    },
    {
      "comparison": "res_2 vs other samplers for LTX",
      "verdict": "No clear visual benefits from res_2, almost double the time for minimal gains",
      "from": "The Shadow (NYC)"
    },
    {
      "comparison": "Linear-euler vs res_2 family",
      "verdict": "Linear-euler preferred for LTX2 until proper dialed workflow allows grid tests",
      "from": "The Shadow (NYC)"
    },
    {
      "comparison": "LTX-2 vs WAN 2.2",
      "verdict": "LTX-2 seems better than WAN 2.2 once kinks are sorted out, at least for certain styles",
      "from": "crinklypaper"
    },
    {
      "comparison": "Dev model quality vs distilled model",
      "verdict": "If dev model can't do good details at high resolution, it's much more similar to WAN 2.2 High Noise model in functionality",
      "from": "Ablejones"
    },
    {
      "comparison": "LTX-2 I2V vs WAN I2V",
      "verdict": "WAN had dedicated I2V model, LTX-2 is half-way between T2V and I2V so might not be as good as dedicated model",
      "from": "Ablejones"
    },
    {
      "comparison": "LTX vs WAN for 2D animation",
      "verdict": "LTX too mushy for 2D styles compared to WAN, but good for 3D animation and dialogue scenes",
      "from": "dj47"
    },
    {
      "comparison": "Dev model vs Distill model quality",
      "verdict": "Haven't seen any dev model generation as good as distill model output",
      "from": "Ablejones"
    },
    {
      "comparison": "LTX vs WAN for different use cases",
      "verdict": "Use WAN with hand drawn keyframes for action, LTX or infinite talk for dialogue scenes",
      "from": "dj47"
    },
    {
      "comparison": "VACE 2.1 vs other models for hand drawn animation",
      "verdict": "VACE 2.1 still best for purely hand drawn animation",
      "from": "dj47"
    },
    {
      "comparison": "HuMo 14b vs 1.7b",
      "verdict": "14b much better quality but 1.7b is 3x faster. 540p vid: 4:25 with 14b vs 1:26 with 1.7b",
      "from": "Ablejones"
    },
    {
      "comparison": "Flux Klein vs ZImage",
      "verdict": "ZImage more realistic, Klein has better adherence. 9b Klein on par with ZImage",
      "from": "dj47"
    },
    {
      "comparison": "HeartMula vs Suno",
      "verdict": "HeartMula quality like Suno v2, limited to schlager/pop, Suno still better",
      "from": "burgstall"
    },
    {
      "comparison": "LTX i2v LoRA vs without",
      "verdict": "LoRA improves image quality and motion details in T2V, better hands and natural colors",
      "from": "Kevin 'Literally Who?' Abanto"
    },
    {
      "comparison": "LTX2 vs WAN 2.2",
      "verdict": "LTX2 beats WAN 2.2 on video leaderboard and offers more features like audio, longer videos, variable framerates",
      "from": "Phr00t"
    },
    {
      "comparison": "Dev model + distilled lora vs full distilled model",
      "verdict": "Dev model with distilled lora at 0.4-0.6 strength gives better results, especially for I2V",
      "from": "N0NSens"
    },
    {
      "comparison": "30 FPS vs 24/25 FPS",
      "verdict": "30 FPS gives much better results with less artifacts, not significantly slower",
      "from": "Elvaxorn"
    },
    {
      "comparison": "60 steps vs lower steps",
      "verdict": "60 steps makes huge difference in quality",
      "from": "NC17z"
    }
  ],
  "tips": [
    {
      "tip": "Use specific singing descriptions for better lip sync",
      "context": "Instead of just 'singing', use 'rapping with fierce intensity', 'lip-syncing with swagger', or 'music video performer'",
      "from": "Arts Bro"
    },
    {
      "tip": "Add grain to input image for I2V",
      "context": "Signals to model that image is already in transition, recommended by devs along with preprocessing node",
      "from": "EnragedAntelope"
    },
    {
      "tip": "Use image strength 0.6-0.9 for I2V",
      "context": "1.0 is too high, 0.6-0.9 is good range to try",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "Set compression to 35 instead of 33",
      "context": "35 is default for preprocessor node",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "Apply FPS conditioning twice with different values",
      "context": "1 node for 1st stage with base fps, 1 node for 2nd stage with base/doubled fps",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "For faster generations skip temporal upscaling",
      "context": "Or lower resolution, use fewer steps with CFG > 1.0, or change samplers to CFG 1.0",
      "from": "Ablejones"
    },
    {
      "tip": "Slightly open mouths help with non-human singing",
      "context": "Especially for non-human characters to avoid human mouth bleeding",
      "from": "Arts Bro"
    },
    {
      "tip": "Add consistent grain before upscaling",
      "context": "Normalize sharpness and textures, then do detail upscaling for best results",
      "from": "hudson223"
    },
    {
      "tip": "Use large overlap of 256 to avoid tiling artifacts in VAE decode",
      "context": "When using tiled VAE decoding",
      "from": "Ablejones"
    },
    {
      "tip": "Resize extreme resolution output to 720p to save disk space and make Wan detailer more reasonable",
      "context": "After LTX 2x upscale stage",
      "from": "Ablejones"
    },
    {
      "tip": "Use as large tile_size and temporal_size as hardware can handle, with smallest overlap possible without artifacts",
      "context": "For optimal tiled VAE performance",
      "from": "Ablejones"
    },
    {
      "tip": "Use melband roformer if audio has background noise or isn't lip syncing properly",
      "context": "When working with audio that has unwanted background sounds",
      "from": "Arts Bro"
    },
    {
      "tip": "Keep denoising low on Wan detailer stage to avoid interfering with movement or lip sync",
      "context": "When using Wan as detailer pass after LTX",
      "from": "Ablejones"
    },
    {
      "tip": "Basic audio processing and frequency filters can help remove weird rumbling tones from LTX-2 output",
      "context": "Post-processing LTX-2 generated audio",
      "from": "Ablejones"
    },
    {
      "tip": "Use dev model for first stage, then distill model/lora for upscale",
      "context": "This is the intended pipeline for best quality video according to devs",
      "from": "Ablejones"
    },
    {
      "tip": "Don't add detailer in first pass",
      "context": "Adding detailer in first pass makes output much worse than without it",
      "from": "scf"
    },
    {
      "tip": "Only run detailer when happy with LTX-2 result",
      "context": "Detailer stage takes significant time, so optimize main generation first",
      "from": "Ablejones"
    },
    {
      "tip": "Use memory_usage_factor of 0.1 with KJ's memory usage factor override node",
      "context": "Helps prevent VRAM overflow and improves performance",
      "from": "Ablejones"
    },
    {
      "tip": "Lower I2V strength to 0.8 instead of 1.0",
      "context": "Strength of 1 can be too much for I2V, varies based on frame count",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "Start with 97 frames for testing mouth movements",
      "context": "Judge mouth movements and camera on LQ first, if mouths aren't moving properly, prompt is likely the issue",
      "from": "Arts Bro"
    },
    {
      "tip": "Use detailed character descriptions for ZiT character consistency",
      "context": "When working with character-based content",
      "from": "burgstall"
    },
    {
      "tip": "Train a LoRA for single character used across multiple videos",
      "context": "When you have one main character appearing in multiple videos",
      "from": "burgstall"
    },
    {
      "tip": "Don't try to cut too many corners with animation",
      "context": "Quality animation still requires mindful iteration even with AI tools",
      "from": "hudson223"
    },
    {
      "tip": "Animation LoRAs help get better looking inbetweens",
      "context": "When working on animation projects",
      "from": "JUSTSWEATERS"
    },
    {
      "tip": "Use lower denoise for better facial expression preservation",
      "context": "When concerned about maintaining expressions, try 2 steps instead of 3 (set start step to 6/8)",
      "from": "Ablejones"
    },
    {
      "tip": "Higher FPS helps motion quality significantly",
      "context": "More fps helps a lot with LTX-2 output quality",
      "from": "dj47"
    },
    {
      "tip": "Use Klein for different characters instead of training LoRAs",
      "context": "For music videos with different characters, Klein works like omni model - generate consistent character images then animate with LTX",
      "from": "dj47"
    },
    {
      "tip": "Keep resolution reasonable for Wan models",
      "context": "720p was high resolution one month ago, LTX's compressed latent space means higher res isn't always better",
      "from": "Ablejones"
    },
    {
      "tip": "Prompt complexity affects generation duration significantly",
      "context": "Prompt length and complexity have big impact on processing time",
      "from": "hudson223"
    },
    {
      "tip": "Use additional face close-up images as batch inputs into HuMo reference",
      "context": "For character consistency, especially with loras",
      "from": "Ablejones"
    },
    {
      "tip": "Try changing unsampler steps_to_run from 2 to 3 for face vibration issues",
      "context": "Will take longer (6 steps total instead of 4) but may fix face stability",
      "from": "Ablejones"
    },
    {
      "tip": "Generate at 32fps and convert to 25fps for HuMo workflow",
      "context": "This formula works well for LTX2 to WAN21 HuMo pipeline",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "Don't use crazy long LLM-generated prompts with LTX2",
      "context": "Simple prompts work better, model doesn't follow complex prompts well anyway",
      "from": "Phr00t"
    },
    {
      "tip": "Use increments of 8 when increasing VAE temporal overlap",
      "context": "Should be 24 or 32, not arbitrary numbers",
      "from": "Phr00t"
    },
    {
      "tip": "For likeness LoRA only need 30-50 clips, style LoRA needs a little more since it's more conceptual",
      "context": "LoRA training data requirements",
      "from": "Golden"
    },
    {
      "tip": "Try bypassing mel separator node and send full audio instead of just vocals",
      "context": "When you want LTX to react to all audio elements, not just optimize for lip sync",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "tip": "Second prompt box is for short extra instructions for both T2I and I2V",
      "context": "Like making sure main character wears exact same outfit all the time",
      "from": "BitPoet (Chris)"
    }
  ],
  "news": [
    {
      "update": "LTX added latent normalization sampler",
      "details": "New sampler type available in official repo",
      "from": "TK_999"
    },
    {
      "update": "ComfyUI now supports native LoRA training",
      "details": "Training node available that works on any supported model",
      "from": "TK_999"
    },
    {
      "update": "Resource page will be updated Monday with fresh Discord scrape",
      "details": "Will include new information from early days of LTX-2 usage",
      "from": "Nathan Shipley"
    },
    {
      "update": "LTX team plans to fix audio rumbling issues in version 1.5",
      "details": "Acknowledged issue with early rumbling tones in audio generation",
      "from": "TK_999"
    },
    {
      "update": "RES4LYF merged to main branch",
      "details": "Latest updates now available in main branch",
      "from": "TK_999"
    },
    {
      "update": "New VRAM reduction node released by KJ",
      "details": "Memory usage factor node that allows pushing system limits properly",
      "from": "BNP4535353"
    },
    {
      "update": "New ComfyUI commit reduces LTX-2 VAE VRAM usage",
      "details": "Recent update that drastically reduces VRAM requirements for LTX-2 VAE",
      "from": "TK_999"
    },
    {
      "update": "New version of RES4LYF workflow coming",
      "details": "Ablejones working on fixes and improvements, should be available today or tomorrow",
      "from": "Ablejones"
    },
    {
      "update": "VEO now supports 4K output",
      "details": "VEO model has been updated with 4K resolution capability",
      "from": "dj47"
    },
    {
      "update": "Z-image base model release coming soon",
      "details": "Expected to be even better than current models, has been teased for months",
      "from": "LarpsAI"
    },
    {
      "update": "LTX-2 Image2Video Adapter LoRA released",
      "details": "High-rank LoRA trained on 30k videos to improve i2v generation without complex workflows",
      "from": "Fill"
    },
    {
      "update": "Multiple datasets in processing",
      "details": "General anime version on 50k clips, native 4k upscale model, and IC control LoRA (pose-based)",
      "from": "Fill"
    },
    {
      "update": "RES4LYF dev fork updated with LTX-2 fixes",
      "details": "Major changes made for proper LTX-2 compatibility, available in dev branch",
      "from": "Ablejones"
    },
    {
      "update": "New version 2 of WanHuMo Detailer workflow released",
      "details": "Uses unsampling and resampling with noise instead of denoising, maintains color better",
      "from": "Ablejones"
    },
    {
      "update": "KJ released new lora node to bypass audio blocks",
      "details": "Fixes sync issues with audio input",
      "from": "N0NSens"
    },
    {
      "update": "New multimodal guider nodes available",
      "details": "Seem to improve many things, may help with audio",
      "from": "TK_999"
    },
    {
      "update": "01.29.26 LTX2 Updates released",
      "details": "Better control for real workflows",
      "from": "The Shadow (NYC)"
    }
  ],
  "workflows": [
    {
      "workflow": "I2V workflow with RES4LYF PR",
      "use_case": "Image to video generation with proper noise mask handling",
      "from": "Ablejones"
    },
    {
      "workflow": "Audio replacement workflow based on Kijai's video-audio-continue",
      "use_case": "Replace existing video's audio, prompt can be blank or used for guidance",
      "from": "garbus"
    },
    {
      "workflow": "Simplified audio-to-voice converter",
      "use_case": "Streamlined audio conversion process",
      "from": "Nokai"
    },
    {
      "workflow": "I2V + audio + v2v workflow optimized for fp4/fp8/dev",
      "use_case": "Low quality preview for blocking/movements, then fine control and quality maximization",
      "from": "Arts Bro"
    },
    {
      "workflow": "LTX2-Rapid-Merges LTXV-DoEverything-v2 workflow handles I2V, First to Last Frame and T2V with speed and quality optimizations",
      "use_case": "Complete LTX workflow with chunk feed forward, distilled sigmas/lcm/tiled vae",
      "from": "Phr00t"
    },
    {
      "workflow": "Audio + first image to video (A2V) workflow",
      "use_case": "Creating videos from audio input and starting image",
      "from": "avataraim"
    },
    {
      "workflow": "Two-pass workflow using dev model with positive distill LoRAs and Wan detailer",
      "use_case": "High quality video generation with detailing pass",
      "from": "Ablejones"
    },
    {
      "workflow": "Audio reactive workflow using canny input video and IC LoRA",
      "use_case": "Creating music-reactive videos for shorts content",
      "from": "Trap City"
    },
    {
      "workflow": "Z-Image / LTX-2 Unlimited-Length Image-to-Video Automation",
      "use_case": "One-run automated pipeline for full-length music videos from lyrics/audio",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Split samplers with different loras",
      "use_case": "Apply different loras to each sampler stage without model reload",
      "from": "Ablejones"
    },
    {
      "workflow": "Pure dev model with full CFG first stage",
      "use_case": "Current experimental setup for potentially better quality",
      "from": "Ablejones"
    },
    {
      "workflow": "LTX to HuMo detailing pipeline",
      "use_case": "Using LTX for initial generation then HuMo for upscaling with better lip sync and ID preservation",
      "from": "Ablejones"
    },
    {
      "workflow": "Single pass dev+distill chainsampling",
      "use_case": "16 steps cfg 4 with lora 0.0, then rest are cfg 1 lora 0.75",
      "from": "TK_999"
    },
    {
      "workflow": "3D projection with AI backgrounds",
      "use_case": "Live action actors with out of focus AI generated backgrounds",
      "from": "hudson223"
    },
    {
      "workflow": "Motion model for inbetween frames",
      "use_case": "Using motion models to generate inbetween frames that can be edited or cut",
      "from": "JUSTSWEATERS"
    },
    {
      "workflow": "LTX-2 + HuMo detailing with context windows",
      "use_case": "Infinite length video detailing on any system that can run Wan, stronger artifact correction",
      "from": "Ablejones"
    },
    {
      "workflow": "Custom length scenes with SRT timestamps",
      "use_case": "Music videos with variable scene lengths based on beat markers and lyric segments",
      "from": "VRGameDevGirl84"
    },
    {
      "workflow": "32fps generation with frame rate conversion",
      "use_case": "Generate at 32fps, downsample to 16fps, then RIFE back to 32fps for cleaner edges",
      "from": "The Shadow (NYC)"
    },
    {
      "workflow": "Flux Klein keyframe generation adapted from VRGameDevGirl workflow",
      "use_case": "Alternative to Chinese models for keyframe generation",
      "from": "Eiw / Dennis"
    },
    {
      "workflow": "WanHuMo Detailer v2 using unsampling/resampling",
      "use_case": "Video enhancement with better color maintenance",
      "from": "Ablejones"
    },
    {
      "workflow": "LTX2 first pass + WAN21 HuMo second pass",
      "use_case": "Two-stage video generation for higher quality output",
      "from": "The Shadow (NYC)"
    },
    {
      "workflow": "Context windows with HuMo reference control for long videos",
      "use_case": "Generate videos of any length as long as you can do 41+ frames at desired resolution",
      "from": "Ablejones"
    },
    {
      "workflow": "Audio to video with voice cloning pipeline",
      "use_case": "Create YouTube shorts using TTS and video generation",
      "from": "NC17z"
    },
    {
      "workflow": "Fixed LTXV-DoAlmostEverything workflow",
      "use_case": "Comprehensive LTX video generation",
      "from": "Phr00t"
    },
    {
      "workflow": "Custom node for generating prompts and style from theme",
      "use_case": "Automated prompt generation based on theme",
      "from": "randomanum"
    }
  ],
  "settings": [
    {
      "setting": "ModelComputeDtype",
      "value": "bf16 or fp16",
      "reason": "Left over from bf16 model testing, may not make difference for fp8 being cast",
      "from": "Ablejones"
    },
    {
      "setting": "Image strength for I2V",
      "value": "0.6-0.9",
      "reason": "1.0 is too high, this range works better",
      "from": "Gleb Tretyak"
    },
    {
      "setting": "Compression setting",
      "value": "35",
      "reason": "Default for preprocessor node instead of 33",
      "from": "Gleb Tretyak"
    },
    {
      "setting": "CFG for upscaler pass",
      "value": "1.0",
      "reason": "Not much benefit to higher CFG, causes memory spikes",
      "from": "TK_999"
    },
    {
      "setting": "AnimateDiff LoRA strength",
      "value": "2.0",
      "reason": "Recommended starting strength for AnimateDiff combinations",
      "from": "NebSH"
    },
    {
      "setting": "Latent normalization",
      "value": "1,1,0.5,1 per step",
      "reason": "Applies 0.5 on step 3 out of 8, leaves rest unchanged",
      "from": "Ablejones"
    },
    {
      "setting": "Distill LoRA strength for Deforum Morphing",
      "value": "1.5",
      "reason": "Higher strength for more intense morphing effect",
      "from": "S4f3ty_Marc"
    },
    {
      "setting": "Second pass distill LoRA",
      "value": "0.75",
      "reason": "Good balance for second pass refinement",
      "from": "TK_999"
    },
    {
      "setting": "Jinx LoRA strength",
      "value": "0.9",
      "reason": "Recommended to avoid overcooking from 29000 training steps",
      "from": "Cseti"
    },
    {
      "setting": "VAE tiled decode overlap",
      "value": "256",
      "reason": "Large overlap to avoid tiling artifacts",
      "from": "Ablejones"
    },
    {
      "setting": "Resolution buckets for training",
      "value": "960x544x41;960x544x49;960x544x57;960x544x65;960x544x73;960x544x81;960x544x97;960x544x105;960x544x121;960x544x137;960x544x169;960x544x217",
      "reason": "Various frame lengths for comprehensive training",
      "from": "Cseti"
    },
    {
      "setting": "FPS",
      "value": "12 for generation, upscale to 24",
      "reason": "Manages computational load while achieving target framerate",
      "from": "crinklypaper"
    },
    {
      "setting": "Resolution for high detail runs",
      "value": "1280x720, 30fps, 361-391 frames",
      "reason": "Achievable settings for detailed generation",
      "from": "veldrin"
    },
    {
      "setting": "Distilled lora strength",
      "value": "0.42",
      "reason": "Working value, though bypassing distilled sometimes gives better results",
      "from": "veldrin"
    },
    {
      "setting": "Static camera and detailer values",
      "value": "0.10 static camera, 0.55 detailer",
      "reason": "Settings used in successful 720p generation",
      "from": "veldrin"
    },
    {
      "setting": "Memory usage factor",
      "value": "0.1",
      "reason": "Prevents VRAM overflow and improves performance",
      "from": "Ablejones"
    },
    {
      "setting": "H264 compression level",
      "value": "33 (CRF)",
      "reason": "Default compression level used by LTXVpreprocess node",
      "from": "TK_999"
    },
    {
      "setting": "Context window context_length",
      "value": "Decrease to get model to run",
      "reason": "Prevents situations where model can't run",
      "from": "Ablejones"
    },
    {
      "setting": "HuMo render framerate",
      "value": "25 fps",
      "reason": "Must be 25 fps for proper lip syncing, works well with LTX-2",
      "from": "Ablejones"
    },
    {
      "setting": "HuMo sampling fps",
      "value": "25 fps",
      "reason": "HuMo trained on 25fps, strict requirement for audio sync",
      "from": "Ablejones"
    },
    {
      "setting": "HuMo context length",
      "value": "97 frames",
      "reason": "Model trained at 97 frames, though 81 or lower also works",
      "from": "Ablejones"
    },
    {
      "setting": "CFG for CausVid 1.3b",
      "value": "2.0",
      "reason": "Recommended setting when using with HuMo 1.7b",
      "from": "Ablejones"
    },
    {
      "setting": "Memory usage factor for 1024x1024",
      "value": "3.0",
      "reason": "Required patch to run higher resolutions without OOM",
      "from": "Ablejones"
    },
    {
      "setting": "HuMo denoise steps",
      "value": "2-3 steps minimum",
      "reason": "Start step 6 gives 2 steps, minimum recommended for quality",
      "from": "Ablejones"
    },
    {
      "setting": "Memory override node",
      "value": "0.01",
      "reason": "Fixes OOM issues on 4090",
      "from": "scf"
    },
    {
      "setting": "Shift value in ModelSamplingSD3",
      "value": "40-50",
      "reason": "Increases denoise amount safely (equals ~0.5 denoise)",
      "from": "Ablejones"
    },
    {
      "setting": "Distilled lora strength with dev model",
      "value": "0.4-0.6",
      "reason": "Better results than full distilled model",
      "from": "N0NSens"
    },
    {
      "setting": "Frame rate",
      "value": "30 FPS",
      "reason": "Much better results than 24/25 FPS with less artifacts",
      "from": "Elvaxorn"
    },
    {
      "setting": "Steps for first pass",
      "value": "60",
      "reason": "Makes huge difference compared to lower steps",
      "from": "NC17z"
    },
    {
      "setting": "VAE temporal overlap",
      "value": "16+ frames",
      "reason": "Minimum recommended, use increments of 8 (24, 32)",
      "from": "Phr00t"
    },
    {
      "setting": "Context size for 720x480",
      "value": "97 frames",
      "reason": "Should work easily on 4090 for any length video",
      "from": "Ablejones"
    },
    {
      "setting": "BTNB animation LoRA weight",
      "value": "various weights",
      "reason": "Works well for animation prompting",
      "from": "hudson223"
    }
  ],
  "concepts": [
    {
      "term": "Temporal upscaling",
      "explanation": "Uses upscale model for temporal dimension instead of spatial, doubles fps (25->50) and frame count",
      "from": "Ablejones"
    },
    {
      "term": "Latent normalization sampling",
      "explanation": "New sampling method to prevent audio latents from blowing out early to distorted sounds",
      "from": "TK_999"
    },
    {
      "term": "ReModel series nodes",
      "explanation": "Patches for doing style guides, the ltxv one won't work with ltx2",
      "from": "Ablejones"
    },
    {
      "term": "Latent normalization",
      "explanation": "Technique that modifies latents on the fly during sampling to improve audio-video coherence, works better inside sampler than at outer level",
      "from": "Ablejones"
    },
    {
      "term": "Distilled model requirement",
      "explanation": "Dev model alone produces noise artifacts and requires distilled model or LoRA component for proper artifact removal and upscaling",
      "from": "Ablejones"
    },
    {
      "term": "Sparse latent space",
      "explanation": "LTX has intentionally sparse latent space for faster training and inference, which may limit effectiveness of advanced sampling techniques",
      "from": "Ablejones"
    },
    {
      "term": "IC LoRA",
      "explanation": "Input Control LoRA that can be trained on specific reference inputs like luminance maps to drive aspects of video generation",
      "from": "oumoumad"
    },
    {
      "term": "Context length limit",
      "explanation": "The context limit of audio and video dimension and length that affects artifact levels - ideally under 15k, max 20k",
      "from": "TK_999"
    },
    {
      "term": "LTXVpreprocess node",
      "explanation": "Node that applies h264 video compression to images to better match training data and fight against static video issues",
      "from": "TK_999"
    },
    {
      "term": "VAE decoding tiled artifacts",
      "explanation": "Visual artifacts that appear during VAE decoding, especially visible in areas like sky, caused by tiling process",
      "from": "Ablejones"
    },
    {
      "term": "Attention tuning on audio",
      "explanation": "Method for improving audio quality in multimodal generation",
      "from": "Ablejones"
    },
    {
      "term": "ClownGuides for audio",
      "explanation": "Potential concept for controlling audio generation similar to visual guides",
      "from": "Ablejones"
    },
    {
      "term": "Context windows with HuMo",
      "explanation": "Allows processing infinite length videos by splitting into manageable chunks while maintaining reference",
      "from": "Ablejones"
    },
    {
      "term": "Per-key merging for LoRAs",
      "explanation": "Intelligent mixing method that performs separate audio and visual strength merging rather than simple loading",
      "from": "Phr00t"
    },
    {
      "term": "Audio stripped i2v LoRA",
      "explanation": "Applying only visual weights from i2v LoRA while preserving original audio quality",
      "from": "Phr00t"
    },
    {
      "term": "Unsampling and resampling with noise",
      "explanation": "Alternative to denoising that maintains color better in video enhancement",
      "from": "Ablejones"
    },
    {
      "term": "Bongmath with eta = 0.0",
      "explanation": "Just repeats same model call unmodified, wastes time and compute without SDE noise",
      "from": "Ablejones"
    },
    {
      "term": "8n + 1 frame issue",
      "explanation": "LTX generates frames in 8n+1 format which causes timing issues when trying to cut exactly on beats",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "term": "Chopping up movie for training data",
      "explanation": "A 1.5 hour movie would yield 540 ten-second clips for training",
      "from": "Fill"
    }
  ],
  "resources": [
    {
      "resource": "Ltxv-Deforum-Morphing-Style_v1-2025",
      "url": "https://huggingface.co/s4f3tymarc/Ltxv-Deforum-Morphing-Style_v1-2025",
      "type": "model",
      "from": "S4f3ty_Marc"
    },
    {
      "resource": "Ltxv-Deforum-Circle-Effect_v1-2025",
      "url": "https://huggingface.co/s4f3tymarc/Ltxv-Deforum-Circle-Effect_v1-2025",
      "type": "model",
      "from": "S4f3ty_Marc"
    },
    {
      "resource": "Clay stop motion LoRA",
      "url": "https://huggingface.co/oumoumad/clay-stop-motion-lora-ngtvspc",
      "type": "model",
      "from": "oumoumad"
    },
    {
      "resource": "Paper stop motion LoRA",
      "url": "https://huggingface.co/oumoumad/paper-stop-motion-lora-ptprnc",
      "type": "model",
      "from": "oumoumad"
    },
    {
      "resource": "Deep zoom LoRA",
      "url": "https://huggingface.co/oumoumad/deepzoom-lora",
      "type": "model",
      "from": "oumoumad"
    },
    {
      "resource": "Hand Transition LoRA",
      "url": "https://huggingface.co/Nebsh/HandTransition",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "LTX2_Handheld_run LoRA",
      "url": "https://huggingface.co/Nebsh/LTX2_Handheld_run",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "LTX2_AtomicExplosion LoRA",
      "url": "https://huggingface.co/Nebsh/LTX2_AtomicExplosion",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "LTX2_Outfitswitch LoRA",
      "url": "https://huggingface.co/Nebsh/LTX2_Outfitswitch",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "Arcane Jinx LoRA",
      "url": "https://civitai.com/models/2314428/ltx-2-19b-arcane-jinx-lora",
      "type": "model",
      "from": "Cseti"
    },
    {
      "resource": "LTX2 updates Reddit post",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1qdug07/ltx2_updates/",
      "type": "resource",
      "from": "TK_999"
    },
    {
      "resource": "RES4LYF PR for I2V fix",
      "url": "https://github.com/ClownsharkBatwing/RES4LYF/pull/231",
      "type": "repo",
      "from": "TK_999"
    },
    {
      "resource": "DiscordChatExporter",
      "url": "https://github.com/Tyrrrz/DiscordChatExporter",
      "type": "tool",
      "from": "Nathan Shipley"
    },
    {
      "resource": "Discord text cleaner web tool",
      "url": "https://nathanshipley.github.io/discord-text-cleaner/",
      "type": "tool",
      "from": "Nathan Shipley"
    },
    {
      "resource": "LTX2-Rapid-Merges workflow",
      "url": "https://huggingface.co/Phr00t/LTX2-Rapid-Merges/blob/main/LTXV-DoEverything-v2.json",
      "type": "workflow",
      "from": "Phr00t"
    },
    {
      "resource": "LTXV Spatio Temporal Tiled VAE Decode node",
      "url": "",
      "type": "node",
      "from": "avataraim"
    },
    {
      "resource": "Gemma 3 text encoder models",
      "url": "https://huggingface.co/Comfy-Org/ltx-2/tree/main/split_files/text_encoders",
      "type": "model",
      "from": "Ablejones"
    },
    {
      "resource": "RES4LYF fork with latent normalization",
      "url": "https://github.com/drozbay/res4lyf",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "AI Bongmath tutorial videos",
      "url": "https://www.youtube.com/@ai_bongmath",
      "type": "tutorial",
      "from": "Ablejones"
    },
    {
      "resource": "ComfyUI RIFE TensorRT Auto",
      "url": "https://github.com/silveroxides/ComfyUI_RIFE_TensorRT_Auto",
      "type": "tool",
      "from": "EnragedAntelope"
    },
    {
      "resource": "KJNodes",
      "url": "https://github.com/kijai/ComfyUI-KJNodes",
      "type": "repo",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "MilkDrop3",
      "url": "https://github.com/milkdrop2077/MilkDrop3",
      "type": "tool",
      "from": "Arts Bro"
    },
    {
      "resource": "Gemma 3 12B quantized model",
      "url": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized/tree/main",
      "type": "model",
      "from": "Arts Bro"
    },
    {
      "resource": "LTX-2 resources compilation",
      "url": "https://github.com/wildminder/awesome-ltx2",
      "type": "resource",
      "from": "Abyss"
    },
    {
      "resource": "Updated prompting resource",
      "url": "https://discord.com/channels/1076117621407223829/1459223128139104436/1463353304422809653",
      "type": "resource",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "Z-Image / LTX-2 Unlimited-Length Music Video Workflow",
      "url": "https://github.com/vrgamegirl19/comfyui-vrgamedevgirl/tree/main/Workflows",
      "type": "workflow",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "ComfyUI VRGameDevGirl Custom Nodes",
      "url": "https://github.com/vrgamegirl19/comfyui-vrgamedevgirl/tree/main",
      "type": "custom nodes",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "WAN Humo LTX-2 Music Video Prompt Creator GPT",
      "url": "https://chatgpt.com/g/g-68d0a5a9f7b4819189bdb15c826c789c-wanhumo-ltx-2-music-video-prompt-creator",
      "type": "tool",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "Ablejones RES4LYF fork",
      "url": "https://github.com/drozbay/res4lyf",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "LTX-2 VRAM Memory Management",
      "url": "https://github.com/RandomInternetPreson/ComfyUI_LTX-2_VRAM_Memory_Management",
      "type": "tool",
      "from": "BNP4535353"
    },
    {
      "resource": "LTX-2 Deforum Evolution LoRA",
      "url": "https://huggingface.co/s4f3tymarc/LTX-2_Deforum_Evolution_v1",
      "type": "lora",
      "from": "S4f3ty_Marc"
    },
    {
      "resource": "Cutout Satire LoRA",
      "url": "https://huggingface.co/Nebsh/cutout-satire-lora/",
      "type": "lora",
      "from": "NebSH"
    },
    {
      "resource": "WAN 2.1 I2V LoRA",
      "url": "https://huggingface.co/lightx2v/Wan2.1-Distill-Loras/blob/main/wan2.1_i2v_lora_rank64_lightx2v_4step.safetensors",
      "type": "lora",
      "from": "Ablejones"
    },
    {
      "resource": "LTX-2 19B GGUF 12GB ComfyUI workflows",
      "url": "https://civitai.com/models/2304098/ltx-2-19b-gguf-12gb-comfyui-workflows-5-total-t2vi2vv2via2vta2v",
      "type": "workflow",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "resource": "Urabewe audio nodes",
      "url": "",
      "type": "node",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "resource": "VRGameDevGirl ComfyUI workflows",
      "url": "https://github.com/vrgamegirl19/comfyui-vrgamedevgirl",
      "type": "repo",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "Honda Dream Generator project",
      "url": "https://www.rpa.com/work/project/honda-the-dream-generator",
      "type": "project",
      "from": "hudson223"
    },
    {
      "resource": "LTX-2 Image2Video Adapter LoRA",
      "url": "https://huggingface.co/MachineDelusions/LTX-2_Image2Video_Adapter_LoRa",
      "type": "lora",
      "from": "Fill"
    },
    {
      "resource": "ComfyUI Nano Banana",
      "url": "https://github.com/ru4ls/ComfyUI_Nano_Banana",
      "type": "node",
      "from": "LarpsAI"
    },
    {
      "resource": "ComfyUI AudioTools",
      "url": "https://github.com/Urabewe/ComfyUI-AudioTools",
      "type": "node",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "resource": "RES4LYF dev fork",
      "url": "https://github.com/drozbay/RES4LYF",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "HeartMula open source song generator",
      "url": "https://github.com/HeartMuLa/heartlib",
      "type": "tool",
      "from": "dj47"
    },
    {
      "resource": "LTX2 Rapid Merges script",
      "url": "https://huggingface.co/Phr00t/LTX2-Rapid-Merges/blob/main/MergingScript/fancy-apply.py",
      "type": "script",
      "from": "Phr00t"
    },
    {
      "resource": "WanExperiments",
      "url": "https://github.com/drozbay/WanExperiments",
      "type": "repo",
      "from": "N0NSens"
    },
    {
      "resource": "ComfyUI-WanVaceAdvanced",
      "url": "https://github.com/drozbay/ComfyUI-WanVaceAdvanced",
      "type": "repo",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "Ablejones res4lyf fork",
      "url": "https://github.com/drozbay/res4lyf",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "Phr00t LTX2 merged models",
      "url": "https://huggingface.co/Phr00t/LTX2-Rapid-Merges",
      "type": "model",
      "from": "Phr00t"
    },
    {
      "resource": "Next Scene LoRA",
      "url": "https://huggingface.co/lovis93/next-scene-qwen-image-lora-2509",
      "type": "lora",
      "from": "Eiw / Dennis"
    },
    {
      "resource": "LTXV-DoEverything-v2 workflow",
      "url": "https://huggingface.co/Phr00t/LTX2-Rapid-Merges/blob/main/LTXV-DoEverything-v2.json",
      "type": "workflow",
      "from": "Phr00t"
    },
    {
      "resource": "Video Generation Arena Leaderboard",
      "url": "https://huggingface.co/spaces/ArtificialAnalysis/Video-Generation-Arena-Leaderboard",
      "type": "tool",
      "from": "Phr00t"
    },
    {
      "resource": "LTXV-DoAlmostEverything-v3.json",
      "url": "https://huggingface.co/Phr00t/LTX2-Rapid-Merges/blob/main/LTXV-DoAlmostEverything-v3.json",
      "type": "workflow",
      "from": "Phr00t"
    },
    {
      "resource": "Beauty and Beast LoRA for general 2D animation",
      "url": "Available on Civitai",
      "type": "model",
      "from": "JUSTSWEATERS"
    },
    {
      "resource": "WanExperiments nodes",
      "url": "https://github.com/drozbay/WanExperiments",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "Kimi 2.5 LLM",
      "url": "https://www.kimi.com/ai-models/kimi-k2-5",
      "type": "tool",
      "from": "pom"
    },
    {
      "resource": "LTX implementation with GUI and memory management",
      "url": "https://github.com/maybleMyers/ltx",
      "type": "tool",
      "from": "Benjimon"
    },
    {
      "resource": "LTX2 prompt reference library",
      "url": "https://ltx.io/model/model-blog/ltx-2-better-control-for-real-workflows",
      "type": "resource",
      "from": "The Shadow (NYC)"
    }
  ],
  "limitations": [
    {
      "limitation": "LTXVImgToVideoInplace node is broken",
      "details": "Breaks on 'resample', needs investigation",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "CFG hits VRAM very hard",
      "details": "Causes OOM issues especially on second pass",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "Character bleeding in multi-character scenes",
      "details": "Jinx LoRA mostly bleeds out when prompting for other characters but sometimes works well",
      "from": "Cseti"
    },
    {
      "limitation": "Random loss of movement or lipsync",
      "details": "Can turn into Ken Burns effect when changing single words",
      "from": "MiggyCabrera"
    },
    {
      "limitation": "Slowest part is stopping and starting sampling",
      "details": "Same model but doesn't start up again quickly",
      "from": "Ablejones"
    },
    {
      "limitation": "LTX-2 breaks basic ComfyUI functions",
      "details": "Can't split sampling with audio+video in regular ComfyUI, only works in RES4LYF",
      "from": "Ablejones"
    },
    {
      "limitation": "Dev model cannot generate coherent video alone",
      "details": "Always needs distilled component for proper results",
      "from": "TK_999"
    },
    {
      "limitation": "Spatial upscaler is fixed at 2x scale",
      "details": "No flexibility to choose other scaling factors like 1.3x or 1.5x",
      "from": "Arts Bro"
    },
    {
      "limitation": "Character LoRAs sometimes bleed out with multiple people",
      "details": "Jinx LoRA works sometimes with multiple people but often bleeds characteristics",
      "from": "Cseti"
    },
    {
      "limitation": "Audio generation has weird rumbling tones",
      "details": "Basic frequency filters needed to remove unwanted low-frequency artifacts",
      "from": "Ablejones"
    },
    {
      "limitation": "Requires large amounts of system RAM",
      "details": "Even 64GB RAM needs swap space for these large models, commonly crashes without proper swap",
      "from": "Ablejones"
    },
    {
      "limitation": "Dev model incomplete sampling effect",
      "details": "Dev model shows incomplete sampling artifacts that are concerning for quality",
      "from": "Ablejones"
    },
    {
      "limitation": "I2V doesn't stick well to original image",
      "details": "Model doesn't adhere rigidly to input image, tends toward creative interpretation rather than faithful reproduction",
      "from": "Ablejones"
    },
    {
      "limitation": "Audio conditioning convergence",
      "details": "Audio conditioning shows interesting behaviors but often converges toward similar soundscapes rather than following unique inputs",
      "from": "S4f3ty_Marc"
    },
    {
      "limitation": "Prompt sensitivity",
      "details": "Single word can totally misdirect video output, model is very sensitive to prompting",
      "from": "Ablejones"
    },
    {
      "limitation": "H.264 pixel size limits",
      "details": "H.264 compression has hard limits on frame size, can't handle very large stadium-sized displays",
      "from": "The Shadow (NYC)"
    },
    {
      "limitation": "LTX model heavily biased toward white subjects",
      "details": "Generates mostly white people, lots of hockey haircuts",
      "from": "hudson223"
    },
    {
      "limitation": "High speed artifacts and motion artifacts",
      "details": "Serious issues with fast motion and general motion artifacts",
      "from": "TK_999"
    },
    {
      "limitation": "Copyright content generation",
      "details": "Will generate copyrighted content, music, voices without awareness",
      "from": "TK_999"
    },
    {
      "limitation": "Anatomy issues",
      "details": "Loads of anatomical problems in generated content",
      "from": "TK_999"
    },
    {
      "limitation": "Baked-in camera focus pulls",
      "details": "Model seems to have certain camera movements built in",
      "from": "TK_999"
    },
    {
      "limitation": "Raw dev model quality issues",
      "details": "Dev checkpoint is unwieldy and produces worse results than distill",
      "from": "hudson223"
    },
    {
      "limitation": "Latent upscale doesn't support nested tensors",
      "details": "Cannot use standard latent upscale nodes with LTX's nested tensor format",
      "from": "TK_999"
    },
    {
      "limitation": "HuMo changes everyone into girls at high denoise",
      "details": "Especially noticeable with male characters, may need reference from last frame instead of first",
      "from": "dj47"
    },
    {
      "limitation": "HuMo not great at wide shots",
      "details": "Best used as detailer for medium-close up character content",
      "from": "hudson223"
    },
    {
      "limitation": "First frame of LTX-2 i2v usually poor quality",
      "details": "Blurry with compression artifacts, better to use 2nd or 3rd frame as reference",
      "from": "Ablejones"
    },
    {
      "limitation": "Detail injection difficult with distilled models",
      "details": "Only 2-3 enormous steps makes it hard to inject fine details like facial pores",
      "from": "Ablejones"
    },
    {
      "limitation": "LTX-2 8n + 1 frame constraint affects scene timing",
      "details": "Custom length scenes don't align perfectly with beat markers due to frame constraints",
      "from": "VRGameDevGirl84"
    },
    {
      "limitation": "Owl mouth not detected by HuMo",
      "details": "Reference matching fails when mouth location unclear, try lower denoise or different seed",
      "from": "Ablejones"
    },
    {
      "limitation": "LTX upscaling models introduce motion artifacts",
      "details": "Designed more for speed, better to do 1 stage and use RIFE later",
      "from": "Phr00t"
    },
    {
      "limitation": "Quality degrades over time in long video generation",
      "details": "Not recommended for extending videos without enhancement",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "LTX2 doesn't follow prompts well, especially with distilled model and CFG 1",
      "details": "Simply doesn't care what you prompt with those settings",
      "from": "N0NSens"
    },
    {
      "limitation": "Normalizing sampler doesn't fix broken audio",
      "details": "Just an equalizer - if sound was broken, it stays broken. Doesn't work on dev models",
      "from": "N0NSens"
    },
    {
      "limitation": "Chunk feed forward node significantly slows generation",
      "details": "Makes generation about half speed",
      "from": "Ivoxx"
    },
    {
      "limitation": "Only one audio output in certain workflows",
      "details": "Refined audio out not connected by default, connecting it disconnects pre-fine tuned output",
      "from": "buggz"
    },
    {
      "limitation": "Cycles rebounds can be finicky",
      "details": "May cause errors with certain guiders",
      "from": "Ablejones"
    },
    {
      "limitation": "Custom nodes run each time new chunk is generated",
      "details": "When you only want them to run once at beginning for prompt/style creation",
      "from": "randomanum"
    }
  ],
  "hardware": [
    {
      "requirement": "16GB VRAM minimum for default workflows",
      "details": "Need to apply 16 VRAM measurements for complex workflows",
      "from": "Gleb Tretyak"
    },
    {
      "requirement": "OOM issues on 5090 with 128GB RAM",
      "details": "At 24fps/721 frames/1280x720 at HQ stage with sage attn + chunking",
      "from": "drbaph"
    },
    {
      "requirement": "24GB RTX4090 can handle workflows but freezes with high tile settings",
      "details": "User needs to use lower tile sizes to avoid system freeze",
      "from": "avataraim"
    },
    {
      "requirement": "400 seconds generation time for 15 second video",
      "details": "Using full workflow with temporal upscaling on RTX4090",
      "from": "avataraim"
    },
    {
      "requirement": "130 seconds after optimizations",
      "details": "Skipping distilled LoRA, temporal upscaling, and frame rate doubling",
      "from": "avataraim"
    },
    {
      "requirement": "VRAM usage",
      "details": "RTX 5090 mentioned as being spoiled by resources, RTX 4090 working well with optimizations",
      "from": "Ablejones"
    },
    {
      "requirement": "Training VRAM",
      "details": "H100 peaked around 60GB VRAM for LoRA training, could use cheaper GPUs next time",
      "from": "Cseti"
    },
    {
      "requirement": "System RAM",
      "details": "64GB RAM still insufficient without proper swap space, crashes are common with small swap",
      "from": "Ablejones"
    },
    {
      "requirement": "Training time",
      "details": "Around 1 day training time on H100 for LoRA with multiple resume sessions",
      "from": "Cseti"
    },
    {
      "requirement": "RIFE acceleration",
      "details": "TensorRT version is blazing fast compared to standard ComfyUI Frame Interpolation",
      "from": "EnragedAntelope"
    },
    {
      "requirement": "VRAM for HQ long generations",
      "details": "48GB VRAM or more needed for HQ versions of longer generations",
      "from": "Arts Bro"
    },
    {
      "requirement": "4090 performance",
      "details": "Takes around 4 minutes for first stage with GGUF, 20 minutes without optimizations",
      "from": "AI_Fan"
    },
    {
      "requirement": "System with 20GB VRAM and 28GB system RAM",
      "details": "Can run GGUF Q5 LTX-2/Gemma models at high resolutions like 3072x2048",
      "from": "The Shadow (NYC)"
    },
    {
      "requirement": "GGUF model for lower VRAM",
      "details": "GGUF workflow works well for users with VRAM limitations while maintaining quality",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "HuMo detailing context windows",
      "details": "Works on any system that can run Wan, enables infinite length processing",
      "from": "Ablejones"
    },
    {
      "requirement": "1024x1024 resolution",
      "details": "Requires memory_usage_factor patch to 3.0 to avoid OOM",
      "from": "Ablejones"
    },
    {
      "requirement": "Low-end system performance",
      "details": "A4500 with 20GB VRAM and 28GB system RAM can run workflows with heavy paging",
      "from": "The Shadow (NYC)"
    },
    {
      "requirement": "Mac M4 compatibility",
      "details": "64GB M4 Mac Mini Pro can load LTX-2 completely (35997.23 MB)",
      "from": "buggz"
    },
    {
      "requirement": "8GB VRAM limitations",
      "details": "Causes slowdown in second chunk generation, first chunk 4min vs second 13min",
      "from": "army"
    },
    {
      "requirement": "4090 VRAM for 720x480 at 97 frames context",
      "details": "Should work easily for any length video, more limited by RAM",
      "from": "Ablejones"
    },
    {
      "requirement": "128GB CPU RAM helpful for long videos",
      "details": "Memory becomes limiting factor more than VRAM for extended generation",
      "from": "NC17z"
    },
    {
      "requirement": "RTX 6000 Pro Blackwell (96GB VRAM) for training",
      "details": "Used for 5000 steps training, took at least 10 hours total",
      "from": "oumoumad"
    },
    {
      "requirement": "24GB VRAM (RTX 3090) for 20-30 second videos",
      "details": "Possible at 720x480 resolution range",
      "from": "NC17z"
    },
    {
      "requirement": "Works on laptop 4090",
      "details": "46GB RAM used successfully",
      "from": "randomanum"
    },
    {
      "requirement": "Not great option for low VRAM/RAM",
      "details": "Regarding the GUI implementation with memory management",
      "from": "Benjimon"
    }
  ],
  "community_creations": [
    {
      "creation": "Deforum Morphing Style LoRA",
      "type": "lora",
      "description": "Originally trained for LTXv-13b.0.97, works with LTX-2, will retrain for v2",
      "from": "S4f3ty_Marc"
    },
    {
      "creation": "Deforum Circle Effect LoRA",
      "type": "lora",
      "description": "Circle effect style for video generation",
      "from": "S4f3ty_Marc"
    },
    {
      "creation": "Clay stop motion LoRA",
      "type": "lora",
      "description": "Creates clay stop motion style with trigger word NGTVSPC",
      "from": "oumoumad"
    },
    {
      "creation": "Paper stop motion LoRA",
      "type": "lora",
      "description": "Creates paper stop motion style with trigger word PTPRNC",
      "from": "oumoumad"
    },
    {
      "creation": "Deep zoom LoRA",
      "type": "lora",
      "description": "Extreme zoom effect, intended to be reversed for reveal effect, trained on hand-held objects",
      "from": "oumoumad"
    },
    {
      "creation": "Hand Transition LoRA",
      "type": "lora",
      "description": "Creates hand transition effects",
      "from": "NebSH"
    },
    {
      "creation": "Handheld run LoRA",
      "type": "lora",
      "description": "Creates handheld camera movement effects",
      "from": "NebSH"
    },
    {
      "creation": "Atomic Explosion LoRA",
      "type": "lora",
      "description": "Generates atomic explosion effects",
      "from": "NebSH"
    },
    {
      "creation": "Outfit Switch LoRA",
      "type": "lora",
      "description": "Creates outfit switching transitions",
      "from": "NebSH"
    },
    {
      "creation": "Arcane Jinx LoRA",
      "type": "lora",
      "description": "Character LoRA for Jinx from Arcane",
      "from": "Cseti"
    },
    {
      "creation": "Audio replacement workflow",
      "type": "workflow",
      "description": "Easily replace existing video's audio based on Kijai's video-audio-continue workflow",
      "from": "garbus"
    },
    {
      "creation": "Simplified audio-to-voice converter",
      "type": "workflow",
      "description": "Streamlined version of audio to voice conversion",
      "from": "Nokai"
    },
    {
      "creation": "I2V + audio + v2v workflow",
      "type": "workflow",
      "description": "Multi-stage workflow for preview and quality control",
      "from": "Arts Bro"
    },
    {
      "creation": "LTXV Spatio Temporal Tiled VAE Decode",
      "type": "node",
      "description": "Improved VAE decode that prevents freezing and improves speed",
      "from": "avataraim"
    },
    {
      "creation": "Luminance Map IC LoRA",
      "type": "lora",
      "description": "IC LoRA trained on luminance map references for driving video brightness/particle effects",
      "from": "oumoumad"
    },
    {
      "creation": "WHATUSEE transition effect LoRA",
      "type": "lora",
      "description": "Creates eye zoom transition effect from person's view to first-person perspective",
      "from": "burgstall"
    },
    {
      "creation": "Updated Deforum Morphing LoRA",
      "type": "lora",
      "description": "Morphing effects LoRA being retrained for LTX-2",
      "from": "S4f3ty_Marc"
    },
    {
      "creation": "Jinx character LoRA",
      "type": "lora",
      "description": "29000 step trained character LoRA, recommended strength 0.9",
      "from": "Cseti"
    },
    {
      "creation": "LTX2 prompting resource with GPT integration",
      "type": "workflow",
      "description": "Comprehensive prompting system with reference files for GPT 5.2 Thinking model",
      "from": "The Shadow (NYC)"
    },
    {
      "creation": "LTX-2 Deforum Evolution LoRA",
      "type": "lora",
      "description": "Fine-tuned on Deforum animations with Flux, creates hypnotic morphing animations with trigger [deforumorph]",
      "from": "S4f3ty_Marc"
    },
    {
      "creation": "Cutout Satire LoRA",
      "type": "lora",
      "description": "LoRA for cutout/satire style video generation",
      "from": "NebSH"
    },
    {
      "creation": "Z-Image/LTX-2 Unlimited Music Video Automation",
      "type": "workflow",
      "description": "Fully automated pipeline for creating full-length music videos from audio and lyrics",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "LTX-2 VRAM Memory Management custom node",
      "type": "tool",
      "description": "Custom node for reducing VRAM usage with LTX-2",
      "from": "BNP4535353"
    },
    {
      "creation": "Urabewe audio nodes",
      "type": "node",
      "description": "Audio processing nodes now available in Comfy Manager, search for 'Urabewe'",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "creation": "VRGameDevGirl automation workflow",
      "type": "workflow",
      "description": "LTX automation workflow with prompt creator",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "DeterministicFPSResample node",
      "type": "node",
      "description": "Frame rate conversion without interpolation, works for both upsample and downsample",
      "from": "The Shadow (NYC)"
    },
    {
      "creation": "ComfyUI AudioTools",
      "type": "node",
      "description": "Enhance and normalize audio for LTX-2, prevents dramatic audio changes in extended videos",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "creation": "LTX2 Rapid Merges",
      "type": "script",
      "description": "Per-key LoRA merging with separate audio/visual strengths and FP8 conversion",
      "from": "Phr00t"
    },
    {
      "creation": "Custom length scenes with SRT import",
      "type": "workflow",
      "description": "App to create scene markers by playing song and hitting keyboard, generates SRT file for variable length scenes",
      "from": "VRGameDevGirl84"
    },
    {
      "creation": "WanHuMo Detailer v2",
      "type": "workflow",
      "description": "Enhanced video detailing using unsampling/resampling technique",
      "from": "Ablejones"
    },
    {
      "creation": "Motion enhancement LoRA",
      "type": "lora",
      "description": "LoRA for adding more motion to LTX2 generations",
      "from": "Fill"
    },
    {
      "creation": "Deterministic FPS converter node",
      "type": "node",
      "description": "Converts image batches between different frame rates",
      "from": "The Shadow (NYC)"
    },
    {
      "creation": "Beat-driven scene segmentation system",
      "type": "workflow",
      "description": "Analyzes audio beats and creates scene cuts with SRT timeline",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "Character LoRA training on 33 videos",
      "type": "lora",
      "description": "Trained on 24fps 5-second clips at 1024x576 for 6000 steps",
      "from": "NC17z"
    },
    {
      "creation": "BTNB animation LoRA",
      "type": "lora",
      "description": "Good for general 2D animation, Beauty and the Beast style",
      "from": "Cseti"
    },
    {
      "creation": "LTX GUI implementation",
      "type": "tool",
      "description": "Built upon source repo with more features, GUI and memory management",
      "from": "Benjimon"
    },
    {
      "creation": "Custom prompt generation node",
      "type": "node",
      "description": "Makes rules from prompt theme automatically",
      "from": "randomanum"
    }
  ]
}