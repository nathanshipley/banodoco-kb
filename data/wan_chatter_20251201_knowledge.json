{
  "channel": "wan_chatter",
  "date_range": "2025-12-01 to 2026-01-01",
  "messages_processed": 14134,
  "chunks_processed": 36,
  "api_usage": {
    "input_tokens": 459323,
    "output_tokens": 93396,
    "estimated_cost": 2.778909
  },
  "extracted_at": "2026-02-03T05:29:56.381961Z",
  "discoveries": [
    {
      "finding": "Wan VAE can use way less VRAM if you offload the cache to CPU",
      "details": "At 720p it's like ~5GB less VRAM used, makes it slower but beats tiled VAE if necessary, has no effect on output unlike tiling",
      "from": "Kijai"
    },
    {
      "finding": "Low model for Wan 2.2 is just a more trained/fine tuned version of 2.1",
      "details": "You can generate entire video with just the LN model. HN models are specifically good at motion, prompt following, and composition, but have lost ability to create details. Run HN model for first few steps to establish what it's good at, then let any low noise model or Wan 2.1 model take over",
      "from": "Ablejones"
    },
    {
      "finding": "SteadyDancer works with LightX2V LoRA",
      "details": "Tested and confirmed working combination",
      "from": "Kijai"
    },
    {
      "finding": "Wrapper updates providing significant VRAM reduction",
      "details": "Reports ranging from 10% to 50% less VRAM used since latest update",
      "from": "Kijai"
    },
    {
      "finding": "UltraViCo (DiT-Extrapolation) breaks looping and increases video quality when going above 81 frames",
      "details": "Normal I2V at 2x the model's trained length just loops back to start, not allowing much movement. UltraViCo prevents this and improves quality for longer videos",
      "from": "Kijai"
    },
    {
      "finding": "Base motion is defined very early in Wan models",
      "details": "Models define the base motion really early, so you might not see much difference unless you end sampling super early, especially with low step workflows",
      "from": "Kijai"
    },
    {
      "finding": "Context stride causes terrible stutter in Wan",
      "details": "Because it packs 4 frames into one latent, it causes terrible stutter. Only real way to utilize it is splitting the process and using stride on early steps, like 2.2 A14B workflows",
      "from": "Kijai"
    },
    {
      "finding": "First frame in Wan VAE takes 4x memory compared to other frames",
      "details": "1st frame = context/references for the rest (the root image), which is why it uses significantly more memory",
      "from": "Scruffy"
    },
    {
      "finding": "Wan frame count should follow 4n+1 formula",
      "details": "Frames should be 4n+1 format (like 145 = 144 or 36 sets of 4 frames, plus 1) for proper processing",
      "from": "Scruffy"
    },
    {
      "finding": "Diffusion models under 1000 steps have error rates of 1e-3 to 1e-5",
      "details": "This enormous error means discretization precision barely matters for quality",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "8 euler steps will always beat 4 steps of any other 1NFE solver",
      "details": "More euler steps consistently outperforms higher order solvers with fewer steps",
      "from": "spacepxl"
    },
    {
      "finding": "First-step undistilled sampling improves results",
      "details": "Using 1-2 undistilled steps at the beginning helps with motion and overall quality, especially for I2V",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "Motion blur issues often come from VAE, not the sampler",
      "details": "VAE doesn't handle fast motion well due to compression architecture",
      "from": "spacepxl"
    },
    {
      "finding": "TCD/TDD distillation works on arbitrary schedules",
      "details": "Unlike some adversarial methods, TCD distillation respects geometry and can handle different timestep schedules",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "SVI 2.0 uses 5 frames for continuation, not 1 frame as initially thought",
      "details": "Updated script shows it uses 5 frames to continue and pads with the original reference image, combining both film and shot approaches",
      "from": "Kijai"
    },
    {
      "finding": "SVI 2.0 combines SVI-Shot v1 and SVI-film v1 functionality",
      "details": "New version provides motion dynamics of film with shot-style continuation",
      "from": "42hub"
    },
    {
      "finding": "Steadydancer has attention mechanism additions, multiple convolution types, and distillation step",
      "details": "Architecture includes low rank training to preserve original model capacity better, addressing trade-off between appearance and motion control",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "UltraViCo works with everything but drops start image/reference over longer generations",
      "details": "Not perfect for maintaining reference consistency in extended generations",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 14B and 2.1 are capped at 81 frames, will start looping after that",
      "details": "Training was done on 81 frames, model tends to go backwards on longer generations without control signals",
      "from": "Kijai"
    },
    {
      "finding": "SVI-Shot for Wan 2.2 only needs 1 frame masking instead of 5",
      "details": "The svi version 2.0 for Wan 2.2 is differently trained than the 2.0 for Wan 2.1. 2.1 can use 5 frames while 2.2 can just use one frame",
      "from": "Kijai"
    },
    {
      "finding": "Masking only first frame prevents flash between extensions",
      "details": "Using 5 frames causes flash between extensions, masking only the first frame works better for Wan 2.2",
      "from": "Kijai"
    },
    {
      "finding": "Concat technique reduces flash in Wan Animate",
      "details": "Using concat_latent_image made much less flash during 14 hours of generation",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "Flash attention required for stability",
      "details": "It is necessary to install flash_attn; otherwise, severe artifacts and instability will appear",
      "from": "Ablejones"
    },
    {
      "finding": "SVI-Shot can use 3-4 frames with minimal flashing",
      "details": "You can get away with 3 or 4 frames with minimal flashing, and it still maintains the reference",
      "from": "Ablejones"
    },
    {
      "finding": "PyTorch 2.8 causes memory leaks with image resize nodes",
      "details": "Updating to PyTorch 2.9.1+cu130 fixes the memory leak issue where resize nodes don't release cache properly",
      "from": "spacepxl"
    },
    {
      "finding": "SVI 2.2 works better with SageAttention",
      "details": "Testing showed outputs were better with SageAttention enabled vs disabled",
      "from": "Ablejones"
    },
    {
      "finding": "FILM VFI in Fill-Nodes pack is significantly faster",
      "details": "6-8 times faster than other FILM implementations",
      "from": "lostintranslation"
    },
    {
      "finding": "3 overlap frames seems to be the highest possible with no artifacts for SVI",
      "details": "3 overlap frames works better than 1, but going higher causes artifacts",
      "from": "Ablejones"
    },
    {
      "finding": "SVI should not have flash at transition if used correctly",
      "details": "Flashes indicate incorrect usage rather than model limitation",
      "from": "Ablejones"
    },
    {
      "finding": "Reference strength can be modified in SVI workflows",
      "details": "Been experimenting with changing reference strength as alternative approach",
      "from": "Ablejones"
    },
    {
      "finding": "HuMo can be used as low noise model in Wan 2.2",
      "details": "Works both on wrapper and native implementations",
      "from": "Elvaxorn"
    },
    {
      "finding": "Magref can be better on I2V context window generations",
      "details": "Can even use it as the low noise model in Wan 2.2, although you lose a little identity preservation in exchange for some better motion and blending",
      "from": "blake37"
    },
    {
      "finding": "Flash attention massively affects SVI performance",
      "details": "Selected attention type has significant impact on SVI results",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "Merged vs unmerged LoRAs produce different effects",
      "details": "The effect is bit different depending on model/LoRA, sometimes stronger sometimes weaker. If merge fixes some issue then increasing strength might do so too",
      "from": "Kijai"
    },
    {
      "finding": "OneToAll Animation performs better pose retargeting than WanAnimate",
      "details": "OneToAll does better at keeping reference when used correctly and has better pose alignment capabilities",
      "from": "Kijai"
    },
    {
      "finding": "Token replacement technique in Wan models",
      "details": "Sets timestep of next 2 frames after reference frame to 0 to avoid them changing too much from init. Only used when continuing from previous frames",
      "from": "Kijai"
    },
    {
      "finding": "OneToAll Animation uses reference + controlnet modules for Wan T2V",
      "details": "Includes alignment code for WanAnimate preprocessor to align reference pose better",
      "from": "Kijai"
    },
    {
      "finding": "OneToAll Animation has two pose modes",
      "details": "Either aligns input pose to reference, or reference to input pose",
      "from": "Kijai"
    },
    {
      "finding": "Fraudulent model discovery - aquif-ai uploaded stolen Magic Wan model",
      "details": "Hash comparison confirmed aquif-ai's model is identical to Magic-Wan-Image-v1.0, not a finetune as claimed",
      "from": "Kijai"
    },
    {
      "finding": "WanAnimate better init adherence with 25 steps euler vs 8 steps lightx2v lora",
      "details": "cfg 1.2 25 steps euler shows better init adherence compared to cfg 1.2 8 steps lightx2v lora, though looks a little scuffed",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "finding": "OneToAll extension method uses token replacement with 5 frame overlap",
      "details": "5 frame overlap, mark that part of sequence with zero timestep, and don't include that part in the scheduler. Similar to S2V technique",
      "from": "Kijai"
    },
    {
      "finding": "OneToAll can extend videos seemingly indefinitely",
      "details": "Generated 613 frames with One-to-all, though it does burn with lightx2v. Can reduce burning by lowering lightx2v strength, lower shift, ending pose control early",
      "from": "Kijai"
    },
    {
      "finding": "RCM distill version released for Wan 2.2 I2V",
      "details": "worstcoder/rcm-Wan released RCM6.0 merged model for faster generation with fewer steps",
      "from": "DawnII"
    },
    {
      "finding": "Pose aligner can be stopped early to maintain reference better",
      "details": "You can stop the pose control after few steps and then it keeps the ref better in OneToAll",
      "from": "Kijai"
    },
    {
      "finding": "Not scaling pose at all sometimes leads to best results",
      "details": "Sometimes not scaling the pose at all still leads to best results in OneToAll",
      "from": "Kijai"
    },
    {
      "finding": "HuMo fix allows using start image",
      "details": "Recent HuMo fix for wrapper allows using start image as HuMo is I2V model, normally it's not used",
      "from": "Kijai"
    },
    {
      "finding": "WanMove works at 121 frames, longer than initial 81 frame limit",
      "details": "Model can generate beyond 81 frames and supports custom fps creation",
      "from": "Kijai"
    },
    {
      "finding": "WanMove maintains better style consistency with I2V models",
      "details": "I2V models keep the style well when using control, better than expected",
      "from": "Kijai"
    },
    {
      "finding": "WanMove supports mask for tracks, unlike ATI",
      "details": "Has ability to use mask batch to create mask for single track, multiple track masking not yet implemented",
      "from": "Kijai"
    },
    {
      "finding": "WanMove is compatible with context, unlike ATI",
      "details": "Nothing is hard coded, should work better with context than ATI",
      "from": "Kijai"
    },
    {
      "finding": "Zooming in using video models works as upscale method",
      "details": "Can use video models for upscaling by zooming in, then use second model to unwarp geometry",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "HuMo can be used as low noise with Wan 2.2 as high noise for lip sync",
      "details": "Works well by using HuMo as the low-noise side and Wan 2.2 as high-noise side at higher shift",
      "from": "Ablejones"
    },
    {
      "finding": "SVI helps overcome video degradation over time",
      "details": "SVI lora keeps character reference but pushes camera back towards reference, preventing degradation but limiting camera movement",
      "from": "Kijai"
    },
    {
      "finding": "WanMove trajectory control works better than ATI",
      "details": "Like ATI but better trajectory control system",
      "from": "Kijai"
    },
    {
      "finding": "SVI 2.2 designed for static shots, not dynamic camera movement",
      "details": "Wan 2.2 is equivalent of 'shot' for 2.1, there is no film version for 2.2",
      "from": "DawnII"
    },
    {
      "finding": "LightX2V loras cause color drift after 40-45 seconds",
      "details": "Color drift becomes noticeable around 40-45 second generation when using single reference, 30-35s with additional lora",
      "from": "Vardogr"
    },
    {
      "finding": "Native WanMove implementation uses new Tracks data type",
      "details": "Instead of JSON coords, uses Tracks type that holds path and mask tensors for easier node development",
      "from": "Kijai"
    },
    {
      "finding": "NAG is implemented as additional attention in the positive pass only, separate from negative CFG passes",
      "details": "NAG ignores negative passes and only affects positive cross attention, shouldn't interfere with CFG",
      "from": "Kijai"
    },
    {
      "finding": "HuMo generates 4 extra frames automatically",
      "details": "For every reference image given to HuMo, it adds 4 frames to generation, then sampler cuts those 4 frames off",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "finding": "SVI 2.0 lora for Wan2.1 works better with HuMo than SVI 2.2 Wan2.2 LN lora",
      "details": "Plus you get 5 frames of overlap, helps because you can toss out first 5 frames where most HuMo i2v artifacts show up",
      "from": "Ablejones"
    },
    {
      "finding": "Lightning loras completely remove ability to do dark scenes with T2V",
      "details": "They make everything bright and change results too much",
      "from": "Kijai"
    },
    {
      "finding": "Q8 GGUF is an improvement overall",
      "details": "No OOM or VRAM clearing needed via Distorch nodes, pushed to 97 frames with no issue on 128GB RAM",
      "from": "metaphysician"
    },
    {
      "finding": "rCM LoRAs trained on Wan 2.1 don't work properly on Wan 2.2",
      "details": "Can't use LoRAs on different models because consistency models require operating on the same ODE they were trained on",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "OneToAll 1.3B model performs better than expected",
      "details": "Actually produces decent results, not as terrible as anticipated",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "SCAIL model uses 20 input channels and concatenated pose embeddings",
      "details": "I2V model with separate pose patch embed concatenated in sequence dim instead of channel, requires split rope",
      "from": "Kijai"
    },
    {
      "finding": "SCAIL pose detector uses 3D + 2D mashup format",
      "details": "Uses warm colors on right side and cool colors on left side, which seems important for proper functioning",
      "from": "Kijai"
    },
    {
      "finding": "Mixing e5m2 normal and scaled quantization can cause issues",
      "details": "Can lead to black generations and NaN outputs",
      "from": "Kijai"
    },
    {
      "finding": "SCAIL model generalizes well to different pose inputs",
      "details": "SCAIL works with pose inputs it wasn't specifically trained with, showing good generalization capabilities",
      "from": "Kijai"
    },
    {
      "finding": "Preview as test node works for conditioning outputs",
      "details": "The Preview as test node works for outputs that aren't classes, including conditioning (conds)",
      "from": "Kijai"
    },
    {
      "finding": "SCAIL pose downsampling by default",
      "details": "By default the pose is downsampled to half the size as it's heavy otherwise, and the default res is 895x512. Model has separate patch embed for pose so doesn't have to match in resolution, but currently only full or half size are supported",
      "from": "Kijai"
    },
    {
      "finding": "Context windows work with SCAIL",
      "details": "SCAIL works with context windows for longer video generation, though it's slow",
      "from": "Kijai"
    },
    {
      "finding": "V2 nodes design philosophy",
      "details": "V2 nodes are designed to be modular with separate components: text_embeds, image_embeds, scheduler, extra_args for cleaner workflow",
      "from": "Scruffy"
    },
    {
      "finding": "ComfyUI-RMBG nodes globally override torch.load to unsafe loading",
      "details": "This breaks NLF model functionality and creates security vulnerabilities by disabling safe pickle loading",
      "from": "Kijai"
    },
    {
      "finding": "WanMove works with context windows",
      "details": "Can now split longer camera movements across multiple 81-frame context windows using GetTrackRange node",
      "from": "Kijai"
    },
    {
      "finding": "SCAIL pose input needs half resolution of main input",
      "details": "Model is trained this way - if main input is 512x896, pose input should be 256x448",
      "from": "Kijai"
    },
    {
      "finding": "Uni3c with SCAIL improves background consistency",
      "details": "Background rotates properly instead of being frozen in longer videos",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Multiple spline interpolation methods available in WanMove",
      "details": "Cardinal, Polar, Step, and other interpolation methods for different motion effects",
      "from": "zelgo_"
    },
    {
      "finding": "3D pose detection now working for SCAIL",
      "details": "Single person detection implemented with proper finger tracking",
      "from": "Kijai"
    },
    {
      "finding": "WanMove has speed mode for path control",
      "details": "Can adjust movement speed at specific points by adding control points",
      "from": "Kijai"
    },
    {
      "finding": "low_rank algorithm is 100x faster than full extraction with almost same quality",
      "details": "For LoRA extraction, low_rank provides massive speed improvement with minimal quality loss",
      "from": "Kijai"
    },
    {
      "finding": "TurboWan 2.2 model uses activation quantization and sparse attention",
      "details": "Claims 100x speedup over base model, uses RCM distillation with SAGE optimization",
      "from": "yi"
    },
    {
      "finding": "SCAIL now supports multi-person pose detection",
      "details": "Updated pose node can detect and handle 2 people in videos",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Taichi backend in SCAIL can run on CPU, GPU, OpenGL, or Vulkan",
      "details": "Fast even on CPU, auto-detects architecture when set to 'gpu'",
      "from": "Kijai"
    },
    {
      "finding": "Swapping left and right hand coordinates fixes SCAIL pose issues",
      "details": "There was an error in the SCAIL_Pose repo where left and right hand coordinates were swapped, causing incorrect hand positioning",
      "from": "teal024"
    },
    {
      "finding": "SCAIL works better with longer prompts",
      "details": "Long prompts usually yield better control for both motions and background in SCAIL",
      "from": "teal024"
    },
    {
      "finding": "SCAIL pose control at 0.25 strength gives good results",
      "details": "Using pose control for only 2 out of 6 steps (ending at 0.25) provides effective control without overdoing it",
      "from": "Kijai"
    },
    {
      "finding": "First step only pose control works well for some scenarios",
      "details": "Using pose control only on the first diffusion step can be effective for certain types of motion",
      "from": "Kijai"
    },
    {
      "finding": "NLF returns bounding boxes in addition to pose data",
      "details": "The NLF pose detection system outputs bbox information that can be useful for additional processing",
      "from": "Kijai"
    },
    {
      "finding": "SCAIL performs better than Wan Animate for long-duration generation",
      "details": "For extended video sequences, SCAIL shows superior performance compared to Wan Animate",
      "from": "Karthik"
    },
    {
      "finding": "SCAIL has better ID retention than WanAnimate but WanAnimate has better facial performance",
      "details": "SCAIL retained ID much better than Wan Animate but facial performance replication was missing",
      "from": "A.I.Warper"
    },
    {
      "finding": "Higher resolution generates significantly better quality with SCAIL",
      "details": "1080p looks way better than lower resolutions, difference is more evident than usual",
      "from": "A.I.Warper"
    },
    {
      "finding": "First CFG step with real CFG boosts motion",
      "details": "1-2 steps with no distill loras and real CFG, then use distill and cfg=1 for rest helps motion",
      "from": "spacepxl"
    },
    {
      "finding": "VACE works better with trained LoRAs than image references",
      "details": "VACE is basically just a controlnet on top of unmodified t2v model, so t2v loras work incredibly well. Trained lora will almost always beat image references",
      "from": "spacepxl"
    },
    {
      "finding": "2.2 low model can refine 2.1 VACE outputs",
      "details": "Often use 2.2 low as a refiner for 2.1 vace, 1-2 steps just to clean it up",
      "from": "spacepxl"
    },
    {
      "finding": "SCAIL excels at complex motions and generalizing for reference images",
      "details": "Better for dance movements and complex choreography, though not better for simple motions or facial controls",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.6 pricing revealed",
      "details": "$0.10 per second for 720p, $0.15 per second for 1080p",
      "from": "JohnDopamine"
    },
    {
      "finding": "SCAIL has context window morphing issues",
      "details": "Slight background shifts at context windows, can see gaps even with 64 frame overlap",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Multi-person pose tracking in SCAIL flashes between colors",
      "details": "Colors should stay separate but currently flash between them, multi-person needs segmented approach",
      "from": "Kijai"
    },
    {
      "finding": "SCAIL pose detection works better with 2D anime than DWPose",
      "details": "User reported SCAIL tracking from 2D anime driving video which had serious issues with DWPose",
      "from": "metaphysician"
    },
    {
      "finding": "SCAIL ref image acts as the start frame",
      "details": "The reference image is already the start frame, not a separate input",
      "from": "Kijai"
    },
    {
      "finding": "SCAIL can do higher fps output matching control video",
      "details": "SCAIL follows pose video frame by frame, so 24fps input creates 24fps output when not skipping frames",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.5+ confirmed closed source",
      "details": "Wan team wants to open source but 'someone big does not' - Wan 2.2 is the last open source model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Rope cache issue fixed in WanVideoWrapper",
      "details": "ComfyUI rope cache issue causing errors has been resolved",
      "from": "Kijai"
    },
    {
      "finding": "SCAIL pose input needs to be half the resolution of output",
      "details": "Pose resolution must be half of the output resolution for SCAIL to work properly",
      "from": "Kijai"
    },
    {
      "finding": "Fun InP fl2v mode needed for First-Last-Frame to work properly",
      "details": "Enabling fl2v mode in the I2V encode node prevents yellow flash when using same image for start and end frames",
      "from": "Kijai"
    },
    {
      "finding": "SCAIL works with vitpose input for non-humanoid subjects",
      "details": "SCAIL can work with vitpose input for animals and non-human subjects, vitpose can even detect objects",
      "from": "Kijai"
    },
    {
      "finding": "New T2V distill LoRAs released for Wan 2.2",
      "details": "Latest T2V distill LoRAs available at lightx2v/Wan2.2-Distill-Loras",
      "from": "yi"
    },
    {
      "finding": "Wan Move LoRA works with Wan 2.2 but not 2.1",
      "details": "2.2 understood the conditioning while 2.1 was meh and added walking people. Works better when prompting for static elements",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "SCAIL cannot use Wan Move LoRA due to architectural differences",
      "details": "SCAIL is all in 20 channels while WanMove conditioning is in extra channels that SCAIL doesn't have. WanMove is identical to base Wan 2.1 I2V structure, just finetuned",
      "from": "Kijai"
    },
    {
      "finding": "Models output whatever bit depth they're run in but quality won't be better just by increasing bit depth",
      "details": "Model trained on 8bit data learns continuous functions, so outputs are smooth up to fp16 precision but range/accuracy similar to 8bit training data",
      "from": "spacepxl"
    },
    {
      "finding": "Context windows prevent camera from moving much from initial position",
      "details": "Every window has same start image/camera position, tries to get to that and if moves too far away, snaps back",
      "from": "Kijai"
    },
    {
      "finding": "Differential diffusion works with mask in WanVideo Encode node",
      "details": "Adding mask uses differential diffusion automatically, can also use Set Latent Noise Mask",
      "from": "Kijai"
    },
    {
      "finding": "UniAnimate strength can be unlocked beyond 1.0",
      "details": "No technical reason for 1.0 limit, was only too strong for 2.1. Can experiment with higher values for 2.2",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 works with strength LoRAs",
      "details": "Strength LoRAs definitely affect 2.2 output, unlike being locked at 1.0 for 2.1",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "WanMove works with UniAnimate",
      "details": "Nice to know UniAnimate can work with WanMove for combined motion control",
      "from": "Kijai"
    },
    {
      "finding": "Prompt weighting works in WanVideoWrapper",
      "details": "Parentheses like (16:9) cause weight multiplication - (16:9) multiplied token '16' weight by 9x causing artifacts",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "InfiniteTalk Multi needs clean first latent frame",
      "details": "If first latent frame is noise, you get flash/noise artifacts. Must use clean image as first frame",
      "from": "Kijai"
    },
    {
      "finding": "WorldCanvas uses 2.2 weights with 53 input channels",
      "details": "New model with 53 input channels (vs Fun Control 2.2's 52), appears to be 2.2 with VACE-like features",
      "from": "Kijai"
    },
    {
      "finding": "LongCat Avatar uses 'enhance_hf' scheduler modification",
      "details": "Their scheduler modification is quite drastic, which could explain implementation issues",
      "from": "Kijai"
    },
    {
      "finding": "FlashPortrait uses the same face encoder as WanAnimate",
      "details": "FlashPortrait is a 14B 2.1 I2V model that seems to be using the same face encoder as WanAnimate and appears to be using the Fun model architecture",
      "from": "Kijai"
    },
    {
      "finding": "FlashPortrait acceleration is just lightx2v LoRA",
      "details": "The claimed 6x acceleration does not exist independently - it's literally just using the lightx2v LoRA, and using context windows actually loses that speed advantage",
      "from": "Kijai"
    },
    {
      "finding": "FlashPortrait renamed lightx2v LoRA without credit",
      "details": "FlashPortrait repo contains 'fast_lora_rank64.safetensors' which is identical to lightx2v's wan2.1_i2v_lora_rank64_lightx2v_4step.safetensors down to the hash, with no credit given",
      "from": "Kijai"
    },
    {
      "finding": "LongCat Avatar uses previous latents directly as model input",
      "details": "The continuation is mostly from latent space with minimal decode-encode between windows, using previous latents directly as model input rather than full image reconstruction",
      "from": "Kijai"
    },
    {
      "finding": "LongCat Avatar has method to prevent repetitive actions in long generations",
      "details": "Main advantage over Infinite is a built-in method that prevents repetitive actions during long video generation",
      "from": "Kijai"
    },
    {
      "finding": "LongCat Avatar ref_latent bug was causing degradation",
      "details": "Kijai discovered a bug where ref_latent wasn't being used, causing identity degradation in longer generations. Fixed in latest commits.",
      "from": "Kijai"
    },
    {
      "finding": "LongCat Avatar allows significant camera movement without breaking",
      "details": "The model surprisingly handles camera movement well, even with dynamic shots and rotations",
      "from": "Kijai"
    },
    {
      "finding": "Stand-In LoRA works on Wan 2.2 without code changes",
      "details": "The new Stand-In LoRA can be used directly with Wan 2.2, uploaded as .safetensors format",
      "from": "Kijai"
    },
    {
      "finding": "LongVie2 controlnet works with just Wan 2.1 I2V",
      "details": "Successfully applied LongVie2 controlnet to Wan 2.1 I2V without using their attention weights, just the controlnet portion",
      "from": "Kijai"
    },
    {
      "finding": "LongCat-Avatar can generate up to 301 frames successfully",
      "details": "301 frames took 560sec on 5090, but 141 frames took 154sec - more than doubling frames results in more than triple the time",
      "from": "burgstall"
    },
    {
      "finding": "LongCat-Avatar works at various resolutions",
      "details": "480x480x141 and 480x480x201 frames both work, 93 frame windows aren't entirely necessary if you have memory",
      "from": "burgstall"
    },
    {
      "finding": "Stand-In LoRAs need to be loaded differently",
      "details": "Stand-In LoRAs must be loaded in the model loader lora input as it's a different kind of LoRA",
      "from": "Kijai"
    },
    {
      "finding": "Pose input resolution requirement for dark output fix",
      "details": "Pose should be half the resolution of the main input, which is done automatically in example workflow",
      "from": "Kijai"
    },
    {
      "finding": "Context Options don't affect main steps",
      "details": "Context options do multiple model passes per step but don't change the step count",
      "from": "Kijai"
    },
    {
      "finding": "Denoise value affects actual steps executed",
      "details": "Denoise at 0.21 means only doing 20% of the steps, which is why console shows fewer steps than set",
      "from": "Kijai"
    },
    {
      "finding": "PUSA LoRA made for 2.2 works better on WAN 2.1 than the PUSA 2.1 version",
      "details": "When testing Stand-In for WAN 2.1, the PUSA 2.2 LoRA actually improved facial consistency better than the version specifically made for 2.1",
      "from": "\u25b2"
    },
    {
      "finding": "2.2 LN LoRAs work reliably in WAN 2.1 based models",
      "details": "2.2 LN wan is just 2.1+++, so you can use 2.2 LN loras pretty reliably in wan 2.1 based models",
      "from": "ucren"
    },
    {
      "finding": "LongCat-Avatar uses 32 fps as input and samples audio with stride of 2",
      "details": "The model uses 32 fps as input and samples the audio with stride of 2, so the output is 16 fps video",
      "from": "Kijai"
    },
    {
      "finding": "WanMove depth control strength affects tree persistence",
      "details": "Lowering the control strength to 0.8 seems to allow the tree to persist in depth-controlled generations",
      "from": "Kijai"
    },
    {
      "finding": "NVFP4 support now available for Blackwell cards",
      "details": "New Wan-NVFP4 model released with NVFP4 kernel support, mostly supported in pytorch nightly",
      "from": "yi"
    },
    {
      "finding": "Windows 11 shows improved VRAM efficiency over Windows 10",
      "details": "User reports significantly reduced VRAM usage and ability to generate at Full HD directly without running out of VRAM after switching from Windows 10 to Windows 11",
      "from": "David Snow"
    },
    {
      "finding": "Frame rates need to match between input and output for SCAIL",
      "details": "Using 16fps for both input video and output video improves SCAIL motion transfer quality",
      "from": "ucren"
    },
    {
      "finding": "vitpose-h-wholebody model improves pose detection",
      "details": "Better pose detection results compared to vitpose-l-wholebody for SCAIL workflows",
      "from": "ucren"
    },
    {
      "finding": "Padding reference images improves SCAIL results",
      "details": "Reference images that are too tightly cropped cause disproportionate results, padding helps fit the character properly",
      "from": "Kijai"
    },
    {
      "finding": "StoryMem LoRAs require special alpha scaling",
      "details": "StoryMem LoRAs are rank stabilized and need strength around 22 or automatic scaling code to work properly",
      "from": "Kijai"
    },
    {
      "finding": "Block swap is relatively fast for model loading",
      "details": "PCI Express 4 interface allows fast transfer between RAM and VRAM, adding only seconds max to generation",
      "from": "42hub"
    },
    {
      "finding": "vitpose files need to be in models/detection folder",
      "details": "vitpose-h-wholebody.onnx should be placed in models/detection rather than models/onnx for proper detection",
      "from": "JohnDopamine"
    },
    {
      "finding": "q4_km GGUF uses less VRAM than fp8 scaled",
      "details": "q4km allows more frames for upscaling compared to fp8 scaled version",
      "from": "craftogrammer"
    },
    {
      "finding": "Context windows work with WanAnimate after bug fix",
      "details": "WanVideo Encode Latent Batch with context windows now compatible with WanAnimate",
      "from": "Kijai"
    },
    {
      "finding": "Multiple images and prompts can be used in I2V",
      "details": "Can use multiple images with pipe-separated prompts to generate different video segments in one generation",
      "from": "Kijai"
    },
    {
      "finding": "SVI 2.0 Pro no longer uses first frame copies for conditioning",
      "details": "Now uses last latent index instead of last frame, allowing cond channel to be utilized by other controls like pose. You lose less frames in cond channel.",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "SVI 2.0 Pro settings for lightx compatibility",
      "details": "Setting overlap to 4 and motion frames to 1 stops glitches when using lightx2v",
      "from": "Benjimon"
    },
    {
      "finding": "Wan Alpha 2.0 uses VAE LoRA for transparency",
      "details": "Uses separate vae_fgr and vae_pha models with decoder LoRA loading for foreground and alpha channels",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "SCAIL scales skeleton of input video TO input image",
      "details": "DWPose from reference image gets applied, then video motion is applied to that skeleton rather than original video bones",
      "from": "DawnII"
    },
    {
      "finding": "Native QwenVL 2.5 implementation in progress",
      "details": "Kijai working on native implementation, considering 3.0 support. Useful when model uses same text encoder for embeddings",
      "from": "Kijai"
    },
    {
      "finding": "LightX2V achieves 25x speedup by reducing model passes from 100 (50 steps with cfg) to 4 (4 steps without cfg)",
      "details": "SageAttention provides additional 2x faster model pass, making total speedup 50x faster",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 works better with fp8 fast compared to 2.1",
      "details": "fp8 fast is even usable with 2.2 while it never was for 2.1",
      "from": "Kijai"
    },
    {
      "finding": "I2V models have 36 channels (32 + 4 mask channels) while T2V models have 16 channels",
      "details": "This causes tensor mismatch errors when trying to mix I2V and T2V models",
      "from": "cyncratic"
    },
    {
      "finding": "Wan training with 1k videos at 121 frames requires 32GB VRAM for LoRA training",
      "details": "Can possibly work on 24GB with max block swap but will be slow",
      "from": "CJ"
    },
    {
      "finding": "SVI 2.0 Pro doesn't need encoding between extensions",
      "details": "Only the init image needs encoding alone, no black padding frames are encoded - padding is just torch zeros directly",
      "from": "Kijai"
    },
    {
      "finding": "SVI Pro uses clean anchor frame for extensions",
      "details": "Always uses the same first image as anchor, should be able to go long without decode-encode degradation",
      "from": "Kijai"
    },
    {
      "finding": "Wan wrapper VRAM usage improved significantly",
      "details": "20% improvement in VRAM usage, user went from 96% to 73% VRAM usage with 1280x720 81f",
      "from": "Lumifel"
    },
    {
      "finding": "Open-OmniVCus works but feels realism biased",
      "details": "Returns consistent results but has a bias toward realistic content",
      "from": "Kijai"
    },
    {
      "finding": "Scene changes cause SVI Pro degradation",
      "details": "When scene changes significantly, the original reference frame doesn't apply well, causing degradation",
      "from": "Kijai"
    },
    {
      "finding": "Stand-in LoRA for Wan 2.2 works best with specific dimensions",
      "details": "Reference must be exactly 512x512 and output 480x832 for best results, other dimensions lose likeness",
      "from": "ucren"
    },
    {
      "finding": "SVI Pro uses offset parameter to handle different dimensions",
      "details": "Changing offset to 2 for 1088x640 resolution helps with generation",
      "from": "ucren"
    },
    {
      "finding": "SVI Pro uses previous generation latent directly instead of frames",
      "details": "No decoding/encoding between generations, uses last 4 frames essentially through latent space",
      "from": "DawnII"
    },
    {
      "finding": "LightX2V causes motion issues with inconsistent pacing",
      "details": "Makes entire video feel choppy, motion becomes unpredictable and slow",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "High noise models work better without LightX2V",
      "details": "Much better motion quality when not using LightX2V lora on high noise model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "SVI Pro has hard time following simple prompts",
      "details": "Difficulty with prompt adherence compared to regular generation",
      "from": "Kijai"
    },
    {
      "finding": "SVI Pro works in portrait orientation",
      "details": "Successfully generates portrait videos",
      "from": "DawnII"
    },
    {
      "finding": "Ultravico (modified attention) works with frame_tokens corresponding to 832x480",
      "details": "Hard coded frame_tokens may be why ultravico works badly at other resolutions",
      "from": "Kijai"
    },
    {
      "finding": "First frame alpha decay modification improves longer generations",
      "details": "Modified code to never decay the first frame amount of tokens, though might ruin motion",
      "from": "Kijai"
    },
    {
      "finding": "SVI Pro can replicate StorymMem functionality with independent anchor latents",
      "details": "Using different anchor samples independently can achieve similar results to StorymMem without needing the actual StorymMem model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Qwen Image Edit 2511 can generate multiple view anchor samples",
      "details": "Used to generate multiple views as single anchor samples for each phase in SVI workflows",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "RCM LoRA improves cohesion and adherence in Wan 2.2",
      "details": "Using RCM LoRA with old lightx2v at 3.0 strength provides better prompt adherence and visual cohesion",
      "from": "faceismus"
    },
    {
      "finding": "SVI Pro works well for video extension",
      "details": "Can create videos up to 52-57 seconds long with good quality",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Ultravico can generate longer sequences",
      "details": "Can push generations past 121/149 frames, works better with T2V than I2V",
      "from": "Kijai"
    },
    {
      "finding": "Motion scale control node works with VACE",
      "details": "The new motion scale control node can work with VACE unlike PainterI2V",
      "from": "Elvaxorn"
    },
    {
      "finding": "Smoothmix has lightx2v LoRA built-in",
      "details": "Smoothmix model includes lightx2v LoRA so you don't need to load it separately",
      "from": "Tachyon"
    },
    {
      "finding": "SVI Pro can generate up to 2 minute videos",
      "details": "Maximum tested length is 57 seconds but theoretically can go up to 2 minutes",
      "from": "avataraim"
    },
    {
      "finding": "Augment empty frames strength of 0.2-0.3 works well for motion enhancement",
      "details": "0.4 introduced artifacts, 0.0-0.3 range produces good comparable results",
      "from": "David Snow"
    },
    {
      "finding": "VACE 2.1 modules can work with Wan 2.2 T2V models",
      "details": "At least with the Low Noise ones, works fine for certain tasks",
      "from": "Ablejones"
    },
    {
      "finding": "HuMo I2V capabilities slightly degraded but still functional",
      "details": "Tiny amount of noise on first frame but still works well for I2V",
      "from": "Ablejones"
    },
    {
      "finding": "Color matching all frames to first frame helps with color drift",
      "details": "Using Color Match node from KJNodes to correct all frames to the first one",
      "from": "42hub"
    },
    {
      "finding": "VACE training lost original functionality but improved simple control + reference for realistic content",
      "details": "Training the VACE model seemed to reduce some original capabilities while improving specific use cases",
      "from": "Kijai"
    },
    {
      "finding": "FreeLong node doesn't actually use FreeLong for long generation",
      "details": "Despite claims, the node just uses last frame and hopes FreeLong within 81 frames does something",
      "from": "Kijai"
    },
    {
      "finding": "Fun models can work at 161 frames with control",
      "details": "You lose video quality but motion quality is fine due to the control when going past training limits",
      "from": "Kijai"
    },
    {
      "finding": "FreeLong blends low-frequency global video features with high-frequency local video features",
      "details": "Maintains global consistency while incorporating diverse spatiotemporal details from local videos",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "Enhance-a-video separates temporal tokens from spatial",
      "details": "Similar approach to FreeLong in handling different aspects of video generation",
      "from": "Kijai"
    }
  ],
  "troubleshooting": [
    {
      "problem": "Torch compile causing noise/artifacts in Fun VACE",
      "solution": "Upgrade to PyTorch 2.9.1 from 2.9.0",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "Lynx not working with I2V after recent wrapper update",
      "solution": "Fixed with code changes, was related to tensor device placement issues",
      "from": "Kijai"
    },
    {
      "problem": "LoRAs never fully offloaded at end of sampling in multi-sampler workflows",
      "solution": "Fixed in wrapper update, was increasing VRAM cost and leaving less VRAM for VAE decode",
      "from": "Kijai"
    },
    {
      "problem": "Performance degradation after wrapper updates",
      "solution": "Clear Triton caches at C:\\Users\\<username>\\.triton and torch inductor cache",
      "from": "Kijai"
    },
    {
      "problem": "Out of memory errors that appear as CUDA errors",
      "solution": "These are actually RAM errors not VRAM, reboot to clear fragmented memory",
      "from": "Kijai"
    },
    {
      "problem": "SAGE attention errors with S2V",
      "solution": "Input tensors must be in dtype of torch.float16 or torch.bfloat16, falls back to pytorch attention",
      "from": "blake37"
    },
    {
      "problem": "ComfyUI tab loads infinitely after upgrading frontend",
      "solution": "Move all custom nodes to new directory outside custom_nodes, then move half back in. Use binary search approach to find problematic node",
      "from": "Scruffy"
    },
    {
      "problem": "SageAttention installation issues on Ubuntu 24.04",
      "solution": "Downgrade to Ubuntu 22.04 or Pop OS 22.04, as CUDA toolkit compatibility is better",
      "from": "seitanism"
    },
    {
      "problem": "torch compile error with QuantizedTensor after ComfyUI update",
      "solution": "Issue identified but no specific solution provided in this chat",
      "from": "Baku"
    },
    {
      "problem": "VAEUtils_CustomVAELoader error: VAE is invalid",
      "solution": "The linked file is actually a latent upscale model, not a VAE. Use the latent upscale node instead",
      "from": "spacepxl"
    },
    {
      "problem": "Negative embeddings error with CFG scale > 1.0 in NAG setup",
      "solution": "Issue was with CFG schedule float list, resolved by changing that component",
      "from": "David Snow"
    },
    {
      "problem": "UltraViCo loses start image influence at 241 frames",
      "solution": "Known limitation identified but no solution provided",
      "from": "Kijai"
    },
    {
      "problem": "Fast motion causes blur in generated videos",
      "solution": "Use 2x VFI (video frame interpolation) and v2v to slow down motion, then clean up with v2v",
      "from": "spacepxl"
    },
    {
      "problem": "Multistep samplers underperform with 2+2 split step samplers",
      "solution": "Stay away from multistep samplers for anything lower than 8 steps, especially for split-model workflows",
      "from": "Ablejones"
    },
    {
      "problem": "FP8 model giving horrid output compared to Q5 GGUF",
      "solution": "Use Q8 GGUF with distorch model loaders for better control over VRAM/RAM loading",
      "from": "David Snow"
    },
    {
      "problem": "CUDA kernel error on 4090 with --fast and --use-sage-attention flags",
      "solution": "Remove the custom parameters, error was caused by sage attention needing rebuild for different GPU architectures",
      "from": "mamad8"
    },
    {
      "problem": "UltraViCo returns 'NoneType' object error",
      "solution": "Works with T2V but fails with I2V and HuMo due to attention calls and input size limitations with block masking",
      "from": "aiacsp"
    },
    {
      "problem": "OOM issues with Steadydancer on 3090",
      "solution": "Reduce blocks to swap and/or disable non-blocking in block swap settings",
      "from": "Kijai"
    },
    {
      "problem": "SVI 2.0 not working properly with LightX2V",
      "solution": "LightX2V breaks SVI 2.0, works better without it",
      "from": "Kijai"
    },
    {
      "problem": "SVI 2.0 looking washed out and not working as expected",
      "solution": "Use 5 frame overlap with proper padding, updated implementation needed",
      "from": "Hashu"
    },
    {
      "problem": "Context stride causing fatal errors with uniform_standard",
      "solution": "Keep stride at 1, don't use stride value of 4",
      "from": "Kijai"
    },
    {
      "problem": "Flash between video extensions",
      "solution": "Mask only the first frame instead of all 5 frames for Wan 2.2",
      "from": "Kijai"
    },
    {
      "problem": "Artifacts around mouth in video",
      "solution": "Install flash_attn to prevent severe artifacts and instability",
      "from": "Ablejones"
    },
    {
      "problem": "DWPose not detecting bones in complex movements",
      "solution": "Try vit pose using WanAnimate preprocess stuff, or mask out subject on black background before running pose",
      "from": "Ablejones"
    },
    {
      "problem": "ComfyUI memory not releasing properly",
      "solution": "Use explicit caching by saving and loading intermediate images/videos, or switch to API",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "problem": "Memory leak with image resize nodes causing crashes",
      "solution": "Update to PyTorch 2.9.1+cu130 and uninstall version-dependent packages like xformers and flash-attn",
      "from": "spacepxl"
    },
    {
      "problem": "ComfyUI crashes after PyTorch update",
      "solution": "Update SageAttention and other custom nodes that depend on PyTorch version",
      "from": "lostintranslation"
    },
    {
      "problem": "Memory leak affects multiple resize node types",
      "solution": "Issue is in ComfyUI's node caching system, not specific nodes - updating PyTorch fixes it",
      "from": "spacepxl"
    },
    {
      "problem": "RuntimeError: GET was unable to find an engine to execute this computation with WanVideo ImageToVideo Encode node",
      "solution": "Changed bf16 to fp16",
      "from": "xwsswww"
    },
    {
      "problem": "RAM leak issue with torch 2.8.0",
      "solution": "Update from torch 2.8.0 to 2.9.1",
      "from": "Kijai"
    },
    {
      "problem": "InfiniteTalk modifying too much of the video beyond just mouth",
      "solution": "Use masking or low steps like 2 to have Infinite modify only the mouth. More steps = more quality is not always true in this specific situation",
      "from": "Stef"
    },
    {
      "problem": "HuMo model loading taking too long (minutes instead of seconds)",
      "solution": "Issue is usually slow cloud server drives, should load in seconds with fast SSD",
      "from": "Kijai"
    },
    {
      "problem": "OOM with HuMo at 1280x720",
      "solution": "Use lower resolution like 1024x576",
      "from": "AmirKerr"
    },
    {
      "problem": "Merging LoRAs breaks HuMo model",
      "solution": "Don't merge LoRAs with HuMo, use unmerged",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "OneToAll Animation model not working from early download",
      "solution": "Re-download model or rename norm keys (attn1.norm_added_k -> ref_attn_norm_k_img)",
      "from": "Kijai"
    },
    {
      "problem": "SVI LoRAs not working with native ComfyUI",
      "solution": "Use WanVideoWrapper - native ComfyUI LoRA loader doesn't work with SVI models",
      "from": "Ablejones"
    },
    {
      "problem": "Pose control bias issues",
      "solution": "Models need to encode videos directly instead of relying on openpose",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "problem": "VACE mask loading issue",
      "solution": "Load masked video into 'input frames' slot on VACE encoder with first frame replaced, not as control images",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "ComfyUI launch breaking",
      "solution": "Remove westNeighbor_ComfyUI-ultimate-openpose-editor custom node",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "WanVideoDecode error when reusing decoded video",
      "solution": "Fresh sidecar install with basics resolves the issue - caused by some other custom node conflict",
      "from": "Nathan Shipley"
    },
    {
      "problem": "Memory issues with OneToAll at 81 frames 512x even with fp8 and 30 block swap",
      "solution": "Issue is with VAE conv3d before sampling starts. 832x480x81 works under 12GB. Apply conv3d fix from hunyuan VAE",
      "from": "patientx"
    },
    {
      "problem": "Saturation changes in normal wan i2v",
      "solution": "Issue was motion amplitude above 1 in painteri2v node",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "problem": "Dynamic face expressions missing in 2D character control",
      "solution": "Extract face from ref image as 512x512 then repeat for N frames, use that for face_images",
      "from": "FlipYourBits"
    },
    {
      "problem": "Mask input issues in VACE workflow",
      "solution": "Mask is only needed in first vace encode for start frame and inpaint areas. Unplug input_masks from pose and depth vace encodes",
      "from": "Hashu"
    },
    {
      "problem": "TinyVAE not showing in vae_approx folder",
      "solution": "Use normal VAE loader, or use separate tiny vae loader in wrapper. Don't rename the file. Requires recent ComfyUI update",
      "from": "Kijai"
    },
    {
      "problem": "Sage Attention installation on fresh install",
      "solution": "Install two wheels from woct0rdho repos - super easy now, no need for visual studio or cuda-toolkit",
      "from": "Kijai"
    },
    {
      "problem": "Fp8 scaled models producing black/NaN outputs",
      "solution": "Issue with recent ComfyUI quantization update, some LoRAs affected differently - lightx2v T2V lora still works",
      "from": "Kijai"
    },
    {
      "problem": "Frame count mismatch error in spline editor",
      "solution": "Bug when using multiple splines gives wrong count, use single spline or set frame count manually",
      "from": "Kijai"
    },
    {
      "problem": "SVI kills prompt adherence",
      "solution": "Reduce SVI LoRA strength on high noise to 0.5-0.75 to balance SVI effect with prompt adherence",
      "from": "Hashu"
    },
    {
      "problem": "VAE decoding taking extremely long time",
      "solution": "First triton run creates cache files causing 17+ minute delays, subsequent runs are normal 20 seconds",
      "from": "patientx"
    },
    {
      "problem": "KJNodes backward compatibility broken",
      "solution": "Boolean value changes from False to false, need to refresh nodes or update manually",
      "from": "Kijai"
    },
    {
      "problem": "HuMo unhappy being used as I2V, creates transition issues",
      "solution": "Use SVI 2.0 for 2.1 on the HuMo pass, and do color matching with true HuMo initial generation",
      "from": "Ablejones"
    },
    {
      "problem": "WanMove native spline editor IndexError when adding second spline",
      "solution": "Update KJNodes - spline editor was outputting combined point count causing mismatch",
      "from": "Kijai"
    },
    {
      "problem": "Character jerking movement at start when reference frame put into first frame",
      "solution": "Use LCM sampler instead of res4lyf schedulers which cause the problem",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "problem": "Black video output with LightX2V",
      "solution": "Update ComfyUI - was fixed yesterday",
      "from": "Kijai"
    },
    {
      "problem": "Cannot use CFG with WanMove",
      "solution": "Bug fixed - negative prompt wasn't using tracks and needed proper casting",
      "from": "Kijai"
    },
    {
      "problem": "InfiniteTalk producing black outputs",
      "solution": "Issue acknowledged but no solution provided",
      "from": "WhimsicalZ"
    },
    {
      "problem": "WanVideoVAE object has no attribute 'get' error",
      "solution": "Re-download the model",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "problem": "cfg float list error when using NAG",
      "solution": "Need actual negative prompt in main text encode, not just NAG. Empty prompt used for CFG steps",
      "from": "Kijai"
    },
    {
      "problem": "Flashing when using both reference images and start frame in HuMo",
      "solution": "Use just start frame, avoid connecting both reference_images and start frame together",
      "from": "Hashu"
    },
    {
      "problem": "Snowing/washed out greyish artifacts in low noise video",
      "solution": "Steps are too large - lower shift, change schedule",
      "from": "Ablejones"
    },
    {
      "problem": "'free model and node cache' no longer works for recognizing new models",
      "solution": "F5 to refresh page or use 'R' hotkey",
      "from": "David Snow"
    },
    {
      "problem": "Snow artifacts in video generation",
      "solution": "Check sigma curves with Kijai's WAN Scheduler node, avoid splitting steps too far from 0.875 T2V or 0.9 I2V boundaries",
      "from": "pookyjuice"
    },
    {
      "problem": "Black generations with VACE",
      "solution": "Use matching quantization types (both BF16) instead of mixing e5m2 normal and scaled",
      "from": "Dream Making"
    },
    {
      "problem": "NaN outputs and black generations",
      "solution": "Try disabling SageAttention and using SDPA, or update SageAttention to latest version",
      "from": "Kijai"
    },
    {
      "problem": "VRAM getting stuck during generation",
      "solution": "Add --reserve-vram 1 argument to ComfyUI startup to reserve 1GB VRAM",
      "from": "Kijai"
    },
    {
      "problem": "Custom node breaking unrelated workflows",
      "solution": "Use --disable-all-custom-nodes to identify problematic nodes that monkey patch other components",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "White and black frames inverted in VACE keyframe node",
      "solution": "Update the node as the mask inversion was fixed",
      "from": "Flipping Sigmas"
    },
    {
      "problem": "Dark output videos with SCAIL",
      "solution": "Try the pose downscaling instead of full pose, and use the I2V LoRA (lightx2v 2.1 I2V for SCAIL)",
      "from": "Kijai"
    },
    {
      "problem": "OOM error on 5090 with e4m3fn versions",
      "solution": "Try deleting triton cache (C:\\Users\\username\\.triton) and disable non-blocking on block swap if using wrapper workflow",
      "from": "NebSH and Kijai"
    },
    {
      "problem": "RAM error with torch script model",
      "solution": "Try manually downloading the model file again, could be model file error",
      "from": "Kijai"
    },
    {
      "problem": "Pose frames leaking into main causing black artifacts",
      "solution": "Use pose downscaling instead of full pose resolution",
      "from": "Kijai"
    },
    {
      "problem": "Issues with low sampler on Wan 2.2",
      "solution": "Change the sampler - some nodes don't work properly with 2.2",
      "from": "Dream Making"
    },
    {
      "problem": "Keyframes too close together causing issues",
      "solution": "Add auto spacing feature between keyframes",
      "from": "Flipping Sigmas"
    },
    {
      "problem": "Black border on top and left of FlashVSR output",
      "solution": "Caused by ComfyUI-RMBG nodes - remove or update them",
      "from": "Kijai"
    },
    {
      "problem": "Merge_loras making VAE decoding slower",
      "solution": "Disable force_offload or lower tile size to avoid RAM-VRAM transfer overhead",
      "from": "Kijai"
    },
    {
      "problem": "Uni3c incompatible with SCAIL (20 vs 21 channels)",
      "solution": "Add 4 more frames to uni3c input to account for SCAIL ref taking one latent",
      "from": "Kijai"
    },
    {
      "problem": "RuntimeError: invalid vector subscript in NLF",
      "solution": "Remove ComfyUI-RMBG nodes that globally override torch.jit settings",
      "from": "Kijai"
    },
    {
      "problem": "Face detection error in multi-person SCAIL",
      "solution": "Disconnect dwpose input or disable face drawing if no faces detected",
      "from": "Kijai"
    },
    {
      "problem": "triton-windows compatibility issues",
      "solution": "Downgrade from version 3.5 to 3.3 to match torch version",
      "from": "boorayjenkins"
    },
    {
      "problem": "SCAIL pose background leaking to main sequence",
      "solution": "Ensure pose resolution is exactly half the main input resolution",
      "from": "Kijai"
    },
    {
      "problem": "SCAIL pose input makes everything dark/black if not downscaled",
      "solution": "Use 2x downscaling for pose input as model was trained with downsampling",
      "from": "teal024"
    },
    {
      "problem": "REMBG nodes causing dependency conflicts",
      "solution": "Uninstall REMBG custom nodes - they clash with newer ML dependencies",
      "from": "cyncratic"
    },
    {
      "problem": "SCAIL errors with landscape resolutions",
      "solution": "Use portrait aspect ratios, resolution must be divisible by 32",
      "from": "Flipping Sigmas"
    },
    {
      "problem": "Hand drifting in SCAIL retargeting",
      "solution": "Try crop instead of pad, use VITPose-H instead of L, bicubic interpolation",
      "from": "Kijai"
    },
    {
      "problem": "Multi-person SCAIL detection issues",
      "solution": "Disconnect DWPose when using multi-person, feature not fully complete",
      "from": "Kijai"
    },
    {
      "problem": "Tensor mismatch caused by corrupted cached state when changing frame counts",
      "solution": "Restart ComfyUI to clear the RoPE cache when switching between different frame counts or pose resolutions",
      "from": "Flipping Sigmas"
    },
    {
      "problem": "Bicubic interpolation fails on driving video but works on reference",
      "solution": "Use Lanczos interpolation instead, which works reliably though slightly slower",
      "from": "Kijai"
    },
    {
      "problem": "SCAIL skeleton appears off-center regardless of input resolution changes",
      "solution": "Issue appears related to retargeting alignment, may need manual adjustment of thresholds or turning off retargeting",
      "from": "DawnII"
    },
    {
      "problem": "First frame color bugs out or appears blurry/flashed",
      "solution": "Avoid using LCM sampler which can discolor first frame, issue may also be related to LightX2V LoRA or scheduler",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "problem": "IndexError when no pose detected in first frame",
      "solution": "Ensure pose detection works on first frame, or code will use first valid pose found in sequence",
      "from": "Kijai"
    },
    {
      "problem": "Resolution mismatch errors in SCAIL",
      "solution": "Width and height must be divisible by 32, and resized node dimensions must match the set width/height parameters",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "CUDA allocation error with pose conditioning",
      "solution": "Increase block swap amount as pose conditioning requires more VRAM than normal generation",
      "from": "Kijai"
    },
    {
      "problem": "RuntimeError with normalized_shape=[1280] in WanVideo Sampler v2",
      "solution": "Re-download the clip_vision model - same name but file was corrupted",
      "from": "patientx"
    },
    {
      "problem": "NLF prediction hardcoded to use CUDA, won't work with CPU/ROCm",
      "solution": "No current solution - would need to rebuild NLF and smplfitter repos",
      "from": "Kijai"
    },
    {
      "problem": "DEIS sampler throwing TypeError with cuda tensor conversion",
      "solution": "Switch to other samplers - DEIS sampler has issues",
      "from": "Dream Making"
    },
    {
      "problem": "UnboundLocalError with callback_latent in UniPC sampler",
      "solution": "Check if free_init connected by accident, verify doing actual steps, or use custom sigmas meant for 4 total steps",
      "from": "Kijai"
    },
    {
      "problem": "SCAIL nodes appearing red after installation",
      "solution": "Update WanVideoWrapper nodes - SCAIL nodes were only merged into WanVideoWrapper recently",
      "from": "Ablejones"
    },
    {
      "problem": "No module named 'taichi' error with SCAIL",
      "solution": "Install taichi dependency",
      "from": "xwsswww"
    },
    {
      "problem": "Resolution mismatch errors",
      "solution": "Make sure all resolutions match exactly (e.g., not 512x895 vs 512x896), use crop to fix mismatches",
      "from": "hicho"
    },
    {
      "problem": "Wan 2.6 prompt rejection for violence terms",
      "solution": "Remove words like 'scuffle', 'lunges', 'grabs', 'wrestling' - reframe as accidents or clumsiness instead of fights",
      "from": "JohnDopamine"
    },
    {
      "problem": "SCAIL pose skeleton drawing as flat surfaces instead of cylinders",
      "solution": "Check taichi version with 'pip show taichi' or try changing render device to CPU",
      "from": "Kijai"
    },
    {
      "problem": "Getting 2 second video with 81 frames in SCAIL",
      "solution": "FPS automatically pulled from input video - add Set FPS node to control output duration",
      "from": "cyncratic"
    },
    {
      "problem": "SCAIL multi-person hands/face jumping between people",
      "solution": "Disconnect ref_dw_pose when doing multiple people - it doesn't work even in original implementation",
      "from": "Kijai"
    },
    {
      "problem": "VitPose to DWPose node crashing PC",
      "solution": "Noted as system stability issue requiring restart",
      "from": "Janosch Simon"
    },
    {
      "problem": "WanVideoSamplerv2 shape error at 1280x720",
      "solution": "Resolution must be divisible by 32 (use 1280x704) and set in the node properly",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "VACE losing 2 frames (32 input, 30 output)",
      "solution": "Use 4*n+1 frame count rule for Wan models",
      "from": "42hub"
    },
    {
      "problem": "I2V model needs I2V embeds not empty embeds",
      "solution": "Use proper embed type - empty embeds are for T2V only",
      "from": "Kijai"
    },
    {
      "problem": "SCAIL context windows cause background shift",
      "solution": "Context windows don't work well with moving camera - try continue from last frame instead",
      "from": "Kijai"
    },
    {
      "problem": "Vitpose only returns one person detection",
      "solution": "Use dwpose for multiple people and face detection",
      "from": "Kijai"
    },
    {
      "problem": "ImportError: Selected attention mode not available with SCAIL",
      "solution": "Switch from SageAttention to SDPE attention mode - it's a version compatibility issue",
      "from": "A.I.Warper"
    },
    {
      "problem": "SCAIL generations coming out super dark",
      "solution": "Make sure pose input is half the resolution of output - use math nodes to halve it",
      "from": "Kijai"
    },
    {
      "problem": "Yellow flash when using same image for start and end frames",
      "solution": "Enable fl2v mode in the I2V encode node in WanVideoWrapper",
      "from": "Kijai"
    },
    {
      "problem": "Background changes during long SCAIL generations",
      "solution": "Context windowing issue where each window tries to return to reference camera position/background, uni3c can help override this",
      "from": "Kijai"
    },
    {
      "problem": "VRAM memory issues with uni3c doubling time",
      "solution": "Swap more blocks to avoid running out of memory",
      "from": "Kijai"
    },
    {
      "problem": "WAN move conditioning crashing with multiple tracks",
      "solution": "When using different amounts of splines between spline editors, use separate spline editors for each track (3 spline editors with 3 masks)",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "SCAIL example workflow cuts off frames (41 input to 37 output)",
      "solution": "Follow the num_frames value of empty embeds - it generates as many frames as you input there",
      "from": "Kijai"
    },
    {
      "problem": "Uni3C controlnet loading error 'controlnet_blocks.0.bias'",
      "solution": "No solution provided in chat",
      "from": "A.I.Warper"
    },
    {
      "problem": "Compounding error in long video generation causing oversaturation",
      "solution": "Split video into separate 81 frame runs and splice together, or use depth-controlled second pass with context windows",
      "from": "spacepxl"
    },
    {
      "problem": "Pose detection issues with DWPose nodes",
      "solution": "Connect Image dw Pose extractor output to Video Pose detection input, connect that to nlf dw pose. Set width/height from bottom Image resizer to top Video resizer",
      "from": "Jumper"
    },
    {
      "problem": "ComfyUI preview issues after update",
      "solution": "Add --preview-method auto to command line or check new preview settings location in ComfyUI settings",
      "from": "Hashu"
    },
    {
      "problem": "InfiniteTalk Multi initial flash",
      "solution": "Use clean image as first latent frame instead of noise. Can also use MultiTalk model which doesn't have this requirement",
      "from": "Kijai"
    },
    {
      "problem": "WanAnimate character replacement masking complexity",
      "solution": "Use same mask for both background removal and generation area. Alternatively, add character in second pass instead of replacing to use smaller mask",
      "from": "Kijai"
    },
    {
      "problem": "Massive RAM usage with Wan 2.2 after ComfyUI update",
      "solution": "Uncheck merge/low mem load options. Issue affects systems with 96GB RAM that previously worked",
      "from": "Hoernchen"
    },
    {
      "problem": "CFG > 1.0 error with mixed models",
      "solution": "Error occurs when mixing 4 models (WanMove, UniAnimate, InfiniteTalk). Likely due to negative conditioning override from one of the models",
      "from": "Kijai"
    },
    {
      "problem": "SCAIL blackouts with animal pose",
      "solution": "Use VitPose output directly with higher stick width (20), bypass conversion nodes. Draw thicker lines resembling NLF format",
      "from": "Kijai"
    },
    {
      "problem": "Multiple WanVideoWrapper versions causing issues",
      "solution": "Check custom_nodes folder for different forks/branches like separate -bindweave folder that can cause conflicts",
      "from": "Kijai"
    },
    {
      "problem": "LongCat Avatar T2V giving NoneType error for audio_emb_slice",
      "solution": "Need to still use extension node and provide audio somehow, can use old multitalk embeds input or set overlap to 0",
      "from": "Kijai"
    },
    {
      "problem": "First frame flash/noise issues with reference models",
      "solution": "Decreasing mask value on first frame makes it worse, decreasing first_strength leads to stronger contrast shift - issue may be related to encode/decode differences",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "problem": "ComfyUI animated previews broken after latest update",
      "solution": "Add start flag '--preview-method auto' without quotes",
      "from": "Jumper"
    },
    {
      "problem": "Error 'not enough values to unpack (expected 5, got 4)' after ComfyUI update",
      "solution": "Update VHS (Video Helper Suite) nodes",
      "from": "Kijai"
    },
    {
      "problem": "TAESD preview errors",
      "solution": "Disable TAESD previews, use latent2rgb instead. TAESD live previews never existed for video anyway",
      "from": "Kijai"
    },
    {
      "problem": "SCAIL BF16 model producing black screen",
      "solution": "Try disabling prefetch_blocks and/or non-blocking in block swap node, or redownload model file",
      "from": "Kijai"
    },
    {
      "problem": "LongCat Avatar not working without blockswap on 5090",
      "solution": "Model requires blockswap even at low resolutions like 480x480 due to 31GB model size",
      "from": "burgstall"
    },
    {
      "problem": "Mask list creating frames squared instead of single frame masks",
      "solution": "Use 'mask list to batch' node to convert properly",
      "from": "Kijai"
    },
    {
      "problem": "Front end UI freezes in large workflows",
      "solution": "Try using show any node from Comfy-easy-use instead of show text node",
      "from": "David Snow"
    },
    {
      "problem": "LongCat-Avatar not loading audio_proj model",
      "solution": "Using wrong model - need to use avatar branch model, not normal model",
      "from": "Kijai"
    },
    {
      "problem": "LongCat-Avatar doesn't work in fp16",
      "solution": "Use bf16 instead - original also uses bf16, fp16 causes issues",
      "from": "Kijai"
    },
    {
      "problem": "Dark output with speed-up LoRAs",
      "solution": "Try different sageattn modes - overflow issues cause darkening",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "Triton compilation error",
      "solution": "Clear triton and torch inductor cache at %temp%\\torchinductor_%username% and C:\\Users\\%username%\\.triton\\cache",
      "from": "Ablejones"
    },
    {
      "problem": "White flash at start of stand-in videos",
      "solution": "Crop closer to the head, too much white background causes issues",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI crashes with 'Allocation on device' error",
      "solution": "Not enough VRAM for the workflow, reduce blockswap or use lower settings",
      "from": "Kijai"
    },
    {
      "problem": "VACE RuntimeError: not enough memory allocating 724775731200 bytes",
      "solution": "Issue with memory allocation when using 9 images, needs investigation of image dimensions and processing",
      "from": "NebSH"
    },
    {
      "problem": "LongCat Avatar scaled model gives noise with wrong quantization setting",
      "solution": "LongCat-Avatar-single_fp8_e4m3fn_scaled_mixed_KJ.safetensors works with fp8_e4m3fn_scaled quantization (base_precision: bf16), not regular fp8_e4m3fn",
      "from": "Wildminder"
    },
    {
      "problem": "LongCat Avatar only shows first reference frame, rest black",
      "solution": "Need to use bf16 base precision with LongCat models, ensure SageAttention 2.2.0 (not 1.0.6), and don't use refinement lora",
      "from": "Kijai"
    },
    {
      "problem": "Multiple WanVideoWrapper versions causing node import issues",
      "solution": "Make sure there's no other wrapper nodes in custom_nodes folder, use latest version",
      "from": "Kijai"
    },
    {
      "problem": "LongCat Avatar breakdown after third pass in long generations",
      "solution": "Looks like what happens when you don't give the same initial ref_latent to all extensions",
      "from": "Kijai"
    },
    {
      "problem": "SCAIL causing hand distortions and character inconsistency",
      "solution": "Set pose control strength to 1.0 instead of 0.5, use reference images with similar proportions and positioning",
      "from": "Kijai"
    },
    {
      "problem": "Character head appearing gigantic and disproportionate in SCAIL",
      "solution": "Ensure reference image and video have same resolution/aspect ratio, pad reference image if too tightly cropped",
      "from": "Kijai"
    },
    {
      "problem": "vitpose-h-wholebody.onnx not appearing in ONNX Detection Model Loader",
      "solution": "Move the file from models/onnx to models/detection folder",
      "from": "JohnDopamine"
    },
    {
      "problem": "OOM on VAE decode for long videos",
      "solution": "Use VAE Decode Batched from VHS nodes or reduce resolution from 1080p to 1280x720",
      "from": "boorayjenkins"
    },
    {
      "problem": "ComfyUI 0.5.1 and 0.6.0 breaking WanVideo",
      "solution": "Downgrade to ComfyUI 0.5.0 or update to latest WanVideoWrapper which fixes the bug",
      "from": "hicho"
    },
    {
      "problem": "WanVideoWrapper getting stuck at sampling with cfg zero star",
      "solution": "Update to latest version of WanVideoWrapper which fixes this bug",
      "from": "Kijai"
    },
    {
      "problem": "RuntimeError about tensors on different devices with context windows",
      "solution": "Fixed in latest WanVideoWrapper update",
      "from": "Kijai"
    },
    {
      "problem": "Triton cache causing VRAM issues after updates",
      "solution": "Clear Triton cache folders: C:\\Users\\<username>\\.triton and C:\\Users\\<username>\\AppData\\Local\\Temp\\torchinductor_<username>",
      "from": "Kijai"
    },
    {
      "problem": "Text encoder taking forever to load",
      "solution": "Use cached text encoder node to eliminate RAM/VRAM impact from text encoder",
      "from": "Kijai"
    },
    {
      "problem": "'WanVideoSampler' object has no attribute 'noise_front_pad_num' error",
      "solution": "Issue was with WanVideo Enhance-A-Video node breaking whole code when enabled. Fixed by Kijai.",
      "from": "avataraim/Kijai"
    },
    {
      "problem": "Sampling previews gone after ComfyUI update",
      "solution": "Check main comfy menu settings (execution -> live preview method -> taesd), or add --preview-method taesd to cmd args. Refresh browser after changing.",
      "from": "Gleb Tretyak/Kijai"
    },
    {
      "problem": "Memory leaks with QwenVL implementation",
      "solution": "Issues noted with existing qwenvl causing performance problems",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "Docker dependency infinite loop issue",
      "solution": "Disable automatic upgrade on ComfyUI startup to prevent conflicts between downgrade/upgrade cycles",
      "from": "Gill Bastar"
    },
    {
      "problem": "CUDA error with sageattention wheel",
      "solution": "Error 'no kernel image available' likely due to torch version mismatch - wheel compiled with torch 2.7 but user has torch 2.9",
      "from": "scf"
    },
    {
      "problem": "WanVideoWrapper doing wrong number of steps after update",
      "solution": "Rolled back to previous version, adjusted sigma to step node from 0.875 to 0.980 to get proper 50% step split",
      "from": "Kenk"
    },
    {
      "problem": "S2V not picking up input image after update",
      "solution": "Issue noted but no specific solution provided",
      "from": "Kenk"
    },
    {
      "problem": "Getting blurry outputs with latest commit",
      "solution": "Check step splitting - was caused by incorrect step allocation between high/low noise samplers",
      "from": "Kenk"
    },
    {
      "problem": "SVI Pro color changes between clips",
      "solution": "Could be due to using fp8, sageattn, or different seeds - try weaker samplers like euler or LCM",
      "from": "Kijai"
    },
    {
      "problem": "Apple Silicon compatibility issues with Wan",
      "solution": "Change Lanczos to CPU, set force offload to false, use WanVideo Sampler instead of V2 node",
      "from": "buggz"
    },
    {
      "problem": "Second sampler in SVI Pro eating RAM and becoming slow",
      "solution": "Restart ComfyUI or do a short warmup generation first",
      "from": "Zabo"
    },
    {
      "problem": "SVI Pro not moving, staying at reference image",
      "solution": "Need overlap frames of 8 for proper functionality",
      "from": "DawnII"
    },
    {
      "problem": "Load latent node looking in wrong folder",
      "solution": "Core saves to output folder but loads from input folder - need to move files manually",
      "from": "Kijai"
    },
    {
      "problem": "Bright/gray output with SVI Pro native workflow",
      "solution": "Issue was with LightX2V lora causing problems, removing it fixed the issue",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "problem": "Light flashing between generations with SageAttention",
      "solution": "Switching back to SDPA fixed the flashing issue",
      "from": "DawnII"
    },
    {
      "problem": "Gray pictures in wrapper vs native",
      "solution": "Use Kijai's converted SVI loras instead of original repo loras, or set LightX2V 1030 strength to 0.6",
      "from": "ucren"
    },
    {
      "problem": "Preview not showing up in SVI Pro",
      "solution": "Enable 'Show Progress Images During Generation' in ComfyUI main settings menu and refresh page before sampling",
      "from": "Kijai"
    },
    {
      "problem": "WanImageToVideoSVIPro node not found",
      "solution": "Update ComfyUI manager first, then install KJNodes again",
      "from": "MOV"
    },
    {
      "problem": "LightX2V 1030 creates hard cuts instead of morphing in FMML",
      "solution": "Use different LightX lora - older versions work better for morphing workflows",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "LoRA key not loaded errors when using SVI V2 Pro",
      "solution": "Use the converted LoRAs from Kijai's HuggingFace repo",
      "from": "Kijai"
    },
    {
      "problem": "VAE loading error with Wan2_1_VAE_bf16.safetensors",
      "solution": "Re-download the VAE file",
      "from": "Kijai"
    },
    {
      "problem": "Grey outputs in SVI wrapper",
      "solution": "Update the wrapper nodes",
      "from": "David Snow"
    },
    {
      "problem": "Running out of 96GB system RAM when interpolating multiple scenes",
      "solution": "Combine all batches first, then interpolate, rather than interpolating each batch separately",
      "from": "Persoon"
    },
    {
      "problem": "Loop nodes require ComfyUI cache to function properly",
      "solution": "Do not disable ComfyUI cache when using loop nodes",
      "from": "pagan"
    },
    {
      "problem": "Image concatenate node fails on first loop iteration",
      "solution": "Patch the node with: if source_images is None and new_images is not None: return new_images, new_images, new_images",
      "from": "pagan"
    },
    {
      "problem": "Missing WanImageToSVIPro node after updating",
      "solution": "Update KJNodes pack and ComfyUI, delete and re-clone KJNodes if needed",
      "from": "MOV"
    },
    {
      "problem": "Numpy 2.2.6 breaks multiple nodes",
      "solution": "Downgrade numpy version as older nodes won't work on numpy 2 versions",
      "from": "Kijai"
    },
    {
      "problem": "LoRA tensor size mismatch errors",
      "solution": "Kijai fixed the issue with dora scale needing to be reshaped to 2 dim tensor - re-download required",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI preview stops working after restart",
      "solution": "Use command arg --preview-method auto, latent2rgb, or taesd",
      "from": "LukeG89"
    },
    {
      "problem": "SVI Pro jumps back to original frame with camera movement",
      "solution": "Works fine with static camera, use last frame of previous generation as anchor frame for camera movement",
      "from": "Cseti"
    },
    {
      "problem": "SAM3 not masking watermarks",
      "solution": "Using 'watermark' as prompt doesn't work effectively",
      "from": "Persoon"
    },
    {
      "problem": "First frame appears burned in ultravico",
      "solution": "Trim the first latent off prior to decoding or 4 frames prior to final save",
      "from": "JohnDopamine"
    },
    {
      "problem": "SVI 2.0 Pro causes video contrast drift",
      "solution": "Use color matching techniques, batch image color matching can help",
      "from": "BNP4535353"
    },
    {
      "problem": "Washed up colors with color match node active",
      "solution": "Use augment empty frames at 0.2-0.3 strength",
      "from": "David Snow"
    },
    {
      "problem": "High VRAM usage with VACE nodes",
      "solution": "Use CPU cache for VAE, reduces to just under 24GB",
      "from": "Lumifel"
    },
    {
      "problem": "Tiled VAE degrades quality with VACE",
      "solution": "Use CPU cache for VAE instead of tiled VAE",
      "from": "Lumifel"
    },
    {
      "problem": "SDE samplers too sensitive with DiT models in native",
      "solution": "Use Clown samplers for stable SDE noise",
      "from": "Ablejones"
    },
    {
      "problem": "Loading model checkpoints with weights_only=False creates RCE vulnerability",
      "solution": "Use .safetensors files which are generally considered safe, or ensure weights_only=True (default since PyTorch 2.6)",
      "from": "tazztone"
    },
    {
      "problem": "FreeLong workflows don't combine segments correctly",
      "solution": "Fixed workflow to properly handle 1 frame overlap between segments",
      "from": "Ablejones"
    },
    {
      "problem": "SCAIL background shifts around 81 frame boundaries",
      "solution": "Use reference image with homogeneous background like green screen, or use Uni3C camera control with static image to lock camera",
      "from": "42hub"
    }
  ],
  "comparisons": [
    {
      "comparison": "Seed vs WAN vs Flash for upscaling",
      "verdict": "Seed is nice but has flickering issues, WAN tends to change stuff (playing with Noise can help), FlashVSR was not really good",
      "from": "Gill Bastar"
    },
    {
      "comparison": "SAM3 vs SeC vs SAM2 for masking",
      "verdict": "SAM3 > SeC > SAM2 in experience, SAM3 has text prompt to segment feature",
      "from": "BitJuggler"
    },
    {
      "comparison": "SteadyDancer vs WanAnimate",
      "verdict": "Didn't see anything better than WanAnimate, lack of face control and long gen makes it complex to use",
      "from": "Kijai"
    },
    {
      "comparison": "Classic/tiled animatediff upscale vs newer solutions",
      "verdict": "Feel like classic/tiled animatediff upscale pass is still probably the best for larger resolutions and higher framecounts",
      "from": "Disco_Tek"
    },
    {
      "comparison": "UltraViCo vs normal SageAttn",
      "verdict": "UltraViCo runs twice as slow as normal sageattn on same frame count, but provides quality benefits for longer videos",
      "from": "Kijai"
    },
    {
      "comparison": "Kling O1 vs WanAnimate",
      "verdict": "O1 matches input more closely especially in head area, but WanAnimate has better lighting and feels more natural. O1 is very cut and paste with awful lighting/color blend",
      "from": "A.I.Warper"
    },
    {
      "comparison": "Open source vs commercial models",
      "verdict": "It's insane that we can even compare big commercial models to local free models",
      "from": "dj47"
    },
    {
      "comparison": "LongCat vs other extension methods",
      "verdict": "LongCat is the best one for video extension, but otherwise extension methods are not great and degrade over time",
      "from": "Kijai"
    },
    {
      "comparison": "4 steps res_2s vs 8 steps euler",
      "verdict": "8 steps euler likely still beats 4 steps res_2s despite same NFE count",
      "from": "spacepxl"
    },
    {
      "comparison": "Higher order samplers vs euler for low step sampling",
      "verdict": "If heun doesn't improve significantly over euler, higher order samplers don't matter",
      "from": "spacepxl"
    },
    {
      "comparison": "Q8 vs Q6 GGUF",
      "verdict": "Differences are marginal, only slight noticeable errors in Q6",
      "from": "David Snow"
    },
    {
      "comparison": "Adversarial distillation vs trajectory-focused distillation",
      "verdict": "Adversarial methods are timestep-sensitive and fragile, trajectory-focused methods like TCD are more robust",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "comparison": "Steadydancer vs WanAnimate for dancing",
      "verdict": "Stepvideo is exceptional at dancing, Steadydancer not the smoothest but has good architecture",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "comparison": "SVI v1 vs v2",
      "verdict": "V2 solves static object/camera issues that v1 had, but v2 feels washed out",
      "from": "Hashu"
    },
    {
      "comparison": "Training large LoRA vs training-free methods",
      "verdict": "Still prefer taking largest affordable model and training big LoRA with lots of examples for character/subject and movement",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "comparison": "Aquif-Image vs Z-image",
      "verdict": "Aquif has natural look vs Z-Image's over RLd artificial look",
      "from": "yi"
    },
    {
      "comparison": "Using lightx2v vs without",
      "verdict": "With lightx2v there's more room for error, without everything is more sensitive to wrong params",
      "from": "Kijai"
    },
    {
      "comparison": "Radau vs Gauss solvers",
      "verdict": "Radau more stable, Gauss higher order. Gauss wins until ODE instability gets bad, then Radau is better",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "comparison": "FILM vs RIFE interpolation",
      "verdict": "FILM better for fast movement, RIFE better speed/quality tradeoff",
      "from": "lostintranslation"
    },
    {
      "comparison": "Mitchell vs Lanczos resampling",
      "verdict": "Mitchell nicer to the eye, Lanczos still competitive and close to optimal",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "comparison": "SVI v2.0/SVI_Wan2.1 vs v2.0/SVI_Wan2.2",
      "verdict": "v2.0/SVI_Wan2.1 permits 5 overlap frames while v2.0/SVI_Wan2.2 only allows up to 3 overlap frames",
      "from": "42hub"
    },
    {
      "comparison": "Video DiTs vs Image DiTs parameter requirements",
      "verdict": "Video DiTs need around 10x the tokens of img DiTs. 6B for video gonna be a bit squiffy, while img DiTs get at least somewhat okay at 2B",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "comparison": "OneToAll Animation vs WanAnimate",
      "verdict": "OneToAll better at pose retargeting and reference keeping, but WanAnimate better at adherence. OneToAll potentially much better for long clips as WanAnimate degrades over time",
      "from": "Kijai"
    },
    {
      "comparison": "HunyuanVideo 1.5 vs Wan 2.2",
      "verdict": "Wan 2.2 much better - HunyuanVideo 1.5 feels dumb and has uncontrolled camera movement",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "SteadyDancer vs WanAnimate",
      "verdict": "SteadyDancer better at some things but doesn't handle longer videos well",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "comparison": "DepthAnything V3 vs V2",
      "verdict": "V2 better quality - V3 looks like garbage for this use case",
      "from": "David Snow"
    },
    {
      "comparison": "OneToAll vs WanAnimate",
      "verdict": "Still prefers wananimate, but hasn't spent much time optimizing OneToAll",
      "from": "Josiah"
    },
    {
      "comparison": "RCM vs regular on 2.2 I2v",
      "verdict": "RCM has less artifacting and contrast variance, movement is less but not much movement was prompted",
      "from": "DawnII"
    },
    {
      "comparison": "Context windows vs extension method",
      "verdict": "Context windows don't degrade (plus), but work worse overall (negative). Extension method is only option for really long sequences",
      "from": "Kijai"
    },
    {
      "comparison": "WanMove vs ATI",
      "verdict": "WanMove seems better than ATI with less artifacts and clearer results, reuses most ATI functionality",
      "from": "Kijai"
    },
    {
      "comparison": "rCM LoRA vs Lightning LoRA for motion",
      "verdict": "1030 lightx2v LoRA on high noise gives much better motion than lightning LoRA",
      "from": "Zabo"
    },
    {
      "comparison": "IndexTTS-2 vs VibevoIce for emotional voice control",
      "verdict": "IndexTTS-2 better for emotional voice control, VibevoIce has limited emotional range",
      "from": "Gleb Tretyak"
    },
    {
      "comparison": "TTS Audio Suite TTS-2 vs VibevoIce reliability",
      "verdict": "Switched from VibevoIce to TTS-2 due to VibevoIce problems, much happier with TTS-2",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "comparison": "OneToAll vs WanAnimate",
      "verdict": "O2A has great movement coherence but doesn't adhere to input/likeness as well as WanAnimate",
      "from": "Josiah"
    },
    {
      "comparison": "Wan 2.2 fp16 vs fp8_scaled vs GGUF Q8",
      "verdict": "FP16 works fine if you have enough RAM, but gain isn't huge vs fp8_scaled or GGUF Q8",
      "from": "Kijai"
    },
    {
      "comparison": "Sage attention vs SDPA",
      "verdict": "Sage uses more VRAM than SDPA by design, though SDPA can have issues on some setups",
      "from": "Kijai"
    },
    {
      "comparison": "1030 lora vs Lightning 4-step vs 480p lora",
      "verdict": "1030 good at prompt following but basic motion. 480p more active but erratic. Lightning 4-step not recommended",
      "from": "metaphysician"
    },
    {
      "comparison": "rCM vs lightx2v",
      "verdict": "Couldn't get same results from rCM as 2.1 lightx2v in testing",
      "from": "Kijai"
    },
    {
      "comparison": "SVI vs regular continuation for consistency",
      "verdict": "SVI great for consistency, awful for dynamic flow. Works by building image embedding differently",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "comparison": "Using quantization vs scaled fp8 or GGUF",
      "verdict": "Quantization is slower to load and e5m2 is lowest quality, scaled fp8 or GGUF preferred",
      "from": "Kijai"
    },
    {
      "comparison": "SCAIL vs WanAnimate",
      "verdict": "SCAIL feels better than WanAnimate for motion capture",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "Wrapper vs Native implementation",
      "verdict": "Wrapper is much nicer than native and perhaps in wider use. Wrapper remains faster to embrace new models/experiments",
      "from": "Scruffy"
    },
    {
      "comparison": "Diffusers vs ComfyUI scheduler handling",
      "verdict": "In diffusers beta sigmas are applied after shift, while in comfy it's other way around, leading to completely different sigmas when using beta",
      "from": "Kijai"
    },
    {
      "comparison": "FlowFrames vs ComfyUI RIFE node",
      "verdict": "FlowFrames is much better - it's lightweight, has scene change detection and is fast",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "comparison": "Kandinsky vs Wan 2.2",
      "verdict": "Kandinsky better at human interactions, 24fps more natural motion, but slower. Wan has better ecosystem",
      "from": "Benjimon"
    },
    {
      "comparison": "STAR vs SeedVR2 for video upscaling",
      "verdict": "SeedVR2 is better quality, STAR is old and nobody talks about it anymore",
      "from": "Gill Bastar"
    },
    {
      "comparison": "InspyreNet vs other background removal",
      "verdict": "InspyreNet is probably my favorite for background removal",
      "from": "cyncratic"
    },
    {
      "comparison": "SCAIL vs Wan Animate",
      "verdict": "SCAIL is better for plain pose control, Wan Animate is more for replacement and lipsync",
      "from": "Kijai"
    },
    {
      "comparison": "SCAIL vs Wan Animate for long duration",
      "verdict": "SCAIL performs better than Wan Animate for long-duration generation",
      "from": "Karthik"
    },
    {
      "comparison": "SCAIL vs WanAnimate",
      "verdict": "SCAIL better for ID retention and works better when image isn't same camera angle/size. WanAnimate better for facial performance but nukes ID too much",
      "from": "A.I.Warper"
    },
    {
      "comparison": "VACE vs WanAnimate for VFX work",
      "verdict": "VACE is 10x more useful than wanimate for real VFX work",
      "from": "spacepxl"
    },
    {
      "comparison": "Lightning LoRAs vs base model quality",
      "verdict": "Lightning gives less motion/structure variety but better detail quality. Lightning with 8 steps gives much better detail quality than base model with ~30 steps",
      "from": "spacepxl"
    },
    {
      "comparison": "Wan 2.6 vs Veo 2",
      "verdict": "Veo 2 still looks more real than Wan 2.6",
      "from": "Ruairi Robinson"
    },
    {
      "comparison": "Wan 2.6 quality assessment",
      "verdict": "Quality looks ok at start then degrades, looks overlit, oversaturated, over contrasty instead of natural and photographic",
      "from": "Ruairi Robinson"
    },
    {
      "comparison": "Wan 2.6 vs 2.5",
      "verdict": "2.6 outputs are genuinely worse than 2.5, lacks visual fidelity despite being newer",
      "from": "DawnII"
    },
    {
      "comparison": "Wan 2.6 vs LTX",
      "verdict": "Looks like LTX but worse than wan 2.1, has same plastic coating look",
      "from": "spacepxl"
    },
    {
      "comparison": "Wan 2.6 sound vs Kling/Sora",
      "verdict": "Sound is shit compared to Kling 2.6 or Sora, uses mmaudio",
      "from": "asd"
    },
    {
      "comparison": "SCAIL vs other models",
      "verdict": "SCAIL is the best for now, especially for complex motions",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "CFG settings impact on speed",
      "verdict": "50 steps with cfg = 100 model passes vs 4 steps with cfg 1.0 = 4 model passes",
      "from": "Kijai"
    },
    {
      "comparison": "TurboDiffusion vs LightX2V",
      "verdict": "Weights didn't seem better than LightX2V, misleading 'lossless' claims with visible quality loss",
      "from": "Kijai"
    },
    {
      "comparison": "SCAIL vs WanAnimate for character retargeting",
      "verdict": "SCAIL better for very different character sizes (chibi vs human), WanAnimate better for backgrounds without shifts",
      "from": "Juan Gea"
    },
    {
      "comparison": "Wan 2.6 cost vs other services",
      "verdict": "Wan 2.6 costs $6.70 for 15s video on FAL, extremely expensive",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "DF11 vs fp16 models",
      "verdict": "100% identical quality to fp16 but 30% smaller file size, 5-20% slower than fp8 but faster than bf16",
      "from": "Ada"
    },
    {
      "comparison": "SCAIL vs WanAnimate for character swapping",
      "verdict": "WanAnimate holds up much better for long runs, SCAIL has issues with close-up face detection",
      "from": "xwsswww"
    },
    {
      "comparison": "Wrapper vs native nodes for Wan 2.1",
      "verdict": "Wrapper generates significantly better videos, default LoRAs (480p cfg distill) work better than 1030 high + 1022 low",
      "from": "Danial"
    },
    {
      "comparison": "SCAIL vs WanAnimate for reconfiguration",
      "verdict": "SCAIL does much better with re-configuring body proportions, especially for non-human and exaggerated features. WanAnimate claims to reconfigure but doesn't actually do it well",
      "from": "amli"
    },
    {
      "comparison": "SCAIL vs WanAnimate for faces",
      "verdict": "SCAIL cannot do faces very well at all, WanAnimate is better for faces",
      "from": "AIGambino"
    },
    {
      "comparison": "SCAIL vs WanAnimate for video processing",
      "verdict": "SCAIL animates image as whole, WanAnimate still king for swapping someone in existing video. WanAnimate better for long gen stability",
      "from": "HeadOfOliver"
    },
    {
      "comparison": "GIMM VFI vs RIFE for frame interpolation",
      "verdict": "GIMM seems much better than RIFE",
      "from": "dj47"
    },
    {
      "comparison": "fp16 vs bf16 precision",
      "verdict": "fp16 has significantly more precision than bf16. bf16 gives roughly 8 bit precision in half the image range (0.5-1.0)",
      "from": "spacepxl"
    },
    {
      "comparison": "1.3B vs 5B models",
      "verdict": "1.3B is better than 5B",
      "from": "spacepxl"
    },
    {
      "comparison": "New LightX2V 1217 vs Lightning vs Dyno",
      "verdict": "Old lightning and dyno still better - new 1217 has weird colors, too much contrast, less dynamic movement",
      "from": "crinklypaper"
    },
    {
      "comparison": "InfiniteTalk Multi vs MultiTalk",
      "verdict": "MultiTalk slightly better at single speaker tasks, both can do single speaker but optimized for their specific use cases",
      "from": "Kijai"
    },
    {
      "comparison": "Adding character vs replacing character in WanAnimate",
      "verdict": "Adding character in second pass works better - smaller mask, cleaner results, faster processing",
      "from": "boorayjenkins"
    },
    {
      "comparison": "WorldCanvas vs WanMove",
      "verdict": "WorldCanvas more feature rich, uses 2.2 weights with VACE-like features vs WanMove's plain 2.1 I2V",
      "from": "Kijai"
    },
    {
      "comparison": "LongCat Avatar vs Infinite Talk",
      "verdict": "LongCat is slower (10 steps vs 4-6 steps) but runs at 16fps vs 25fps, so actually faster in that sense. Main advantage is prevention of repetitive actions in long gens",
      "from": "Kijai"
    },
    {
      "comparison": "FlashPortrait vs FantasyPortrait for short generations",
      "verdict": "FlashPortrait is worse than FantasyPortrait for single short generations, seems made for long generation method only",
      "from": "Kijai"
    },
    {
      "comparison": "SVI prompt following and movement",
      "verdict": "SVI is very bad at following prompt and movements, difficult to fix",
      "from": "Zabo"
    },
    {
      "comparison": "10 steps with distill LoRA vs 40 steps with cfg 3.0",
      "verdict": "Both produce similar quality results for LongCat Avatar",
      "from": "Kijai"
    },
    {
      "comparison": "LongVie2 with vs without self_attn layers on Wan 2.1",
      "verdict": "Both work for depth control, unclear how much the additional layers improve results",
      "from": "Kijai"
    },
    {
      "comparison": "Uni3C compatibility with LongCat Avatar",
      "verdict": "Not compatible at all - LongCat is based on Wan architecture but trained from scratch",
      "from": "Kijai"
    },
    {
      "comparison": "HUMO vs STAND-IN facial consistency",
      "verdict": "HUMO does facial consistency very well, but below ~65 frames starts losing consistency and becomes grainy. STAND-IN works fine even with very few frames (as low as 9) and is close to HUMO in reference preservation, though not quite at the same level",
      "from": "\u25b2"
    },
    {
      "comparison": "LongCat Avatar vs InfiniteTalk",
      "verdict": "For long gen it's better simply because it doesn't repeat itself like InfiniteTalk does",
      "from": "Kijai"
    },
    {
      "comparison": "WanMove vs ATI",
      "verdict": "It's like ATI but a lot better",
      "from": "Kijai"
    },
    {
      "comparison": "Sage vs Flash vs SDPA attention",
      "verdict": "sage >>>>>>>>>>>>>>> flash > sdpa with slight exaggeration",
      "from": "Kijai"
    },
    {
      "comparison": "q4_km GGUF vs fp8 scaled Wan models",
      "verdict": "q4km uses less VRAM and allows more frames for upscaling",
      "from": "craftogrammer"
    },
    {
      "comparison": "vitpose-h-wholebody vs vitpose-l-wholebody",
      "verdict": "H version provides better pose detection results",
      "from": "ucren"
    },
    {
      "comparison": "4090 vs dual 3090 performance",
      "verdict": "Single 4090 is twice as fast for Wan, 5090 is maybe 3x faster. Dual 3090 better for LLM due to more VRAM",
      "from": "Benjimon"
    },
    {
      "comparison": "SageAttention 2.1/2.2 vs SageAttention 3",
      "verdict": "Sage 2.1/2.2 are still mainstream options, Sage 3 has way too high quality loss on 2.1 to be useful",
      "from": "Kijai"
    },
    {
      "comparison": "SageAttention vs TeaCache",
      "verdict": "SageAttention is almost lossless compared to TeaCache which caused noticeable detail loss and limb deformities",
      "from": "slmonker"
    },
    {
      "comparison": "Foley vs MMAudio for audio generation",
      "verdict": "MMAudio preferred, foley sucks",
      "from": "Benjimon"
    },
    {
      "comparison": "fp4 vs fp8 quality",
      "verdict": "fp4 has noticeable quality drop compared to fp8",
      "from": "cyncratic"
    },
    {
      "comparison": "Kandinsky vs Wan",
      "verdict": "Better at some things but doesn't listen as well and is slower, worth using for some specific cases",
      "from": "Benjimon"
    },
    {
      "comparison": "Lightning LoRAs vs newer distill LoRAs",
      "verdict": "Avoid Lightning-based LoRAs especially for T2V due to big impact on generation content, use newer distill LoRAs from 11 days ago",
      "from": "Kijai"
    },
    {
      "comparison": "SVI vs SVI Pro motion adherence",
      "verdict": "Regular SVI had more change/variety compared to SVI Pro which is more restrictive",
      "from": "NebSH"
    },
    {
      "comparison": "LightX2V 1030 vs 1022",
      "verdict": "1022 works better with SVI Pro, 1030 had issues with gray output",
      "from": "DawnII"
    },
    {
      "comparison": "High noise with vs without LightX2V",
      "verdict": "Without LightX2V produces much better motion, more natural pacing",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "SVI vs StorymMem for camera movement",
      "verdict": "StorymMem better for prompt adherence, SVI has better continuity but fights camera prompts",
      "from": "Kijai"
    },
    {
      "comparison": "Different alpha values for ultravico",
      "verdict": "0.95 alpha doesn't allow subject to leave frame, first frame alpha 0.95 with rest 0.8 shows promise",
      "from": "Kijai"
    },
    {
      "comparison": "SVI Pro vs manual I2V continuation",
      "verdict": "SVI Pro is amazing and better than manual last image continuation method",
      "from": "avataraim"
    },
    {
      "comparison": "Motion scale node vs Painter",
      "verdict": "Motion scale node works better than painter and doesn't make camera go wild",
      "from": "Zabo"
    },
    {
      "comparison": "Ultravico with I2V vs T2V",
      "verdict": "Harder to use with I2V because it decays the image conditioning, works better with T2V",
      "from": "Kijai"
    },
    {
      "comparison": "Original 2.1 VACE vs Fun VACE 2.2",
      "verdict": "Original 2.1 better for reference images and inpainting, Fun 2.2 better for depth and pose controls",
      "from": "Ablejones"
    },
    {
      "comparison": "SVI vs HuMo for lipsync",
      "verdict": "SVI doesn't do lipsync, HuMo still the goto for lipsync",
      "from": "NebSH"
    },
    {
      "comparison": "Storymem vs SVI Pro 2.0",
      "verdict": "Storymem introduces huge color drift, SVI Pro makes prompts almost fully ignored",
      "from": "Gleb Tretyak"
    },
    {
      "comparison": "SCAIL vs WanAnimate",
      "verdict": "SCAIL better at complex movements like spinning and multiple characters; WanAnimate better at facial expressions and has two mechanisms for >81 frames vs SCAIL's one",
      "from": "42hub"
    },
    {
      "comparison": "FreeLong vs raw I2V vs SVI 2.0 Pro",
      "verdict": "Raw I2V expected to do similar job, SVI 2.0 Pro expected to be even better than FreeLong",
      "from": "42hub"
    },
    {
      "comparison": "FreeLong vs non-FreeLong generation",
      "verdict": "FreeLong doesn't improve continuity, right side looked nicer but took twice as long with more complex background",
      "from": "Ablejones"
    }
  ],
  "tips": [
    {
      "tip": "For maximum speed with compile working, use 'comfy' rope function and enable allow_unmerged_lora_compile if you don't merge loras",
      "context": "When torch compile is working properly",
      "from": "Kijai"
    },
    {
      "tip": "For VACE inpainting with masks, use full denoise to get VACE to recreate original unmasked part",
      "context": "When doing inpainting operations",
      "from": "Ablejones"
    },
    {
      "tip": "Can simulate denoise by using non-binary mask to set how much you want area to change, keeping denoise at 1",
      "context": "Alternative to lowering denoise for masked areas",
      "from": "Ablejones"
    },
    {
      "tip": "Do control only for few steps by ending it early",
      "context": "For getting creative variations with VACE pose/depth control",
      "from": "Kijai"
    },
    {
      "tip": "Vace works with lower denoise for video-to-video, but for inpainting must keep denoise at 1",
      "context": "Different denoise strategies for different use cases",
      "from": "hicho"
    },
    {
      "tip": "For VACE 17 frames to match OpenPose properly, feed it to ControlVideo",
      "context": "When working with VACE workflows",
      "from": "NodeMancer"
    },
    {
      "tip": "Use blank background/huge homogenous level floor + subsequent background outpainting with VACE for SteadyDancer",
      "context": "Background may be problematic with SteadyDancer, works better with context windows for long videos",
      "from": "42hub"
    },
    {
      "tip": "Make bird subject fill the whole reference frame and increase mask size",
      "context": "When inserting small subjects like birds into scenes with Wan 2.2 Animate",
      "from": "\ud83e\udd99rishappi"
    },
    {
      "tip": "Use pose closer to first frame if you don't want transition effects",
      "context": "When using start image with pose control in Wan",
      "from": "Kijai"
    },
    {
      "tip": "For CFG schedules, avoid CFG schedule float list with certain samplers",
      "context": "When setting up NAG workflows",
      "from": "David Snow"
    },
    {
      "tip": "Use eta of 0.5 or greater for SDE sampling",
      "context": "Works fine even with low steps if implementation is correct",
      "from": "Ablejones"
    },
    {
      "tip": "Test heun sampler to check if higher order sampling helps",
      "context": "Quick test to see if 2NFE per step improves quality enough to be worth it",
      "from": "spacepxl"
    },
    {
      "tip": "Use gradient estimation sampler for 1NFE with momentum",
      "context": "Sometimes gives good results but depends on the model, works well with restart sampler",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "Upscale whole video then downscale and blur to match",
      "context": "Gets better details when working with limited resolution source",
      "from": "spacepxl"
    },
    {
      "tip": "Use film grain injection before upscaling for more detail",
      "context": "When doing creative upscaling to add detail to video generations",
      "from": "David Snow"
    },
    {
      "tip": "Use last 5 frames of first video as start_image for next video with SVI-Film lora",
      "context": "For 15 second I2V generations when SVI isn't working properly",
      "from": "Ablejones"
    },
    {
      "tip": "Motion frame setting of 5 works for new SVI 2.0, higher values create jumpy overlaps",
      "context": "Using SVI 2.0 lora for video extension",
      "from": "xiver2114"
    },
    {
      "tip": "Image upscale is better than latent upscale",
      "context": "Multi-pass video generation workflow",
      "from": "David Snow"
    },
    {
      "tip": "Always look at hair quality to judge video generation quality",
      "context": "Evaluating AI video generation results",
      "from": "FL13"
    },
    {
      "tip": "Use explicit caching by saving/loading images instead of relying on ComfyUI cache",
      "context": "When working with image batches and masks to avoid recomputation",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "Switch to ComfyUI API to control with python and dodge half the bugs",
      "context": "When dealing with memory management issues",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "Use Claude for code review to check for malicious code",
      "context": "When installing unknown ComfyUI nodes",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use minimal workflow to diagnose memory issues",
      "context": "When troubleshooting memory leaks or other ComfyUI issues",
      "from": "Kijai"
    },
    {
      "tip": "Test with --disable-all-custom-nodes to isolate problems",
      "context": "When experiencing crashes or memory issues that might be caused by custom nodes",
      "from": "Kijai"
    },
    {
      "tip": "Always update torch, torchvision, and torchaudio together",
      "context": "When updating PyTorch to avoid version sync issues",
      "from": "spacepxl"
    },
    {
      "tip": "Treat SVI like svi-film for overlap frames",
      "context": "Frames should be unmasked, wasn't trained the same way as SVI 2.0 for wan2.1",
      "from": "Ablejones"
    },
    {
      "tip": "Use high noise LoRA at 0.5 to get more motion/prompt following",
      "context": "When working with SVI workflows",
      "from": "Hashu"
    },
    {
      "tip": "Check KJ's example workflow for cleaner SVI implementation",
      "context": "Has nice new-ish node that collapses complexity",
      "from": "Hashu"
    },
    {
      "tip": "Important to note GGUF behavior when using LoRAs",
      "context": "Models like GGUF don't allow merging, thus lora weights are added on top of the de-quantized weight when used",
      "from": "Kijai"
    },
    {
      "tip": "HuMo does amazing job using reference inputs without too much bias for their input positions",
      "context": "For extension work without using start images",
      "from": "Ablejones"
    },
    {
      "tip": "Use strength 1.0 with LightX2V LoRAs",
      "context": "Normal usage",
      "from": "DawnII"
    },
    {
      "tip": "Better hardware doesn't improve quality, only speed",
      "context": "Quality is same on all cards, just faster on better hardware due to memory hierarchy",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "OneToAll Animation sensitive to prompts",
      "context": "Prompting for something else can override the reference",
      "from": "Kijai"
    },
    {
      "tip": "Reference adherence suffers from misaligned pose and LightX2V",
      "context": "When using OneToAll Animation",
      "from": "Kijai"
    },
    {
      "tip": "PainterI2V motion amplitude tradeoff",
      "context": "More motion with distill at cost of contrast changes and prompt following. Motion amplitude of 1 might as well not use painteri2v",
      "from": "DawnII"
    },
    {
      "tip": "Use TTM for best results without audio facial expressions",
      "context": "Time to Move is considered best so far, only issue is can't do audio facial expressions and mouth movements",
      "from": "xwsswww"
    },
    {
      "tip": "Normalize latent mean and standard deviation each step",
      "context": "Can help with saturation changes in video generation",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "SeedVR2VideoUpscale removes artifacts",
      "context": "Running init image through SeedVR2VideoUpscale first helps with eyes and finer details",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "tip": "Use 1030 or 1030+1020 on high noise, any wan 2.1 t2v/i2v on low for best I2V results",
      "context": "LightX2V LoRA configuration",
      "from": "FL13"
    },
    {
      "tip": "Higher LoRA rank closer to full model but diminishing returns above rank 64",
      "context": "LoRA rank selection, larger file size matters for unmerged/GGUF",
      "from": "Kijai"
    },
    {
      "tip": "Sometimes rank 4-8 can be 90% effective with much smaller size",
      "context": "LoRA optimization",
      "from": "Kijai"
    },
    {
      "tip": "Use noise injection for wider range of settings tolerance",
      "context": "Better results with SDE that injects noise every step",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "Bump audio cfg to 1.4 or 1.5 for better lip sync in InfiniteTalk",
      "context": "Audio-driven lip sync optimization",
      "from": "Charlie"
    },
    {
      "tip": "Can adjust trajectory paths instead of tweaking variables for better control",
      "context": "WanMove provides more intuitive control method",
      "from": "Kijai"
    },
    {
      "tip": "For SVI motion problems, outpaint input image to 480\u00d7832 horizontal ratio",
      "context": "SVI trained at 480\u00d7832 horizontal, doesn't perform well with very different aspect ratios especially vertical with full-body person",
      "from": "xiver2114"
    },
    {
      "tip": "Use HuMo at end of flow for better lip sync",
      "context": "When combining multiple models, HuMo works better at the end rather than beginning",
      "from": "Scruffy"
    },
    {
      "tip": "Adjusting SVI lora strength may affect camera movement",
      "context": "When trying to get more camera movement with SVI",
      "from": "Kijai"
    },
    {
      "tip": "For SVI 2.2, set 1 frame overlap but provide first 5 frames anyway",
      "context": "Proper setup for SVI 2.2 compared to SVI shot",
      "from": "Ablejones"
    },
    {
      "tip": "Use precise prompts for NAG instead of huge negative spam prompts",
      "context": "NAG works better with targeted negatives rather than long lists",
      "from": "Kijai"
    },
    {
      "tip": "Very low CFG should be used with low steps",
      "context": "When using CFG with few steps",
      "from": "Kijai"
    },
    {
      "tip": "At least one step with CFG makes such a difference",
      "context": "For video generation quality",
      "from": "Kijai"
    },
    {
      "tip": "Lightning lora on low produces sharper results but changes too much",
      "context": "For I2V generation",
      "from": "David Snow"
    },
    {
      "tip": "Use manual venv install for ComfyUI",
      "context": "Easiest in the long run for maintaining stable setup over time",
      "from": "Kijai"
    },
    {
      "tip": "VACE uses T2V sigma boundary of 0.875",
      "context": "Not 0.9 like I2V, important for proper VACE functionality",
      "from": "pookyjuice"
    },
    {
      "tip": "For 5090 users, use 'auto' mode",
      "context": "Optimal performance setting for RTX 5090",
      "from": "Kijai"
    },
    {
      "tip": "SCAIL model works best with 896x512 resolution",
      "context": "Not meant for face-only input, needs more body content",
      "from": "Kijai"
    },
    {
      "tip": "WanAnimate output frames must be divisible by 4 after first frame",
      "context": "Explains why sometimes output has 2-3 fewer frames than input",
      "from": "Kijai"
    },
    {
      "tip": "Use LightX2V LoRA with SCAIL",
      "context": "For SCAIL use the lightx2v 2.1 I2V LoRA for better results",
      "from": "Kijai"
    },
    {
      "tip": "Use Skyreel LoRA with T2V for extended frames",
      "context": "Can use Skyreel LoRA with T2V 2.2 to push to 121 frames, use strength higher than 1.0",
      "from": "NebSH and Kijai"
    },
    {
      "tip": "Use e4m3fn for Blackwell GPUs",
      "context": "Usually e4 for blackwell architecture GPUs if I remember rightly",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "Join multiple reference images manually",
      "context": "For VACE with multiple references, join them into a single image yourself as the model can only use one image",
      "from": "Kijai"
    },
    {
      "tip": "Buy RAM/SSDs quickly",
      "context": "RAM and SSD prices are rising rapidly - buy as quickly as possible to avoid paying double",
      "from": "David Snow"
    },
    {
      "tip": "Use 6 images max for 81 frames",
      "context": "6 images is probably too much for 81 frames, try with 5 only",
      "from": "NebSH"
    },
    {
      "tip": "Use fp16_fast for faster WanMove generation",
      "context": "When using WanMove model",
      "from": "Kijai"
    },
    {
      "tip": "Add stationary points where swing chains attach to prevent unwanted movement",
      "context": "When creating swing motion in WanMove",
      "from": "Kijai"
    },
    {
      "tip": "Use path filters with noise or sine wave for more natural movement",
      "context": "Making camera movements look less mechanical",
      "from": "Kijai"
    },
    {
      "tip": "720p makes significant difference over 480p for face quality",
      "context": "When working with face-focused content",
      "from": "FlipYourBits"
    },
    {
      "tip": "Segment faces without stretching for better results",
      "context": "Face processing workflows",
      "from": "FlipYourBits"
    },
    {
      "tip": "Use 5-frame overlap with SVI-film for context window transitions",
      "context": "When splitting long sequences across context windows",
      "from": "Kijai"
    },
    {
      "tip": "Clamp should be turned off for LoRA extraction",
      "context": "It seems unnecessary and even detrimental for quality",
      "from": "Kijai"
    },
    {
      "tip": "Don't use adaptive setting for LoRA extraction anymore",
      "context": "Standard setting works better than adaptive",
      "from": "Kijai"
    },
    {
      "tip": "Use rank 256 for LoRA extraction as starting point",
      "context": "Good balance for testing, can adjust higher if needed",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "512x896 resolution yields optimal SCAIL results",
      "context": "This is the tested and recommended resolution",
      "from": "teal024"
    },
    {
      "tip": "Use longer prompts for better control",
      "context": "When using SCAIL for both motion and background control",
      "from": "teal024"
    },
    {
      "tip": "Generate at lower resolution first before upscaling",
      "context": "720p or lower first gives better motion and prompt adherence, then upscale",
      "from": "FL13"
    },
    {
      "tip": "Use 1080p direct generation for best quality",
      "context": "Output size is the biggest contributor to quality, larger generations have better motion",
      "from": "ingi // SYSTMS"
    },
    {
      "tip": "Crop ultra long driving hands",
      "context": "Cropping can fix hand extension issues in driving videos",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "Use v2v workflow for enhanced quality",
      "context": "Upscale then v2v with low noise model works well though takes longer",
      "from": "FL13"
    },
    {
      "tip": "Use CFG schedule for better motion",
      "context": "Using 2 on first step and cfg 1 for rest with lightx2v LoRAs",
      "from": "FL13"
    },
    {
      "tip": "Match distill LoRA strengths to total around 1",
      "context": "When mixing multiple distill loras",
      "from": "spacepxl"
    },
    {
      "tip": "Refresh page after downloading models",
      "context": "Press 'R' to refresh or F5 after downloading SCAIL models",
      "from": "David Snow"
    },
    {
      "tip": "Higher resolution helps reduce glitchy fingers more than steps",
      "context": "Going higher res helps get less glitchy fingers more than steps, even 12",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "Use thicker sticks and correct colors for pose visualization",
      "context": "When working with pose generation and visualization",
      "from": "Kijai"
    },
    {
      "tip": "Use Wan 2.6 generation to access 2.5 for free without queue",
      "context": "Best thing about 2.6 is bypassing 2.5 queues",
      "from": "DawnII"
    },
    {
      "tip": "SCAIL works better for complex motions, use VACE/Fun Control for simple ones",
      "context": "When choosing between pose control models",
      "from": "Kijai"
    },
    {
      "tip": "Lora training could help with SCAIL context window morphing",
      "context": "When features not in init image cause morphing issues",
      "from": "Kijai"
    },
    {
      "tip": "Two-pass approach for longer SCAIL videos",
      "context": "First continue from last frame, then run vid2vid context windows to smooth transitions",
      "from": "Kijai"
    },
    {
      "tip": "Use dwpose output with SCAIL for face detection",
      "context": "When you need faces in SCAIL pose detection",
      "from": "Kijai"
    },
    {
      "tip": "Increase overlap for better context windows",
      "context": "Makes generation slower but improves quality",
      "from": "Kijai"
    },
    {
      "tip": "Choose 'all frames' to avoid frame skipping",
      "context": "When you want higher fps output matching input video",
      "from": "Kijai"
    },
    {
      "tip": "Use cross dissolve for SCAIL continuations",
      "context": "Overlap 16 frames and cross dissolve in Comfy or externally until model supports continuations",
      "from": "42hub"
    },
    {
      "tip": "Use first frame of video as reference with similar pose/position for better SCAIL results",
      "context": "When doing character replacement in SCAIL",
      "from": "avataraim"
    },
    {
      "tip": "End uni3c early to speed up generation",
      "context": "Uni3c doesn't have much effect after 1-2 steps, can end at 0.01",
      "from": "Kijai"
    },
    {
      "tip": "Keep fl2v mode always enabled in wrapper",
      "context": "Only affects last frame encoding when provided, makes workflows more consistent",
      "from": "Kijai"
    },
    {
      "tip": "Use separate prompts for different context windows",
      "context": "Remove/add prompt elements between windows for better consistency (like 'holding onto her dress' or 'glasses')",
      "from": "Kijai"
    },
    {
      "tip": "Use SCAIL first then WanAnimate for professional results",
      "context": "Use SCAIL for initial character retarget, then WanAnimate for motion detection based on SCAIL video for stability",
      "from": "Juan Gea"
    },
    {
      "tip": "Use DWPose instead of VitPose for multiple people",
      "context": "VitPose only does one person at a time while DWPose can handle faces for multiple people",
      "from": "Kijai"
    },
    {
      "tip": "Use first frame as pose control for initial image render",
      "context": "When working with multiple people to avoid positioning issues",
      "from": "boorayjenkins"
    },
    {
      "tip": "For HDR without training, lower exposure and inpaint highlights with differential diffusion",
      "context": "Can do this multiple times to get more range",
      "from": "spacepxl"
    },
    {
      "tip": "Use separate processes/models as different tools",
      "context": "Like going from Houdini simulation to Blender for shading - use SCAIL for retargeting, then WanAnimate for stability",
      "from": "Juan Gea"
    },
    {
      "tip": "For precise face detailing, create stable animated mask",
      "context": "SAM3 masks vibrate, better to use steady animated mask with spline editor to avoid stitching artifacts",
      "from": "Juan Gea"
    },
    {
      "tip": "Reduce LoRA strength to improve character consistency",
      "context": "When using multiple control methods like WanAnimate, it's a tradeoff with model accuracy (motion, noise, artifacting)",
      "from": "Josiah"
    },
    {
      "tip": "Generate driving video with character first",
      "context": "Makes I2V/character swap more effective by using WanAnimate to create the driving video",
      "from": "Josiah"
    },
    {
      "tip": "Use T2V with FOMO instead of I2V for full VACE access",
      "context": "T2V gives full access to VACE features vs limitations with I2V",
      "from": "Grimm1111"
    },
    {
      "tip": "Skip first 4 frames and add more features if first frame issues",
      "context": "When having trouble with first frame requirements in models like InfiniteTalk",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "Avoid complex prompts with parentheses in WanVideoWrapper",
      "context": "Parentheses trigger weight multiplication that can cause severe artifacts",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "For SCAIL with animals, match NLF colors",
      "context": "Draw poses where left side is 'warm' colors and right side is 'cool' colors to match NLF format",
      "from": "Kijai"
    },
    {
      "tip": "Lower audio_cfg can tone down excessive motion",
      "context": "When using audio-driven models like LongCat Avatar",
      "from": "Kijai"
    },
    {
      "tip": "Re-color skeleton to same color scheme for better rotations",
      "context": "When not using SCAIL pose, the model learned left/right sides by cool/warm colors",
      "from": "Kijai"
    },
    {
      "tip": "Old distill LoRA probably ruins longer generations",
      "context": "When using LongCat Avatar with older distillation LoRAs",
      "from": "Kijai"
    },
    {
      "tip": "Use humanoid rig for humanoid characters rather than primitives",
      "context": "For animation control, free rigs from mixamo or accurig work well with SCAIL",
      "from": "dj47"
    },
    {
      "tip": "Use first segment latent as reference for consistency in LongCat Avatar extensions",
      "context": "For clips around 20 seconds, changes won't be very noticeable, especially without drastic head turns",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "tip": "Set overlap to 0 for T2V LongCat Avatar",
      "context": "When using empty image as first frame for T2V generation",
      "from": "Kijai"
    },
    {
      "tip": "Use previous latent from last sample as reference for extensions",
      "context": "When extending videos, use the first latent of the result as reference for next extension",
      "from": "Kijai"
    },
    {
      "tip": "Lower audio_cfg to avoid excessive tooth exposure",
      "context": "High audio_cfg (like 3.0) often causes over-exposed teeth, especially with distill LoRA or low steps",
      "from": "Kijai"
    },
    {
      "tip": "Use higher temporal offset for better results",
      "context": "Made default in stand-in implementation after testing showed improvement",
      "from": "Kijai"
    },
    {
      "tip": "Use EasyCache starting at step 5 for LongCat",
      "context": "When total steps are 10, don't start cache at default 10 steps",
      "from": "burgstall"
    },
    {
      "tip": "Adjust start step instead of denoise for easier understanding",
      "context": "When doing refinement passes with low denoise values",
      "from": "Kijai"
    },
    {
      "tip": "Enable add_noise option when using start step",
      "context": "When forgetting denoise value and using start step method",
      "from": "Kijai"
    },
    {
      "tip": "Always share full error and workflow when asking for help",
      "context": "Saves everyone time when troubleshooting",
      "from": "Kijai"
    },
    {
      "tip": "Use chunked FFN for LongCat by default",
      "context": "Drops peak VRAM by ~2GB without visible speed loss",
      "from": "Kijai"
    },
    {
      "tip": "Use vocal extraction for better lip sync results",
      "context": "For lip-sync models, extract vocals first, use only the vocal track as audio input for lip-syncing, then add instrumental back in editing",
      "from": "Kijai"
    },
    {
      "tip": "Add noise after vocal extraction for LongCat-Avatar",
      "context": "LongCat-Avatar doesn't handle absolute silence, so some noise is added after vocal extraction for generated embeds",
      "from": "Kijai"
    },
    {
      "tip": "PUSA LoRA helps with facial consistency",
      "context": "Adding PUSA lora to Stand-In at 0.5 to 1 strength improves facial consistency significantly",
      "from": "\u25b2"
    },
    {
      "tip": "Use SEGS Detailer for distant face issues",
      "context": "For faces in the distance that lose likeness, use the SEGS Detailer from Impact node pack fork as an extra pass",
      "from": "42hub"
    },
    {
      "tip": "Quality loss from sage is offset by speed gain",
      "context": "The quality loss from sage is so small in most cases that you can more than offset it from the speed gain",
      "from": "Kijai"
    },
    {
      "tip": "Remove background from reference video for better motion transfer",
      "context": "When using SCAIL for pose transfer",
      "from": "ucren"
    },
    {
      "tip": "Use similar proportions between reference image and video",
      "context": "Improves SCAIL motion transfer quality",
      "from": "souoNeo"
    },
    {
      "tip": "Force reference video and output to same frame rate",
      "context": "16fps recommended for both input and output in SCAIL workflows",
      "from": "ucren"
    },
    {
      "tip": "Use Qwen Image Edit to prepare reference images",
      "context": "Get character closer to initial frame of reference video before using SCAIL",
      "from": "souoNeo"
    },
    {
      "tip": "Cache latents to disk for multi-step workflows",
      "context": "Saves time when experimenting with long video workflows that take an hour to generate",
      "from": "boorayjenkins"
    },
    {
      "tip": "Turn off merge_lora and use minimum block_swap",
      "context": "Reduces 'Loading transformer parameters' time",
      "from": "trykiss"
    },
    {
      "tip": "Use batch encode node instead of normal encode for multiple reference images",
      "context": "When working with multiple reference images in windowed context",
      "from": "Kijai"
    },
    {
      "tip": "Disconnect DWPose from input image and NLF node if you want to adapt input image to original video instead of scaling video skeleton to image",
      "context": "When using SCAIL and want different behavior",
      "from": "DawnII"
    },
    {
      "tip": "Use 1030 iteration after 1022 for lightx models",
      "context": "1030 is next iteration and might give better results",
      "from": "42hub"
    },
    {
      "tip": "Start with 0.3 strength for motion morph LoRAs",
      "context": "When using FlippinRad Motion Morph LoRA for improving morphs",
      "from": "42hub"
    },
    {
      "tip": "Use 6 steps instead of 4 for LightX2V when mixing in CFG",
      "context": "4 is hard to tune properly, 6 gives more room especially with CFG",
      "from": "Kijai"
    },
    {
      "tip": "Use first step with CFG and remaining steps without",
      "context": "Works better with at least 6 steps total",
      "from": "Kijai"
    },
    {
      "tip": "Train individual LoRAs in smaller batches instead of one large finetune",
      "context": "For datasets with multiple concepts - easier to confirm adherence and merge later",
      "from": "CJ"
    },
    {
      "tip": "Use cfg under 2.0 for low CFG with LightX2V",
      "context": "When doing 4 steps with low CFG",
      "from": "Kijai"
    },
    {
      "tip": "Always run second pass or upscale pass for solid quality",
      "context": "SageAttention loss becomes negligible with additional processing",
      "from": "cyncratic"
    },
    {
      "tip": "Use start/end steps instead of denoise for clarity",
      "context": "Denoise rounds values and causes confusion with low step counts",
      "from": "Kijai"
    },
    {
      "tip": "Multiply entire sigmas schedule by 0.0-1.0 for fine grain denoise control",
      "context": "Gives infinite control, 0.90 factor may be like 0.5 denoise",
      "from": "Ablejones"
    },
    {
      "tip": "Use fp32 VAE only for initial encode in SVI Pro",
      "context": "Minor difference for single frame encode, not worth it for decode",
      "from": "Kijai"
    },
    {
      "tip": "Consider changing anchor frame when scene changes completely",
      "context": "May help with degradation when moving far from initial reference",
      "from": "Kijai"
    },
    {
      "tip": "Use lower latent strength for initial image",
      "context": "0.5 strength works nicely for better results",
      "from": "DawnII"
    },
    {
      "tip": "Use block wise attention selection for SVI Pro",
      "context": "Could use SDPA on first/last block only like with sage3 to avoid flashing",
      "from": "Kijai"
    },
    {
      "tip": "Set different seeds for each clip in extension",
      "context": "For better variation in extended sequences",
      "from": "avataraim"
    },
    {
      "tip": "Keep overlap at 5 frames for SVI Pro",
      "context": "Standard overlap is 4 frames from last latent + 1, don't change this value",
      "from": "Kijai"
    },
    {
      "tip": "Use motion_latent_count of 2 for extensions",
      "context": "One for anchor latent (init image) and second for last latent of prior generation",
      "from": "DawnII"
    },
    {
      "tip": "Use different alpha values for first frame vs rest",
      "context": "When using ultravico attention, first frame alpha 0.95, rest 0.8",
      "from": "Kijai"
    },
    {
      "tip": "Reduce anchor latent strength for better prompt following",
      "context": "When SVI is too anchored to initial image and not following camera prompts",
      "from": "Kijai"
    },
    {
      "tip": "Use multiple anchor samples instead of previous samples",
      "context": "For better scene transitions without degradation",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "tip": "Merge images in batches to avoid RAM issues",
      "context": "When interpolating long videos with many frames",
      "from": "Cubey"
    },
    {
      "tip": "CFG around 20-30 steps works well without LightX",
      "context": "For Wan animate without speed LoRAs",
      "from": "TK_999"
    },
    {
      "tip": "Use overlap 8-10 with motion_latent_count 2 for better results",
      "context": "When using SVI Pro for stable longer videos",
      "from": "avataraim"
    },
    {
      "tip": "Save videos at 24fps or 30fps for better speed appearance",
      "context": "Post-processing generated videos",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use DDIM uniform scheduler for faster motion but requires more work for visual quality",
      "context": "When needing faster motion in videos",
      "from": "Elvaxorn"
    },
    {
      "tip": "Use motion scale 1.3 as good middle ground, can push to 2.0 for very good motion",
      "context": "Using motion scale control node",
      "from": "Elvaxorn"
    },
    {
      "tip": "Don't use prompt with MMAudio, works better without one",
      "context": "When generating audio for videos",
      "from": "Benjimon"
    },
    {
      "tip": "Use 0.2-0.3 strength for augment empty frames",
      "context": "For motion enhancement without artifacts",
      "from": "David Snow"
    },
    {
      "tip": "Use augment empty frames only on high noise for Wan 2.2",
      "context": "Allows low noise to fix artifacts",
      "from": "Kijai"
    },
    {
      "tip": "Set pad_frame_value to 0.5 for VACE",
      "context": "When using empty frames padding",
      "from": "Kijai"
    },
    {
      "tip": "Turn down lightx2v strength on high noise for better motion",
      "context": "Instead of using painter motion fixes",
      "from": "ucren"
    },
    {
      "tip": "Use split tabs in Brave browser for copying nodes across workflows",
      "context": "Great for managing multiple ComfyUI instances",
      "from": "David Snow"
    },
    {
      "tip": "Better models are harder to train because they have stronger ideas of what the world is",
      "context": "When training models with strong priors",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "Do each segment manually instead of using looping workflow",
      "context": "For long video generation, as one bad gen will scuttle the entire sequence",
      "from": "David Snow"
    },
    {
      "tip": "Run high noise separate from low noise",
      "context": "For better control over generation process",
      "from": "DawnII"
    },
    {
      "tip": "Use 3 sampler method to get right speed, then use VACE to interpolate to 24fps",
      "context": "Alternative to dealing with slow motion look from LightX2V",
      "from": "dj47"
    }
  ],
  "news": [
    {
      "update": "Wan 2.6 upgrade announced",
      "details": "Benchmarking against Sora2, improves reference video generation, multi-camera storytelling, generation quality, and duration. Currently recruiting testers but unclear if open source",
      "from": "Yan"
    },
    {
      "update": "Speculation about Higgsfield acquiring Wan 2.5",
      "details": "Claims of $100M acquisition, but community skeptical calling it fake news or worst deal in history if true",
      "from": "Benjimon"
    },
    {
      "update": "Black Forest Labs working on video model",
      "details": "Temporally consistent Flux basically, likely to be released fully open source",
      "from": "cyncratic"
    },
    {
      "update": "DiT-Extrapolation (UltraViCo) code released",
      "details": "GitHub repository released for the long video generation method",
      "from": "JohnDopamine"
    },
    {
      "update": "Long gen i2v implementation added to WanVideoWrapper",
      "details": "New implementation added to the wrapper for longer video generation",
      "from": "JohnDopamine"
    },
    {
      "update": "Lotus-2 released by EnVision Research",
      "details": "New model/tool released, not yet runnable in ComfyUI",
      "from": "A.I.Warper"
    },
    {
      "update": "ComfyUI Wan context options PR in progress",
      "details": "Pull request working through ComfyUI approval process",
      "from": "spacepxl"
    },
    {
      "update": "Flux 2 uses Mistral encoder instead of CLIP",
      "details": "Move away from CLIP encoders to more capable language models",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "update": "SVI 2.0 released for both Wan 2.1 and 2.2",
      "details": "New version combines film and shot approaches, uses 5 frame continuation with reference padding",
      "from": "Gleb Tretyak"
    },
    {
      "update": "Steadydancer tutorial available",
      "details": "Video tutorial for Steadydancer implementation",
      "from": "David Snow"
    },
    {
      "update": "LongCat-Image-Edit model released",
      "details": "New image edit model on par with Seedream 4.0, smaller size, includes camera change editing",
      "from": "DawnII"
    },
    {
      "update": "Aquif-Image-14B released",
      "details": "Image model finetuned from Wan2.2, claims to be SOTA image gen model for its size",
      "from": "yi"
    },
    {
      "update": "SVI 2.0 has different training for Wan 2.1 vs 2.2",
      "details": "2.1 can use 5 frames while 2.2 can just use one frame",
      "from": "Kijai"
    },
    {
      "update": "SVI 2.0 model released for Wan 2.2",
      "details": "Updated Sequential Video Interpolation model available",
      "from": "JohnDopamine"
    },
    {
      "update": "Kijai pushed update to allow I2V start image in HuMo",
      "details": "Was only allowed in the infinite talk loop before, now works for regular I2V and can work both start image + references at same time",
      "from": "Kijai"
    },
    {
      "update": "Context windows PR merged with bugfixes and freenoise",
      "details": "Still need to figure better way to handle controls for cond retain and prompt travel, so those are commented out for now",
      "from": "Kijai"
    },
    {
      "update": "Kijai added easier way to do padding yesterday for SVI",
      "details": "Nothing special, just workflow improvement",
      "from": "Kijai"
    },
    {
      "update": "OneToAllAnimation pose alignment node added",
      "details": "Uses same detection as WanAnimate preprocessor, could be useful for other models too, doesn't do mad limb stretching",
      "from": "Kijai"
    },
    {
      "update": "OneToAll Animation model released",
      "details": "New pose retargeting model with better alignment capabilities, fp16 and fp8 versions available",
      "from": "Kijai"
    },
    {
      "update": "aquif-ai fraudulent model removed",
      "details": "HuggingFace account deleted after being caught uploading stolen Magic Wan model",
      "from": "Nathan Shipley"
    },
    {
      "update": "SCAIL project discovered",
      "details": "From original CogVideo team, handles multiple people and rotation well, but model was pulled",
      "from": "Kijai"
    },
    {
      "update": "LTX2 postponed to January 2025",
      "details": "Previously expected in 2025, now specifically January",
      "from": "NebSH"
    },
    {
      "update": "New models released end of year",
      "details": "Wan-Move-14B-480P and Live-Avatar models released, possibly due to end of year deadlines",
      "from": "DawnII"
    },
    {
      "update": "Live Avatar supports real-time streaming",
      "details": "20 FPS on 5\u00d7H800 GPUs with 4-step sampling, infinite-length interactive avatar video generation",
      "from": "David Snow"
    },
    {
      "update": "New rCM LoRA released for Wan 2.2 I2V high noise",
      "details": "Wan22-I2V-A14B-HIGH-rCM6_0_lora_rank_64_bf16 available",
      "from": "42hub"
    },
    {
      "update": "rCM low noise LoRA now available",
      "details": "Wan22-I2V-A14B-LOW-rCM1_0_lora_rank_64_bf16.safetensors released after multiple requests",
      "from": "Kijai"
    },
    {
      "update": "WanMove native ComfyUI implementation released",
      "details": "Native node for WanMove added, compatible with ATI workflows",
      "from": "Kijai"
    },
    {
      "update": "OneToAll merged into main WanVideoWrapper branch",
      "details": "OneToAll animation feature now in main branch, no longer separate",
      "from": "Karthik"
    },
    {
      "update": "New image model based on Wan 2.2 released",
      "details": "Reddit post about new image generation model using Wan 2.2 as base",
      "from": "Dream Making"
    },
    {
      "update": "Wan 2.6 announcement expected at offline event on 10th",
      "details": "Event shows 'try all new wan face to face', possible Wan 2.6 release",
      "from": "yi"
    },
    {
      "update": "WanMove native support added with new Tracks data type",
      "details": "Native ComfyUI implementation using custom Tracks type for better integration",
      "from": "Kijai"
    },
    {
      "update": "RAM pressure cache feature coming to ComfyUI",
      "details": "Feature will unload models from RAM if needed, should solve RAM issues",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI 2.9.1 update fixed crashing issues with VRGDG workflow",
      "details": "Sorted problems users were having",
      "from": "burgstall"
    },
    {
      "update": "New Tracks node input type PR submitted to core ComfyUI",
      "details": "Should make custom nodes simpler, no need for JSON string stuff",
      "from": "Kijai"
    },
    {
      "update": "SpatialTracker V2 released",
      "details": "Major improvement over V1",
      "from": "Kijai"
    },
    {
      "update": "SCAIL model implementation in progress",
      "details": "New pose-driven video model with 3D+2D pose control, available in SCAIL branch of wrapper",
      "from": "Kijai"
    },
    {
      "update": "OneToAll 1.3B v2 model available",
      "details": "Updated version shows improved performance over original",
      "from": "Kijai"
    },
    {
      "update": "SCAIL preview release available",
      "details": "SCAIL (Wan 2.1 14B) preview version available but has no long video generation - full version will have it. Research only license for NLF model component",
      "from": "Kijai"
    },
    {
      "update": "V2 nodes in development",
      "details": "New V2 node architecture being developed with modular design - separate components for text_embeds, image_embeds, scheduler, extra_args",
      "from": "Kijai"
    },
    {
      "update": "Auto spacing feature added",
      "details": "Auto spacing feature added to address keyframe spacing issues",
      "from": "Flipping Sigmas"
    },
    {
      "update": "fp8_e4m3fn_scaled version of SCAIL uploaded",
      "details": "New fp8_e4m3fn_scaled version of Wan21-14B-SCAIL uploaded to HuggingFace",
      "from": "Kijai"
    },
    {
      "update": "Alibaba announcing WAN model launch event",
      "details": "https://www.alibabacloud.com/en/events/wan-model-launch",
      "from": "seruva19"
    },
    {
      "update": "WanMove now available in native ComfyUI",
      "details": "No longer requires wrapper-only usage",
      "from": "Kijai"
    },
    {
      "update": "SCAIL-Pose node package released",
      "details": "3D pose detection for SCAIL with single and multi-person support",
      "from": "Kijai"
    },
    {
      "update": "TurboWan 2.2 I2V 14B model released on HuggingFace",
      "details": "Includes activation quantization and sparse attention optimizations",
      "from": "JohnDopamine"
    },
    {
      "update": "Wan 2.6 launch event scheduled for January 17th",
      "details": "Appears to be closed source release based on Alibaba Cloud event",
      "from": "TK_999"
    },
    {
      "update": "SCAIL official version coming in later months",
      "details": "Will have native long video support, higher-res, possibly facial injection methods",
      "from": "teal024"
    },
    {
      "update": "RAM prices have tripled recently",
      "details": "128GB RAM kit tripled in value over a few months, making open source harder",
      "from": "blake37"
    },
    {
      "update": "Wan 2.6 announced by Alibaba Cloud",
      "details": "New cloud-only model, not available locally. 50B parameters mentioned",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "update": "SCAIL merged to main branch",
      "details": "SCAIL pose control functionality now available in main branch of WanVideoWrapper",
      "from": "Kijai"
    },
    {
      "update": "Hand coordinate swap fixed in SCAIL",
      "details": "Left and right hand coordinates were corrected, significantly improving hand control",
      "from": "Kijai"
    },
    {
      "update": "Dependencies cleaned up for SCAIL",
      "details": "Dependencies reduced to just taichi and opencv-python, removing essentials requirement",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.6 released but not open source",
      "details": "Changed from 24fps to 30fps, 1080p looks upscaled and filtered",
      "from": "Ruairi Robinson"
    },
    {
      "update": "TurboDiffusion released Turbo models for Wan",
      "details": "TurboWan2.2-I2V-A14B-720P and TurboWan2.1-T2V-14B-720P available on HuggingFace, claims 720P 5 secs in 35 secs on 5090",
      "from": "Ada"
    },
    {
      "update": "T3-Video models released",
      "details": "1.3B and 5B wan models finetuned on 4k dataset with 10x faster 4k inference",
      "from": "yi"
    },
    {
      "update": "Wan2.2-Turbo appeared on HuggingFace",
      "details": "Suspicious release possibly related to removed aquif-ai accounts, released same day as Turbo diffusion model",
      "from": "hicho"
    },
    {
      "update": "Wan 2.6 released but not open source",
      "details": "15s consistency, multi-shot control, character ID replication, commercial pricing model",
      "from": "L\u00e9on"
    },
    {
      "update": "CogVideo devs released new video VAE",
      "details": "Recently released as potential improvement",
      "from": "yi"
    },
    {
      "update": "Seedance 1.5 launched",
      "details": "New model with paper available",
      "from": "yi"
    },
    {
      "update": "LongCat-Video-Avatar released",
      "details": "New video avatar model with GitHub implementation",
      "from": "yi"
    },
    {
      "update": "Wan 2.2 turbo models available",
      "details": "New faster inference variants of 2.2",
      "from": "Zabo"
    },
    {
      "update": "SCAIL dwpose compatibility added",
      "details": "New node created to convert dwpose output to be compatible with SCAIL",
      "from": "Kijai"
    },
    {
      "update": "SCAIL prompt generation snippets released",
      "details": "Google Gemini snippets for reading reference image and driving motion to generate detailed prompts available in gen_prompts_gemini.py",
      "from": "teal024"
    },
    {
      "update": "T3-Video released by UltraWan team",
      "details": "New model from APRIL-AIGC/T3-Video on HuggingFace",
      "from": "hicho"
    },
    {
      "update": "New T2V distill LoRAs released",
      "details": "Available at huggingface.co/lightx2v/Wan2.2-Distill-Loras/tree/main",
      "from": "yi"
    },
    {
      "update": "Alibaba Wan team went closed source",
      "details": "Live stream showing new closed-source model, community feedback negative due to quality drop vs open 2.2",
      "from": "Ruairi Robinson"
    },
    {
      "update": "Uni3c optimization update",
      "details": "Added option to disable offloading for faster performance, can run in fp8 with quantization",
      "from": "Kijai"
    },
    {
      "update": "DF11 models now support Wan",
      "details": "30% VRAM reduction models available for Wan 2.2 I2V and T2V at DFloat11 HuggingFace",
      "from": "Ada"
    },
    {
      "update": "SCAIL team planning to merge SCAIL_Pose into main repo",
      "details": "Will provide implementation based on Wan Official Framework instead of SAT, add training scripts for community",
      "from": "teal024"
    },
    {
      "update": "New LightX2V T2V LoRAs released on 12/17",
      "details": "Available on HuggingFace lightx2v/Wan2.2-Distill-Loras",
      "from": "aipmaster"
    },
    {
      "update": "New LongCat Video Avatar model released",
      "details": "From same team, different from Wan but similar architecture. Not very usable without distill LoRA (30 mins for 5 secs). 16fps with audio stride issues",
      "from": "Kijai"
    },
    {
      "update": "Comfy-org now paying Kijai for open source contributions",
      "details": "Official sponsorship for ComfyUI development work",
      "from": "Kijai"
    },
    {
      "update": "Wan Alpha v2.0 released",
      "details": "New version available on GitHub",
      "from": "Hashu"
    },
    {
      "update": "Hand scaling fix and DWPose bug fix pushed",
      "details": "Fixed hand scaling from upstream and bug when using DWPose instead of VitPose (hands no longer swapped)",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI Manager integrated, preview options moved",
      "details": "Manager now integrated into ComfyUI, preview options relocated to main ComfyUI settings",
      "from": "TK_999"
    },
    {
      "update": "LongCat team working on 24fps version",
      "details": "Current 16fps setup has lipsync issues, team is developing 24fps version to address this",
      "from": "Kijai"
    },
    {
      "update": "LongCat team working on new distill LoRA",
      "details": "Team is training a distill LoRA specifically for LongCat Avatar model, said to be coming 'soon' and tested at 16 steps",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "SCAIL now available on main branch",
      "details": "SCAIL has been moved from branch to main branch and available for few days",
      "from": "Kijai"
    },
    {
      "update": "LongCat Avatar branch updated with ref_latent bug fix",
      "details": "Major bug fix where ref_latent wasn't being used properly, causing identity degradation",
      "from": "Kijai"
    },
    {
      "update": "Stand-In LoRA weights released for Wan 2.2",
      "details": "New Stand-In LoRA available as .safetensors format, works without code changes",
      "from": "Kijai"
    },
    {
      "update": "Loop decode node added to native ComfyUI",
      "details": "New node for creating looping videos with native context windows",
      "from": "Kijai"
    },
    {
      "update": "LongCat-Avatar fp8 quantized model released",
      "details": "Mixed precision fp8_e4m3fn scaled model available, nodes need update to use it",
      "from": "Kijai"
    },
    {
      "update": "Stand-In preprocessing officially released",
      "details": "Official Stand-In preprocessing available for better results",
      "from": "V\u00e9role"
    },
    {
      "update": "LongVie2 model available",
      "details": "Control part is interesting, continuation doesn't seem great, but control might be usable with other workflows",
      "from": "Kijai"
    },
    {
      "update": "HuMo dataset released",
      "details": "HuMoSet dataset containing 670K video samples with diverse reference images, dense video captions, and strict audio-visual synchronization. 133GB download, already 700+ downloads",
      "from": "JohnDopamine"
    },
    {
      "update": "NVFP4 model for Blackwell cards",
      "details": "Wan-NVFP4 released on HuggingFace with support for Blackwell cards using NVFP4 kernel",
      "from": "yi"
    },
    {
      "update": "StoryMem model released",
      "details": "New model for memory-to-video generation with MI2V and MM2V capabilities for connecting adjacent shots",
      "from": "Karthik"
    },
    {
      "update": "WanVideoWrapper version 1.4.5 released",
      "details": "Updated version with fixes for mixed model support and improved compatibility",
      "from": "Kijai"
    },
    {
      "update": "LongCat team working on 24fps version",
      "details": "They are working on 24fps version because the current 32/16 fps has some lipsync issues",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.6 available as API",
      "details": "New version released but only accessible through API, not open source",
      "from": "Ryzen"
    },
    {
      "update": "Wan-Alpha v2.0 released",
      "details": "Based on Wan2.1-14B-T2V with adapted weights and inference code open-sourced",
      "from": "Gleb Tretyak"
    },
    {
      "update": "EgoX 2.1 I2V LoRA released",
      "details": "Available on HuggingFace from DAVIAN-Robotics",
      "from": "DawnII"
    },
    {
      "update": "OmniVCus model getting release",
      "details": "Model uploading to HuggingFace with recent GitHub updates",
      "from": "JohnDopamine"
    },
    {
      "update": "WanVideoWrapper bug fixes",
      "details": "Fixed issues with cfg zero star and ComfyUI compatibility",
      "from": "Kijai"
    },
    {
      "update": "SVI 2.0 Pro released",
      "details": "New version with improved workflow compatibility and better handling of conditioning channels",
      "from": "DawnII"
    },
    {
      "update": "Open-OmniVCus code is up",
      "details": "Code released but repo is messy as it contains all of diffsynth",
      "from": "JohnDopamine"
    },
    {
      "update": "Distilled version of Wan might be possible",
      "details": "Rumors that Wan team might release a distilled version due to pressure, though just rumors",
      "from": "slmonker"
    },
    {
      "update": "Kijai waiting for new distill LoRA before touching Turbodiffusion again",
      "details": "Development pause on Turbodiffusion implementation",
      "from": "Kijai"
    },
    {
      "update": "Wan NVFP4 28x boost released but needs custom kernels",
      "details": "Requires building cutlass and custom implementation, not just pytorch",
      "from": "Kijai"
    },
    {
      "update": "SVI Pro available with significant code changes",
      "details": "Might work with old implementation but has many modifications",
      "from": "Benjimon"
    },
    {
      "update": "Wan going to paid API",
      "details": "Wan video generation moving to paid API model",
      "from": "Lumifel"
    },
    {
      "update": "New SVI 2.0 Pro model released",
      "details": "Available on HuggingFace, works as LoRA with improved extension capabilities",
      "from": "Kijai"
    },
    {
      "update": "New distill LoRAs released 11 days ago",
      "details": "Better quality than Lightning LoRAs, recommended for current use",
      "from": "Kijai"
    },
    {
      "update": "Native SVI Pro node added to KJNodes",
      "details": "New WanImageToVideoSVIPro node available with converted LoRAs",
      "from": "Kijai"
    },
    {
      "update": "SVI Pro LoRAs converted and available",
      "details": "Converted LoRAs available at huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Stable-Video-Infinity/v2.0",
      "from": "Kijai"
    },
    {
      "update": "Ultravico implementation for Wan 2.2 available",
      "details": "Custom implementation working with parameters: 0.95 alpha, 0.3 beta, 4 gamma",
      "from": "Benjimon"
    },
    {
      "update": "FreeLong++ implementation for MultiTalk and VACE",
      "details": "Algorithm that does up to 640 frames in one batch, compatible with VACE/T2V/MultiTalk+VACE",
      "from": "campeonchik"
    },
    {
      "update": "Smooth Mix model available",
      "details": "Fusion model with higher motion dynamics available on Civitai for Wan 2.2",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "Added ultravico support to SVI workflows",
      "details": "Kijai added ultravico (modified attention) functionality",
      "from": "Kijai"
    },
    {
      "update": "Wan Alpha 2.0 DoRA conversion completed",
      "details": "Successfully converted Wan Alpha 2.0 LoRA to DoRA format for ComfyUI",
      "from": "Kijai"
    },
    {
      "update": "Tencent released HY-Motion 1.0",
      "details": "New motion model that could be useful for animation production as first pass before Wan",
      "from": "NebSH"
    },
    {
      "update": "Kijai added step-specific ultravico control",
      "details": "Can now set ultravico for specific steps, with first step being most important",
      "from": "Kijai"
    },
    {
      "update": "New motion scale control node released",
      "details": "Custom node that gives ability to control scale of motion, works with VACE",
      "from": "brbbbq"
    },
    {
      "update": "HuMo I2V PR being considered for native ComfyUI",
      "details": "Kosinkadink asking for examples and test workflows to expedite review",
      "from": "Kosinkadink"
    },
    {
      "update": "PyTorch 2.6 made weights_only=True the default",
      "details": "Security improvement to prevent RCE vulnerabilities from malicious model files",
      "from": "Kijai"
    }
  ],
  "workflows": [
    {
      "workflow": "Fun VACE v2v with reference image",
      "use_case": "Video stylization with 0.6-0.8 denoise using reference images",
      "from": "David Snow"
    },
    {
      "workflow": "Using Lynx with I2V for basic v2v guiding",
      "use_case": "Follows basic motion but with more flexibility than VACE depth",
      "from": "hablaba"
    },
    {
      "workflow": "3-chained KSampler setup for I2V quality",
      "use_case": "K1 -> High noise, step 0-2 no lora, K2 -> High noise + Lightx2v lora, step 2-8, K3 -> low noise, Lightx2v lora step 8-14 for 81 frames at 1920x800",
      "from": "harryB"
    },
    {
      "workflow": "Crop&stitch for masked subject animation",
      "use_case": "Select masked area and feed cropped area to Wan with reference image for subject insertion",
      "from": "Valle"
    },
    {
      "workflow": "Context windows with overlapping for long video generation",
      "use_case": "Standard_static context: first window 1-81 frames, second window 65-156, third window 140-221 with proper overlap handling",
      "from": "Ablejones"
    },
    {
      "workflow": "Output clean latent, upscale, then add noise again",
      "use_case": "Upscaling latents between LN and HN phases by outputting clean, upscaling clean, then adding noise in low noise ksampler",
      "from": "spacepxl"
    },
    {
      "workflow": "Three-sampler approach: priming + high + low",
      "use_case": "Prime with undistilled 1-2 steps, then use high noise model, finish with low noise model",
      "from": "Scruffy"
    },
    {
      "workflow": "Split schedule with euler/gradient estimation sampler",
      "use_case": "Helped converge details better than unipc, but gradient estimation had issues with high noise steps",
      "from": "spacepxl"
    },
    {
      "workflow": "SVI 2.0 extension workflow",
      "use_case": "Extending videos by using 5 frames for continuation with original reference padding",
      "from": "Kijai"
    },
    {
      "workflow": "VACE long generation with 6 shots",
      "use_case": "25 second total generation using open pose, depth, and driving video for color with mitigated flashes and color shift",
      "from": "Juan Gea"
    },
    {
      "workflow": "4-pass I2V generation with mid-step image upscale",
      "use_case": "Outputting 2K resolution videos with better quality than latent upscale",
      "from": "FL13"
    },
    {
      "workflow": "SVI-Shot video extension",
      "use_case": "Maintaining scene continuity by applying reference image into i2v frames, can switch scenes by changing reference frame",
      "from": "Ablejones"
    },
    {
      "workflow": "One-to-all pose control with text2vid",
      "use_case": "Pose control using just text prompts without additional inputs",
      "from": "Kijai"
    },
    {
      "workflow": "FFLF with VACE for perfect image looping",
      "use_case": "Creating seamless loops, works up to about 200 frames but degrades after 3-4 generations",
      "from": "Juan Gea"
    },
    {
      "workflow": "SVI 2.0 workflow adaptation",
      "use_case": "Sequential video interpolation for long video generation",
      "from": "Hashu"
    },
    {
      "workflow": "HuMo extension with context windows",
      "use_case": "Better handling of color shift issues compared to pure overlap extensions",
      "from": "Ablejones"
    },
    {
      "workflow": "Using few extra overlap frames with SVI",
      "use_case": "Better results than 1 frame overlap, 3 frames seems optimal",
      "from": "Ablejones"
    },
    {
      "workflow": "VACE for video extensions",
      "use_case": "Tried and tested method for extensions, has faith it should get good results",
      "from": "42hub"
    },
    {
      "workflow": "SVI Wan 2.2 workflow",
      "use_case": "Stable Video Infinity using first and last frame for I2V generation",
      "from": "Ablejones"
    },
    {
      "workflow": "OneToAll Animation with WanAnimate",
      "use_case": "Combining OneToAll preprocessing with WanAnimate for better pose control",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "OneToAll workflow available",
      "use_case": "Video generation and extension",
      "from": "Josiah"
    },
    {
      "workflow": "Main WanAnimate setup with optimization",
      "use_case": "Optimized since WanAnimate dropped, getting incredible results",
      "from": "Josiah"
    },
    {
      "workflow": "Two-step TTS to video generation",
      "use_case": "Text to speech video when single-step not available - use TTS Audio Suite then feed to InfiniteTalk/HuMo",
      "from": "Gleb Tretyak"
    },
    {
      "workflow": "VACE for video interpolation smoothing",
      "use_case": "Smooth out jumps in first-frame-last-frame videos using masking",
      "from": "DawnII"
    },
    {
      "workflow": "TTM (Time-to-Move) with vid2vid",
      "use_case": "Masked noise injection for controlled video generation, steps get skipped in vid2vid",
      "from": "Kijai"
    },
    {
      "workflow": "HuMo + SVI 2.2 combination with overlap blending",
      "use_case": "Smooth lip sync video generation with transition smoothing",
      "from": "Cseti"
    },
    {
      "workflow": "Wan 2.2 FMML high noise + HuMo low noise",
      "use_case": "Automated lip sync video generation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "SVI with reference images for character consistency",
      "use_case": "Long video generation with stable character appearance",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "workflow": "Wan 2.2 high noise + HuMo low for lip sync with FMML for I2V",
      "use_case": "Infinite length generation with lip sync",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "SVI with HuMo using SVI 2.0 lora for Wan2.1 instead of 2.2",
      "use_case": "Better long video generation with less artifacts",
      "from": "Ablejones"
    },
    {
      "workflow": "Native ComfyUI implementation for SVI + HuMo",
      "use_case": "More control over sampling and embedding combination",
      "from": "Ablejones"
    },
    {
      "workflow": "Audio reactive video with HuMo and high pass filter",
      "use_case": "Creating audio-responsive video content",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Using latent mask with MultiTalk for character-specific LoRAs",
      "use_case": "Applying different effects to specific characters in lip-sync videos",
      "from": "boorayjenkins"
    },
    {
      "workflow": "Multikeyframe qwen edit with VACE",
      "description": "Workflow using subgraphed workflows of new multikeyframe qwen edit as options for inputs, where one image goes through qwen image edit to make keyframes which are then sent to VACE",
      "from": "Flipping Sigmas"
    },
    {
      "workflow": "SCAIL pose-driven animation",
      "description": "Using SCAIL model with pose inputs for character animation, works with poses it wasn't trained on",
      "from": "Kijai"
    },
    {
      "workflow": "Context windows for longer videos",
      "description": "Using context windows with SCAIL for longer video generation, though it's slow",
      "from": "Kijai"
    },
    {
      "workflow": "Uni3c + SCAIL for background consistency",
      "use_case": "Preventing frozen backgrounds in longer videos over 81 frames",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "WanMove with spline paths across context windows",
      "use_case": "Creating continuous camera movement in videos longer than 81 frames",
      "from": "zelgo_"
    },
    {
      "workflow": "3D pose-driven SCAIL animation",
      "use_case": "Creating character animations with proper pose control and face tracking",
      "from": "Kijai"
    },
    {
      "workflow": "Multi-person SCAIL pose detection",
      "use_case": "Generating videos with two characters interacting using pose control",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "Chaining multiple LoRA nodes vs single multi-LoRA node",
      "use_case": "Both methods work identically for loading multiple LoRAs",
      "from": "Kijai"
    },
    {
      "workflow": "SCAIL pose control workflow",
      "use_case": "Character animation with 3D pose control using NLF and optional face/hand control",
      "from": "Kijai"
    },
    {
      "workflow": "Upscale then v2v with low noise",
      "use_case": "Enhanced quality workflow using Wan 2.2 i2v low noise fp16 with LightX2V LoRA",
      "from": "FL13"
    },
    {
      "workflow": "Context window extension for long videos",
      "use_case": "Creating videos longer than 5 seconds using context window morphing",
      "from": "Kijai"
    },
    {
      "workflow": "SCAIL pose transfer",
      "use_case": "Transfer pose from video to reference image while maintaining better ID retention",
      "from": "David Snow"
    },
    {
      "workflow": "2.2 low model as refiner for 2.1 VACE",
      "use_case": "Use 1-2 steps of 2.2 low model to clean up 2.1 VACE outputs",
      "from": "spacepxl"
    },
    {
      "workflow": "Mixed CFG scheduling",
      "use_case": "1-2 steps with no distill loras and real CFG, then distill and cfg=1 for rest",
      "from": "spacepxl"
    },
    {
      "workflow": "Chain SCAIL generations with SVI lora",
      "use_case": "Creating longer videos with smoother transitions",
      "from": "Kijai"
    },
    {
      "workflow": "Two-pass SCAIL processing",
      "use_case": "First continue from last frame, then vid2vid context windows for smoothing",
      "from": "Kijai"
    },
    {
      "workflow": "SCAIL with context windows",
      "use_case": "Longer generations with 48-64 frame overlap",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "Multi-person SCAIL with segmentation",
      "use_case": "Handling multiple people in pose-driven animation",
      "from": "Kijai"
    },
    {
      "workflow": "Using SCAIL generation as pose input for WanAnimate",
      "use_case": "Better retargeting for very different character sizes while avoiding background shifts",
      "from": "Juan Gea"
    },
    {
      "workflow": "VACE outpainting after SCAIL generation",
      "use_case": "Add new background after SCAIL generation to fix background issues",
      "from": "42hub"
    },
    {
      "workflow": "SCAIL + WanAnimate + Uni3c combination",
      "use_case": "Professional character replacement with stable background and perfect retarget",
      "from": "Juan Gea"
    },
    {
      "workflow": "Multi-person dance with updated SCAIL-pose",
      "use_case": "Using dwpose and convert openpose keypoints to DWpose node for multiple people",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "Perfect loop generation with same start/end image",
      "use_case": "Using same image for start and end frame with fl2v mode enabled for seamless loops",
      "from": "Danial"
    },
    {
      "workflow": "Two-step SCAIL + WanAnimate process",
      "use_case": "Use SCAIL for character retargeting, then WanAnimate for stable long generation with same reference frame",
      "from": "Juan Gea"
    },
    {
      "workflow": "Split 1.3B tile + 2.2 low noise upscale",
      "use_case": "Clean upscaling results with early changeover point",
      "from": "spacepxl"
    },
    {
      "workflow": "Splice seams method for long videos",
      "use_case": "Take last 16 frames of one video and first 16 of another, run inbetween frames to create seamless transitions",
      "from": "NodeMancer"
    },
    {
      "workflow": "Multi-model combination for character animation",
      "use_case": "Combining Wan 2.2 I2V, WanMove, UniAnimate and InfiniteTalk Multi for character consistency",
      "from": "Gleb Tretyak"
    },
    {
      "workflow": "Two-pass character animation",
      "use_case": "First pass animates main character, second pass drives idle secondary character for multi-character scenes",
      "from": "boorayjenkins"
    },
    {
      "workflow": "Z-image controlnet with T2V",
      "use_case": "Using start image with z-image controlnet for T2V generation",
      "from": "Kijai"
    },
    {
      "workflow": "Per-window reference images for context windows",
      "use_case": "Inject different reference images for each context window using WanVideo Encode Latent Batch \u2192 reference_latent input",
      "from": "chrisd0073"
    },
    {
      "workflow": "TTM with cut and drag for primitive animation",
      "use_case": "Move primitive image via cut and drag, can be masked to work with finalized first frame background",
      "from": "Gleb Tretyak"
    },
    {
      "workflow": "VACE with wireframes or point blobs",
      "use_case": "Control motion with primitive shapes, can cut off early and do half steps with control signal then half with solid gray inpaint",
      "from": "spacepxl"
    },
    {
      "workflow": "LongCat Avatar multi-angle generation",
      "use_case": "Creating videos with multiple camera angles by passing different images as encoded latents to ref and prev latents",
      "from": "burgstall"
    },
    {
      "workflow": "LongCat Avatar V2V with masking",
      "use_case": "Video-to-video generation with ability to mask specific parts to affect, using video slice node for correct input video parts",
      "from": "Kijai"
    },
    {
      "workflow": "VACE inpainting with multiple ideal frames",
      "use_case": "Input start frame + end frame + middle frames (like 44th, 69th) with masks for better inpainting results",
      "from": "Dannhauer80"
    },
    {
      "workflow": "Face-only MultiTalk refinement",
      "use_case": "Apply MultiTalk only to face using SAM masking, then refine with WAN 2.2",
      "from": "boorayjenkins"
    },
    {
      "workflow": "Frame windowing for long generation",
      "use_case": "Use 141 frame windows instead of 301 frames for better time efficiency",
      "from": "burgstall"
    },
    {
      "workflow": "Vocal extraction workflow for lip sync",
      "use_case": "Improving lip sync quality by using clean vocal tracks",
      "from": "Kijai"
    },
    {
      "workflow": "WanMove with depth control",
      "use_case": "Creating consistent camera movements with depth control at 0.8 strength",
      "from": "Kijai"
    },
    {
      "workflow": "VACE with keyframing",
      "use_case": "Multiple first to last frame samplers in serial for long video generation",
      "from": "DawnII"
    },
    {
      "workflow": "3D camera control rendering",
      "use_case": "Recording camera moves and outputting as images for video generation",
      "from": "Kijai"
    },
    {
      "workflow": "SCAIL + Qwen Image Edit combination",
      "use_case": "First use Qwen to get character proportions similar to reference video, then apply SCAIL for motion transfer",
      "from": "souoNeo"
    },
    {
      "workflow": "Multi-step video workflow with caching",
      "use_case": "Cache latents to disk between steps to avoid re-running expensive sampling when experimenting with decode settings",
      "from": "boorayjenkins"
    },
    {
      "workflow": "Multiple images with context windows in I2V",
      "use_case": "Generate different video segments using multiple reference images and pipe-separated prompts",
      "from": "Kijai"
    },
    {
      "workflow": "SVI 2.0 Pro with lightx2v",
      "use_case": "Extended video generation with speed optimization",
      "from": "Benjimon"
    },
    {
      "workflow": "Using batch encode for multiple reference images",
      "use_case": "Windowed context with multiple reference images and prompts",
      "from": "avataraim"
    },
    {
      "workflow": "SCAIL with DWPose disconnected",
      "use_case": "When you want input image to adapt to original video rather than video adapting to image",
      "from": "DawnII"
    },
    {
      "workflow": "InfiniteTalk v2v for lip sync",
      "use_case": "Lip syncing existing video with custom audio using partial denoise",
      "from": "42hub"
    },
    {
      "workflow": "Three pass workflow with Z Image model",
      "use_case": "High quality image generation with refiner",
      "from": "David Snow"
    },
    {
      "workflow": "Using SaveLatent and LoadLatent nodes for chaining video generations",
      "use_case": "Save state between video generations to avoid regenerating from start",
      "from": "42hub"
    },
    {
      "workflow": "SVI 2.0 Pro extension workflow",
      "use_case": "Creating seamless video extensions without decode-encode steps",
      "from": "Kijai"
    },
    {
      "workflow": "Save/load latents for memory efficiency",
      "use_case": "Avoid model offloading and reloading when result not needed instantly",
      "from": "Kijai"
    },
    {
      "workflow": "SVI Pro + StoryMem combination",
      "use_case": "Maintaining character consistency across multiple clips",
      "from": "avataraim"
    },
    {
      "workflow": "SVI Pro extension workflow",
      "use_case": "Extending videos by copying extension groups and connecting prev_samples and decoded results",
      "from": "Kijai"
    },
    {
      "workflow": "Full auto infinite SVI Pro workflow",
      "use_case": "Automated infinite video generation with multiple loops, achieved 52 second generation",
      "from": "V\u00e9role"
    },
    {
      "workflow": "Character sheet with SVI Pro",
      "use_case": "Using character reference sheets with SVI Pro's continuation preference",
      "from": "DawnII"
    },
    {
      "workflow": "T2V + I2V combined workflow",
      "use_case": "Generate first video with T2V then continue with I2V, but requires grabbing first frame as input",
      "from": "avataraim"
    },
    {
      "workflow": "SVI Pro with multiple anchor samples workflow",
      "use_case": "Creating consistent scene transitions without using previous samples",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "SVI loop workflow with prompt splitting",
      "use_case": "Automated multiple SVI passes with different prompts per loop",
      "from": "Quality_Control"
    },
    {
      "workflow": "VACE with multiple clip stitching",
      "use_case": "Creating longer videos by combining several shorter clips",
      "from": "ingi // SYSTMS"
    },
    {
      "workflow": "SVI Pro with HuMo integration",
      "use_case": "Audio-reactive video generation, though produces flash artifacts",
      "from": "Ablejones"
    },
    {
      "workflow": "First-Last frame workflow for cinematic results",
      "use_case": "Creating slower, more fluid, cinematic videos instead of fast-moving sequences",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Automated long video generation with LLM integration",
      "use_case": "Creating long videos with automated prompting and audio length-based segmentation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "SVI Pro with anchor image changes",
      "use_case": "Changing scenes or subjects mid-generation by using different anchor images",
      "from": "NebSH"
    },
    {
      "workflow": "SVI 2.0 Pro with color matching",
      "use_case": "Long video generation with color drift correction",
      "from": "BNP4535353"
    },
    {
      "workflow": "Using different LoRAs for each SVI chunk",
      "use_case": "Character LoRA for entire run, motion LoRA only on specific chunks",
      "from": "Dever"
    },
    {
      "workflow": "SVI 2.0 Pro for infinite length sequences",
      "use_case": "Avoiding slow motion issues with LightX2V by rendering longer sequences and speeding up in post",
      "from": "dj47"
    },
    {
      "workflow": "VACE character replacement workflow with Phantom and Context Windows",
      "use_case": "Advanced video editing combining multiple techniques for character replacement in existing videos",
      "from": "42hub"
    }
  ],
  "settings": [
    {
      "setting": "Fun VACE frame load cap",
      "value": "162 frames",
      "reason": "Set as 81x2 for the dual model architecture",
      "from": "David Snow"
    },
    {
      "setting": "SteadyDancer workflow FPS",
      "value": "16 fps input, 24 fps output",
      "reason": "Standard configuration in workflow",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "setting": "VACE pose/depth control for variations",
      "value": "Pose 0.4-0.7, Depth 0.1-0.4, Shift 8+",
      "reason": "For getting creative variations with large mask",
      "from": "Dannhauer80"
    },
    {
      "setting": "UltraViCo block size",
      "value": "64 instead of 128",
      "reason": "Block size 128 isn't supported on 4090 hardware level, reduced to 64 to make it run",
      "from": "Kijai"
    },
    {
      "setting": "Context stride",
      "value": "1 (mostly not used)",
      "reason": "Higher values cause terrible stutter, only used on early steps in 2.2 A14B workflows",
      "from": "Kijai"
    },
    {
      "setting": "Fun Control default frames",
      "value": "162 instead of 81",
      "reason": "For 10 second videos",
      "from": "David Snow"
    },
    {
      "setting": "VACE FP16",
      "value": "Standard setting",
      "reason": "Working configuration for VACE workflows",
      "from": "NodeMancer"
    },
    {
      "setting": "SDE eta",
      "value": "0.5 or greater",
      "reason": "Works fine even with low steps if properly implemented",
      "from": "Ablejones"
    },
    {
      "setting": "Euler steps for quality",
      "value": "10,000 steps",
      "reason": "Euler becomes good again with enough steps",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "setting": "Context stride",
      "value": "1",
      "reason": "Higher values like 4 can cause fatal errors and crashes",
      "from": "Kijai"
    },
    {
      "setting": "SVI 2.0 motion frames",
      "value": "5",
      "reason": "Works properly, higher values create jumpy overlaps",
      "from": "xiver2114"
    },
    {
      "setting": "Block size for UltraViCo",
      "value": "Modified from default 128",
      "reason": "Default 128 not supported on 4090 hardware level",
      "from": "Kijai"
    },
    {
      "setting": "Frame limit for stable generation",
      "value": "81 frames",
      "reason": "Models trained on 81 frames, going over causes looping behavior",
      "from": "Juan Gea"
    },
    {
      "setting": "return_with_leftover_noise",
      "value": "disabled",
      "reason": "To get denoised latent in native nodes",
      "from": "Ablejones"
    },
    {
      "setting": "CFG for different scenarios",
      "value": "3.5/1/1 vs 3.5/3.5/3.5",
      "reason": "Higher CFG 30% slower but better prompt following",
      "from": "N0NSens"
    },
    {
      "setting": "SVI overlap frames",
      "value": "3-4 frames",
      "reason": "Minimal flashing while maintaining reference",
      "from": "Ablejones"
    },
    {
      "setting": "Context windows for long generation",
      "value": "context_length 81, context_overlap 30",
      "reason": "For looped generation, though results vary",
      "from": "Benjimon"
    },
    {
      "setting": "SVI overlap frames",
      "value": "3 frames",
      "reason": "Highest number without artifacts, better than 1 frame",
      "from": "Ablejones"
    },
    {
      "setting": "High noise LoRA strength",
      "value": "0.5",
      "reason": "Gets more motion and prompt following",
      "from": "Hashu"
    },
    {
      "setting": "InfiniteTalk steps",
      "value": "2 steps",
      "reason": "Low steps to modify only mouth without affecting rest of video",
      "from": "Stef"
    },
    {
      "setting": "HuMo resolution for 16GB VRAM",
      "value": "1024x576",
      "reason": "1280x720 causes OOM",
      "from": "AmirKerr"
    },
    {
      "setting": "OneToAll Animation default resolution",
      "value": "576x1024",
      "reason": "Default resolution used by the model",
      "from": "Kijai"
    },
    {
      "setting": "OneToAll Animation scheduler",
      "value": "shift 7 with euler",
      "reason": "Recommended settings, though doesn't matter much with LightX2V",
      "from": "Kijai"
    },
    {
      "setting": "OneToAll Animation steps",
      "value": "30 steps with cfg 2.0",
      "reason": "Good results without LightX2V, 50 steps didn't improve much",
      "from": "Kijai"
    },
    {
      "setting": "OneToAll default resolution",
      "value": "576x1024",
      "reason": "Default resolution for proper results",
      "from": "Kijai"
    },
    {
      "setting": "CFG comparison settings",
      "value": "cfg 1.2 8 steps lightx2v vs cfg 1.2 25 steps euler",
      "reason": "Better init adherence with euler but looks scuffed",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "setting": "Max frame window size for WanAnimate",
      "value": "1000",
      "reason": "Maximum setting for frame_window_size in WanVideo Animate Embeds",
      "from": "Valle"
    },
    {
      "setting": "LightX2V I2V LoRA strength",
      "value": "3.0 on low noise",
      "reason": "Standard setting for I2V with lightx2v distillation",
      "from": "patientx"
    },
    {
      "setting": "SVI LoRA strength on high noise",
      "value": "0.5-0.75",
      "reason": "Balance between SVI effect and prompt adherence",
      "from": "Hashu"
    },
    {
      "setting": "Audio CFG for InfiniteTalk",
      "value": "1.4-1.5",
      "reason": "Better lip sync quality",
      "from": "Charlie"
    },
    {
      "setting": "TTM steps",
      "value": "3 steps with vid2vid",
      "reason": "Vid2vid skips steps, actual generation uses 2 steps",
      "from": "Kijai"
    },
    {
      "setting": "SVI 2.2 noise split point",
      "value": "0.85 for T2V, 0.9 for I2V",
      "reason": "Original defaults for proper model switching",
      "from": "Kijai"
    },
    {
      "setting": "SVI 2.2 frame overlap",
      "value": "1 frame overlap with 5 frames provided",
      "reason": "Designed setup for SVI 2.2 vs SVI shot",
      "from": "Ablejones"
    },
    {
      "setting": "Black levels for image embed nodes",
      "value": "0 instead of 0.17",
      "reason": "User found 0.17 seemed wrong for some nodes",
      "from": "Vardogr"
    },
    {
      "setting": "1030 lora strength",
      "value": "1.0",
      "reason": "Proper strength for Wan2.2",
      "from": "WorldX"
    },
    {
      "setting": "480p lora on high",
      "value": "3.0",
      "reason": "Works well but results can be blurry at other strengths",
      "from": "metaphysician"
    },
    {
      "setting": "SVI loras for Wan2.2",
      "value": "Both high and low set to 1",
      "reason": "Recommended setting",
      "from": "BarleyFarmer"
    },
    {
      "setting": "Old 2.1 lightx2v lora",
      "value": "3.0 strength with cfg",
      "reason": "Never found anything better",
      "from": "Kijai"
    },
    {
      "setting": "Frame limit for generation",
      "value": "125 frames maximum",
      "reason": "Anything over 125 frames gets weird, quality ok up to 200 but effects undesirable",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "setting": "VACE sigma boundary",
      "value": "0.875",
      "reason": "VACE is T2V model, not I2V",
      "from": "pookyjuice"
    },
    {
      "setting": "Reserve VRAM argument",
      "value": "1GB",
      "reason": "Prevents getting stuck at VRAM limit",
      "from": "Kijai"
    },
    {
      "setting": "SCAIL default resolution",
      "value": "895x512",
      "reason": "Default resolution for SCAIL model",
      "from": "Kijai"
    },
    {
      "setting": "Pose downsampling",
      "value": "half size",
      "reason": "Default pose downsampling to half size as full pose is heavy",
      "from": "Kijai"
    },
    {
      "setting": "Skyreel LoRA strength with T2V 2.2",
      "value": "higher than 1.0",
      "reason": "Recommended strength for extending frames to 121",
      "from": "Kijai"
    },
    {
      "setting": "SCAIL context windows with block swap",
      "value": "20 blocks swapped",
      "reason": "Example setting for 4090 - very slow but works",
      "from": "Kijai"
    },
    {
      "setting": "WanMove memory usage",
      "value": "Same as Wan 2.1 14B I2V",
      "reason": "For VRAM planning",
      "from": "Kijai"
    },
    {
      "setting": "SCAIL pose resolution",
      "value": "Half of main input resolution",
      "reason": "Model training requirement",
      "from": "Kijai"
    },
    {
      "setting": "Uni3c frame requirement with SCAIL",
      "value": "Add 4 more frames",
      "reason": "SCAIL ref takes one latent",
      "from": "Kijai"
    },
    {
      "setting": "LoRA extraction type",
      "value": "standard",
      "reason": "Better than 'full' which is actually full model",
      "from": "Kijai"
    },
    {
      "setting": "SCAIL resolution",
      "value": "512x896",
      "reason": "Optimal results, must be divisible by 32",
      "from": "teal024"
    },
    {
      "setting": "Image resize method for SCAIL",
      "value": "crop instead of pad",
      "reason": "Reduces hand drifting issues",
      "from": "Kijai"
    },
    {
      "setting": "VITPose model",
      "value": "VITPose-H instead of L",
      "reason": "Better accuracy for pose detection",
      "from": "Kijai"
    },
    {
      "setting": "Pose control strength",
      "value": "0.25 (ending control at 0.25)",
      "reason": "Uses control for only 2 out of 6 steps, provides effective control without overdoing it",
      "from": "Kijai"
    },
    {
      "setting": "Sampling steps for fast motion",
      "value": "6 steps instead of 4",
      "reason": "Fast motion doesn't look good with just 4 steps, leaves noise on moving parts",
      "from": "Kijai"
    },
    {
      "setting": "Resolution requirements",
      "value": "Width and height divisible by 32",
      "reason": "Required for proper tensor dimensions and rope calculations",
      "from": "Gleb Tretyak"
    },
    {
      "setting": "Block swap for SCAIL",
      "value": "20/40 blocks swapped with fp8_scaled",
      "reason": "SCAIL requires more VRAM than normal, block swap helps manage memory",
      "from": "Kijai"
    },
    {
      "setting": "SageAttention version",
      "value": "2.2.0",
      "reason": "Balance between quality and speed, SageAttention 3 considered drop in quality",
      "from": "FL13"
    },
    {
      "setting": "LightX2V LoRA configuration",
      "value": "wan 2.2 latest lightx2v 1030/1022 LoRa on high model + wan 2.1 old lightx2v for low model",
      "reason": "Recommended setup for best results",
      "from": "FL13"
    },
    {
      "setting": "CFG schedule",
      "value": "2 on first step, cfg 1 for rest",
      "reason": "Better motion with distill LoRAs",
      "from": "FL13"
    },
    {
      "setting": "Base precision",
      "value": "fp16",
      "reason": "Highest quality you can get",
      "from": "FL13"
    },
    {
      "setting": "SageAttention installation",
      "value": "pip install sageattention==2.2.0 --no-build-isolation",
      "reason": "Proper installation method",
      "from": "FL13"
    },
    {
      "setting": "SCAIL context window overlap",
      "value": "64 frames",
      "reason": "Better than 48 but still shows gaps",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "setting": "Default SCAIL framerate",
      "value": "16fps",
      "reason": "Default output framerate",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "setting": "CFG vs steps trade-off",
      "value": "4 steps with cfg 1.0 = 4 passes vs 50 steps with cfg = 100 passes",
      "reason": "Massive speed difference",
      "from": "Kijai"
    },
    {
      "setting": "SCAIL frame skipping",
      "value": "Choose 'all frames' for higher fps",
      "reason": "Model can do sparse frames by default, disable skipping for full fps matching input",
      "from": "Kijai"
    },
    {
      "setting": "Resolution for Wan models",
      "value": "Must be divisible by 32",
      "reason": "Prevents shape errors in WanVideoSamplerv2",
      "from": "Gleb Tretyak"
    },
    {
      "setting": "Wan frame count",
      "value": "4*n+1 frames",
      "reason": "Prevents frame loss in VACE processing",
      "from": "42hub"
    },
    {
      "setting": "SCAIL pose resolution",
      "value": "Half of output resolution",
      "reason": "Required for proper SCAIL function, use math nodes to halve",
      "from": "Kijai"
    },
    {
      "setting": "Uni3c end percentage",
      "value": "0.01 (after 1-2 steps)",
      "reason": "Doesn't have much effect after initial steps, saves time",
      "from": "Kijai"
    },
    {
      "setting": "fl2v mode",
      "value": "Always enabled",
      "reason": "Only affects last frame encoding, prevents issues with same start/end frames",
      "from": "Kijai"
    },
    {
      "setting": "Block swap for SCAIL on 24GB",
      "value": "Around 16GB used with half blocks swapped",
      "reason": "Default resolution with normal Wan 14B requirements",
      "from": "Kijai"
    },
    {
      "setting": "New T2V distill LoRA",
      "value": "Better at 2 steps for i2v (1022), 4-6 steps for lightx2v",
      "reason": "Surprisingly better movement at lower steps",
      "from": "patientx"
    },
    {
      "setting": "Wan 2.2 as high noise + Wan 1.3B as low noise",
      "value": "1.3B at 0.15 denoise",
      "reason": "Good motion but graphics suffer, lower denoise improves appearance",
      "from": "hicho"
    },
    {
      "setting": "SCAIL embed strength and end_percent",
      "value": "Default 1 and 0.5",
      "reason": "Example workflow values, some experimenting with strength 2 for finger matching",
      "from": "ucren"
    },
    {
      "setting": "Tile LoRA strength",
      "value": "Higher than 0.1",
      "reason": "0.1 strength won't do much, especially at low denoise",
      "from": "spacepxl"
    },
    {
      "setting": "UniAnimate strength",
      "value": "Above 1.0 possible",
      "reason": "1.0 limit was only for 2.1, 2.2 can handle higher values",
      "from": "Kijai"
    },
    {
      "setting": "VitPose stick width for animals",
      "value": "20",
      "reason": "Thicker lines help with animal pose detection in SCAIL",
      "from": "Kijai"
    },
    {
      "setting": "SCAIL control end percent",
      "value": "0.5",
      "reason": "For speed and because control has no big effect after that point, hands/faces may benefit from higher values",
      "from": "Kijai"
    },
    {
      "setting": "Uni3C strength",
      "value": "0.1 for one step",
      "reason": "Enough to lock background, higher values may cause weird camera movement",
      "from": "Kijai"
    },
    {
      "setting": "LongCat Avatar steps",
      "value": "20",
      "reason": "Seems enough for it to work, though single window takes 10 minutes to test",
      "from": "Kijai"
    },
    {
      "setting": "SVI CFG with Lightx2v LoRA",
      "value": "CFG 2 with LoRA strength 0.5",
      "reason": "Prevents burning and keeps motion stable, updated from original CFG 1",
      "from": "Hashu"
    },
    {
      "setting": "LongCat Avatar steps",
      "value": "10 steps",
      "reason": "Works with less steps than normal model, diminishing returns when increasing step count",
      "from": "Kijai"
    },
    {
      "setting": "LongCat Avatar audio_cfg",
      "value": "3.0",
      "reason": "Maintains good lipsync but doubles generation time",
      "from": "Kijai"
    },
    {
      "setting": "LongCat Avatar text_cfg",
      "value": "1.0",
      "reason": "Used with distill LoRA at 0.8 strength for stable results",
      "from": "Kijai"
    },
    {
      "setting": "LongCat Avatar steps",
      "value": "12 steps with distill schedule",
      "reason": "Balanced quality/speed, much faster than 40+ steps",
      "from": "Kijai"
    },
    {
      "setting": "Context window overlap for Wan",
      "value": "2",
      "reason": "Proper overlap setting for Wan models",
      "from": "Kijai"
    },
    {
      "setting": "Blockswap for LongCat Avatar",
      "value": "20",
      "reason": "Required even on 5090 due to 31GB model size, enables 960x960x97 generation",
      "from": "burgstall"
    },
    {
      "setting": "EasyCache start step",
      "value": "5",
      "reason": "When using 10 total steps, don't start at default 10",
      "from": "burgstall"
    },
    {
      "setting": "Blockswap",
      "value": "10-15 on 5090",
      "reason": "Good balance for 720x720 resolution before upscaling",
      "from": "burgstall"
    },
    {
      "setting": "Steps",
      "value": "10 steps",
      "reason": "6 steps doesn't seem that bad but 10 is better",
      "from": "burgstall"
    },
    {
      "setting": "Frequency offset",
      "value": "4",
      "reason": "Better than default 1, improves stand-in results",
      "from": "Kijai"
    },
    {
      "setting": "Audio scale",
      "value": "varies by input",
      "reason": "Some inputs need higher audio scale, others go crazy with same amount",
      "from": "Kijai"
    },
    {
      "setting": "PUSA LoRA strength",
      "value": "0.5 to 1",
      "reason": "Helps with facial consistency in Stand-In",
      "from": "\u25b2"
    },
    {
      "setting": "WanMove control strength",
      "value": "0.8",
      "reason": "Allows tree to persist in depth-controlled generations",
      "from": "Kijai"
    },
    {
      "setting": "Audio CFG",
      "value": "adjustable",
      "reason": "Has major impact on lipsync quality",
      "from": "Kijai"
    },
    {
      "setting": "Base precision for LongCat",
      "value": "bf16",
      "reason": "Required for LongCat models to work properly",
      "from": "Kijai"
    },
    {
      "setting": "WAN 2.1 with LightX2V resolution",
      "value": "640x320",
      "reason": "Demo uses tiny resolution, around 30 secs for each 5s segment on 5090",
      "from": "Kijai"
    },
    {
      "setting": "SCAIL pose control strength",
      "value": "1.0",
      "reason": "Better than default 0.5 for more faithful pose copying",
      "from": "Kijai"
    },
    {
      "setting": "Frame rate for SCAIL input/output",
      "value": "16fps",
      "reason": "Input and output fps must match for proper motion transfer",
      "from": "ucren"
    },
    {
      "setting": "AudioCFG for audio issues",
      "value": "Increase value",
      "reason": "Can help resolve audio-related generation problems",
      "from": "AmirKerr"
    },
    {
      "setting": "StoryMem LoRA strength",
      "value": "22",
      "reason": "Rank stabilized LoRAs need high strength values to have effect",
      "from": "Kijai"
    },
    {
      "setting": "Block swap",
      "value": "minimum",
      "reason": "Reduces loading time for transformer parameters",
      "from": "trykiss"
    },
    {
      "setting": "Merge LoRA",
      "value": "off",
      "reason": "Reduces loading time for transformer parameters",
      "from": "trykiss"
    },
    {
      "setting": "SVI Pro overlap",
      "value": "4",
      "reason": "Stops glitches when using with lightx2v",
      "from": "Benjimon"
    },
    {
      "setting": "SVI Pro motion frames",
      "value": "1",
      "reason": "Works with overlap=4 to prevent glitches",
      "from": "Benjimon"
    },
    {
      "setting": "Motion morph LoRA strength",
      "value": "0.3",
      "reason": "Good starting point for FlippinRad Motion Morph",
      "from": "42hub"
    },
    {
      "setting": "Sigma to step",
      "value": "0.875 for T2V, 0.9 for I2V",
      "reason": "Proper handoff between high and low noise samplers",
      "from": "CJ"
    },
    {
      "setting": "CFG value for LightX2V",
      "value": "Under 2.0",
      "reason": "Works well for low CFG scenarios",
      "from": "Kijai"
    },
    {
      "setting": "Shift value range",
      "value": "Can go as high as 17",
      "reason": "To force dramatic changes in step allocation",
      "from": "CJ"
    },
    {
      "setting": "SVI Pro overlap frames",
      "value": "8",
      "reason": "Needed for proper functionality and seamless transitions",
      "from": "DawnII"
    },
    {
      "setting": "SVI Pro sigma",
      "value": "0.9",
      "reason": "Good middle ground for extension quality",
      "from": "Kijai"
    },
    {
      "setting": "Steps for SVI Pro",
      "value": "6 with 3 split",
      "reason": "Equivalent to sigma 0.9, provides good quality",
      "from": "Kijai"
    },
    {
      "setting": "Stand-in LoRA dimensions",
      "value": "512x512 reference, 480x832 output",
      "reason": "Only dimension combo that maintains likeness properly",
      "from": "ucren"
    },
    {
      "setting": "Offset parameter",
      "value": "2",
      "reason": "Works better for 1088x640 resolution",
      "from": "ucren"
    },
    {
      "setting": "Latent strength for initial image",
      "value": "0.5",
      "reason": "Provides better balance in SVI Pro generations",
      "from": "DawnII"
    },
    {
      "setting": "LightX2V 1030 strength",
      "value": "0.6",
      "reason": "Prevents gray/weird output when using converted loras",
      "from": "ucren"
    },
    {
      "setting": "Motion latent count",
      "value": "1 for first generation, 2 for extensions",
      "reason": "First gen doesn't need overlap, extensions need anchor and previous latent",
      "from": "DawnII"
    },
    {
      "setting": "Overlap frames",
      "value": "5",
      "reason": "4 frames from last latent + 1, standard for SVI Pro",
      "from": "Kijai"
    },
    {
      "setting": "High noise CFG",
      "value": "3.5",
      "reason": "Good balance for high noise model generation",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "setting": "High noise steps",
      "value": "10",
      "reason": "Standard steps for high noise model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "setting": "Low noise steps",
      "value": "4",
      "reason": "Used with LightX2V for low noise model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "setting": "Anchor latent strength",
      "value": "Variable/reduced",
      "reason": "Lower values improve prompt following when SVI is too anchored",
      "from": "Kijai"
    },
    {
      "setting": "Alpha for ultravico",
      "value": "0.9 default",
      "reason": "Default setting in original implementation, no gamma parameter",
      "from": "Kijai"
    },
    {
      "setting": "CFG and steps without LightX",
      "value": "20-30 steps",
      "reason": "Works well for Wan animate without speed LoRAs",
      "from": "TK_999"
    },
    {
      "setting": "LightX2V 2.1 with RCM combination",
      "value": "LightX2V with 0.5 RCM strength",
      "reason": "Provides realistic physics and good motion coherence",
      "from": "faceismus"
    },
    {
      "setting": "motion_latent_count",
      "value": "2",
      "reason": "Gives better results than default",
      "from": "avataraim"
    },
    {
      "setting": "overlap",
      "value": "8-10",
      "reason": "Works well with motion_latent_count 2",
      "from": "avataraim"
    },
    {
      "setting": "RCM LoRA strength",
      "value": "1.5",
      "reason": "Good balance, can be too strong at higher values",
      "from": "avataraim"
    },
    {
      "setting": "motion scale amplitude",
      "value": "1.2-1.3",
      "reason": "Good middle ground, 2.0 gives very good motion but more artifacts",
      "from": "Elvaxorn"
    },
    {
      "setting": "ultravico alpha",
      "value": "0.91 or higher",
      "reason": "Needed for proper scene changes in I2V, but creates looping issues",
      "from": "Kijai"
    },
    {
      "setting": "augment_empty_frames",
      "value": "0.2-0.3",
      "reason": "Good motion enhancement without artifacts",
      "from": "David Snow"
    },
    {
      "setting": "pad_frame_value",
      "value": "0.5",
      "reason": "Proper color for empty frames with VACE",
      "from": "Kijai"
    },
    {
      "setting": "lightx2v strength on high noise",
      "value": "lower values",
      "reason": "Improves motion quality",
      "from": "ucren"
    }
  ],
  "concepts": [
    {
      "term": "Wan 2.2 High/Low model architecture",
      "explanation": "High noise models good at motion, prompt following, composition but lost detail ability. Low noise models are fine-tuned Wan 2.1 that handle details. Use HN for first few steps, then LN to finish",
      "from": "Ablejones"
    },
    {
      "term": "Context windows for SteadyDancer",
      "explanation": "Model didn't have long gen method, so context windows only option for longer generations",
      "from": "Kijai"
    },
    {
      "term": "Context windows in video generation",
      "explanation": "Process splits video into overlapping windows (e.g., 1-81, 65-156, 140-221), encodes frames to latents, processes each window separately through sampler, then glues results together. Each window is processed independently by the model",
      "from": "Ablejones"
    },
    {
      "term": "Graph breaks in torch compilation",
      "explanation": "Failure of torch compilation to understand python code resulting in higher VRAM usage. sageattn_compiled prevents these breaks",
      "from": "42hub"
    },
    {
      "term": "4n+1 frame format",
      "explanation": "Wan models work with frame counts following 4n+1 formula because they pack 4 frames into one latent plus 1 reference frame",
      "from": "Scruffy"
    },
    {
      "term": "Discretization error vs model loss",
      "explanation": "They're additive - if score function isn't accurate, no amount of accurate integration will fix it. Model loss is 1.0 to 0.1 average",
      "from": "spacepxl"
    },
    {
      "term": "Diffusion trajectory boomerang shape",
      "explanation": "2nd/3rd order samplers can account for expected boomerang curve, which is why they show outsized gains over 4th/5th order",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "term": "Distillation baking curvature into euler steps",
      "explanation": "Distillation process essentially bakes higher order solver or many small euler substeps into one large step",
      "from": "spacepxl"
    },
    {
      "term": "SVI padding method",
      "explanation": "Uses 5 frames to continue generation and pads with original reference image, combining film motion with shot continuation",
      "from": "Kijai"
    },
    {
      "term": "Autoregressive-diffusion hybrid",
      "explanation": "Future approach for long generations using some causal masking, not necessarily block causal like magi",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "term": "Low rank training regularization",
      "explanation": "Form of regularization that preserves original model capacity better, known advantage of LoRA training",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "term": "SVI noise injection training",
      "explanation": "They inject calibrated noise during training to match expected errors in inference and change the prediction target slightly, making the model more robust to noise from recycling reference images",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "term": "Latent space color matching",
      "explanation": "Color matching with mean/std works pretty well in latent space for style transfer",
      "from": "spacepxl"
    },
    {
      "term": "SVI (Sequential Video Interpolation)",
      "explanation": "Trains model to be robust to noise from sequential steps and repeated conditioning on end frames, shifts prediction target to be more noise-robust",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "term": "Memory leak vs normal cache behavior",
      "explanation": "Leak is when same node run multiple times grows memory each time. Different nodes caching different image sizes is normal behavior",
      "from": "Kijai"
    },
    {
      "term": "SVI LoRA frame overlap mechanism",
      "explanation": "v2.0/SVI_Wan2.1 permits overlap of 5 frames while v2.0/SVI_Wan2.2 permits less. Each time people write 'SVI 2.0' it's unclear which specific safetensors they're using",
      "from": "42hub"
    },
    {
      "term": "WAN latent encoding structure",
      "explanation": "1st frame in sequence on WAN is encoded differently - with 4x as much space as other frames and subsequent latents use encoding of that frame as reference. Cannot simply chop last latents from previous generation as they're not meaningful on their own",
      "from": "42hub"
    },
    {
      "term": "LoRA merging vs unmerged behavior",
      "explanation": "Loads weights and merges LoRA weights into them (native comfy behavior). GGUF models don't allow merging, so lora weights are added on top of de-quantized weight when used. Effect differs by model/LoRA",
      "from": "Kijai"
    },
    {
      "term": "Token replacement",
      "explanation": "Technique that sets timestep of next 2 frames after reference frame to 0 to prevent them from changing too much from initialization",
      "from": "Kijai"
    },
    {
      "term": "Video DiT parameter requirements",
      "explanation": "Video DiTs seem to need around 20B parameters, with 15B (like Wan and HunV) doing okay",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "term": "Extension method token replacement",
      "explanation": "Frame 0 gets ref_cond and skipped (0 timestep), Frame 1 and 2 get replaced, rest uses normal timestep. Similar to noise mask after certain timestep",
      "from": "Kijai"
    },
    {
      "term": "Conv3d memory issue",
      "explanation": "Memory problem from hunyuan VAE parts before sampling starts, needs conv3d fix applied",
      "from": "Kijai"
    },
    {
      "term": "Patch embed reshaping",
      "explanation": "RCM models have fused patch embed that needs reshaping - view(v.shape[0], 36, 1, 2, 2) for I2V, different for T2V",
      "from": "Kijai"
    },
    {
      "term": "rCM (rectified Consistency Model)",
      "explanation": "Alternative distillation method for speed LoRAs, not just higher resolution",
      "from": "Kijai"
    },
    {
      "term": "TTM (Time-to-Move)",
      "explanation": "Vid2vid method with masked noise injection for controlled video generation",
      "from": "Kijai"
    },
    {
      "term": "LoRA rank",
      "explanation": "Higher rank closer to full model but diminishing returns above 64, affects file size especially for unmerged/GGUF",
      "from": "Kijai"
    },
    {
      "term": "SVI (Stable Video Interpolation)",
      "explanation": "Video control system that trains on error noise to overcome degradation, helps with character consistency but limits camera movement",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "term": "High noise vs Low noise model splitting",
      "explanation": "Technique where different models handle different noise levels - HuMo as low noise, Wan 2.2 as high noise",
      "from": "Ablejones"
    },
    {
      "term": "Tracks data type",
      "explanation": "New ComfyUI data type that holds path and mask tensors for trajectory control, replacing JSON coordinates",
      "from": "Kijai"
    },
    {
      "term": "NAG (Negative Attention Guidance)",
      "explanation": "Additional attention in positive pass only, separate from CFG negative passes. Less powerful than true negative but better targeted effect",
      "from": "Kijai"
    },
    {
      "term": "SVI embedding approach",
      "explanation": "Places continuation frame as first image, rest are copies of initial reference image for consistency",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "term": "Lightning vs lightx2v-distill",
      "explanation": "Different methods - Lightning is adversarial (higher quality, less diversity), lightx2v-distill was self-forcing trained",
      "from": "Kijai"
    },
    {
      "term": "Consistency model (rCM)",
      "explanation": "Works by making jumps along ODE solver trajectory, requires operating on same ODE it was trained on, essentially a ksampler with trained big jumps",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "term": "SCAIL pose format",
      "explanation": "Uses warm colors on right side, cool colors on left side, combines 3D and 2D pose information",
      "from": "Kijai"
    },
    {
      "term": "SCAIL",
      "explanation": "Wan 2.1 14B model variant with pose control capabilities, uses 3D skeleton rendering different from standard OpenPose",
      "from": "Kijai"
    },
    {
      "term": "NLF (Natural Language Foundation)",
      "explanation": "3D pose detection system used by SCAIL, requires 3D libraries and has research-only license",
      "from": "Kijai"
    },
    {
      "term": "Patch embed for pose",
      "explanation": "SCAIL model has separate patch embedding for pose input, so pose doesn't have to match video resolution exactly",
      "from": "Kijai"
    },
    {
      "term": "Beta sigmas application order",
      "explanation": "In diffusers beta sigmas are applied after shift, while in ComfyUI it's the other way around, leading to completely different sigma schedules",
      "from": "Kijai"
    },
    {
      "term": "V2 nodes architecture",
      "explanation": "New modular node design with separate components: text_embeds, image_embeds, scheduler, extra_args for cleaner workflows",
      "from": "Scruffy"
    },
    {
      "term": "Monkey patching",
      "explanation": "Globally overriding library functions like torch.load - considered bad practice",
      "from": "FlipYourBits"
    },
    {
      "term": "GetTrackRange",
      "explanation": "Node that splits spline paths across multiple context windows for continuous movement",
      "from": "Kijai"
    },
    {
      "term": "Speed mode in WanMove",
      "explanation": "Feature that allows adjusting movement speed at specific points by adding control points",
      "from": "Kijai"
    },
    {
      "term": "Activation quantization",
      "explanation": "Optimization technique used in TurboWan for faster inference",
      "from": "Kijai"
    },
    {
      "term": "RCM distillation",
      "explanation": "Method used in TurboWan for model compression and speedup",
      "from": "yi"
    },
    {
      "term": "Taichi backend",
      "explanation": "Computing framework that can run on CPU, GPU, OpenGL or Vulkan",
      "from": "Kijai"
    },
    {
      "term": "NLF (Neural Light Field)",
      "explanation": "3D pose capture method that's more tolerant to input videos than other existing 3D human-motion capture methods, can handle weird portion inputs",
      "from": "teal024"
    },
    {
      "term": "Retargeting in SCAIL",
      "explanation": "Process that adjusts poses using thresholds for horizontal and portrait scaling to align reference and driving poses",
      "from": "teal024"
    },
    {
      "term": "RoPE cache",
      "explanation": "Rotary Position Embedding cache that can cause tensor mismatches when switching between different frame counts or resolutions",
      "from": "Flipping Sigmas"
    },
    {
      "term": "Context window morphing",
      "explanation": "Technique used for generating videos longer than base model limits, can cause some inconsistencies at transition points",
      "from": "Kijai"
    },
    {
      "term": "SCAIL",
      "explanation": "Video control system that transfers poses from video to reference images",
      "from": "David Snow"
    },
    {
      "term": "Distill LoRAs",
      "explanation": "LoRAs that enable fewer sampling steps but reduce motion/structure variety while improving detail quality",
      "from": "spacepxl"
    },
    {
      "term": "VACE architecture",
      "explanation": "Basically just a controlnet on top of an unmodified t2v model",
      "from": "spacepxl"
    },
    {
      "term": "Multi-person NLF tracking",
      "explanation": "3D pose tracking that should handle multiple people but currently flashes between identities without proper segmentation",
      "from": "Kijai"
    },
    {
      "term": "Context window morphing",
      "explanation": "Background shifts that occur at the boundaries of context windows in longer SCAIL generations",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "term": "Patch size optimization",
      "explanation": "VIT research confirms patch size 16 is optimal, patch size 32 used in newer models isn't good yet",
      "from": "spacepxl"
    },
    {
      "term": "SCAIL reference image",
      "explanation": "The reference image in SCAIL acts as the start frame of the generation, not a separate style reference",
      "from": "Kijai"
    },
    {
      "term": "Context window bleeding",
      "explanation": "Background inconsistency that occurs when using context windows with moving cameras in video generation",
      "from": "Kijai"
    },
    {
      "term": "fl2v mode",
      "explanation": "First-Last-Frame mode in I2V encode node, affects how last frame is encoded when provided",
      "from": "Kijai"
    },
    {
      "term": "Context windowing background drift",
      "explanation": "Each window tries to return to reference camera position/background since same reference is used, causing background shifts",
      "from": "Kijai"
    },
    {
      "term": "DF11 quantization",
      "explanation": "Lossless 30% VRAM reduction technique, 100% identical to fp16 but smaller file size",
      "from": "Ada"
    },
    {
      "term": "Differential diffusion",
      "explanation": "Automatically used when adding mask to WanVideo Encode node or Set Latent Noise Mask",
      "from": "Kijai"
    },
    {
      "term": "Context windows vs frame extension",
      "explanation": "Context windows don't usually degrade quality like frame extension, they have blend seams instead",
      "from": "spacepxl"
    },
    {
      "term": "fp8 scaled diffusion",
      "explanation": "Still uses fp16/bf16 for calculations, only stored weights are fp8. Gets upcast and scaled on the fly",
      "from": "spacepxl"
    },
    {
      "term": "Clean latent frame",
      "explanation": "In InfiniteTalk, first frame must be actual image data rather than noise to prevent flash artifacts",
      "from": "Kijai"
    },
    {
      "term": "NLF (pose detection)",
      "explanation": "Detection process that creates skeleton style unique to SCAIL with specific color coding",
      "from": "Kijai"
    },
    {
      "term": "Prompt splitting with |",
      "explanation": "Using | symbol to provide different prompts per context window in long generations",
      "from": "Gleb Tretyak"
    },
    {
      "term": "ref_target_masks",
      "explanation": "Masks for InfiniteTalk Multi to target specific speakers in multi-person scenarios",
      "from": "Gleb Tretyak"
    },
    {
      "term": "fun_or_fl2v_model toggle",
      "explanation": "off = encode last image on its own like first image; on = insert last image as last pixel image and encode all together",
      "from": "Kijai"
    },
    {
      "term": "FlashPortrait long generation method",
      "explanation": "Uses context windows similar to FantasyPortrait, not actually a novel acceleration technique",
      "from": "Kijai"
    },
    {
      "term": "ref_latent in LongCat Avatar",
      "explanation": "Reference latent inserted at ref_frame_index position in latent space to maintain consistency in longer extensions",
      "from": "Kijai"
    },
    {
      "term": "prev_latents in LongCat Avatar",
      "explanation": "Where generation continues from, overlap amount of frames are taken from previous latents",
      "from": "Kijai"
    },
    {
      "term": "VACE control batches",
      "explanation": "VACE supports multiple control frames - you can input start/end frames plus any intermediate frames for better control",
      "from": "Kijai"
    },
    {
      "term": "Frame windows",
      "explanation": "93 frame processing windows aren't entirely necessary if you have enough memory",
      "from": "Kijai"
    },
    {
      "term": "Denoise step skipping",
      "explanation": "Denoise value skips steps - 0.21 denoise means only doing 20% of total steps",
      "from": "Kijai"
    },
    {
      "term": "Mixed precision quantization",
      "explanation": "New ComfyUI system allows customizable precision per layer instead of downcasting everything",
      "from": "Kijai"
    },
    {
      "term": "LN",
      "explanation": "Low noise - refers to the low noise variant of models",
      "from": "ucren"
    },
    {
      "term": "MI2V",
      "explanation": "Memory + first-frame image conditioning to connect adjacent shots when scene_cut is False",
      "from": "NebSH"
    },
    {
      "term": "MM2V",
      "explanation": "Memory + first 5 motion frames conditioning to connect adjacent shots when scene_cut is False",
      "from": "NebSH"
    },
    {
      "term": "Asset pop-in",
      "explanation": "A learned 'feature' where objects appear/disappear inconsistently in generated videos",
      "from": "Kijai"
    },
    {
      "term": "NVFP4",
      "explanation": "New precision format for Blackwell cards with dedicated kernel support",
      "from": "JohnDopamine"
    },
    {
      "term": "Rank stabilized LoRAs",
      "explanation": "LoRAs that use different alpha scaling and require special handling, need high strength values or automatic scaling",
      "from": "Kijai"
    },
    {
      "term": "Context windows with reference images",
      "explanation": "Can use multiple reference images for different segments of video generation, each image applies to corresponding prompt segment",
      "from": "Kijai"
    },
    {
      "term": "StoryMem memory frames",
      "explanation": "Extracted based on quality score using HPS model, model uses max 3 at once with position being random",
      "from": "Kijai"
    },
    {
      "term": "Last latent conditioning",
      "explanation": "SVI 2.0 Pro now feeds last latent index instead of last frame for better compatibility with other controls",
      "from": "DawnII"
    },
    {
      "term": "VAE LoRA loading",
      "explanation": "Wan Alpha 2.0 uses separate foreground and alpha VAE models with LoRA applied to decoder only",
      "from": "Gleb Tretyak"
    },
    {
      "term": "Skeleton scaling in SCAIL",
      "explanation": "SCAIL scales the skeleton of input video TO the input image, not the other way around",
      "from": "DawnII"
    },
    {
      "term": "NVFP4",
      "explanation": "Advanced 4-bit quantization method that's more sophisticated than naive 4-bit downcasting, preserves core information while compressing non-essential content",
      "from": "Kijai"
    },
    {
      "term": "Distillation in LightX2V context",
      "explanation": "Process that reduces inference steps while maintaining quality, allows faster generation",
      "from": "Kijai"
    },
    {
      "term": "Tensor format BCHW",
      "explanation": "Batch Channel Height Width - how tensors are arranged in ComfyUI, I2V=36 channels, T2V=16 channels",
      "from": "cyncratic"
    },
    {
      "term": "Distilling a model",
      "explanation": "Training it to use fewer steps, sometimes by training on its own outputs",
      "from": "Benjimon"
    },
    {
      "term": "FreeLong spectral blending",
      "explanation": "Windowing attention using sliding windows for more frames than model trained for, but not the same as continuing from last frame",
      "from": "Kijai"
    },
    {
      "term": "SVI Pro latent continuation",
      "explanation": "Uses previous generation latent directly instead of encoding/decoding frames, providing seamless continuity",
      "from": "DawnII"
    },
    {
      "term": "FreeLong++ technique",
      "explanation": "Low bandpass filter on latents for consistent outputs, can do 81x2, x4, or x8 frame options",
      "from": "campeonchik"
    },
    {
      "term": "Anchor samples in SVI Pro",
      "explanation": "Initial first frame that keeps generation from degrading over extensions",
      "from": "Kijai"
    },
    {
      "term": "Motion latent count",
      "explanation": "Number of latents to carry over between generations - includes anchor and previous generation latents",
      "from": "DawnII"
    },
    {
      "term": "Ultravico",
      "explanation": "Modified attention mechanism with hard coded frame_tokens for 832x480, works poorly at other resolutions",
      "from": "Kijai"
    },
    {
      "term": "Anchor latent in SVI",
      "explanation": "The initial frame used as anchor to prevent degradation, but if scene changes completely it won't help much",
      "from": "Kijai"
    },
    {
      "term": "DoRA vs LoRA",
      "explanation": "DoRA uses magnitude_vector weights, ComfyUI can load DoRAs by adding proper prefixes to keys",
      "from": "Kijai"
    },
    {
      "term": "StorymMem functionality",
      "explanation": "MI2V LoRA continues from last frame, MMI2V from 5 last frames, both can use memory images",
      "from": "Kijai"
    },
    {
      "term": "sageattn_ultravico",
      "explanation": "An attention_mode that can be chosen on WanVideoModelLoader in Wrapper for extended length generations",
      "from": "42hub"
    },
    {
      "term": "Motion scale control",
      "explanation": "Ability to control the speed and timing of motion in video generation, essentially motion-level prompt enforcement",
      "from": "Ada"
    },
    {
      "term": "Context Windows",
      "explanation": "Method used for partial denoising in models like HuMo and InfiniteTalk to turn them into V2V",
      "from": "42hub"
    },
    {
      "term": "augment_empty_frames",
      "explanation": "Adds inverse of start image into empty frames of I2V conditioning to push away from start image, can introduce artifacts",
      "from": "Kijai"
    },
    {
      "term": "pad_frame_value",
      "explanation": "The color of the empty frames in video generation",
      "from": "Kijai"
    },
    {
      "term": "ScaleROPE",
      "explanation": "Tells model that frames are different count than actual (like 81 frames are actually 40), can lead to unwanted behavior",
      "from": "Kijai"
    },
    {
      "term": "Context Windows",
      "explanation": "Generic mechanism to make models generate videos longer than 81 frames even if models weren't originally intended for it",
      "from": "42hub"
    },
    {
      "term": "FreeLong frequency blending",
      "explanation": "Blends low-frequency components of global video features with high-frequency components of local video features for consistency",
      "from": "mallardgazellegoosewildcat2"
    }
  ],
  "resources": [
    {
      "resource": "Upscalers gossip collection",
      "url": "https://wanx-troopers.github.io/upscalers.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "SteadyDancer FP8 model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/SteadyDancer",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SAM3 ComfyUI nodes",
      "url": "https://github.com/PozzettiAndrea/ComfyUI-SAM3",
      "type": "repo",
      "from": "BitJuggler"
    },
    {
      "resource": "DiT-Mem-1.3B model",
      "url": "https://huggingface.co/Thrcle/DiT-Mem-1.3B",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Apple StarFlow model",
      "url": "https://huggingface.co/apple/starflow",
      "type": "model",
      "from": "shaggss"
    },
    {
      "resource": "DiT-Extrapolation (UltraViCo)",
      "url": "https://github.com/thu-ml/DiT-Extrapolation/commit/bd97f25dd06830d4f01a5eafcb1f07a442bbaa5f",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "ComfyUI-AnimateDiff-Evolved Context Options documentation",
      "url": "https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved/tree/main/documentation/nodes#context-optionsstandard-uniform",
      "type": "documentation",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI-outputlists-combiner nodepack",
      "url": "https://github.com/geroldmeisinger/ComfyUI-outputlists-combiner",
      "type": "node pack",
      "from": "Gleb Tretyak"
    },
    {
      "resource": "Lotus-2",
      "url": "https://github.com/EnVision-Research/Lotus-2",
      "type": "model",
      "from": "A.I.Warper"
    },
    {
      "resource": "LongCat example workflow",
      "url": "workflow file shared",
      "type": "workflow",
      "from": "Kijai"
    },
    {
      "resource": "TCD sampler for ComfyUI",
      "url": "https://github.com/JettHu/ComfyUI-TCD",
      "type": "tool",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "resource": "ComfyUI Wan context options PR",
      "url": "https://github.com/comfyanonymous/ComfyUI/pull/10975",
      "type": "repo",
      "from": "spacepxl"
    },
    {
      "resource": "Distillation paper on trajectory analysis",
      "url": "https://arxiv.org/abs/2511.22475",
      "type": "paper",
      "from": "spacepxl"
    },
    {
      "resource": "ComfyUI-MultiGPU loader",
      "url": "https://github.com/pollockjj/ComfyUI-MultiGPU",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "Long Penis LoRA",
      "url": "https://civitai.com/models/2181569?modelVersionId=2467448",
      "type": "model",
      "from": "harrisonwells"
    },
    {
      "resource": "VACE custom node for keyframe insertion",
      "url": "https://old.reddit.com/r/comfyui/comments/1l93f7w/my_weird_custom_node_for_vace/",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "SVI 2.0 converted LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/Stable-Video-Infinity/v2.0/SVI_Wan2.1-I2V-14B_lora_v2.0_rank_128_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Steadydancer tutorial",
      "url": "https://www.youtube.com/watch?v=53QdhJcUjvQ",
      "type": "tutorial",
      "from": "David Snow"
    },
    {
      "resource": "One-to-All Animation",
      "url": "https://github.com/ssj9596/One-to-All-Animation",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "SVI 2.0 original model",
      "url": "https://huggingface.co/vita-video-gen/svi-model/tree/main/version-2.0",
      "type": "model",
      "from": "Gleb Tretyak"
    },
    {
      "resource": "LongCat-Image-Edit model",
      "url": "https://huggingface.co/meituan-longcat/LongCat-Image-Edit",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "WanEx_I2VCustomEmbeds node",
      "url": "https://github.com/drozbay/WanExperiments",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "ComfyScript",
      "url": "https://github.com/Chaoses-Ib/ComfyScript",
      "type": "tool",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "resource": "DrPanGloss ComfyUI fork",
      "url": "https://github.com/hiddenswitch/ComfyUI",
      "type": "repo",
      "from": "Scruffy"
    },
    {
      "resource": "TrentNodes keyframe node",
      "url": "https://github.com/TrentHunter82/TrentNodes/tree/main",
      "type": "repo",
      "from": "Flipping Sigmas"
    },
    {
      "resource": "ComfyUI-PromptHelper",
      "url": "https://github.com/LeonQ8/ComfyUI-PromptHelper",
      "type": "tool",
      "from": "L\u00e9on"
    },
    {
      "resource": "Aquif-Image-14B",
      "url": "https://huggingface.co/aquif-ai/aquif-Image-14B",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "ComfyUI-GIMM-VFI",
      "url": "https://github.com/kijai/ComfyUI-GIMM-VFI",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI_Fill-Nodes",
      "url": "https://github.com/filliptm/ComfyUI_Fill-Nodes/",
      "type": "repo",
      "from": "lostintranslation"
    },
    {
      "resource": "ComfyUI-EasyColorCorrector",
      "url": "https://github.com/regiellis/ComfyUI-EasyColorCorrector",
      "type": "repo",
      "from": "lostintranslation"
    },
    {
      "resource": "SVI model 2.0",
      "url": "https://huggingface.co/vita-video-gen/svi-model/tree/main/version-2.0",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "WanExperiments repository",
      "url": "https://github.com/drozbay/WanExperiments",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "SVI 2.0 LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Stable-Video-Infinity/v2.0",
      "type": "model",
      "from": "Ablejones"
    },
    {
      "resource": "ViSAudio video->audio tracking",
      "url": "https://github.com/kszpxxzmc/ViSAudio",
      "type": "repo",
      "from": "Kiwv"
    },
    {
      "resource": "MAGI-1 hybrid autoregressive/diffusion model",
      "url": "https://github.com/SandAI-org/MAGI-1",
      "type": "repo",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "resource": "One-to-All-Animation",
      "url": "https://github.com/ssj9596/One-to-All-Animation",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Flash Attention for Windows",
      "url": "https://huggingface.co/ussoewwin/Flash-Attention-2_for_Windows/blob/main/flash_attn-2.8.3%2Bcu130torch2.9.1cxx11abiTRUE-cp312-cp312-win_amd64.whl",
      "type": "tool",
      "from": "Gleb Tretyak"
    },
    {
      "resource": "WanX Troopers SVI documentation",
      "url": "https://wanx-troopers.github.io/svi.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "WanX Troopers extensions documentation",
      "url": "https://wanx-troopers.github.io/extensions.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "WanX Troopers HuMo documentation",
      "url": "https://wanx-troopers.github.io/humo.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "Spiritus skeletal AI animation paper",
      "url": "https://dl.acm.org/doi/10.1145/3746059.3747707",
      "type": "research",
      "from": "Gleb Tretyak"
    },
    {
      "resource": "NotebookLM Wan chat model",
      "url": "https://notebooklm.google.com/notebook/a08901b9-0511-4926-bbf8-3c86a12dc306?pli=1",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "OneToAll Animation fp8 model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/blob/main/OneToAllAnimation/Wan21-OneToAllAnimation_fp8_e4m3fn_scaled_KJ.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SVI Wan2.2 workflow",
      "url": "https://github.com/vita-epfl/Stable-Video-Infinity/blob/svi_wan22/comfyui_workflow/SVI-Wan22-1207.json",
      "type": "workflow",
      "from": "Ablejones"
    },
    {
      "resource": "Wan ecosystem documentation",
      "url": "https://wanx-troopers.github.io/",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "LightX2V Distill LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Wan22_Lightx2v",
      "type": "lora",
      "from": "DawnII"
    },
    {
      "resource": "SCAIL project",
      "url": "https://teal024.github.io/SCAIL/",
      "type": "project",
      "from": "Kijai"
    },
    {
      "resource": "RCM Wan 2.2 I2V model",
      "url": "https://huggingface.co/worstcoder/rcm-Wan/blob/main/Wan2.2-I2V-A14B-high-rCM6.0-merged.pth",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "OneToAll workflow",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1447650547841630381",
      "type": "workflow",
      "from": "Josiah"
    },
    {
      "resource": "SCAIL Preview model",
      "url": "https://huggingface.co/zai-org/SCAIL-Preview",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Triton Windows wheels",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "SageAttention wheels",
      "url": "https://github.com/woct0rdho/SageAttention",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "RCM LoRA conversion",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/rCM/Wan22-I2V-A14B-HIGH-rCM6_0_lora_rank_64_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Manim Community repo",
      "url": "https://github.com/ManimCommunity/manim",
      "type": "tool",
      "from": "Scruffy"
    },
    {
      "resource": "Wan-Move-14B model",
      "url": "https://huggingface.co/Ruihang/Wan-Move-14B-480P",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Live Avatar model",
      "url": "https://huggingface.co/Quark-Vision/Live-Avatar",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "WanMove OneToAll animation workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/Wan21_OneToAllAnimation_example_01.json",
      "type": "workflow",
      "from": "42hub"
    },
    {
      "resource": "LightX2V LoRA guide and links",
      "url": "https://wanx-troopers.github.io/loras/part-01.html#22-i2v",
      "type": "guide",
      "from": "42hub"
    },
    {
      "resource": "rCM high noise LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/rCM/Wan22-I2V-A14B-HIGH-rCM6_0_lora_rank_64_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "rCM low noise LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/rCM/Wan22-I2V-A14B-LOW-rCM1_0_lora_rank_64_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "TTS Audio Suite",
      "url": "https://github.com/diodiogod/TTS-Audio-Suite",
      "type": "node pack",
      "from": "Gleb Tretyak"
    },
    {
      "resource": "WanMove fp8 scaled models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/WanMove",
      "type": "model",
      "from": "slmonker"
    },
    {
      "resource": "Ovi TTS workflow",
      "url": "https://github.com/character-ai/Ovi",
      "type": "model",
      "from": "Lumi"
    },
    {
      "resource": "Upscaler recommendations",
      "url": "https://wanx-troopers.github.io/upscalers.html",
      "type": "guide",
      "from": "42hub"
    },
    {
      "resource": "VRGameDevGirl84 ComfyUI workflows and custom nodes",
      "url": "https://github.com/vrgamegirl19/comfyui-vrgamedevgirl/tree/main/Workflows",
      "type": "workflow",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "UCPE paper/implementation",
      "url": "https://github.com/chengzhag/UCPE",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "WanMove repository",
      "url": "https://github.com/ali-vilab/Wan-Move",
      "type": "repo",
      "from": "Charlie"
    },
    {
      "resource": "SVI model on HuggingFace",
      "url": "https://huggingface.co/vita-video-gen/svi-model",
      "type": "model",
      "from": "Cseti"
    },
    {
      "resource": "FL-Path-Animator for trajectory control",
      "url": "https://github.com/filliptm/ComfyUI_FL-Path-Animator",
      "type": "tool",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "resource": "WanMove example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_WanMove_I2V_example_01.json",
      "type": "workflow",
      "from": "CaptHook"
    },
    {
      "resource": "Wan2.2 Lightning I2V lora",
      "url": "https://huggingface.co/Aitrepreneur/FLX/blob/main/Wan2.2-Lightning_I2V-A14B-4steps-lora_LOW_fp16.safetensors",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "SVI loras for Wan2.2",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Stable-Video-Infinity/v2.0",
      "type": "lora",
      "from": "BarleyFarmer"
    },
    {
      "resource": "SVI ComfyUI workflow",
      "url": "https://github.com/vita-epfl/Stable-Video-Infinity/tree/svi_wan22/comfyui_workflow",
      "type": "workflow",
      "from": "Doctor Diffusion"
    },
    {
      "resource": "WanX Troopers lora collection",
      "url": "https://wanx-troopers.github.io/loras/part-01.html",
      "type": "resource",
      "from": "42hub"
    },
    {
      "resource": "Wan-Move notes",
      "url": "https://wanx-troopers.github.io/wan-move.html",
      "type": "resource",
      "from": "42hub"
    },
    {
      "resource": "SpatialTracker V2",
      "url": "https://spatialtracker.github.io/",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI Tracks node PR",
      "url": "https://github.com/comfyanonymous/ComfyUI/pull/11247",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "OneToAll 1.3B v2 model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/OneToAllAnimation/Wan21-OneToAllAnimation_1_3B_v2_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SageAttention latest release",
      "url": "https://github.com/woct0rdho/SageAttention",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Triton Windows wheels",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "repo",
      "from": "Zabo"
    },
    {
      "resource": "MultiGPU ComfyUI extension",
      "url": "https://github.com/pollockjj/ComfyUI-MultiGPU",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "Trent keyframe nodes",
      "url": "https://github.com/TrentHunter82/TrentNodes/tree/main",
      "type": "node",
      "from": "Dream Making"
    },
    {
      "resource": "Triton/Sage install tutorial",
      "url": "https://discord.com/channels/1076117621407223829/1145677539738665020/1445821826994405516",
      "type": "tutorial",
      "from": "garbus"
    },
    {
      "resource": "SCAIL fp8 model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/blob/main/SCAIL/Wan21-14B-SCAIL-preview_fp8_e4m3fn_scaled_KJ.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Cinematic Fast Cutting LoRA",
      "url": "https://civitai.com/models/2113025/cinematic-fast-cutting-previously-quick-cuts",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "WanVideoWrapper example workflows",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/main/example_workflows",
      "type": "workflow",
      "from": "42hub"
    },
    {
      "resource": "NLF pose detection",
      "url": "https://github.com/isarandi/nlf",
      "type": "repo",
      "from": "42hub"
    },
    {
      "resource": "Online pose editor",
      "url": "https://zhuyu1997.github.io/open-pose-editor/",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "SCAIL documentation",
      "url": "https://wanx-troopers.github.io/wan-animates.html#scail",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "ComfyUI-SCAIL-Pose",
      "url": "https://github.com/kijai/ComfyUI-SCAIL-Pose",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "TrentNodes (problematic)",
      "url": "https://github.com/TrentHunter82/TrentNodes",
      "type": "repo",
      "from": "Flipping Sigmas"
    },
    {
      "resource": "Alibaba WAN model launch event",
      "url": "https://www.alibabacloud.com/en/events/wan-model-launch",
      "type": "announcement",
      "from": "seruva19"
    },
    {
      "resource": "SCAIL official repository",
      "url": "https://github.com/zai-org/SCAIL",
      "type": "repo",
      "from": "teal024"
    },
    {
      "resource": "ComfyUI SCAIL Pose node",
      "url": "https://github.com/kijai/ComfyUI-SCAIL-Pose",
      "type": "repo",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "TurboWan 2.2 I2V 14B model",
      "url": "https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "SCAIL models for ComfyUI",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/SCAIL",
      "type": "model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "TurboDiffusion technical paper",
      "url": "https://jt-zhang.github.io/files/TurboDiffusion_Technical_Report.pdf",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "SCAIL GitHub repository",
      "url": "https://github.com/zai-org/SCAIL",
      "type": "repo",
      "from": "teal024"
    },
    {
      "resource": "Cinematic Quick Cuts LoRA",
      "url": "https://huggingface.co/neph1/cinematic_quick_cuts_wan",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "SCAIL GGUF quantized model",
      "url": "https://huggingface.co/vantagewithai/SCAIL-Preview-GGUF",
      "type": "model",
      "from": "rgeryrfb54r4"
    },
    {
      "resource": "Fill Nodes for RIFE interpolation",
      "url": "https://github.com/filliptm/ComfyUI_Fill-Nodes",
      "type": "node",
      "from": "FL13"
    },
    {
      "resource": "LightX2V LoRA comparison guide",
      "url": "https://wanx-troopers.github.io/loras/part-01.html",
      "type": "guide",
      "from": "42hub"
    },
    {
      "resource": "SCAIL models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/SCAIL",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "LightX2V LoRA guide",
      "url": "https://wanx-troopers.github.io/loras/part-01.html",
      "type": "guide",
      "from": "42hub"
    },
    {
      "resource": "TurboDiffusion Wan models",
      "url": "https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P",
      "type": "model",
      "from": "Ada"
    },
    {
      "resource": "WanViTPoseRetargeter",
      "url": "https://github.com/red-polo/ComfyUI-WanViTPoseRetargeter",
      "type": "node",
      "from": "dj47"
    },
    {
      "resource": "T3-Video models",
      "url": "https://huggingface.co/APRIL-AIGC/T3-Video",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "AI Windows wheels",
      "url": "https://github.com/wildminder/AI-windows-whl",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "SCAIL pose control",
      "url": "https://github.com/zai-org/SCAIL",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "SCAIL AudioReactive ComfyUI",
      "url": "https://github.com/ckinpdx/ComfyUI-SCAIL-AudioReactive",
      "type": "repo",
      "from": "ComfyCod3r"
    },
    {
      "resource": "TurboDiffusion supporting Wan",
      "url": "https://github.com/thu-ml/TurboDiffusion",
      "type": "repo",
      "from": "tintwotin"
    },
    {
      "resource": "LongCat-Video-Avatar",
      "url": "https://meigen-ai.github.io/LongCat-Video-Avatar/ https://github.com/meituan-longcat/LongCat-Video",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "Wan 2.6 creation interface",
      "url": "https://create.wan.video/",
      "type": "tool",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "resource": "TrentNodes for video transitions",
      "url": "https://github.com/TrentHunter82/TrentNodes",
      "type": "repo",
      "from": "cyncratic"
    },
    {
      "resource": "SCAIL prompt generation snippets",
      "url": "https://github.com/zai-org/SCAIL",
      "type": "repo",
      "from": "teal024"
    },
    {
      "resource": "T3-Video model",
      "url": "https://huggingface.co/APRIL-AIGC/T3-Video",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "ComfyUI-WarperNodes",
      "url": "https://github.com/AIWarper/ComfyUI-WarperNodes",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "LongCat-Video",
      "url": "https://github.com/meituan-longcat/LongCat-Video",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "LongCat-Video-Avatar",
      "url": "https://github.com/MeiGen-AI/LongCat-Video-Avatar",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Skyreels models collection",
      "url": "https://huggingface.co/collections/Skywork/skyreels-v2",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "New T2V Distill LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Loras/tree/main",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "DF11 Wan Models",
      "url": "https://huggingface.co/DFloat11/Wan2.2-I2V-A14B-DF11",
      "type": "model",
      "from": "Ada"
    },
    {
      "resource": "ComfyUI-DFloat11-Extended",
      "url": "https://github.com/BigStationW/ComfyUI-DFloat11-Extended",
      "type": "tool",
      "from": "Ada"
    },
    {
      "resource": "Wan Move Model",
      "url": "https://huggingface.co/Ruihang/Wan-Move-14B-480P",
      "type": "model",
      "from": "Gleb Tretyak"
    },
    {
      "resource": "DCM Model",
      "url": "https://huggingface.co/cszy98/DCM",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "TurboDiffusion",
      "url": "https://github.com/thu-ml/TurboDiffusion",
      "type": "repo",
      "from": "patientx"
    },
    {
      "resource": "Lumos-Custom relighting model",
      "url": "https://github.com/alibaba-damo-academy/Lumos-Custom",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "LightX2V Wan2.2 Distill LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Loras/tree/main",
      "type": "model",
      "from": "aipmaster"
    },
    {
      "resource": "VAE-decode-hdr",
      "url": "https://github.com/netocg/vae-decode-hdr",
      "type": "repo",
      "from": "Juan Gea"
    },
    {
      "resource": "ComfyUI-HQ-Image-Save",
      "url": "https://github.com/spacepxl/ComfyUI-HQ-Image-Save",
      "type": "repo",
      "from": "spacepxl"
    },
    {
      "resource": "Luminance Stack Processor",
      "url": "https://github.com/sumitchatterjee13/Luminance-Stack-Processor",
      "type": "repo",
      "from": "chrisd0073"
    },
    {
      "resource": "WASWanExposureStabilizer",
      "url": "https://github.com/WASasquatch/WAS_Extras",
      "type": "tool",
      "from": "42hub"
    },
    {
      "resource": "IC-Effect research",
      "url": "https://cuc-mipg.github.io/IC-Effect/",
      "type": "research",
      "from": "DawnII"
    },
    {
      "resource": "Wan Alpha v2.0",
      "url": "https://github.com/WeChatCV/Wan-Alpha",
      "type": "repo",
      "from": "Hashu"
    },
    {
      "resource": "Stable Video Infinity",
      "url": "https://github.com/vita-epfl/Stable-Video-Infinity",
      "type": "repo",
      "from": "burgstall"
    },
    {
      "resource": "SVI documentation",
      "url": "https://wanx-troopers.github.io/svi.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "Context Windows guide",
      "url": "https://wanx-troopers.github.io/what-plugs-where/context-windows.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "Extensions documentation",
      "url": "https://wanx-troopers.github.io/extensions.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "NLF pose detection",
      "url": "https://istvansarandi.com/nlf/",
      "type": "research",
      "from": "Kijai"
    },
    {
      "resource": "WorldCanvas",
      "url": "https://worldcanvas.github.io/",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Z-Image Turbo Fun Controlnet Union",
      "url": "https://huggingface.co/alibaba-pai/Z-Image-Turbo-Fun-Controlnet-Union",
      "type": "model",
      "from": "xwsswww"
    },
    {
      "resource": "TurboDiffusion",
      "url": "https://github.com/thu-ml/TurboDiffusion",
      "type": "repo",
      "from": "AiGangster"
    },
    {
      "resource": "FlashPortrait",
      "url": "https://huggingface.co/FrancisRing/FlashPortrait",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "FlashPortrait",
      "url": "https://github.com/Francis-Rings/FlashPortrait",
      "type": "model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "LongCat Avatar workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/longcat_avatar",
      "type": "workflow",
      "from": "asd"
    },
    {
      "resource": "OpenPose skeleton for SCAIL",
      "url": "https://toyxyz.gumroad.com/l/ciojz",
      "type": "tool",
      "from": "manzonif_01"
    },
    {
      "resource": "Pivot Animator for stick figure animation",
      "url": "https://pivotanimator.net/index.html",
      "type": "tool",
      "from": "Jumper"
    },
    {
      "resource": "LongVie for 5-minute videos",
      "url": "https://github.com/Vchitect/LongVie",
      "type": "repo",
      "from": "dj47"
    },
    {
      "resource": "DITTO denoising/enhancing",
      "url": "https://github.com/EzioBy/Ditto",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Ditto LoRA versions",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Ditto",
      "type": "lora",
      "from": "JohnDopamine"
    },
    {
      "resource": "Ditto Full Modules",
      "url": "https://huggingface.co/QingyanBai/Ditto_models/tree/main/models_comfy",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Stand-In LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Stand-In",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "LongVie2 paper",
      "url": "https://arxiv.org/html/2512.13604v1",
      "type": "research",
      "from": "Kijai"
    },
    {
      "resource": "Stand-In model page",
      "url": "https://huggingface.co/BowenXue/Stand-In",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "LongCat-Avatar fp8 model",
      "url": "https://huggingface.co/Kijai/LongCat-Video_comfy/blob/main/Avatar/LongCat-Avatar-single_fp8_e4m3fn_scaled_mixed_KJ.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Stand-In Preprocessor",
      "url": "https://github.com/WeChatCV/Stand-In_Preprocessor_ComfyUI",
      "type": "repo",
      "from": "NebSH"
    },
    {
      "resource": "LongVie2 model",
      "url": "https://huggingface.co/Vchitect/LongVie2/tree/main",
      "type": "model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "EgoX project",
      "url": "https://github.com/DAVIAN-Robotics/EgoX",
      "type": "repo",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "WAN documentation",
      "url": "https://wanx-troopers.github.io/talkies/longcat-avatar.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "HuMoSet dataset",
      "url": "https://modelscope.cn/datasets/leoniuschen/HuMoSet",
      "type": "dataset",
      "from": "JohnDopamine"
    },
    {
      "resource": "Wan-NVFP4 model",
      "url": "https://huggingface.co/lightx2v/Wan-NVFP4",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "ComfyUI-SCAIL-AudioReactive",
      "url": "https://github.com/ckinpdx/ComfyUI-SCAIL-AudioReactive",
      "type": "tool",
      "from": "Dream Making"
    },
    {
      "resource": "ComfyUI-WanSoundTrajectory",
      "url": "https://github.com/ckinpdx/ComfyUI-WanSoundTrajectory",
      "type": "tool",
      "from": "Dream Making"
    },
    {
      "resource": "StoryMem repository",
      "url": "https://github.com/Kevin-thu/StoryMem",
      "type": "repo",
      "from": "Karthik"
    },
    {
      "resource": "InfCam project",
      "url": "https://github.com/emjay73/InfCam",
      "type": "repo",
      "from": "NC17z"
    },
    {
      "resource": "Wan timeline documentation",
      "url": "https://wanx-troopers.github.io/timeline.html",
      "type": "resource",
      "from": "42hub"
    },
    {
      "resource": "SageAttention wheels",
      "url": "https://github.com/woct0rdho/SageAttention",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "Triton and SageAttention installer",
      "url": "https://github.com/DazzleML/comfyui-triton-and-sageattention-installer",
      "type": "tool",
      "from": "NC17z"
    },
    {
      "resource": "SCAIL AudioReactive ComfyUI",
      "url": "https://github.com/ckinpdx/ComfyUI-SCAIL-AudioReactive",
      "type": "repo",
      "from": "mmmgigi"
    },
    {
      "resource": "Wan NVFP4 model",
      "url": "https://huggingface.co/lightx2v/Wan-NVFP4",
      "type": "model",
      "from": "Zeku"
    },
    {
      "resource": "Wan2.1-Turbo-fp8",
      "url": "https://huggingface.co/Aquiles-ai/Wan2.1-Turbo-fp8/tree/main",
      "type": "model",
      "from": "asd"
    },
    {
      "resource": "Wan-Alpha v2.0",
      "url": "https://huggingface.co/htdong/Wan-Alpha-v2.0",
      "type": "model",
      "from": "Gleb Tretyak"
    },
    {
      "resource": "EgoX project",
      "url": "https://keh0t0.github.io/EgoX/",
      "type": "project",
      "from": "Cubey"
    },
    {
      "resource": "EgoX I2V LoRA",
      "url": "https://huggingface.co/DAVIAN-Robotics/EgoX/tree/main",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "OmniVCus model",
      "url": "https://huggingface.co/CaiYuanhao/OmniVCus/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "StoryMem repository",
      "url": "https://huggingface.co/Kevin-thu/StoryMem/tree/main",
      "type": "model",
      "from": "patientx"
    },
    {
      "resource": "LongCat Avatar example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/LongCatAvatar_audio_image_to_video_example_01.json",
      "type": "workflow",
      "from": "42hub"
    },
    {
      "resource": "SVI 2.0 Pro",
      "url": "https://github.com/vita-epfl/Stable-Video-Infinity/tree/svi_wan22",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "FlippinRad Motion Morph LoRA",
      "url": "https://civitai.com/models/2052865/flippinrad-motion-morph",
      "type": "model",
      "from": "42hub"
    },
    {
      "resource": "Morphic Frames to Video LoRA",
      "url": "https://huggingface.co/morphic/Wan2.2-frames-to-video",
      "type": "model",
      "from": "Relven 96gb"
    },
    {
      "resource": "Open-OmniVCus",
      "url": "https://github.com/caiyuanhao1998/Open-OmniVCus",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Wan-NVFP4 speed boost",
      "url": "https://huggingface.co/lightx2v/Wan-NVFP4",
      "type": "model",
      "from": "Zeku"
    },
    {
      "resource": "Lightx2v Distill Models",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Models/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Wan NVFP4 boost",
      "url": "https://huggingface.co/lightx2v/Wan-NVFP4",
      "type": "model",
      "from": "Zeku"
    },
    {
      "resource": "LightX2V Wan 2.2 LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Loras/tree/main",
      "type": "model",
      "from": "devnullblackcat"
    },
    {
      "resource": "Latest Wan 2.2 LightX2V LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Wan22_Lightx2v",
      "type": "model",
      "from": "devnullblackcat"
    },
    {
      "resource": "HiStream research paper",
      "url": "http://haonanqiu.com/projects/HiStream.html",
      "type": "research",
      "from": "Juampab12"
    },
    {
      "resource": "SVI 2.0 Pro model",
      "url": "https://huggingface.co/vita-video-gen/svi-model/tree/main/version-2.0",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "New distill LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Loras/tree/main",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "Wan VAE fp32",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1_VAE_fp32.safetensors",
      "type": "model",
      "from": "blake37"
    },
    {
      "resource": "Open-OmniVCus project",
      "url": "https://caiyuanhao1998.github.io/project/OmniVCus/",
      "type": "repo",
      "from": "cyncratic"
    },
    {
      "resource": "FreeLong Spectral Blending node",
      "url": "https://github.com/shootthesound/comfyUI-LongLook",
      "type": "tool",
      "from": "gaben3801"
    },
    {
      "resource": "SVI Pro converted LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Stable-Video-Infinity/v2.0",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Ultravico Wan 2.2 implementation",
      "url": "https://github.com/maybleMyers/H1111/blob/svipro/wan2_generate_video.py",
      "type": "code",
      "from": "Benjimon"
    },
    {
      "resource": "Smooth Mix Wan 2.2 model",
      "url": "https://civitai.com/models/1995784/smooth-mix-wan-22-i2vt2v-14b",
      "type": "model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "LightX2V LoRAs collection",
      "url": "https://wanx-troopers.github.io/loras/part-01.html",
      "type": "model",
      "from": "42hub"
    },
    {
      "resource": "FlippinRad Motion Morph LoRA",
      "url": "https://civitai.com/models/2052865/flippinrad-motion-morph",
      "type": "model",
      "from": "42hub"
    },
    {
      "resource": "Converted SVI LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Stable-Video-Infinity/v2.0",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Wan Alpha 2.0 DoRA converted",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/WanAlpha/Wan_Alpha_v2.0_DoRA_2.0_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "DITTO stylized VACE fine-tune",
      "url": "https://github.com/EzioBy/Ditto/blob/main/inference/example_prompts.txt",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "InsertAnywhere project",
      "url": "https://github.com/myyzzzoooo/InsertAnywhere",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Qwen Image Edit 2511 workflow for multi-view generation",
      "url": "",
      "type": "workflow",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "ComfyUI Clip Prompt Splitter node",
      "url": "https://github.com/elgalardi/comfyui-clip-prompt-splitter",
      "type": "tool",
      "from": "David Galardi"
    },
    {
      "resource": "Official SVI ComfyUI workflow",
      "url": "https://github.com/vita-epfl/Stable-Video-Infinity/tree/svi_wan22/comfyui_workflow",
      "type": "workflow",
      "from": "Tachyon"
    },
    {
      "resource": "Smoothmix model",
      "url": "https://civitai.com/models/1995784/smooth-mix-wan-22-i2vt2v-14b?modelVersionId=2260110",
      "type": "model",
      "from": "LukeG89"
    },
    {
      "resource": "SVI Pro native workflow",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1454805653599031358",
      "type": "workflow",
      "from": "LukeG89"
    },
    {
      "resource": "Motion scale control node demo",
      "url": "https://youtu.be/Zmkn6_vyMN8",
      "type": "tool",
      "from": "brbbbq"
    },
    {
      "resource": "ComfyUI-LongLook",
      "url": "https://github.com/shootthesound/comfyUI-LongLook",
      "type": "node",
      "from": "Ada"
    },
    {
      "resource": "ModelHunter workflow search tool",
      "url": "https://tr1dae.github.io/ModelHunter/",
      "type": "tool",
      "from": "Quality_Control"
    },
    {
      "resource": "SVI workflows collection",
      "url": "https://wanx-troopers.github.io/svi.html",
      "type": "workflow",
      "from": "42hub"
    },
    {
      "resource": "HY-Motion 1.0",
      "url": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "ComfyUI-PainterI2Vadvanced",
      "url": "https://github.com/princepainter/ComfyUI-PainterI2Vadvanced",
      "type": "repo",
      "from": "LukeG89"
    },
    {
      "resource": "SVI examples",
      "url": "https://wanx-troopers.github.io/svi.html",
      "type": "workflow",
      "from": "42hub"
    },
    {
      "resource": "Wan timeline and documentation",
      "url": "https://wanx-troopers.github.io/",
      "type": "resource",
      "from": "42hub"
    },
    {
      "resource": "ComfyUI-MultiPoseToolkit",
      "url": "https://github.com/Valiant-Cat/ComfyUI-MultiPoseToolkit",
      "type": "repo",
      "from": "harrisonwells"
    },
    {
      "resource": "ComfyUI-prompt-splitter",
      "url": "https://github.com/elgalardi/comfyui-prompt-splitter",
      "type": "node",
      "from": "David Galardi"
    },
    {
      "resource": "SCAIL information and notes",
      "url": "https://wanx-troopers.github.io/wan-animates.html#scail",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "Uni3C camera control documentation",
      "url": "https://wanx-troopers.github.io/control.html#uni3c",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "Context Windows documentation",
      "url": "https://wanx-troopers.github.io/what-plugs-where/context-windows.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "Advanced VACE workflow",
      "url": "https://wanx-troopers.github.io/wan-t2v-advanced.html#combining-latent-vace-masks-phantom-and-context-windows",
      "type": "workflow",
      "from": "42hub"
    },
    {
      "resource": "FreeLong workflow without proprietary nodes",
      "url": "",
      "type": "workflow",
      "from": "Ablejones"
    },
    {
      "resource": "Civitai status page",
      "url": "https://status.civitai.com/status/public",
      "type": "tool",
      "from": "TK_999"
    }
  ],
  "limitations": [
    {
      "limitation": "SteadyDancer lacks face control and long generation method",
      "details": "Makes it complex to use, not that useful without proper long generation",
      "from": "Kijai"
    },
    {
      "limitation": "SteadyDancer seems mainly made for TikTok dances",
      "details": "Limited use case, requires full weights rather than being modular",
      "from": "Kijai"
    },
    {
      "limitation": "Fun VACE 2.2 requires lot of resources",
      "details": "Resource intensive setup, not many people have tried it yet",
      "from": "David Snow"
    },
    {
      "limitation": "Current upscalers can't do 4K videos reasonably",
      "details": "SeedVR takes forever and crashes, none of the solutions handle 4K well",
      "from": "blake37"
    },
    {
      "limitation": "UltraViCo processes whole video at once",
      "details": "If you want to triple video length, you're looking to triple the VRAM cost. Methods like this do the whole video at once",
      "from": "Kijai"
    },
    {
      "limitation": "UltraViCo loses start image influence on very long videos",
      "details": "At 241 frames with ultravico, it seems to lose the start image influence",
      "from": "Kijai"
    },
    {
      "limitation": "Context stride mostly unusable",
      "details": "Context stride causes terrible stutter because it packs 4 frames into one latent, mostly just not used",
      "from": "Kijai"
    },
    {
      "limitation": "Small subjects in video generation",
      "details": "Very small subjects like tiny birds may not be recognized or animated properly, need to fill more of the frame",
      "from": "Juampab12"
    },
    {
      "limitation": "Fixed mask position prevents movement",
      "details": "When using crop&stitch with masks that stay in same position, it can prevent natural movement animation",
      "from": "Juampab12"
    },
    {
      "limitation": "Quantized diffusion models have error multiplication",
      "details": "Quantization errors can multiply each step, making quality degradation worse than other model types",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "limitation": "Adversarial training lacks reliability",
      "details": "Has ugly geometry/topology, super chaotic, tends to oscillate rather than converge",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "limitation": "Distillation reduces variety",
      "details": "Recent paper suggests distilled models have bad variety because their first step is too big, creates deep ruts for common outputs",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "limitation": "SVI 2.0 only works with I2V models, cannot be combined with T2V VACE",
      "details": "I2V models use extra channels for conditioning while VACE runs extra modules on T2V models, architectures incompatible",
      "from": "Kijai"
    },
    {
      "limitation": "Steadydancer has no face control and whole model was trained unfrozen",
      "details": "Also lacks training for extension/long generation, may not work with other methods due to new weights",
      "from": "Kijai"
    },
    {
      "limitation": "UltraViCo doesn't work well with I2V due to attention calls and block masking limitations",
      "details": "Fails with HuMo audio attention calls and input size restrictions",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.2 starts looping after 81 frames",
      "details": "Training limitation causes model to go backwards on generations longer than 81 frames without control signals",
      "from": "Kijai"
    },
    {
      "limitation": "HuMo embeds break after ~30 seconds when using context options",
      "details": "Model breaks completely after approximately 30 seconds with context windows",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "SVI-Shot outside trained regime causes stutter errors",
      "details": "Using 4 overlap frames (outside the regime that svi was trained for) occasionally results in stutter errors",
      "from": "Ablejones"
    },
    {
      "limitation": "ComfyUI memory management issues",
      "details": "Easy to accidentally trigger reload everything, VRAM management is squiffy",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "limitation": "SAM3 ComfyUI nodes lack optimization",
      "details": "They work but don't release VRAM quite as well as expected",
      "from": "pookyjuice"
    },
    {
      "limitation": "SVI 2.2 only supports single start frame",
      "details": "Unlike 2.1 which supported multiple start frames",
      "from": "Kijai"
    },
    {
      "limitation": "Perfect looping beyond 81 frames is challenging",
      "details": "Camera shake and jitter become problematic, context windows don't work reliably for many users",
      "from": "herpderpleton"
    },
    {
      "limitation": "VACE extensions degrade after 3-4 generations",
      "details": "Detail loss increases with each generation, practical limit around 200 frames",
      "from": "Juan Gea"
    },
    {
      "limitation": "SVI 2.0 can only carry over 3-5 frames depending on version",
      "details": "v2.0/SVI_Wan2.1 allows 5 frames, v2.0/SVI_Wan2.2 only allows 3, more causes artifacts",
      "from": "42hub"
    },
    {
      "limitation": "HuMo color shift issues with extensions",
      "details": "Really a HuMo problem because it's not truly an i2v model. Context windows instead of pure overlap extensions do better with this issue",
      "from": "Ablejones"
    },
    {
      "limitation": "Cannot directly use latent space for long video without degradation",
      "details": "Cannot simply take several last latents from previous generation due to WAN's encoding structure where 1st frame is encoded differently",
      "from": "42hub"
    },
    {
      "limitation": "SVI has bad prompt adherence",
      "details": "Prompt adherence is very bad with the SVI lora, gets flashes between 81 frame intervals sometimes, videos become degraded or heavily blurred after multiple generations",
      "from": "xwsswww"
    },
    {
      "limitation": "570 frames cannot work without context windows",
      "details": "Only works with wan animate embeds when using context window options",
      "from": "Kijai"
    },
    {
      "limitation": "OneToAll Animation missing endless generation",
      "details": "Currently only supports short clips, long generation method not yet implemented",
      "from": "Kijai"
    },
    {
      "limitation": "SVI models hard to break away from reference",
      "details": "Feel muted until you push them to prompting threshold, though new ones are better",
      "from": "Ablejones"
    },
    {
      "limitation": "Wan token limit around 200",
      "details": "Approximately 170-180 words maximum for prompts",
      "from": "Zabo"
    },
    {
      "limitation": "OneToAll turning around still not good",
      "details": "While better at pose retargeting, still struggles with character rotation",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "Kung fu motion capture challenges",
      "details": "Large number of motion-blurred frames make kung fu videos challenging",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "OneToAll has low motion issue",
      "details": "Model seems to have low motion problems, may need specific settings",
      "from": "Kijai"
    },
    {
      "limitation": "OneToAll burns with long sequences",
      "details": "613 frames still burns, especially with lightx2v. Quality loss when reducing burning",
      "from": "Kijai"
    },
    {
      "limitation": "Pose aligner fails spectacularly often",
      "details": "Pose alignment frequently fails in OneToAll workflow",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows work worse overall",
      "details": "While they don't degrade, context windows work worse than extension method overall",
      "from": "Kijai"
    },
    {
      "limitation": "WanAnimate is one character only",
      "details": "Current limitation compared to newer models that can handle multiple characters",
      "from": "42hub"
    },
    {
      "limitation": "Ovi audio quality not great",
      "details": "Audio output from Ovi model is subpar, cannot condition voice/ref voice yet",
      "from": "Charlie"
    },
    {
      "limitation": "WanMove trajectory preview dots misleading",
      "details": "Dot size doesn't indicate accuracy level, just visual representation",
      "from": "Kijai"
    },
    {
      "limitation": "Multiple splines in editor broken",
      "details": "Bug in spline editor when using multiple splines gives wrong frame count",
      "from": "Kijai"
    },
    {
      "limitation": "Fp8 scaled models broken with recent ComfyUI update",
      "details": "Recent quantization update causes issues with fp8 scaled models",
      "from": "Kijai"
    },
    {
      "limitation": "HuMo only does 5 seconds",
      "details": "HuMo limited to 5 second generations, has context window issues",
      "from": "Charlie"
    },
    {
      "limitation": "SVI doesn't work well with very different aspect ratios",
      "details": "Trained at 480\u00d7832 horizontal, weak motion/dynamics with vertical aspect ratios especially full-body person",
      "from": "xiver2114"
    },
    {
      "limitation": "HuMo unhappy being used as I2V",
      "details": "Creates transition issues and contrast problems when used improperly",
      "from": "Ablejones"
    },
    {
      "limitation": "SVI limits camera movement",
      "details": "Keeps pushing camera back towards reference image, limiting dynamic camera movement",
      "from": "Vardogr"
    },
    {
      "limitation": "Wan 2.2 has no film version",
      "details": "Only designed for static shots, equivalent of 'shot' version for 2.1",
      "from": "DawnII"
    },
    {
      "limitation": "Lightning loras remove ability to do dark scenes",
      "details": "Make everything bright, particularly with T2V",
      "from": "Kijai"
    },
    {
      "limitation": "SVI suppresses motion quite a bit",
      "details": "When trying with motion, the lora reduces movement significantly",
      "from": "Ablejones"
    },
    {
      "limitation": "HuMo I2V artifacts show up in first 5 frames",
      "details": "Most artifacts appear at beginning of generation",
      "from": "Ablejones"
    },
    {
      "limitation": "Wan Move does weird stuff when pushed too far",
      "details": "Limitations when using extreme parameters",
      "from": "ingi // SYSTMS"
    },
    {
      "limitation": "SCAIL model doesn't work well with face-only input",
      "details": "Needs more body content to function properly, trained for 896x512 resolution",
      "from": "Kijai"
    },
    {
      "limitation": "SVI doesn't work with models it wasn't trained for",
      "details": "Remains stubborn and refuses to cooperate with HuMo",
      "from": "42hub"
    },
    {
      "limitation": "SVI kills prompts",
      "details": "There has to be a cost of pushing the length",
      "from": "Zabo"
    },
    {
      "limitation": "SCAIL multi-person motion capture not implemented",
      "details": "Multi-person motion capture doesn't seem to be implemented yet",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "SCAIL pose detection not fully implemented",
      "details": "None of the pose detection is implemented - just recolored skeleton. Needs 3D libraries for full implementation",
      "from": "Kijai"
    },
    {
      "limitation": "Frame count limitations with background dynamics",
      "details": "If frame count exceeds 81, background dynamics may become unavailable and background might freeze",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "NLF research-only license",
      "details": "NLF model has research-only license meaning can't be used officially in commercial capacity",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL only supports full or half pose resolution",
      "details": "Currently only full or half size pose inputs are supported",
      "from": "Kijai"
    },
    {
      "limitation": "VACE can only use single reference image",
      "details": "The model can only use one reference image, multiple images get concatenated but model limitation remains",
      "from": "Kijai"
    },
    {
      "limitation": "WanMove can't be used simultaneously with VACE",
      "details": "Incompatible control systems",
      "from": "Dream Making"
    },
    {
      "limitation": "Pose retargeting works poorly unless driving pose and reference pose are same",
      "details": "Feature exists in WanAnimate preprocessor but limited effectiveness",
      "from": "Kijai"
    },
    {
      "limitation": "Multi-person SCAIL doesn't have automatic pose alignment",
      "details": "No align code in original multi-people pipeline",
      "from": "Kijai"
    },
    {
      "limitation": "ComfyUI-RMBG nodes break multiple other functionalities",
      "details": "Globally override torch functions affecting NLF and other models",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL multi-person detection not fully complete",
      "details": "DWPose connection causes issues, feature still in development",
      "from": "Kijai"
    },
    {
      "limitation": "NLF pose predictor only works for humans",
      "details": "Cannot detect skeletons for animals or non-human subjects",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL has hand drifting issues in retargeting",
      "details": "Hands scale and drift, especially at distance from center",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "Kandinsky 24fps makes it much slower",
      "details": "Higher frame rate impacts generation speed significantly",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL only works with human or human-like poses",
      "details": "NLF requires human-like poses for 3D capture, animals would need other methods like MocapAnything",
      "from": "teal024"
    },
    {
      "limitation": "Wan 2.6 is cloud-only",
      "details": "New 50B model only available on Alibaba Cloud, not for local use",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "limitation": "Context window causes inconsistencies",
      "details": "Using context windows for long videos can make results inconsistent at transition points",
      "from": "Kijai"
    },
    {
      "limitation": "Resolution stretching at high resolutions",
      "details": "Going bigger than 1080p can cause limb stretching, especially noticeable in 9:16 generations",
      "from": "ingi // SYSTMS"
    },
    {
      "limitation": "SCAIL missing facial performance replication",
      "details": "Retains ID better than WanAnimate but lacks facial performance replication",
      "from": "A.I.Warper"
    },
    {
      "limitation": "NLF prediction CUDA-only",
      "details": "Hardcoded to use CUDA, won't work with CPU or ROCm without rebuilding repos",
      "from": "Kijai"
    },
    {
      "limitation": "Lightning LoRAs reduce motion variety",
      "details": "Give better detail quality but less motion/structure variety due to distillation process",
      "from": "spacepxl"
    },
    {
      "limitation": "Wan 2.6 not open source",
      "details": "Makes it a dead end for many users, can't modify fps or other parameters",
      "from": "Ruairi Robinson"
    },
    {
      "limitation": "DEIS sampler instability",
      "details": "Throws errors during generation, other samplers work fine",
      "from": "Dream Making"
    },
    {
      "limitation": "WanVideoSampler resolution restrictions",
      "details": "Won't do standard TikTok or other common resolutions",
      "from": "Mazrael.Shib"
    },
    {
      "limitation": "Wan 2.6 heavily censored",
      "details": "Omega ultra super duper censored, rejects basic action words, aimed at stopping all conflict",
      "from": "Zabo"
    },
    {
      "limitation": "SCAIL multi-person tracking broken",
      "details": "Can't properly handle multiple people without masking, hands/face jump between people",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL only works with pose control",
      "details": "Cannot use depth maps, only trained for pose skeleton control",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.2 motion consistency issues",
      "details": "Makes things too fast or more frequently too slow, doesn't understand basic motion properly",
      "from": "David Snow"
    },
    {
      "limitation": "Patch size 32 models not good yet",
      "details": "Current models with patch size 32 like Wan 5B, LTX models, Sana aren't optimal",
      "from": "spacepxl"
    },
    {
      "limitation": "SCAIL context windows don't work well with moving cameras",
      "details": "Causes background shift and inconsistencies",
      "from": "Kijai"
    },
    {
      "limitation": "Vitpose only detects one person",
      "details": "Cannot handle multiple people in pose detection",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL doesn't support separate first frame input",
      "details": "Reference image is the start frame, cannot provide different starting frame",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.5+ will remain closed source",
      "details": "Despite team wanting to open source, external forces prevent it",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "SCAIL has issues with close-up face detection",
      "details": "Compared to WanAnimate, SCAIL struggles with facial close-ups and pose detection",
      "from": "xwsswww"
    },
    {
      "limitation": "No proper strength control in WanVideoWrapper",
      "details": "Only has latent multiplier, doesn't work as well as expected",
      "from": "Kijai"
    },
    {
      "limitation": "Context windowing causes background drift",
      "details": "Long generations suffer from background changes due to reference image limitations",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL struggles with non-humanoid shapes",
      "details": "Triangle with arms and head shape not detected properly by pose estimation",
      "from": "amli"
    },
    {
      "limitation": "Wan 2.1 doesn't work well with First-Last-Frame",
      "details": "Only 2.2 works properly with FLF, 2.1 support exists but quality is poor",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL has context window problem with long generations",
      "details": "Background not stable enough for long gen, WanAnimate more stable for long gens",
      "from": "Juan Gea"
    },
    {
      "limitation": "SCAIL cannot do faces very well",
      "details": "Faces are a weak point compared to other models",
      "from": "AIGambino"
    },
    {
      "limitation": "Wan 2.2 low denoise causes ComfyUI crashes with too many frames",
      "details": "Too slow and unstable for long sequences",
      "from": "NodeMancer"
    },
    {
      "limitation": "1.3B tile LoRA adds color shift/flashing around 3 second mark",
      "details": "Makes it unusable despite adding significant detail and good 720p to 1080p upscaling",
      "from": "David Snow"
    },
    {
      "limitation": "No controlnets available for Wan 2.2 I2V",
      "details": "No LoRAs, modules, or anything for pose control specifically for 2.2 I2V model",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "Wan 2.1 and 2.2 limited to 81 frames maximum",
      "details": "16fps in their best quality 14B versions, longer videos remain an open problem",
      "from": "42hub"
    },
    {
      "limitation": "Context windows problematic for I2V",
      "details": "Initial frame gets screwed up as window moves, making I2V with context windows not feasible",
      "from": "42hub"
    },
    {
      "limitation": "LongCat Avatar very slow without distill",
      "details": "Single window takes 10 minutes with 20 steps, also runs at 16fps which looks weird for lipsync",
      "from": "Kijai"
    },
    {
      "limitation": "SVI kills movement and prompt adherence",
      "details": "Trade-off for video stability - reduces dynamic movement and prompt following",
      "from": "Zabo"
    },
    {
      "limitation": "WanAnimate model limitation with detailed masks",
      "details": "Too detailed masks completely break down the model",
      "from": "Kijai"
    },
    {
      "limitation": "Animal pose not compatible with SCAIL retargeter",
      "details": "Cannot directly use animal pose nodes with SCAIL's pose retargeting system",
      "from": "Kijai"
    },
    {
      "limitation": "Latent masking limited with Wan 2.2",
      "details": "4x compression means one mask per 4 frames when using latent masking",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL does not support inpainting with mask",
      "details": "Unlike WanAnimate, SCAIL cannot do masked inpainting",
      "from": "xwsswww"
    },
    {
      "limitation": "FlashPortrait not useful for short generations",
      "details": "Seems made for long generation method only, worse than FantasyPortrait for single short clips",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL struggles with exaggerated character proportions",
      "details": "While SCAIL works well generally, it fails to reconfigure characters with super exaggerated proportions",
      "from": "amli"
    },
    {
      "limitation": "First frame quality issues with reference models",
      "details": "Noisy first frame/flash occurs at boundary between provided first frame and subsequent generated frames",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "LongCat Avatar doesn't support Wan LoRAs",
      "details": "It's not a Wan model - based on Wan architecture but trained from scratch, so no Wan-specific models/LoRAs work",
      "from": "Kijai"
    },
    {
      "limitation": "LongCat Avatar degradation after 20 seconds",
      "details": "Even without distill LoRA, degradation occurs after 20 seconds with 40 steps and CFG 3",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "No multi-character support in LongCat Avatar",
      "details": "Currently cannot handle multiple characters talking or complex masking scenarios",
      "from": "Kijai"
    },
    {
      "limitation": "South Park style doesn't work well in Ditto",
      "details": "Despite ranking high in training dataset analysis, South Park style fails in practice, possibly due to hard style or censoring",
      "from": "JohnDopamine"
    },
    {
      "limitation": "LongCat-Avatar only works with Chinese wav2vec model",
      "details": "Facebook English model doesn't work because it's trained with the Chinese one",
      "from": "Kijai"
    },
    {
      "limitation": "Face quality degrades with distill LoRA and low steps",
      "details": "Hands become horror show, face goes crazy towards the end",
      "from": "Kijai"
    },
    {
      "limitation": "TeaCache and MagCache don't work with LongCat",
      "details": "Tuned per model so wouldn't work properly, EasyCache is model agnostic",
      "from": "Kijai"
    },
    {
      "limitation": "EgoX limited resolution and frames",
      "details": "Trained on 448x448(ego), 448x784(exo) resolutions and 49 frames only",
      "from": "Kijai"
    },
    {
      "limitation": "Extreme values cause fp16 issues",
      "details": "Values like -8716288.0 to 2375680.0 in ffn output explain why fp16 doesn't work",
      "from": "Kijai"
    },
    {
      "limitation": "Character loses likeness in face area at distance",
      "details": "Main issue noticed when using 3D model animated in Blender - character loses facial likeness in output video when at reasonable distance from camera, while clothes remain consistent",
      "from": "xwsswww"
    },
    {
      "limitation": "LongCat Avatar doesn't handle absolute silence",
      "details": "Model requires some noise after vocal extraction, can't process completely silent audio",
      "from": "Kijai"
    },
    {
      "limitation": "Wan TTM completely ignores uni3c",
      "details": "User reports that Wan TTM shows very little camera movement and doesn't respond well to uni3c control",
      "from": "Dream Making"
    },
    {
      "limitation": "StoryMem quality deflation",
      "details": "Looking at more examples shows the quality isn't as impressive as initial previews suggested",
      "from": "Cubey"
    },
    {
      "limitation": "WAN models are too slow animation-wise",
      "details": "Common problem with wan models regarding animation speed",
      "from": "David Snow"
    },
    {
      "limitation": "SCAIL cannot combine with depth control",
      "details": "Chicken and egg problem - can't have depth without already having character moving, and no way to use SCAIL with depth control",
      "from": "Kijai"
    },
    {
      "limitation": "Wan-Alpha v2.0 not working in ComfyUI",
      "details": "Attempts to implement in ComfyUI unsuccessful, model loads but doesn't do anything",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "LongCat Avatar degrades after 3 extensions",
      "details": "Lip-sync gives up and image becomes wonky after multiple time extensions",
      "from": "NC17z"
    },
    {
      "limitation": "OmniVCus is VACE module",
      "details": "Loads without issues but doesn't do anything as is, needs new code to function properly",
      "from": "Kijai"
    },
    {
      "limitation": "SCAIL doesn't work well with partial body videos",
      "details": "Needs full body for pose to work properly, struggles with hip-up or bust-up videos",
      "from": "Hot Hams, the God of Meats"
    },
    {
      "limitation": "Enhance-A-Video has memory issues with compile",
      "details": "Can cause higher than expected memory usage when enabled",
      "from": "Kijai"
    },
    {
      "limitation": "Can't avoid fade with overlap in windowed context",
      "details": "Transitions between different prompts will always have fade effect due to overlap",
      "from": "Kijai"
    },
    {
      "limitation": "Turbodiffusion speed limited to that model itself",
      "details": "Potentially twice as fast as lightx2v + sageattn but only works with specific model",
      "from": "Kijai"
    },
    {
      "limitation": "SageAttention 3 causes visible proportion distortion",
      "details": "Tested on Wan 2.2 T2V - pilot's head bigger than helicopter cockpit",
      "from": "slmonker"
    },
    {
      "limitation": "VACE doesn't work properly with I2V models",
      "details": "Even when forced, it ruins the output",
      "from": "Kijai"
    },
    {
      "limitation": "T2V methods don't work well with WanAnimate",
      "details": "WanAnimate is I2V based so T2V methods like Stand-in are incompatible",
      "from": "Kijai"
    },
    {
      "limitation": "3090 insufficient for large LoRA training",
      "details": "1k videos at 121 frames likely won't work on 3090",
      "from": "CJ"
    },
    {
      "limitation": "SVI Pro degrades with major scene changes",
      "details": "Original anchor frame becomes less relevant when scene changes significantly",
      "from": "Kijai"
    },
    {
      "limitation": "Apple Silicon compatibility poor for video models",
      "details": "Most video models except HV 1.5 AIO and WAN 2.2 AIO take too long or have device errors",
      "from": "buggz"
    },
    {
      "limitation": "UltraVico doesn't work well with I2V",
      "details": "Loses reference/start image effect after normal length, changes characters unexpectedly",
      "from": "Kijai"
    },
    {
      "limitation": "fp8 quality issues with Kandinsky",
      "details": "fp8 weights perform poorly with Kandinsky model",
      "from": "Benjimon"
    },
    {
      "limitation": "SVI Pro has difficulty with prompt adherence",
      "details": "Hard time following simple prompts compared to regular generation",
      "from": "Kijai"
    },
    {
      "limitation": "SVI Pro designed to stay close to start image",
      "details": "Wouldn't work well if it moved away from initial reference, by design",
      "from": "Kijai"
    },
    {
      "limitation": "Cannot combine T2V and I2V in same workflow",
      "details": "Different channel dimensions in latent information cause crashes",
      "from": "DawnII"
    },
    {
      "limitation": "FreeLong implementation issues",
      "details": "Hard coded to 81 frame latents, doesn't work for longer generations",
      "from": "Ablejones"
    },
    {
      "limitation": "Character sheet degrades over time",
      "details": "When using character sheets with SVI Pro, quality degrades in extended sequences",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "SVI degradation after 20-30 seconds",
      "details": "Most models degrade too much after 20-30 seconds of generation",
      "from": "Kijai"
    },
    {
      "limitation": "SVI prompt following issues",
      "details": "Prompt following suffers with SVI, especially camera movement prompts due to anchor image fighting the prompt",
      "from": "Kijai"
    },
    {
      "limitation": "Ultravico subject constraint",
      "details": "0.95 alpha for ultravico doesn't allow subject to leave the frame",
      "from": "Kijai"
    },
    {
      "limitation": "HuMo flash artifacts with SVI",
      "details": "Using HuMo with SVI produces annoying flash artifacts",
      "from": "Ablejones"
    },
    {
      "limitation": "Quality degradation with longer SVI generations",
      "details": "Obvious quality degradation occurs as generation duration increases",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "SVI Pro not good at multiple subjects interacting",
      "details": "Very RNG when trying to generate multiple subjects interacting with each other",
      "from": "Tachyon"
    },
    {
      "limitation": "Camera movement causes issues with SVI Pro",
      "details": "Jumps back to original frame when camera moves like zoom-in",
      "from": "Cseti"
    },
    {
      "limitation": "Prompt adherence degrades after 20 seconds",
      "details": "After 20 seconds of extension, prompt following becomes difficult regardless of prompting",
      "from": "NebSH"
    },
    {
      "limitation": "Ultravico has reliability issues",
      "details": "Not convinced on the reliability of ultravico for consistent results",
      "from": "Kijai"
    },
    {
      "limitation": "Motion scale can ruin images at higher values",
      "details": "1.3 usually completely ruins the image, mainly turning blue or green",
      "from": "Cubey"
    },
    {
      "limitation": "Wan tends towards slow motion",
      "details": "By far its biggest weakness according to users",
      "from": "David Snow"
    },
    {
      "limitation": "SVI 2.0 Pro causes color drift in long videos",
      "details": "Video begins to darken continuously from 10-second mark, progressively worsens",
      "from": "BNP4535353"
    },
    {
      "limitation": "Scaling RoPE leads to unwanted behavior",
      "details": "Often ruins motion as well, never worked great with Wan",
      "from": "Kijai"
    },
    {
      "limitation": "Fun VACE 2.2 struggles with inpainting",
      "details": "Compared to original 2.1 VACE which is better for inpainting",
      "from": "Ablejones"
    },
    {
      "limitation": "SCAIL cannot reproduce facial expressions",
      "details": "Unlike WanAnimate which has this capability",
      "from": "42hub"
    },
    {
      "limitation": "SCAIL can only generate >81 frames via Context Windows",
      "details": "Not trained with beyond-81-frames capability, leading to background shift issues",
      "from": "42hub"
    },
    {
      "limitation": "FreeLong massively increases generation time",
      "details": "Makes many extra model calls during generation",
      "from": "Ablejones"
    },
    {
      "limitation": "Action scenes where subject leaves frame cause everything to change",
      "details": "Prompting dynamic scenes with characters exiting frame leads to poor results",
      "from": "David Snow"
    }
  ],
  "hardware": [
    {
      "requirement": "Fun VACE VRAM usage",
      "details": "VRAM dropped by about 10% after wrapper update, still a RAM/VRAM killer",
      "from": "David Snow"
    },
    {
      "requirement": "Context windows performance",
      "details": "Performance 5x slower without torch compile, 1024x576/105 frames went from 2:05 to over 10 minutes",
      "from": "N0NSens"
    },
    {
      "requirement": "161 frames generation",
      "details": "Works but low pass will OOM if using TTM",
      "from": "Cubey"
    },
    {
      "requirement": "VRAM improvements with wrapper",
      "details": "NVIDIA 30 series cards seeing reduced VRAM usage even without torch compile after PyTorch 2.9.1 upgrade",
      "from": "garbus"
    },
    {
      "requirement": "VRAM for UltraViCo",
      "details": "Triple video length means triple VRAM cost as it processes whole video at once",
      "from": "Kijai"
    },
    {
      "requirement": "4090 block size limitation",
      "details": "Block size 128 isn't supported on 4090 hardware level, need to use 64",
      "from": "Kijai"
    },
    {
      "requirement": "96G VRAM for VACE workflows",
      "details": "NodeMancer uses cloud.comfy with 96G VRAM for VACE workflows",
      "from": "NodeMancer"
    },
    {
      "requirement": "First frame memory usage",
      "details": "1st frame takes up 4x memory compared to rest of frames in Wan VAE",
      "from": "Scruffy"
    },
    {
      "requirement": "VRAM management for 30GB fp16 models",
      "details": "Use Q8 GGUF with distorch model loaders for better control over VRAM/RAM allocation instead of native loader",
      "from": "David Snow"
    },
    {
      "requirement": "4090 VRAM usage",
      "details": "Steadydancer example workflow uses under 10GB VRAM unmodified",
      "from": "Kijai"
    },
    {
      "requirement": "RAM requirements for video generation",
      "details": "32GB minimum, 64GB preferred, some have 96GB or 128GB due to memory leaks and data swapping",
      "from": "42hub"
    },
    {
      "requirement": "Block size compatibility",
      "details": "Default 128 block size not supported on 4090 hardware level, needs modification",
      "from": "Kijai"
    },
    {
      "requirement": "81 frames on 5090 taking 15+ minutes",
      "details": "Wan animate with sage attn on 5090 RunPod taking 15 minutes or more for 81 frames generation",
      "from": "stas"
    },
    {
      "requirement": "RAM limitations for longer generations",
      "details": "Running out of standard RAM in latent space for longer video generations",
      "from": "topmass"
    },
    {
      "requirement": "PyTorch 2.9.1+cu130",
      "details": "Required to fix memory leak issues with resize nodes",
      "from": "spacepxl"
    },
    {
      "requirement": "Memory usage with 4K videos",
      "details": "ComfyUI not great with 4K videos in general",
      "from": "Kijai"
    },
    {
      "requirement": "Gen time improvement with torch upgrade",
      "details": "Gen time went from 21 minutes to 13 minutes after updating torch 2.8 cu128 to torch 2.9.1 cu130",
      "from": "Persoon"
    },
    {
      "requirement": "HuMo VRAM requirements",
      "details": "1280x720 causes OOM, 1024x576 works on unspecified VRAM amount",
      "from": "AmirKerr"
    },
    {
      "requirement": "SVI upscale node memory leak on 16GB",
      "details": "Run once results are good, run again gets blobby and noisy on 5060ti 16GB due to VRAM leak",
      "from": "Scruffy"
    },
    {
      "requirement": "MAGI memory requirements",
      "details": "4.5B-distill+fp8_quant model with window_size=1 works on GPUs with at least 12GB memory",
      "from": "Gleb Tretyak"
    },
    {
      "requirement": "OneToAll Animation generation time",
      "details": "~15 minutes on 4090 at 576x1024 with 30 steps, ~2 minutes with LightX2V",
      "from": "Kijai"
    },
    {
      "requirement": "Low-end GPU compatibility",
      "details": "Even Wan 14B is a lot for low-end GPUs, but will work just slower (potentially 1 hour 45 minutes per prompt on 8GB)",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "requirement": "FP8 E5 support",
      "details": "Only works on 3090+ or with latest triton-windows update, fixed since October",
      "from": "Kijai"
    },
    {
      "requirement": "OneToAll VRAM usage",
      "details": "832x480x81 frames easily under 12GB VRAM",
      "from": "Kijai"
    },
    {
      "requirement": "Live Avatar real-time requirement",
      "details": "20 FPS requires 5\u00d7H800 GPUs (~$150k) with 4-step sampling",
      "from": "David Snow"
    },
    {
      "requirement": "Memory issues with 81 frames 512x",
      "details": "Hits memory problems even with fp8 model and 30 block swap due to VAE conv3d",
      "from": "patientx"
    },
    {
      "requirement": "OneToAll model size",
      "details": "37GB model too large for single 5090, fp8 version available at 18GB",
      "from": "topmass"
    },
    {
      "requirement": "VRAM for fp8 models",
      "details": "Fp8 versions significantly reduce VRAM requirements, 18GB vs 37GB for OneToAll",
      "from": "Josiah"
    },
    {
      "requirement": "Wan 2.2 VRAM usage on 5090",
      "details": "Using 98% VRAM and 98% RAM, 65-83GB RAM usage with Q8 models",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "Wan 2.2 RAM requirements",
      "details": "Need to juggle 2x 27GB models + 10GB text encoder, RAM is bigger issue than VRAM",
      "from": "Kijai"
    },
    {
      "requirement": "5090 + 128GB RAM usage",
      "details": "Some users report 20% RAM usage, others 80%+ - varies significantly by workflow and models used",
      "from": "FlipYourBits"
    },
    {
      "requirement": "128GB RAM for Q8 GGUF",
      "details": "Allows pushing to 97 frames with no OOM or VRAM clearing needed",
      "from": "metaphysician"
    },
    {
      "requirement": "SageAttention performance improvement",
      "details": "5090 user reports 5-second videos in under 2 minutes, 960x960 fp16 videos in 4 minutes, down from 10-50 minutes",
      "from": "ComfyCod3r"
    },
    {
      "requirement": "4090 performance with offloading",
      "details": "Can create 1280x544 video in about 300 seconds using RAM offloading",
      "from": "David Snow"
    },
    {
      "requirement": "SCAIL RAM usage",
      "details": "RAM use is same as Wan 14B in general",
      "from": "Kijai"
    },
    {
      "requirement": "SCAIL VRAM with context windows",
      "details": "With 20 blocks swapped on 4090: Max allocated 16.862 GB, Max reserved 21.594 GB, but very slow (24 minutes for 6 steps)",
      "from": "Kijai"
    },
    {
      "requirement": "RAM pricing trend",
      "details": "Kingston FURY Beast 64GB (2x32GB) 3200MHz DDR4 was \u00a3160, went to \u00a3300, now \u00a3370 - prices rising rapidly",
      "from": "David Snow"
    },
    {
      "requirement": "fp8 format for different GPUs",
      "details": "e4m3fn for Blackwell architecture, e5m2 for older GPUs like 3090",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "requirement": "WanMove VRAM usage",
      "details": "Same memory requirements as Wan 2.1 14B I2V model",
      "from": "Kijai"
    },
    {
      "requirement": "RAM vs VRAM impact with merge_loras",
      "details": "Merged loras moved to RAM at end, unmerged just removed - affects transfer times",
      "from": "Kijai"
    },
    {
      "requirement": "LoRA extraction speed improvement",
      "details": "Went from 1 hour to 77 seconds with low_rank algorithm",
      "from": "Gleb Tretyak"
    },
    {
      "requirement": "RAM pricing impact",
      "details": "128GB RAM kit tripled in price over few months, affecting accessibility",
      "from": "blake37"
    },
    {
      "requirement": "VRAM usage for SCAIL",
      "details": "640x1024, 201 frames, 6 steps, fp8_scaled with 20/40 block swap: Max 14.597 GB allocated, 17.750 GB reserved",
      "from": "Kijai"
    },
    {
      "requirement": "Increased VRAM for pose conditioning",
      "details": "Pose conditioning takes more VRAM than normal generation, may require increased block swap",
      "from": "Kijai"
    },
    {
      "requirement": "NLF warmup time",
      "details": "Takes about 10 seconds for torchscript model warmup",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM usage",
      "details": "1024*512x81 frames wan 2.2 t2v uses significant VRAM during native ksampler",
      "from": "hicho"
    },
    {
      "requirement": "H100 optimization",
      "details": "Use lightx2v lora, sageattn, fp16 precision, consider torch.compile if out of vram",
      "from": "FL13"
    },
    {
      "requirement": "16GB VRAM capabilities",
      "details": "Can handle higher resolutions with 128GB RAM",
      "from": "Gleb Tretyak"
    },
    {
      "requirement": "SCAIL inference speed",
      "details": "1h10m for 5s 720p video with high CFG settings",
      "from": "Juampab12"
    },
    {
      "requirement": "SCAIL torchscript warmup",
      "details": "Requires profiling run causing annoying warm-up time",
      "from": "Kijai"
    },
    {
      "requirement": "Sparse attention hardware limits",
      "details": "Custom kernels will probably limit hardware compatibility",
      "from": "Kijai"
    },
    {
      "requirement": "SCAIL generation time",
      "details": "50 minutes for FastWan with RX 6800, 2 steps",
      "from": "patientx"
    },
    {
      "requirement": "Single 1080p 15sec video",
      "details": "Takes 35-45 minutes to generate",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "requirement": "Cloud GPU pricing",
      "details": "RTX 6000 96GB available around $1.2-1.5/hour on Vast",
      "from": "PeacewalkerOTHV"
    },
    {
      "requirement": "SCAIL VRAM usage",
      "details": "Around 16GB used at default resolution with half blocks swapped on 24GB cards",
      "from": "Kijai"
    },
    {
      "requirement": "Uni3c performance impact",
      "details": "Adds about 1s/iteration at 896x640, 10-20% inference speed hit when offloading",
      "from": "Kijai"
    },
    {
      "requirement": "DF11 performance",
      "details": "5-20% slower than fp8, faster than bf16, can only be faster if makes model fit VRAM fully",
      "from": "Kijai"
    },
    {
      "requirement": "3090 24GB limitations",
      "details": "Nearly obsolete for current workflows, struggles with SCAIL at higher resolutions",
      "from": "TimHannan"
    },
    {
      "requirement": "VRAM usage with multiple models",
      "details": "Running 3+1 different models in one workflow heavily fills SSD and requires significant VRAM",
      "from": "Gleb Tretyak"
    },
    {
      "requirement": "ComfyUI attention performance on AMD",
      "details": "Quad-cross-attention 3-5x faster than SDP on AMD 6800 with new ROCm libraries",
      "from": "patientx"
    },
    {
      "requirement": "RAM usage increase after ComfyUI update",
      "details": "96GB RAM no longer sufficient for Wan 2.2 with 5 LoRAs on RTX 6000, requires disabling merge/low mem load",
      "from": "Hoernchen"
    },
    {
      "requirement": "128GB RAM insufficient for complex workflows",
      "details": "User with RTX 6000 and 128GB RAM hitting limits with complex multi-model workflows",
      "from": "boorayjenkins"
    },
    {
      "requirement": "Model quantization speed",
      "details": "Quantizing models takes less than five minutes",
      "from": "cyncratic"
    },
    {
      "requirement": "LongVie compute requirements",
      "details": "Generate 5s video clip takes ~8-9 minutes on a single A100 GPU",
      "from": "JohnDopamine"
    },
    {
      "requirement": "LongCat Avatar VRAM",
      "details": "Requires blockswap even on RTX 5090 due to 31GB model size. Cannot run BF16 without blockswap on consumer hardware",
      "from": "burgstall"
    },
    {
      "requirement": "LongCat Avatar generation time",
      "details": "Every 5 seconds takes about 2:55 on RTX 5090 (12 steps), audio_cfg doubles the time",
      "from": "Kijai"
    },
    {
      "requirement": "LongCat Avatar with blockswap",
      "details": "Works at 960x960x97 frames with blockswap set to 20, VRAM at 99%",
      "from": "burgstall"
    },
    {
      "requirement": "LongCat-Avatar VRAM",
      "details": "640x352x93fr takes 45 minutes on 16GB VRAM, more about RAM (64GB works)",
      "from": "N0NSens"
    },
    {
      "requirement": "5090 performance",
      "details": "960x960 took 525.75sec with 20 blockswap, 480x480x141 works, 301 frames took 560sec",
      "from": "burgstall"
    },
    {
      "requirement": "Chunked FFN benefit",
      "details": "Drops peak VRAM by ~2GB without visible speed loss on LongCat",
      "from": "Kijai"
    },
    {
      "requirement": "RAM prices",
      "details": "128GB DDR5 was \u00a3360, now almost \u00a31000; 64GB around \u20ac700",
      "from": "Charlie"
    },
    {
      "requirement": "5090 performance with WanMove",
      "details": "832x480 for 801 frames took 16 minutes to generate, ~25 seconds per 5 seconds at 640x352 with lightx2v",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 3090 LongCat compatibility",
      "details": "Requires specific setup with bf16 base precision, SageAttention 2.2.0, and proper model quantization settings",
      "from": "Kijai"
    },
    {
      "requirement": "4090 Full HD generation capability",
      "details": "Can generate 1920p ultrawide directly, VRAM hit 97% but worked, significantly improved performance on Windows 11 vs Windows 10",
      "from": "David Snow"
    },
    {
      "requirement": "Python version for RTX 3090",
      "details": "Had to downgrade to python 3.12 within windows_portable to have latest Triton for RTX 3090 e4m3fn support",
      "from": "NC17z"
    },
    {
      "requirement": "VRAM for long video decode",
      "details": "1080p 700 frames causes OOM on decode, 1280x720 works without OOM",
      "from": "boorayjenkins"
    },
    {
      "requirement": "48GB VRAM + 50GB RAM for Wan 2.2 fp16",
      "details": "A6000 setup may work but will be slower than RTX5090, block swap helps but reduces speed",
      "from": "42hub"
    },
    {
      "requirement": "RTX 5090 performance",
      "details": "3x faster than RTX 3090 for Wan, RTX 4090 is 2x faster than 3090",
      "from": "Benjimon"
    },
    {
      "requirement": "A6000 performance",
      "details": "Miserably slow, block swap cuts down speed a lot, not recommended for inference",
      "from": "MysteryShack"
    },
    {
      "requirement": "Wan 2.5/2.6 size concerns",
      "details": "Models are probably much larger and may not be useful for consumer hardware",
      "from": "Kijai"
    },
    {
      "requirement": "Some open source models need 96GB VRAM",
      "details": "Making open sourcing less meaningful for most users",
      "from": "wallen"
    },
    {
      "requirement": "Activated parameter count in newer Wan models",
      "details": "Won't be massive - Wan team considered community hardware limitations",
      "from": "slmonker"
    },
    {
      "requirement": "LoRA training VRAM",
      "details": "32GB VRAM (5090 equivalent) for 1k videos at 121 frames, possibly 24GB with max block swap but slow",
      "from": "CJ"
    },
    {
      "requirement": "SageAttention 2.2 speed improvement",
      "details": "Around 1 second improvement per iteration",
      "from": "slmonker"
    },
    {
      "requirement": "LightX2V generation speed",
      "details": "17 seconds for 480p video generation",
      "from": "slmonker"
    },
    {
      "requirement": "VRAM optimization in Wan wrapper",
      "details": "20% VRAM reduction, 1600x896 81f possible with only 1 block offloaded",
      "from": "Lumifel"
    },
    {
      "requirement": "SVI Pro VRAM usage",
      "details": "Same as normal I2V since it's just a LoRA, but second sampler can eat RAM on first run",
      "from": "Kijai"
    },
    {
      "requirement": "TorchCompile overhead",
      "details": "40 seconds downtime + 20 seconds generation vs 20 seconds without compile for Qwen",
      "from": "Mngbg"
    },
    {
      "requirement": "VRAM for Ultravico extended generation",
      "details": "1056x1280x161 frames with 21 blocks swapped uses 31.2GB VRAM",
      "from": "Benjimon"
    },
    {
      "requirement": "PCIe bandwidth",
      "details": "PCIe 5.0 x4 should be fine for inference, equivalent to PCIe 3.0 x16, compute bound once models load",
      "from": "Benjimon"
    },
    {
      "requirement": "Power consumption",
      "details": "Can run 3 RTX 5090s at 400 watts each with 1500-1600W PSU",
      "from": "Benjimon"
    },
    {
      "requirement": "Power management",
      "details": "RTX 5090s may need power limiting or undervolting to prevent crashes during generation",
      "from": "ingi // SYSTMS"
    },
    {
      "requirement": "System RAM usage",
      "details": "96GB system RAM can be exceeded when interpolating multiple scenes (972 frames), needs batching approach",
      "from": "Persoon"
    },
    {
      "requirement": "VRAM for long video generation",
      "details": "RTX 5090 with 32GB VRAM and 96GB system RAM can run long video workflows, 16GB VRAM possible with GGUF models and lower res",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "Full Wan 2.2 processing time",
      "details": "Takes about an hour on RTX 5090 for full model",
      "from": "Benjimon"
    },
    {
      "requirement": "VRAM for VACE workflows",
      "details": "Can use CPU cache to reduce to just under 24GB",
      "from": "Lumifel"
    },
    {
      "requirement": "Qwen edit VRAM usage",
      "details": "Takes 42GB of VRAM",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "requirement": "2MP video generation",
      "details": "I2V will fit at 2MP on 24GB but VACE won't",
      "from": "Lumifel"
    }
  ],
  "community_creations": [
    {
      "creation": "Wan Video Blender node",
      "type": "node",
      "description": "Node for video blending/transitions, included with Steerable Motion",
      "from": "BarleyFarmer"
    },
    {
      "creation": "Video transition node",
      "type": "node",
      "description": "Recently made by Kijai with several transition options, consolidates bunch of nodes into one",
      "from": "Kijai"
    },
    {
      "creation": "Load VAE (Utils) node",
      "type": "node",
      "description": "Custom VAE loader utility, though noted issue with incorrect file type",
      "from": "spacepxl"
    },
    {
      "creation": "Dynamic keyframe insertion node",
      "type": "node",
      "description": "Takes up to 16 keyframes and dynamically adjusts where they go in image sequence, made with Claude assistance",
      "from": "Flipping Sigmas"
    },
    {
      "creation": "Video + image frame replacement node",
      "type": "node",
      "description": "Takes image and video input with frame index parameter, outputs new video with replaced frame",
      "from": "BitJuggler"
    },
    {
      "creation": "SVI 2.0 ComfyUI integration",
      "type": "node",
      "description": "Added new input in I2V node for padding with given image and single temporal mask support",
      "from": "Kijai"
    },
    {
      "creation": "Key renaming script for SVI LoRA conversion",
      "type": "script",
      "description": "Convert SVI LoRA keys from 'pipe.dit.' to 'diffusion_model.' format",
      "from": "Kijai"
    },
    {
      "creation": "NormalizeVideoLatentStart",
      "type": "node",
      "description": "Adaptive mean/std normalization from Kandinsky5 now available as core ComfyUI node",
      "from": "Kijai"
    },
    {
      "creation": "Latent Color Match node",
      "type": "node",
      "description": "Color matching in latent space for style transfer",
      "from": "Gleb Tretyak"
    },
    {
      "creation": "mask_channels_preview",
      "type": "tool",
      "description": "Shows different views of mask data - pixel frames and latent space representation",
      "from": "42hub"
    },
    {
      "creation": "WanVideoWrapper SVI support",
      "type": "node",
      "description": "SVI 2.0 support in Kijai's wrapper, tested with 2.1, 2.2 support pending",
      "from": "Kijai"
    },
    {
      "creation": "TrentNodes",
      "type": "node",
      "description": "Includes cross dissolve functionality for video stitching",
      "from": "42hub"
    },
    {
      "creation": "WanEx_I2VCustomEmbeds",
      "type": "node",
      "description": "Custom embeds node that gives full control over conditioning, helps use embeds like start frame",
      "from": "Ablejones"
    },
    {
      "creation": "OneToAllAnimation pose alignment node",
      "type": "node",
      "description": "Uses same detection as WanAnimate preprocessor, doesn't do mad limb stretching, useful for other models too",
      "from": "Kijai"
    },
    {
      "creation": "Reference frame strength modification tool",
      "type": "tool",
      "description": "Useful tool for modifying the strength of reference frames",
      "from": "Ablejones"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "Wrapper implementation for Wan models in ComfyUI",
      "from": "Kijai"
    },
    {
      "creation": "OneToAll Animation implementation",
      "type": "model",
      "description": "ComfyUI implementation of pose retargeting with better alignment",
      "from": "Kijai"
    },
    {
      "creation": "WanAnimate preprocessing improvements",
      "type": "node",
      "description": "Updated preprocessing with auto match capabilities",
      "from": "Kijai"
    },
    {
      "creation": "RCM conversion script",
      "type": "tool",
      "description": "Script to convert RCM models with proper patch embed reshaping",
      "from": "Kijai"
    },
    {
      "creation": "Manim ComfyUI node prototype",
      "type": "node",
      "description": "Claude-built prototype for openpose->manim->comfyui workflow",
      "from": "Scruffy"
    },
    {
      "creation": "WanVideoWrapper OneToAll branch",
      "type": "node",
      "description": "ComfyUI implementation of OneToAll animation, now merged to main",
      "from": "Kijai"
    },
    {
      "creation": "WanMove native node",
      "type": "node",
      "description": "Native ComfyUI node for WanMove with same inputs as ATI",
      "from": "Kijai"
    },
    {
      "creation": "rCM LoRA conversions",
      "type": "lora",
      "description": "Converted rCM models to LoRA format for both high and low noise",
      "from": "Kijai"
    },
    {
      "creation": "HuMo scene extension node",
      "type": "node",
      "description": "Node to handle extending HuMo scenes with audio concatenation like frame overlap",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "creation": "WanVideoWrapper native WanMove support",
      "type": "node",
      "description": "Native ComfyUI implementation with Tracks data type for trajectory control",
      "from": "Kijai"
    },
    {
      "creation": "Dynamic Lora Scheduler",
      "type": "node",
      "description": "Automated lora strength changes while rendering based on noise or motion",
      "from": "L\u00e9on"
    },
    {
      "creation": "Fake point cloud node from depth image",
      "type": "node",
      "description": "Creates fake point cloud from depth image, allows rotation and coord_point string export",
      "from": "ingi // SYSTMS"
    },
    {
      "creation": "3D pose lifting nodes with Manim",
      "type": "node",
      "description": "Lifts 2D poses to 3D and allows camera movement around poses like Matrix camera",
      "from": "Scruffy"
    },
    {
      "creation": "WanEx I2VCustomEmbeds node with mask feature",
      "type": "node",
      "description": "Custom embedding node with masking capabilities similar to VACE",
      "from": "Ablejones"
    },
    {
      "creation": "Audio reactive node with zoom capabilities",
      "type": "node",
      "description": "Combines with 3D nodes for audio-reactive video generation",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "creation": "WAN Scheduler visualization node",
      "type": "node",
      "description": "Visualizes sigma curves to help debug generation issues",
      "from": "Kijai"
    },
    {
      "creation": "Trent VACE Keyframe Builder",
      "type": "node",
      "description": "Dynamic keyframe sequencing with drag-and-drop UI, frame positioning, supports up to 256 frames",
      "from": "Dream Making"
    },
    {
      "creation": "Modified NLF drawing node for SCAIL",
      "type": "node",
      "description": "Adapted to imitate SCAIL mesh format",
      "from": "Kijai"
    },
    {
      "creation": "Multikeyframe qwen edit VACE workflow",
      "type": "workflow",
      "description": "Subgraphed workflows combining qwen image edit with VACE for automated keyframe generation",
      "from": "Flipping Sigmas"
    },
    {
      "creation": "Auto spacing feature",
      "type": "tool",
      "description": "Feature to automatically space keyframes to avoid issues when they're too close together",
      "from": "Flipping Sigmas"
    },
    {
      "creation": "V2 WanVideo nodes",
      "type": "node",
      "description": "Redesigned modular node architecture with separate components for cleaner workflows",
      "from": "Kijai"
    },
    {
      "creation": "GetTrackRange node",
      "type": "node",
      "description": "Splits spline paths across multiple context windows in KJNodes",
      "from": "Kijai"
    },
    {
      "creation": "Simple track generator",
      "type": "node",
      "description": "Core node for generating simple bezier tracks and paths",
      "from": "Kijai"
    },
    {
      "creation": "Path filters concept",
      "type": "node",
      "description": "Proposed node for adding noise, sine wave effects to movement paths",
      "from": "Kijai"
    },
    {
      "creation": "SCAIL-Pose detection",
      "type": "node",
      "description": "3D pose detection with single and multi-person support, face tracking",
      "from": "Kijai"
    },
    {
      "creation": "ComfyUI SCAIL Pose",
      "type": "node",
      "description": "Multi-person pose detection node for SCAIL",
      "from": "Kijai"
    },
    {
      "creation": "WanVideoWrapper SCAIL branch",
      "type": "node",
      "description": "SCAIL integration for Wan video generation",
      "from": "Kijai"
    },
    {
      "creation": "SCAIL-Pose Node",
      "type": "node",
      "description": "ComfyUI node for SCAIL pose control with NLF 3D pose detection and optional face/hand control",
      "from": "Kijai"
    },
    {
      "creation": "CUDA memory history nodes",
      "type": "node",
      "description": "Nodes for tracking and graphing CUDA memory usage during generation, helpful for debugging memory issues",
      "from": "Kijai"
    },
    {
      "creation": "SAT to Wan format converter",
      "type": "tool",
      "description": "Code to convert SAT format models to original Wan format by renaming keys and unfusing qkv and kv",
      "from": "Kijai"
    },
    {
      "creation": "WanViTPoseRetargeter",
      "type": "node",
      "description": "Helps with migrating skeleton proportions to input",
      "from": "dj47"
    },
    {
      "creation": "Pose color mapping",
      "type": "code",
      "description": "Custom color definitions for pose visualization with warm colors for right side, cool colors for left side",
      "from": "Kijai"
    },
    {
      "creation": "Procedural audio-driven SCAIL poses",
      "type": "workflow",
      "description": "Generating poses procedurally from audio but hard to teach math to dance right",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "creation": "SCAIL pose filtering",
      "type": "node",
      "description": "Low-effort filter to align detected pose frames based on proximity of joints for multi-person tracking",
      "from": "Kijai"
    },
    {
      "creation": "ComfyUI node naming optimization",
      "type": "node",
      "description": "PR for shorter node names since ComfyUI charges by the letter now (joke)",
      "from": "Kytra"
    },
    {
      "creation": "SCAIL dwpose compatibility node",
      "type": "node",
      "description": "Converts dwpose output to be compatible with SCAIL",
      "from": "Kijai"
    },
    {
      "creation": "SCAIL prompt generation script",
      "type": "tool",
      "description": "Uses Google Gemini to generate detailed prompts from reference image and driving motion",
      "from": "teal024"
    },
    {
      "creation": "Multi-person SCAIL workflow",
      "type": "workflow",
      "description": "Updated workflow for handling multiple people with dwpose conversion",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "creation": "SCAIL + WanAnimate combination workflow",
      "type": "workflow",
      "description": "Professional pipeline using SCAIL for retarget then WanAnimate for stability",
      "from": "Juan Gea"
    },
    {
      "creation": "Uni3c speed optimization",
      "type": "tool",
      "description": "Added offloading disable option and fp8 quantization support",
      "from": "Kijai"
    },
    {
      "creation": "Wan Move LoRA",
      "type": "lora",
      "description": "Camera movement control LoRA extracted from WanAnimate, works with Wan 2.2 but not 2.1 or SCAIL",
      "from": "Gleb Tretyak"
    },
    {
      "creation": "Split tile upscale workflow",
      "type": "workflow",
      "description": "1.3B tile + 2.2 low noise combination for clean upscaling results",
      "from": "spacepxl"
    },
    {
      "creation": "Hand scaling fix",
      "type": "bugfix",
      "description": "Fixed hand scaling from upstream and DWPose hand swapping bug",
      "from": "Kijai"
    },
    {
      "creation": "LongCat Avatar implementation",
      "type": "node",
      "description": "Working on ComfyUI implementation but extension not working yet and very slow without distill",
      "from": "Kijai"
    },
    {
      "creation": "WorldCanvas analysis",
      "type": "research",
      "description": "Identified as 2.2-based model with 53 input channels and VACE-like features",
      "from": "Kijai"
    },
    {
      "creation": "Model decomposition scripts",
      "type": "tool",
      "description": "Building pyside-based tool to edit DiT blocks, strip and replace blocks from models",
      "from": "cyncratic"
    },
    {
      "creation": "Tinker app for model viewing",
      "type": "tool",
      "description": "App to select any block and save at any dtype, rename keys, view safetensors files",
      "from": "Kijai"
    },
    {
      "creation": "Fake 32-bit nodes for VACE video",
      "type": "node",
      "description": "Nodes with antigravity for sequence output implemented using VACE for video workflow",
      "from": "yukass"
    },
    {
      "creation": "Loop decode node",
      "type": "node",
      "description": "New node for creating looping videos with native context windows, can work with wrapper outputs using rescale node",
      "from": "Kijai"
    },
    {
      "creation": "Video slice node",
      "type": "node",
      "description": "Helps slice correct part of input video for each window in V2V workflows",
      "from": "Kijai"
    },
    {
      "creation": "Face masking refinement node",
      "type": "node",
      "description": "Takes SEGS batch masks and swaps them for fixed size squares for consistent face tracking",
      "from": "boorayjenkins"
    },
    {
      "creation": "ComfyUI-SCAIL-AudioReactive",
      "type": "node",
      "description": "Generate audio-reactive SCAIL pose sequences for character animation without requiring input video tracking",
      "from": "Dream Making"
    },
    {
      "creation": "ComfyUI-WanSoundTrajectory",
      "type": "node",
      "description": "Takes path from SplineEditor and modulates it based on audio analysis for camera/object movement that reacts to music",
      "from": "Dream Making"
    },
    {
      "creation": "3D preview node for NLF poses",
      "type": "node",
      "description": "Node for previewing NLF poses in 3D, though struggles with MoGE output",
      "from": "Kijai"
    },
    {
      "creation": "Torch method NLF pose rendering",
      "type": "tool",
      "description": "LLM generated but functional torch method for rendering NLF poses, making taichi optional",
      "from": "Kijai"
    },
    {
      "creation": "CropAndStitch context mask node",
      "type": "node",
      "description": "Does math to draw consistently sized box around segments for CropAndStitch context mask",
      "from": "boorayjenkins"
    },
    {
      "creation": "Enhanced caching nodes",
      "type": "node",
      "description": "Expanded Cache Node and Load Cache from WAS Node Suite, added caching for stitcher data and masks",
      "from": "boorayjenkins"
    },
    {
      "creation": "WanEx I2V implementation",
      "type": "workflow",
      "description": "Enables I2V behavior through backend model code when WanEx is installed",
      "from": "Ablejones"
    },
    {
      "creation": "OpenPose to tracks conversion node",
      "type": "node",
      "description": "Converts openpose points into tracks, created by Gleb Tretyak",
      "from": "Gleb Tretyak"
    },
    {
      "creation": "Native QwenVL 2.5 implementation",
      "type": "node",
      "description": "Work in progress native implementation by Kijai",
      "from": "Kijai"
    },
    {
      "creation": "h1111 project",
      "type": "tool",
      "description": "Personal implementation for Wan workflows, includes SVI Pro branch",
      "from": "Benjimon"
    },
    {
      "creation": "SVI 2.0 Pro workflow",
      "type": "workflow",
      "description": "ComfyUI workflow for seamless video extension without decode-encode steps",
      "from": "Kijai"
    },
    {
      "creation": "WanVideo wrapper v2 optimizations",
      "type": "node",
      "description": "Major VRAM usage improvements and torch.compile fixes",
      "from": "Kijai"
    },
    {
      "creation": "Native SVI Pro node",
      "type": "node",
      "description": "WanImageToVideoSVIPro node in KJNodes with converted LoRAs support",
      "from": "Kijai"
    },
    {
      "creation": "FreeLong++ Wan implementation",
      "type": "node",
      "description": "Fork of WanVideoWrapper with algorithm for up to 640 frames in one batch",
      "from": "campeonchik"
    },
    {
      "creation": "Ultravico Wan 2.2 implementation",
      "type": "code",
      "description": "Custom implementation of Ultravico technique for Wan 2.2 I2V",
      "from": "Benjimon"
    },
    {
      "creation": "Full auto infinite SVI Pro workflow",
      "type": "workflow",
      "description": "Automated workflow for infinite video generation with loop capabilities",
      "from": "V\u00e9role"
    },
    {
      "creation": "SVI Pro subgraph workflow",
      "type": "workflow",
      "description": "Cleaner native example workflow using subgraphs for better organization",
      "from": "Kijai"
    },
    {
      "creation": "SVI loop workflow with subnet",
      "type": "workflow",
      "description": "Builds in simple loop subnet for multiple SVI passes with different prompts",
      "from": "Quality_Control"
    },
    {
      "creation": "ComfyUI Clip Prompt Splitter",
      "type": "node",
      "description": "CLIP text encoder that splits prompts based on 'Enter' for multiple prompt handling",
      "from": "David Galardi"
    },
    {
      "creation": "Image concatenate node patch",
      "type": "node",
      "description": "Fixes first loop iteration issues in SVI workflows",
      "from": "pagan"
    },
    {
      "creation": "WanMotionScale node",
      "type": "node",
      "description": "Motion control without shift options, essentially rope scale adjustment",
      "from": "Kijai"
    },
    {
      "creation": "Motion scale control custom node",
      "type": "node",
      "description": "Gives ability to control scale of motion in video generation",
      "from": "brbbbq"
    },
    {
      "creation": "ModelHunter",
      "type": "tool",
      "description": "Auto-search shortcut that extracts model names from workflows and creates Google searches",
      "from": "Quality_Control"
    },
    {
      "creation": "ComfyUI-prompt-splitter",
      "type": "node",
      "description": "Separates prompts by line breaks so you can put them all in a single text box",
      "from": "David Galardi"
    },
    {
      "creation": "WanExperiments nodes",
      "type": "node",
      "description": "Includes HuMo I2V functionality in under 80 lines of code",
      "from": "Ablejones"
    },
    {
      "creation": "Longvie2",
      "type": "model",
      "description": "Can do 30 seconds at least with depth/spatracker control, available in test branch",
      "from": "Kijai"
    }
  ]
}