{
  "channel": "wan_chatter",
  "date_range": "2025-04-01 to 2025-05-01",
  "messages_processed": 18910,
  "chunks_processed": 48,
  "api_usage": {
    "input_tokens": 577831,
    "output_tokens": 126049,
    "estimated_cost": 3.624228
  },
  "extracted_at": "2026-02-02T23:36:57.020426Z",
  "discoveries": [
    {
      "finding": "fp16_fast is significantly faster than bf16 for 4090",
      "details": "12.19/it vs 15.26/it - about 3 seconds per iteration faster",
      "from": "ezMan"
    },
    {
      "finding": "SLG has minimal speed impact",
      "details": "10 steps with SLG: 12.51s/it vs without SLG: 12.18s/it - barely noticeable difference",
      "from": "ezMan"
    },
    {
      "finding": "Teacache modulated time embeds doesn't work with dpmpp_sde",
      "details": "Causes rainbows, but time embeds mode works albeit slowly",
      "from": "ezMan"
    },
    {
      "finding": "VACE model released for Wan 1.3B",
      "details": "Preview version available on HuggingFace, works with video control inputs like depth",
      "from": "Kijai"
    },
    {
      "finding": "WhatsApp destroys video quality for vid2vid",
      "details": "Transferring videos from phone to computer through WhatsApp significantly affects output quality compared to original files",
      "from": "3Dmindscaper2000"
    },
    {
      "finding": "Wan 1.3B can effectively improve old generations as second pass",
      "details": "Using depth, highres and aesthetics loras with 1.3B at low step counts (6-8 steps) significantly improves motion coherence of old Hunyuan/Wan generations",
      "from": "David Snow"
    },
    {
      "finding": "DepthCrafter vs Video Depth Anything comparison",
      "details": "DepthCrafter appears better for static shots, VDA may be more temporally consistent but requires size reduction to avoid OOM",
      "from": "David Snow"
    },
    {
      "finding": "VACE uses fp32 format",
      "details": "The VACE model file is in fp32 format, making it larger than typical fp8 models",
      "from": "Kijai"
    },
    {
      "finding": "Batched CFG causes flashy results with VACE",
      "details": "Disabling batched CFG on sampler fixes bad and flashy VACE outputs",
      "from": "TK_999"
    },
    {
      "finding": "VACE blocks are in bf16 while rest of model is fp32",
      "details": "Mixed precision setup causing performance impact",
      "from": "Kijai"
    },
    {
      "finding": "VACE adds 15 additional blocks that run every step",
      "details": "This explains the significantly slower performance compared to standard Wan",
      "from": "Kijai"
    },
    {
      "finding": "Reference images work best with white backgrounds",
      "details": "Need to segment reference images and place on white background for proper likeness",
      "from": "Kijai"
    },
    {
      "finding": "Video extension technique discovered",
      "details": "Send empty frames as gray frames (0.5 works best) with video frames, mask where video frames are white and rest black",
      "from": "Kijai"
    },
    {
      "finding": "VACE encode strength of 0.4 gives control with creativity",
      "details": "Lower strength maintains control while allowing more creative freedom",
      "from": "Cseti"
    },
    {
      "finding": "Padding reference images can make them work when they otherwise fail",
      "details": "Image aspect ratio and canvas placement affects reference image functionality",
      "from": "Kijai"
    },
    {
      "finding": "FP32 provides better quality than FP16 for VACE",
      "details": "FP16 was described as 'trash', FP32 required for good results",
      "from": "VK (5080 128gb)"
    },
    {
      "finding": "DensePosePreprocessor works well with VACE",
      "details": "Works with good results at strength 1",
      "from": "VK (5080 128gb)"
    },
    {
      "finding": "Reference images must have white background for VACE to work",
      "details": "Model doesn't work at all if there aren't any white in the reference",
      "from": "Kijai"
    },
    {
      "finding": "Multiple reference images are composited into single image",
      "details": "Put multiple references into a single image rather than separate inputs",
      "from": "Draken"
    },
    {
      "finding": "VACE puts reference image as first frame during processing",
      "details": "Reference image is literally put into the first frame, trained to pick up likeness from it",
      "from": "Draken"
    },
    {
      "finding": "Context windows can process longer videos in 81-frame chunks",
      "details": "Context options split process to 81 frame windows with overlap, works much better for long videos",
      "from": "Kijai"
    },
    {
      "finding": "Second pass at low denoise removes oversaturation",
      "details": "2nd pass at 6 steps works well to remove the oversaturated look",
      "from": "NebSH"
    },
    {
      "finding": "Higher resolution helps reduce artifacts",
      "details": "1024x576 vs 720p shows improvement, though some artifacting remains",
      "from": "Piblarg"
    },
    {
      "finding": "Force offload halves VRAM usage",
      "details": "Enabling force offload halfed vram requirements",
      "from": "VK (5080 128gb)"
    },
    {
      "finding": "VACE reference images need white background to work properly",
      "details": "Reference images must be on white background, preferably with subject background removed. Position on canvas matters significantly for quality",
      "from": "Kijai"
    },
    {
      "finding": "SLG parameters can help with image arrangement in VACE",
      "details": "SLG 8, 0.1 start, 0.3 end noticed to help with image arrangement",
      "from": "IllumiReptilien"
    },
    {
      "finding": "VACE works with existing 1.3B LoRAs",
      "details": "LoRAs trained on original Wan weights work with VACE, making it better than fun-control",
      "from": "Kytra"
    },
    {
      "finding": "Control inputs need specific formatting",
      "details": "For lineart/canny, model wants black lines on white background",
      "from": "Kijai"
    },
    {
      "finding": "Masking technique for VACE",
      "details": "Use grey solid color composite where mask applies for better results. Prep reference image by removing background and compositing on grey layer",
      "from": "IllumiReptilien"
    },
    {
      "finding": "CFG zero star and zero init helps with VACE",
      "details": "Not SLG that helps, but CFG zero star and zero init settings",
      "from": "IllumiReptilien"
    },
    {
      "finding": "VACE model needs separated subjects or padded images for reference",
      "details": "Full reference images don't work well, need background removed or image padded",
      "from": "Kijai"
    },
    {
      "finding": "VACE strength value really boosts the reference",
      "details": "Using 1.5 strength now actually produces better results",
      "from": "Kijai"
    },
    {
      "finding": "Gray areas represent missing video parts in VACE",
      "details": "Values equal to 127 represent missing video part. White areas in mask represent parts to be generated, black areas represent parts to be retained",
      "from": "AJO"
    },
    {
      "finding": "VACE works well with character LoRAs",
      "details": "Standard 1.3B LoRAs work with VACE for character consistency",
      "from": "Kytra"
    },
    {
      "finding": "VACE pose control produces excellent results",
      "details": "Character lora + pose rig into VACE produces great results with consistency",
      "from": "Kytra"
    },
    {
      "finding": "Hi-Res lora works with VACE",
      "details": "Compatible with VACE model",
      "from": "VK (5080 128gb)"
    },
    {
      "finding": "SemSegPrepprocessor and Coco Sem work well with VACE",
      "details": "Both preprocessors produce surprisingly good results with VACE",
      "from": "VK (5080 128gb)"
    },
    {
      "finding": "VACE can interpret mixed inputs automatically",
      "details": "VACE understands first frame as reference and rest as control without explicit configuration - uses colored frames as-is and control inputs as motion guidance",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "VACE supports creative inpainting with gray overlay",
      "details": "Can do outfit swaps by compositing gray over masked areas in input frames - no explicit mask input needed",
      "from": "Zuko"
    },
    {
      "finding": "Sapiens is SOTA for human preprocessing",
      "details": "Meta's Sapiens models provide better temporal consistency than DWPose for pose, depth, segmentation, and normals",
      "from": "Kijai"
    },
    {
      "finding": "DiffSynth models are full 1.3B models, not LoRAs",
      "details": "The new DiffSynth releases are custom finetunes/distilled versions, not actual LoRAs despite the naming",
      "from": "Kijai"
    },
    {
      "finding": "VACE 1.3B handles high resolution slightly better than standard 1.3B",
      "details": "VACE version of 1.3B model shows improved high resolution performance",
      "from": "DawnII"
    },
    {
      "finding": "VACE mask input controls where VACE can apply in both temporal and spatial terms",
      "details": "The mask input defines the areas where VACE effects are applied across time and space",
      "from": "Kijai"
    },
    {
      "finding": "VACE strength parameter affects different aspects depending on usage",
      "details": "Affects control strength when using control only, and reference strength when using both control and reference",
      "from": "Kijai"
    },
    {
      "finding": "Context windows work well with fewer frames than expected",
      "details": "720p generation with limited context frames still produces good results",
      "from": "Kijai"
    },
    {
      "finding": "Reference images work better on white background, but gray can be better for predominantly white subjects",
      "details": "White canvas is standard for reference, but gray background helps when subject is mostly white",
      "from": "JmySff"
    },
    {
      "finding": "Multiple control images can be stacked by overlaying them",
      "details": "You can combine multiple control inputs by overlaying the control images",
      "from": "ameasure"
    },
    {
      "finding": "WAN 1.3B video quality is beyond typical 1.3B model expectations",
      "details": "The quality output significantly exceeds what's normally expected from a 1.3B parameter model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "VACE works with LoRAs",
      "details": "LoRAs work perfectly with VACE, contrary to some expectations",
      "from": "Kijai"
    },
    {
      "finding": "Main Wan 1.3B model was not even trained",
      "details": "The original VACE model contained untrained 1.3B fp32 model, just the VACE blocks were trained",
      "from": "Kijai"
    },
    {
      "finding": "VACE can be loaded separately",
      "details": "Standalone VACE that can be loaded with any 1.3B model, saves 6GB disk space",
      "from": "Kijai"
    },
    {
      "finding": "Resolution must be divisible by 16",
      "details": "Both width and height need to be divisible by 16, not just 8, for proper operation",
      "from": "Kijai"
    },
    {
      "finding": "DG models don't need white backgrounds",
      "details": "DG models hold onto reference image position tighter and don't require white backgrounds",
      "from": "Hashu"
    },
    {
      "finding": "VACE reference images are inserted at start of frame sequence",
      "details": "If generating 33 frames with 1 ref image, it actually generates 29 frames as ref is first latent then dropped before decode",
      "from": "Kijai"
    },
    {
      "finding": "Combining depth/normals/lineart improves video quality",
      "details": "Using depth, lotus normals, and realistic lineart together provides more detail information to the model",
      "from": "David Snow"
    },
    {
      "finding": "VACE model doesn't train any of the base layers",
      "details": "Kijai extracted it and found VACE can be loaded separately from base models",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "DG models work much better at 4 steps 1 CFG",
      "details": "Better at keeping style and reference image doesn't need white background",
      "from": "Hashu"
    },
    {
      "finding": "prev_embeds allows different control and reference strength",
      "details": "Only way to have different control strength and different ref input strength",
      "from": "Hashu"
    },
    {
      "finding": "Early denoising steps are WAY stronger in prev_embeds",
      "details": "Doing 50/50 split won't do much, early steps have much more influence",
      "from": "Kijai"
    },
    {
      "finding": "Euler scheduler performs worse than UniPC for VACE",
      "details": "Switching back to UniPC from Euler improved results significantly",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "VACE supports inverted canny (black lines on white background)",
      "details": "Works fine with black lines on white background, also tested lineart",
      "from": "Kijai"
    },
    {
      "finding": "Bounding box control works but outlined circle does not",
      "details": "Box moves the subject without drawing the box, circle starts drawing circle shapes",
      "from": "Kijai"
    },
    {
      "finding": "VACE inpainting quality is exceptional",
      "details": "Can replace complex compositing work that would take hours",
      "from": "traxxas25"
    },
    {
      "finding": "VACE works similarly to Tora",
      "details": "Uses start frame + trajectory with bbox + reference image",
      "from": "Kijai"
    },
    {
      "finding": "Model can do extension without context",
      "details": "The base model has extension capabilities, potentially reducing need for context windows",
      "from": "Kijai"
    },
    {
      "finding": "LoRAs trained for base model work with VACE",
      "details": "Base model unchanged so T2V LoRAs are compatible",
      "from": "Kijai"
    },
    {
      "finding": "Reference image background removal improves consistency",
      "details": "User couldn't get VACE to work until removing background from reference image",
      "from": "ingi // SYSTMS"
    },
    {
      "finding": "Skyreels A2 model performs better I2V than base I2V model",
      "details": "Initial tests show SkyreelsA2 gives different and potentially better results for normal I2V compared to base I2V model",
      "from": "Kijai"
    },
    {
      "finding": "SkyreelsA2 can do multiple reference images",
      "details": "Model supports 2-3 reference images, not just one",
      "from": "Kijai"
    },
    {
      "finding": "SkyreelsA2 uses clip embeds unlike VACE",
      "details": "Main difference from VACE is that SkyreelsA2 uses clip embeds since it's based on 14B I2V model, while VACE doesn't use clip embeds at all",
      "from": "Kijai"
    },
    {
      "finding": "Reference image positioning affects final output significantly",
      "details": "Composition and size of subjects in reference images is quite important for final output quality",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "TeaCache optimal threshold is around 30% skipped steps",
      "details": "30% skipped generally gives decent quality, more than 50% usually ends up bad, though I2V is far less sensitive than other tasks",
      "from": "Kijai"
    },
    {
      "finding": "VACE works extremely well with image references and maintains subject consistency",
      "details": "Multiple users demonstrated strong reference adherence in their generations",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "Blurred depth maps work better than clean Blender depth renders",
      "details": "Sharp Blender depth maps don't work at all, but adding just 3 pixel blur makes them function properly",
      "from": "Kijai"
    },
    {
      "finding": "VACE can do temporal inpainting with any number of input frames",
      "details": "Black mask areas are kept, white areas are generated new. Can use for frame extension and morphing",
      "from": "Kijai"
    },
    {
      "finding": "VACE context windows work for longer generations",
      "details": "Successfully generated 960x480x221 frames using reference + control + context",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "VACE sees bounding boxes as control - perfect boxes work, circular shapes make it draw circles",
      "details": "Model interprets shape geometry in control inputs",
      "from": "Kijai"
    },
    {
      "finding": "Lineart and canny need to be inverted for VACE",
      "details": "Control inputs require color inversion to work properly",
      "from": "Kijai"
    },
    {
      "finding": "Character replacement workflow using 2-step approach works very well",
      "details": "First generate clean background, then generate character with proper lighting/environment prompting",
      "from": "traxxas25"
    },
    {
      "finding": "VACE memory optimization breakthrough",
      "details": "Kijai discovered memory optimizations that reduce VACE VRAM usage significantly: 832x480x81 frames now fits in ~12GB without offloading, and 1024x1024x81 fits in ~13GB VRAM",
      "from": "Kijai"
    },
    {
      "finding": "VACE intermediate results can be moved to RAM",
      "details": "Moving VACE intermediate results to RAM after each VACE block calculation reduces VRAM usage to almost nothing extra with only slight speed loss",
      "from": "Kijai"
    },
    {
      "finding": "Depth and pose control blending technique",
      "details": "Successfully blended depth and dwpose controls by using two separate VACE encode nodes with reduced strength (0.5 or less) or switching between them during generation steps",
      "from": "yo9o"
    },
    {
      "finding": "Reward LoRAs released for Wan2.1-Fun models",
      "details": "Wan2.1-Fun-14B-InP-HPS2.1.safetensors and Wan2.1-Fun-14B-InP-MPS.safetensors, 1.46 GB each, use Reward Backpropagation technique for better human preference alignment",
      "from": "Lumi"
    },
    {
      "finding": "VACE supports outpainting functionality",
      "details": "VACE can handle outpainting tasks, addressing a long-standing need",
      "from": "Godhand"
    },
    {
      "finding": "Wan provides better camera movement handling than AnimatedDiff",
      "details": "Wan doesn't create distorted backgrounds like AnimatedDiff when camera moves",
      "from": "xwsswww"
    },
    {
      "finding": "Multi-frame control possible with VACE",
      "details": "Can add frames in middle between start/end frames using white frames in video editor like CapCut",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "VACE memory usage optimization",
      "details": "Kijai halved the VACE memory usage and added offloading - 832x480x81 now uses only 6GB VRAM during sampling",
      "from": "Kijai"
    },
    {
      "finding": "Reward LoRAs work with multiple models",
      "details": "Reward LoRAs work with both Fun Control 1.3B and base 1.3B models, with recommended strengths of 0.5 for HPS and 0.7 for MPS",
      "from": "DawnII"
    },
    {
      "finding": "Reference image requirements for VACE",
      "details": "Reference images need to be on white background for best results - padding is simplest way, BG removal is better",
      "from": "Kijai"
    },
    {
      "finding": "VACE works as single image generation",
      "details": "VACE can work for single frame generation with control, though if using reference image, input needs to be 2 images",
      "from": "Kijai"
    },
    {
      "finding": "VACE works with existing Wan 2.1 LoRAs without retraining",
      "details": "VACE is additional module, original model weights not modified, so LoRAs work as well as they can",
      "from": "Kijai"
    },
    {
      "finding": "DiffSynth converted LoRAs are 2x stronger than originals",
      "details": "DiffSynth removed alpha keys from reward LoRAs, making them twice as powerful at 1.0 strength compared to originals",
      "from": "Kijai"
    },
    {
      "finding": "VACE requires white canvas background for subject detection",
      "details": "Subjects must be on white canvas for VACE to detect them properly, can have multiple subjects on same canvas",
      "from": "Kijai"
    },
    {
      "finding": "Video-depth-anything uses less VRAM than DepthCrafter after xformers install",
      "details": "Takes substantially less vram than depthcrafter after installing xformers",
      "from": "David Snow"
    },
    {
      "finding": "VACE supports spatial and temporal inpainting/outpainting simultaneously",
      "details": "Best part of VACE is that you can do all control, inpainting, outpainting at once",
      "from": "Kijai"
    },
    {
      "finding": "VACE treats white areas as alpha/transparency",
      "details": "The model is picky about too high quality depth maps being detected as grayscale input, and white areas are interpreted as transparent",
      "from": "Kijai"
    },
    {
      "finding": "VACE has soft cap of 81 frames",
      "details": "Artifacts appear around 129 frames, similar to other models trained at specific frame counts",
      "from": "The Shadow (NYC)"
    },
    {
      "finding": "Riflex doesn't work well with Wan models",
      "details": "Can mess up generated content compared to standard generation without riflex",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "finding": "DG model loses character likeness but is faster",
      "details": "Distilled model trades character consistency for speed improvements",
      "from": "burgstall"
    },
    {
      "finding": "Combined VACE strength shouldn't exceed 1.0",
      "details": "When using multiple VACE encodes, the combined strength should stay under 1.0 to avoid overly strong influence",
      "from": "Kijai"
    },
    {
      "finding": "Feta_args value is set by frame count",
      "details": "For 81 frames the default is 4.0, but can be lower for weaker effect. Using wrong values for frame count ruins output",
      "from": "Kijai"
    },
    {
      "finding": "Multiple VACE encode nodes require much lower strength values",
      "details": "When using more than 1 VACE node, the strength on both needs to be much lower, or better to schedule them step wise",
      "from": "Kijai"
    },
    {
      "finding": "Reference images need white canvas padding",
      "details": "Reference image needs to be on a white canvas for the model to use it, if you don't want to remove background you pad it instead. Reference image is NOT same thing as start image",
      "from": "Kijai"
    },
    {
      "finding": "VACE automatically pads encoded video",
      "details": "Made it automatically pad the encoded video to fix tensor errors when using reference images",
      "from": "Kijai"
    },
    {
      "finding": "Multiple subjects can be used on reference canvas",
      "details": "You can have multiple subjects on the reference canvas too",
      "from": "Kijai"
    },
    {
      "finding": "VACE is not strictly just start/end frame",
      "details": "It's not strictly just start/end frame, it's any number of frames and mask",
      "from": "Kijai"
    },
    {
      "finding": "Multiple reference images can be composited",
      "details": "You always could use more than one reference image, just composite them to same canvas",
      "from": "Kijai"
    },
    {
      "finding": "VACE works with DWpose face only for character animation",
      "details": "Using DWpose with face only enabled, reference image, and grey background masking produces good face animation with lip sync accuracy",
      "from": "IllumiReptilien"
    },
    {
      "finding": "Grey background masking technique for VACE",
      "details": "Using 50% grey with normal blend mode at 50% for masked areas in VACE inpainting workflows",
      "from": "IllumiReptilien"
    },
    {
      "finding": "ReCamMaster model available for Wan",
      "details": "New camera control model for Wan 2.1 1.3B that requires specific weights, produces noise without them",
      "from": "Kijai"
    },
    {
      "finding": "IP Adapter announced for Wan 1.3b",
      "details": "Ostris announced IP Adapter for Wan 1.3b with impressive adherence",
      "from": "pom"
    },
    {
      "finding": "VACE can work without controlnet",
      "details": "VACE can be used for lip sync and face animation without control inputs, just reference frames and bbox masks",
      "from": "IllumiReptilien"
    },
    {
      "finding": "VACE can be used for straight I2V without needing input video",
      "details": "You can use VACE with just a starting image and nothing else for image-to-video generation",
      "from": "Hashu"
    },
    {
      "finding": "Multiple VACE controls can be chained together",
      "details": "You can chain one VACE encode into another, just like controlnets, to use multiple controls simultaneously",
      "from": "David Snow"
    },
    {
      "finding": "VACE supports temporal inpainting with flexible frame masking",
      "details": "You can have any number of frames it keeps and any number of frames it tries to generate, not just start/end frames",
      "from": "Kijai"
    },
    {
      "finding": "Two VACE embeds can be used for separate ref and control",
      "details": "One embed with ref only and another with control can be used to separate reference image strength from control strength",
      "from": "Kijai"
    },
    {
      "finding": "White or gray images can be used as neutral reference",
      "details": "Using white or gray images as reference in VACE embed with control can help avoid reference image burn-in",
      "from": "Kijai"
    },
    {
      "finding": "DG_Boost models work better with style LoRAs than base 1.3B",
      "details": "DG_Boost Evol V3 can achieve closer style matches with LoRAs where base model cannot get close",
      "from": "David Snow"
    },
    {
      "finding": "LoRA effect drops off dramatically after 2 steps",
      "details": "First example style only works on 2 step videos, which are too low quality to be useful",
      "from": "David Snow"
    },
    {
      "finding": "VACE works much better than vanilla WAN with LoRAs for style transfer",
      "details": "Got essential style down in about an hour using VACE that couldn't be achieved in three days with vanilla WAN and LoRAs",
      "from": "David Snow"
    },
    {
      "finding": "Reference images add 4 latent frames in VACE",
      "details": "When plugging in a reference image to VACE, it adds 4 latent frames which can cause tensor mismatch errors",
      "from": "DawnII"
    },
    {
      "finding": "Optimal Steps implementation works with WAN T2V native",
      "details": "OptimalSteps from bebebe666 can reduce generation time - 20 optimal steps vs 20 regular steps shows quality improvement",
      "from": "V\u00e9role"
    },
    {
      "finding": "Context windows overlap affects frame calculation",
      "details": "2 context windows would be 81+81+16 = 169 frames total due to overlap, not just 162",
      "from": "Kijai"
    },
    {
      "finding": "VACE allows flexible video generation combining control frames with or without source video encoding",
      "details": "You can use control only, or control + source video. Control frames are generated from source video but you don't need to use the source video itself in diffusion process",
      "from": "David Snow"
    },
    {
      "finding": "Lineart control works opposite with VACE compared to vanilla 1.3B",
      "details": "On vanilla 1.3B, increasing lineart control skews towards lineart style, but with VACE it's the opposite - gets more shaded with higher lineart influence",
      "from": "David Snow"
    },
    {
      "finding": "VACE with both reference image and control video at 1.0 creates noisy outputs",
      "details": "Better results when separating control video from reference image with strength around 0.5 to 0.6 for cleaner outputs",
      "from": "Johnjohn7855"
    },
    {
      "finding": "Higher resolution models seem to produce slower motion",
      "details": "Higher resolution models need to focus more on increased pixels, so have less power to render motions/camera movements",
      "from": "Johnjohn7855"
    },
    {
      "finding": "Context window changes style even when frame count matches",
      "details": "Enabling context options can completely change the look even when testing at same frame number as context window due to overlap calculations",
      "from": "David Snow"
    },
    {
      "finding": "ExVideo LoRA adds extra detail to normal 81-frame generations",
      "details": "When ExVideo LoRA is left enabled on 81-frame generations, it acts as a detail enhancement LoRA, adding more visual detail beyond just frame extension",
      "from": "David Snow"
    },
    {
      "finding": "Fun 1.3B model works with 220+ frames using extended length LoRA",
      "details": "Fun 1.3B with extended length LoRA can generate 220 frames with good quality, taking only 2min 17sec on 4060 ti 16GB at 368x640 resolution",
      "from": "Pol"
    },
    {
      "finding": "Fun models are significantly faster than VACE",
      "details": "Fun 1.3B generates faster than VACE 1.3B and 14B models while using less VRAM",
      "from": "Pol"
    },
    {
      "finding": "VACE uses 15 additional blocks that run on each step",
      "details": "VACE model has 15 more blocks than base Fun model, explaining higher VRAM usage",
      "from": "Kijai"
    },
    {
      "finding": "Wan models have issues with car motion direction",
      "details": "Models tend to get car direction wrong, sending them backwards, sideways, or making them transform instead of moving forward",
      "from": "David Snow"
    },
    {
      "finding": "Video-depth-anything and DepthCrafter are superior to regular depth anything",
      "details": "These specialized video depth models provide better input quality which reflects in output quality",
      "from": "David Snow"
    },
    {
      "finding": "TeaCache can work with Wan Fun Control 14B",
      "details": "Use the '14B T2V' option in the TeaCache node model type dropdown for Wan Fun Control 14B",
      "from": "MilesCorban"
    },
    {
      "finding": "SageAttention provides 100% speed increase with minimal quality impact",
      "details": "About 1% quality drop for 100% speed increase, sometimes sage output is preferable",
      "from": "Kijai"
    },
    {
      "finding": "VACE and Fun Control can give coherent output with single step",
      "details": "Much more efficient than T2V which needs more steps",
      "from": "Kijai"
    },
    {
      "finding": "TeaCache has heaviest quality hit but should be compared to actual step count",
      "details": "30% of steps skipped is acceptable quality tradeoff",
      "from": "Kijai"
    },
    {
      "finding": "Training 1.3B models on synthetic data from 14B works better",
      "details": "Using 14B model outputs as training data for 1.3B models produces better results than mixed video/image datasets",
      "from": "Piblarg"
    },
    {
      "finding": "Wan2.1-FLF2V-14B-720P first-last frame model released",
      "details": "Official first and last frame to video generation model, 720p resolution, works with old workflow but has new pos embed",
      "from": "DawnII"
    },
    {
      "finding": "NormalCrafter produces stable video normals without motion disruption",
      "details": "Video normal maps that maintain stability and don't mess up motion, works better than depth alone for control",
      "from": "David Snow"
    },
    {
      "finding": "Combining depth and normals improves stability",
      "details": "Lotus vs Normalcrafter comparison shows better stability with normals, prefer stability over fine details",
      "from": "David Snow"
    },
    {
      "finding": "SynCamMaster places subjects at exact same position regardless of prompt",
      "details": "Every generation puts subject at same position, background comes out correctly from expected camera angle but subject placement is wrong",
      "from": "Kijai"
    },
    {
      "finding": "FLF2V model only has one new layer: img_emb.emb_pos",
      "details": "Comparison shows 1303 common tensor keys with only img_emb.emb_pos being unique to FLF2V model",
      "from": "Benjimon"
    },
    {
      "finding": "FLF2V model is significantly different from I2V model",
      "details": "Average cosine similarity of 0.873389 with minimum of -0.377899, much less similar than Fun Control was to base model",
      "from": "Benjimon"
    },
    {
      "finding": "Orbit camera generation node added but model struggles with over 90 degrees",
      "details": "New node to generate orbit camera movements, but the model can't handle rotations greater than 90 degrees well",
      "from": "Kijai"
    },
    {
      "finding": "FramePack generates backwards from end to beginning",
      "details": "The model generates ending actions before starting actions due to inverted sampling, requires waiting for earlier frames",
      "from": "JohnDopamine"
    },
    {
      "finding": "FramePack uses DiT patchify technique for temporal coherence",
      "details": "For 3 frames predicting 4th: 3rd frame encoded normal, 2nd frame with fewer patches, 1st frame gets least patches. This technique isn't possible with UNet",
      "from": "Fannovel16"
    },
    {
      "finding": "FramePack can generate 1-minute video with only 6GB VRAM",
      "details": "Can generate 60 seconds at 30fps (1800 frames) using 13B model with minimal 6GB GPU memory, laptop GPUs work",
      "from": "zelgo_"
    },
    {
      "finding": "FramePack processes video in 5-second chunks",
      "details": "Generates approximately 5 seconds at a time, building up longer videos progressively",
      "from": "JohnDopamine"
    },
    {
      "finding": "Wan FLF2V model primarily trained on Chinese text-video pairs",
      "details": "Official repo now recommends using Chinese prompts for better FLF2V results",
      "from": "DawnII"
    },
    {
      "finding": "FLF2V model exists and is available",
      "details": "Wan 2.1 First Frame Last Frame model (FLF2V) is available with fp8 quantization, works with wrapper nodes",
      "from": "DawnII"
    },
    {
      "finding": "VACE temporal masking system",
      "details": "Black mask frames are keyframes (don't change), white mask frames are inpainted temporally. Can mix and match any number of frames",
      "from": "Kijai"
    },
    {
      "finding": "VACE supports spatial masking in addition to temporal",
      "details": "More advanced masking capabilities beyond just temporal control",
      "from": "Kijai"
    },
    {
      "finding": "Fun InP model can do temporal masking like VACE",
      "details": "Just not with control inputs",
      "from": "Kijai"
    },
    {
      "finding": "Chinese prompts work better than English for FLF2V",
      "details": "Recommended to use Chinese prompts for better results",
      "from": "DawnII"
    },
    {
      "finding": "Resolution affects Fun Control performance significantly",
      "details": "Anything below 576x1024 wouldn't follow lips and eye direction properly, need higher resolution on preprocessors too",
      "from": "A.I.Warper"
    },
    {
      "finding": "VACE captures eye movement better than standard processing",
      "details": "When using VACE with stylized Pixar character, eye movement tracking is noticeably improved",
      "from": "A.I.Warper"
    },
    {
      "finding": "Mediapipe proves better than DWPose for realism-to-stylized workflows",
      "details": "When using realistic source footage with stylized character reference, Mediapipe keeps the eyes big instead of conforming them to source video",
      "from": "A.I.Warper"
    },
    {
      "finding": "VACE works with only 20 steps, could work with 8",
      "details": "Quality results achieved with just 20 steps, potentially could get away with 8 steps",
      "from": "A.I.Warper"
    },
    {
      "finding": "Using start frame instead of reference image produces better results",
      "details": "Multiple users confirmed that using start frame approach works better than using separate reference image for VACE",
      "from": "A.I.Warper"
    },
    {
      "finding": "CausVid has very fast inference time",
      "details": "12 seconds for 5 second 832x480 @ 16fps video, mainly limited by model loading time",
      "from": "Cubey"
    },
    {
      "finding": "VACE has strong tendency to colorize and add warm tones",
      "details": "Model consistently adds warm colorized tones to output videos",
      "from": "DawnII"
    },
    {
      "finding": "VACE is trained to do grayscale to color conversion",
      "details": "When using high quality depth maps or normal maps with VACE, it treats them as RGB grayscale inputs and goes into colorize mode",
      "from": "Kijai"
    },
    {
      "finding": "Mediapipe face mesh captures extreme movements better than pose preprocessors",
      "details": "Captures extreme mouth and eyebrow movements better than any pose preprocessors and pushes style further",
      "from": "David Snow"
    },
    {
      "finding": "Fun-control can get eye movement at denoise 1.0 while VACE examples are at 0.7",
      "details": "Control-fun can capture eye movements like eye crossing that VACE struggles with, even at higher denoise levels",
      "from": "David Snow"
    },
    {
      "finding": "Using original video as control_images works like an insanely strong tileControlNet",
      "details": "RGB images going in as control frames provide very strong control influence",
      "from": "Draken"
    },
    {
      "finding": "Upscaling before making depth map or normal map helps quality loads",
      "details": "Can use fast upscale methods like SD 1.5 TCD to improve depth/normal map quality",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "finding": "PyTorch has been improving compile for GGUF weights",
      "details": "They boosted the speed of GGUF weights compilation significantly",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "finding": "FreSca is now available in the WanVideoWrapper",
      "details": "New addition to the wrapper functionality",
      "from": "Kijai"
    },
    {
      "finding": "UniAnimate works with Sapiens pose detection",
      "details": "Can use Sapiens instead of just DWPose for pose guidance",
      "from": "Kijai"
    },
    {
      "finding": "1.3B LoRAs work with Fun models and DG models",
      "details": "Cross-compatibility between different model variants, though not as well as intended",
      "from": "David Snow"
    },
    {
      "finding": "Fun LoRAs won't fully load on normal 1.3B model",
      "details": "Missing image embed keys, so only partial functionality",
      "from": "Kijai"
    },
    {
      "finding": "SkyReels version supports Wan LoRAs",
      "details": "Benjimon's SkyReels version is very close to stock i2v model and supports Wan LoRAs",
      "from": "Benjimon"
    },
    {
      "finding": "UniAnimate works with CFG 1.0",
      "details": "UniAnimate works properly with CFG 1.0, but is bad when it has to add new stuff that's not in the reference",
      "from": "Kijai"
    },
    {
      "finding": "UniAnimate purposely doesn't animate background",
      "details": "The model is designed to focus on pose-driven animation without background animation",
      "from": "Kijai"
    },
    {
      "finding": "UniAnimate works with multiple pose detection methods",
      "details": "Trained with dwpose that has feet added, but works with sapiens/open pose just as well",
      "from": "Kijai"
    },
    {
      "finding": "CFG distilled models don't work with low steps",
      "details": "The DF model produces total garbage even at 8 steps, needs different approach than step distillation",
      "from": "Kijai"
    },
    {
      "finding": "SageAttention and TeaCache performance boost is only 20% for some users",
      "details": "User found that disabling both optimizations only slowed generation by 20%, much less than expected",
      "from": "lostintranslation"
    },
    {
      "finding": "SageAttention speed increase scales dramatically with input size",
      "details": "At higher resolution/longer videos it provides more than 100% increase, especially when attention is the bottleneck",
      "from": "Kijai"
    },
    {
      "finding": "Hunyuan generation time is nearly 90% attention",
      "details": "Attention dominates at higher resolutions according to thunderkittens STA paper",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "finding": "UniAnimate works with reward LoRAs",
      "details": "Only tested reward loras so far with UniAnimate, they did work",
      "from": "Kijai"
    },
    {
      "finding": "UniAnimate gives everything slightly 3D look",
      "details": "Observed effect when using UniAnimate for pose-driven animation",
      "from": "Kijai"
    },
    {
      "finding": "UniAnimate has no speed overhead",
      "details": "Same speed as 14B I2V, not seeing any overhead when using UniAnimate",
      "from": "Kijai"
    },
    {
      "finding": "SkyReels 1.3B I2V model breaks in wrapper due to clip vision model",
      "details": "Using penultimate hidden states instead of last from clip vision output fixes the issue",
      "from": "Kijai"
    },
    {
      "finding": "SkyReels leaked weights are identical to official release",
      "details": "Same upload dates, repo was just private temporarily",
      "from": "Kijai"
    },
    {
      "finding": "SkyReels V2 I2V 1.3B has strong prompt following",
      "details": "Prompt following is so strong it even ignores the image if it can't do it otherwise",
      "from": "Kijai"
    },
    {
      "finding": "SkyReels DF model can be used asynchronously",
      "details": "Diffusion Forcing architecture allows asynchronous usage",
      "from": "Kijai"
    },
    {
      "finding": "VACE works with SkyReels DF models",
      "details": "Successfully tested with 161 frames",
      "from": "DawnII"
    },
    {
      "finding": "DF (Diffusion Forcing) models can continue from any number of frames",
      "details": "Works better at frame continuation than VACE or Fun InP, can theoretically go forever by chaining nodes",
      "from": "Kijai"
    },
    {
      "finding": "Only the 1.3B DF model has fps_embeds",
      "details": "14B DF model lacks fps embedding layers, so fps parameters won't work with it",
      "from": "Kijai"
    },
    {
      "finding": "DF works with T2V concept using input latents",
      "details": "Number of prefix_samples input will overwrite the noise for continuation",
      "from": "Kijai"
    },
    {
      "finding": "addnoise_condition parameter in DF",
      "details": "Seems to add noise and improve the new frames generated",
      "from": "Kijai"
    },
    {
      "finding": "DF requires decode/encode between loops for quality",
      "details": "Tried with latents directly and it degraded quality, need to decode/encode between windows",
      "from": "Kijai"
    },
    {
      "finding": "VACE works with DF models",
      "details": "Can combine VACE control with DF frame continuation, pushed fix to make it work properly",
      "from": "Kijai"
    },
    {
      "finding": "Gray frames (0.5) work better than black frames for VACE",
      "details": "Grey 127 is what the model recognizes for inpainting, use empty_frame_level 0.5 instead of 0",
      "from": "DawnII"
    },
    {
      "finding": "1.3B LoRAs work on SkyReels models",
      "details": "LoRAs trained on Wan base work great on SkyReelsV2-T2V-14B-720P",
      "from": "mamad8"
    },
    {
      "finding": "SageAttention provides significant speed boost",
      "details": "Twice as fast as SDPA with 99% quality retention",
      "from": "Kijai"
    },
    {
      "finding": "TeaCache threshold works like step count",
      "details": "Too high threshold skips too many steps and has huge quality hit",
      "from": "Kijai"
    },
    {
      "finding": "Text encoder precision can be higher than model weights",
      "details": "Can use text encoding at bf16 even with model at fp8, beneficial and separate model",
      "from": "Kijai"
    },
    {
      "finding": "DG models are T2V only",
      "details": "DG models are ~3GB T2V models without layers to handle clip embeds",
      "from": "Kijai"
    },
    {
      "finding": "Native VACE is implemented as a model",
      "details": "Can only use one VACE embed in native, not multiple like wrapper",
      "from": "Kijai"
    },
    {
      "finding": "Wrapper torch compile works with LoRAs",
      "details": "Compile works different with wrapper, will work with loras no problem",
      "from": "Kijai"
    },
    {
      "finding": "VACE cannot be stacked/combined currently",
      "details": "Each VACE includes ref, mask and control together making it difficult to mix multiple ones. Would require ~40 nodes to make it work properly",
      "from": "Draken"
    },
    {
      "finding": "Phantom model puts reference latents at end instead of start",
      "details": "Unlike VACE and A2 which put ref latents at start, Phantom appends them at the end",
      "from": "Kijai"
    },
    {
      "finding": "14B DF model has massive time step embedding memory usage",
      "details": "Time step embedding alone takes 7GB when using DF sampling. TeaCache clones timeembed for cache at ~4GB",
      "from": "Kijai"
    },
    {
      "finding": "VACE is more sensitive to control input artifacts than base 1.3B",
      "details": "VACE picks up on depth preprocessing artifacts like banding, making it worse for second passing when control inputs have issues",
      "from": "David Snow"
    },
    {
      "finding": "Phantom needs 3 model predictions",
      "details": "Based on the model structure, Phantom requires 3 separate model predictions during inference",
      "from": "Kijai"
    },
    {
      "finding": "SkyReels V2 I2V prompting methodology focuses only on dynamics",
      "details": "According to the paper, I2V prompts should only describe temporal action/expression/camera motion and eliminate static information like background/environment descriptions. The image provides static visual context, so prompts should focus purely on what changes over time.",
      "from": "fredbliss"
    },
    {
      "finding": "Phantom model supports multiple reference images",
      "details": "Phantom can take multiple reference images as input for consistent video generation, with a limit of 4 images. Images need to be encoded separately and then batched into different latents.",
      "from": "Kijai"
    },
    {
      "finding": "CFG scheduling can replace two-pass workflows",
      "details": "Instead of using two separate samplers, you can schedule CFG values per step in a single sampler using a list of floats (e.g., '6,6,6,6,6,1,1,1,1,1' for 10 steps)",
      "from": "MilesCorban"
    },
    {
      "finding": "Phantom does 3 passes during sampling with 33% time overhead",
      "details": "Phantom has no memory overhead but adds 33% sampling time due to doing 3 passes during generation",
      "from": "Kijai"
    },
    {
      "finding": "SkyReels V2 uses structured caption fusion methodology",
      "details": "The training uses structured captions fused specifically for I2V, focusing on action/expression/camera motion rather than detailed visual descriptions",
      "from": "fredbliss"
    },
    {
      "finding": "Phantom works well with multiple viewpoints",
      "details": "Tested with 4 different viewpoints as inputs to Phantom, helps a lot with fidelity of the person. Using 4 separate latents works better than feeding a grid of 2x3 images in one latent",
      "from": "mamad8"
    },
    {
      "finding": "TeaCache skips all steps at very low thresholds",
      "details": "At 0.13 threshold with 14B models, TeaCache skipped all conditional and unconditional steps and made generation 30% slower than baseline",
      "from": "lostintranslation"
    },
    {
      "finding": "SLG improves hand motion but causes color burnout",
      "details": "SLG prevents arm disappearance and improves motion, but causes snotty/burned appearance requiring lower CFG values",
      "from": "lostintranslation"
    },
    {
      "finding": "fp16_fast provides significant speed boost",
      "details": "Switching to pytorch nightly and using fp16_fast reduced generation time from 400s to 300s on 4080",
      "from": "lostintranslation"
    },
    {
      "finding": "720p models work better for camera movement prompts",
      "details": "480p model failed to generate rotation with 'Camera slowly rotates to the right' prompt across 5 seeds, 720p model succeeded on first seed",
      "from": "N0NSens"
    },
    {
      "finding": "Color matching fixes brightness drift in DF model",
      "details": "DF model gets progressively brighter after first few frames when chaining videos together. Adding color match referencing initial image resolves this issue",
      "from": "MilesCorban"
    },
    {
      "finding": "Torch compile provides speed improvement on 4090",
      "details": "With compile: 2.94s/it, without: 4s/it on 4090 (640x480x33f)",
      "from": "MilesCorban"
    },
    {
      "finding": "TeaCache memory usage varies dramatically by mode",
      "details": "TeaCache mode 'e' uses 600MB while 'e0' uses 4000MB with 14B model",
      "from": "Kijai"
    },
    {
      "finding": "Phantom works better than VACE for character consistency",
      "details": "Phantom appears better at keeping character identity consistent compared to VACE",
      "from": "NebSH"
    },
    {
      "finding": "VACE ending at 0.5 improves Phantom+VACE combination",
      "details": "When combining Phantom and VACE, ending VACE at 0.5 produces better results",
      "from": "Kijai"
    },
    {
      "finding": "SkyReels V2 supports up to 97 frames (possibly 121) at 24fps compared to base Wan's 16fps",
      "details": "Model allows chaining infinite generations without visible seams",
      "from": "seitanism"
    },
    {
      "finding": "fp8_fast works well with SkyReels V2 720p T2V model",
      "details": "Provides 25% speed boost with crisp outputs, no quality degradation",
      "from": "seitanism"
    },
    {
      "finding": "Second sampler in DF workflow uses more VRAM than first",
      "details": "First sampler uses fewer frames in Kijai's workflow, second sampler is 17 frames more",
      "from": "seitanism"
    },
    {
      "finding": "Teacache causes VRAM spike and memory that doesn't clear between samplers",
      "details": "Turning off teacache prevents increased vram on subsequent samplers",
      "from": "DawnII"
    },
    {
      "finding": "Skyreels models have oversaturated colors compared to base WAN",
      "details": "Skyreels outputs have intensive, oversaturated colors. Adding 'natural color palette' or 'undersaturated colors' to prompts doesn't seem to change this. Base WAN has more natural coloring",
      "from": "seitanism"
    },
    {
      "finding": "Skyreels does scene details much better than base WAN",
      "details": "Despite color issues, Skyreels produces better details in scenes overall",
      "from": "seitanism"
    },
    {
      "finding": "fp8_fast works with T2V but may ruin I2V quality",
      "details": "fp8_fast gives 20-25% speed boost (4.5min to 3.5min) with T2V without noticeable quality loss, but totally destroys quality with I2V",
      "from": "seitanism"
    },
    {
      "finding": "UniAnimate LoRA only works with 14B models, not 1.3B",
      "details": "The UniAnimate LoRA from Kijai's HF is specifically for 14B models and doesn't work with 1.3B variants",
      "from": "Kijai"
    },
    {
      "finding": "Skyreels V2 models are native 24fps",
      "details": "New V2 models output native 24fps instead of 16fps, with 97 frames for 540p and 121 frames for 720p models",
      "from": "Kijai"
    },
    {
      "finding": "fp8_fast quality degradation fix",
      "details": "Kijai discovered that using same dtype for input and weight in fp8 scaled matmul now works, fixing the quality degradation with Wan that was caused by casting inputs to different fp8 precision",
      "from": "Kijai"
    },
    {
      "finding": "30% speedup with fp8_fast fix",
      "details": "The fp8_fast fix provides 30% speedup on 4000 series GPUs and up while maintaining better quality than before",
      "from": "Kijai"
    },
    {
      "finding": "Hypercontrast style fix",
      "details": "Adding 'natural colors. muted tones' to the end of prompts fixes the hyper-saturated, bright white highlights and pure black shadows style that Wan sometimes produces",
      "from": "Screeb"
    },
    {
      "finding": "TeaCache frame count mismatch causes OOM",
      "details": "Example workflow has higher frame count for subsequent samplers than first sampler, causing TeaCache to OOM - needs to be lowered to same count",
      "from": "Cubey"
    },
    {
      "finding": "Fun 1.1 control model now includes reference image functionality similar to Animate Anyone",
      "details": "The Control model can accept both a reference image and a control video as inputs for generation, providing effects similar to Animate Anyone while retaining original functionality",
      "from": "DawnII"
    },
    {
      "finding": "Camera control model supports pan-and-tilt movements",
      "details": "New camera control model supports left, right, up, down movements",
      "from": "DawnII"
    },
    {
      "finding": "Sparge attention tuned parameters available for Wan 2.1 1.3B",
      "details": "Sparge tuned wan 2.1 1.3b model available on HuggingFace, but tuned parameters are very model specific and limited to 1.3B T2V only",
      "from": "yi"
    },
    {
      "finding": "Fun 1.1 control works better than 1.0 for following depth maps",
      "details": "Fun 1.1 follows depth maps better and resolved previous issues with unwanted gesturing and talking",
      "from": "boorayjenkins"
    },
    {
      "finding": "Reference image has limited effect when paired with start image",
      "details": "When using both reference image and start image together, the reference image does not have much effect",
      "from": "DawnII"
    },
    {
      "finding": "Adding noise_aug_strength of 0.03 helps motion following",
      "details": "Adding noise_aug_strength of 0.03 back to WanVideo ImageToVideo Encode makes it follow motion correctly again",
      "from": "boorayjenkins"
    },
    {
      "finding": "TeaCache optimization working in Phantom model",
      "details": "TeaCache skipped 5 conditional, unconditional, and prediction_2 steps at steps [7, 9, 11, 13, 15]",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "finding": "VACE native with auto masking system using segment anything",
      "details": "Created workflow combining VACE native implementation with automatic masking using segment anything",
      "from": "V\u00e9role"
    },
    {
      "finding": "Loop_args parameter exists in WAN sampler",
      "details": "Loop_args parameter found in sampler code but passes value as dict 'as is', samples show null values",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "DG Fun model can do start/end frame morphing that VACE cannot",
      "details": "DG model demonstrated capability for start/end frame transitions that VACE couldn't achieve",
      "from": "N0NSens"
    },
    {
      "finding": "ComfyUI rope function fixes dtype errors",
      "details": "Switching rope function from 'default' to 'comfy' resolves dtype mismatch errors in WAN sampler",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "HiDream base model outperforms Flux with no grid pattern and no 2nd pass needed",
      "details": "Colin deleted all Flux models after trying HiDream base model",
      "from": "Colin"
    },
    {
      "finding": "DF (1.3B 540 fp32) gives proper preview much sooner than base Wan",
      "details": "Noticed during generation of 81 frames at 24fps",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "finding": "Fun 1.1 models perform lot better than previous Fun models",
      "details": "Also supports reference image now",
      "from": "Kijai"
    },
    {
      "finding": "Fun_inp is way better than VACE for start/end images",
      "details": "David Snow found it very impressive in testing",
      "from": "David Snow"
    },
    {
      "finding": "CFG can be traded for very high shift values for 50% inference time reduction",
      "details": "Setting CFG to 1 and shift to ridiculous value like 200 in vid2vid workflow with Wan 1.3 + VACE",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "finding": "Shift values of 10-17 work well for Skyreels, especially for 14B model",
      "details": "Good pointer for improved results",
      "from": "Colin"
    },
    {
      "finding": "Fun models work even when feeding zeros to additional input channels",
      "details": "They're trained to handle this gracefully",
      "from": "Kijai"
    },
    {
      "finding": "FantasyTalking uses additional cross attention for audio conditioning",
      "details": "Can adjust scale of attention and run extra pass with CFG control",
      "from": "Kijai"
    },
    {
      "finding": "Fantasy Talking model has audio CFG bug fixed",
      "details": "Missing brackets in audio CFG calculation caused sync issues, fixed as of 1h ago. Python didn't error from malformed calculation",
      "from": "Kijai"
    },
    {
      "finding": "Fantasy Talking works beyond 81 frames",
      "details": "Model can work with 121 frames but motion quality degrades. Uses 23fps output",
      "from": "Kijai"
    },
    {
      "finding": "Empty audio causes Fantasy Talking sync issues",
      "details": "Model doesn't know what to do with silence, may start lip sync from beginning despite silent audio causing offset",
      "from": "Kijai"
    },
    {
      "finding": "Fantasy Talking effect weakens at higher resolutions",
      "details": "Model becomes less effective at higher resolutions than its training resolution",
      "from": "Kijai"
    },
    {
      "finding": "Fantasy Talking only supports mono audio",
      "details": "If stereo input is provided, only first channel (left) is used",
      "from": "Kijai"
    },
    {
      "finding": "Control LoRAs incompatible with VACE",
      "details": "Control LoRAs modify model input channels making them incompatible with other techniques on same steps",
      "from": "Kijai"
    },
    {
      "finding": "Multiple Fun Control inputs can be composited",
      "details": "Instead of using multiple control nodes, composite control images on top of each other (depth + outlines + pose)",
      "from": "Kijai"
    },
    {
      "finding": "Different seeds help prevent DirectFix overburning",
      "details": "Using different seeds for each sampler helps prevent DF getting stuck in same output and overexposing",
      "from": "Kijai"
    },
    {
      "finding": "Fun control models have different resolution training",
      "details": "1.3B models trained at 480p, 14B models trained at 720p",
      "from": "Gavmakes"
    },
    {
      "finding": "Fun Control 1.1 models support reference images",
      "details": "Can use either reference image or start frame, reference doesn't have to match control closely",
      "from": "Kijai"
    },
    {
      "finding": "VACE can merge depth and normal pass controls",
      "details": "Depth and normal merged work well, but keep other controls like lineart and pose separate",
      "from": "David Snow"
    },
    {
      "finding": "Fantasy Talking has frame onset delay",
      "details": "LoRA transformation always kicks in after 1 second, may be inherent to the model",
      "from": "Stad"
    },
    {
      "finding": "720p model follows prompts better than 480p",
      "details": "Same generation time but 720p has better prompt adherence, though can flicker at low resolutions",
      "from": "N0NSens"
    },
    {
      "finding": "Camera control uses Google's RealEstate10K dataset",
      "details": "Based on Google's dataset for 3D camera movements",
      "from": "Kijai"
    },
    {
      "finding": "TeaCache disables native VACE node",
      "details": "Both KJ nodes TeaCache and original disable VACE functionality",
      "from": "Aaron Jason"
    },
    {
      "finding": "Using a fun ref image latent with an empty embed instead of feeding in the ref image prevents flash/blink at start",
      "details": "Works well with no flash when using this approach",
      "from": "Gavmakes"
    },
    {
      "finding": "14B control camera is significantly better than 1.3B",
      "details": "Much improved camera control quality",
      "from": "Kijai"
    },
    {
      "finding": "Empty frame level 0.5 is what the model is trained with",
      "details": "Corresponds to 127, 127, 127 in RGB - mid gray",
      "from": "Kijai"
    },
    {
      "finding": "Camera movements are fully customizable using Blender paths",
      "details": "Can create custom camera trajectories in Blender and import them",
      "from": "Kijai"
    },
    {
      "finding": "Wan I2V struggles to bring new elements into existence that aren't already in the image",
      "details": "Works for simple additions like 'put on hat' but not for creating entirely new objects",
      "from": "Draken"
    }
  ],
  "troubleshooting": [
    {
      "problem": "T5 TextEncoder crashing",
      "solution": "T5 model file was corrupt - delete and redownload the file",
      "from": "VK (5080 128gb)"
    },
    {
      "problem": "Tiling causing black outputs",
      "solution": "Turn off tiling - only needed for very large resolutions, VAE does 2 latents at once so length doesn't affect VRAM",
      "from": "Kijai"
    },
    {
      "problem": "Grey output when using Fun models with text only",
      "solution": "Fun inpaint model is terrible at T2V, depends on control signal to infer color in some cases",
      "from": "Kijai"
    },
    {
      "problem": "Flash at video start with specific lora",
      "solution": "Turning off flat color lora eliminates the flash at the start",
      "from": "David Snow"
    },
    {
      "problem": "First frame corruption with TeaCache",
      "solution": "Set TeaCache start value to 10 or disable it, or use lower threshold values (0.2 works better with 20 steps)",
      "from": "JohnDopamine"
    },
    {
      "problem": "VAE taking forever with long sequences",
      "solution": "Disable tiling - length doesn't affect VRAM use as VAE processes 2 latents at once",
      "from": "Kijai"
    },
    {
      "problem": "SageAttention error in CLIP vision encode",
      "solution": "Probably using wrong clipvision model",
      "from": "ameasure"
    },
    {
      "problem": "VACE flashy/bad results",
      "solution": "Turn off 'batched cfg' on the sampler",
      "from": "TK_999"
    },
    {
      "problem": "Expected all tensors to be on the same device error",
      "solution": "Remove block swap node when using VACE",
      "from": "Draken"
    },
    {
      "problem": "Reference image not working properly",
      "solution": "Pad the image and place on white background, remove first latent",
      "from": "Kijai"
    },
    {
      "problem": "Control depth not working with VACE",
      "solution": "Disconnect control depth input to fix generation issues",
      "from": "makeitrad"
    },
    {
      "problem": "OOM on 24GB with 33 frames",
      "solution": "Use quantization (fp8_e4m3fn) and proper precision settings",
      "from": "CJ"
    },
    {
      "problem": "VACE embeds widget disappeared from wrapper sampler",
      "solution": "Updated to go into image_embeds input instead",
      "from": "Hashu"
    },
    {
      "problem": "Masking input video causes errors",
      "solution": "Don't mask input video, mask the reference image instead. Model doesn't like alpha channels",
      "from": "VK (5080 128gb)"
    },
    {
      "problem": "Reference image resolution mismatch causes errors",
      "solution": "Keep reference image resolution consistent, errors occur when changing to different resolutions like 480x832",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "problem": "Style transfer not working with background",
      "solution": "Remove background completely from reference image, model trained to look for refs separated by white background",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "Oversaturated and plastic-looking outputs",
      "solution": "Turn off SLG or use specific SLG settings: block 8 with 0.1 start to 0.3-5 end",
      "from": "IllumiReptilien"
    },
    {
      "problem": "Flashing and color shifting on wider aspect ratios",
      "solution": "Resize to 16:9 720p to stop flashing",
      "from": "Faux"
    },
    {
      "problem": "TeaCache not working great with VACE",
      "solution": "Use threshold around 0.1 instead of normal 0.015",
      "from": "Kijai"
    },
    {
      "problem": "VACE reference not passed for context windows",
      "solution": "Known limitation that ref image isn't passed for context windows",
      "from": "Kijai"
    },
    {
      "problem": "TeaCache memory issue with VACE",
      "solution": "Fixed memory leak in TeaCache - reduced max reserved memory from 22.250 GB to 17.031 GB for 81 frames at 480x832",
      "from": "Kijai"
    },
    {
      "problem": "WanModel object has no attribute 'vace_blocks'",
      "solution": "Need to update Wan nodes to latest version, not just ComfyUI",
      "from": "Kijai"
    },
    {
      "problem": "torch._scaled_mm error on RTX 3060",
      "solution": "Don't use fp8_fast models on cards with compute capability < 9.0. Use regular fp8 models instead",
      "from": "Faux"
    },
    {
      "problem": "DPM++_SDE convergence changed after timestep fix",
      "solution": "Switch to Euler scheduler which works properly after the fix",
      "from": "DevouredBeef"
    },
    {
      "problem": "OOM issues on 12GB VRAM with VACE",
      "solution": "Use 27+ block swap, reduce resolution to 480x480 and frames to 33 initially, then scale up",
      "from": "zelgo_"
    },
    {
      "problem": "VACE outputs turning orange",
      "details": "Issue with not using proper masks or reference setup",
      "solution": "Use proper masking with gray areas for missing parts",
      "from": "amli"
    },
    {
      "problem": "Getting weird VACE outputs",
      "details": "First time using wrapper with strange results",
      "solution": "Need the TE bridge for proper setup",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "VACE outputting portrait instead of landscape",
      "details": "Input is 768x432 landscape but output is portrait",
      "solution": "Disable 'swap dimensions' in CR SD1.5 Aspect Ratio node",
      "from": "Ashtar"
    },
    {
      "problem": "Workflow freezing when dragging",
      "details": "Workflow freezes when zooming out or dragging",
      "solution": "Update ComfyUI frontend to resolve compatibility issues",
      "from": "seitanism"
    },
    {
      "problem": "RuntimeError about tensors on different devices",
      "details": "After wrapper update, getting CPU and CUDA device mismatch error",
      "solution": "One node may be on CPU when should be CUDA",
      "from": "PirateWolf"
    },
    {
      "problem": "VACE outputs video too bright",
      "details": "Video comes out overexposed when refining animation",
      "solution": "Try color match node hooking input vid to ref",
      "from": "LarpsAI"
    },
    {
      "problem": "Triton error on Windows",
      "details": "Generic Triton error occurring",
      "solution": "Clear Triton cache",
      "from": "Kijai"
    },
    {
      "problem": "Resolution not working with 856",
      "details": "856 divisible by 8 but causing errors",
      "solution": "Changed to 848 and it works, may need divisible by 16",
      "from": "traxxas25"
    },
    {
      "problem": "Sageattention bfloat/headdim errors",
      "solution": "Remove TRELLIS and Hi3DGen nodes - they override global attention which should never be done",
      "from": "Kijai"
    },
    {
      "problem": "VACE OOMs and memory issues during inference",
      "solution": "Use VACE block swap feature and ensure you're on latest commit with TeaCache memory bug fix",
      "from": "Kijai"
    },
    {
      "problem": "Wrong model error when switching between Wan models",
      "solution": "Purge VRAM and reload models when changing between Fun 1.3B and 14B variants",
      "from": "Seb"
    },
    {
      "problem": "VACE always skipping step 0",
      "solution": "Major bug fixed - start/end percent check was broken, now properly handles first step",
      "from": "Kijai"
    },
    {
      "problem": "Black video outputs with latest WanWrapper version",
      "solution": "Use ComfyUI text encoding with the bridge instead of wrapper's built-in encoding",
      "from": "Kijai"
    },
    {
      "problem": "Tensor size mismatch error with VACE Encode",
      "solution": "Check for Load Video node with select_every_nth parameter causing frame count discrepancy",
      "from": "mamad8"
    },
    {
      "problem": "Hot mess/noisy orange output with VACE workflow",
      "solution": "Ensure pad node is connected and reference image is on white canvas",
      "from": "Kijai"
    },
    {
      "problem": "Gray halo around head when mask is too large",
      "solution": "Reduce mask size to avoid artifacts",
      "from": "Zuko"
    },
    {
      "problem": "Skeleton/corrupted outputs with control",
      "solution": "Check latent strength settings and image blend node clipping",
      "from": "Kijai"
    },
    {
      "problem": "FlowMatchEulerDiscreteScheduler error with use_beta_sigmas",
      "solution": "Update nodes or use UniPC scheduler as alternative",
      "from": "David Snow"
    },
    {
      "problem": "Compile broken with teacache in latest wrapper",
      "solution": "Only compile transformer blocks instead of full model",
      "from": "Kijai"
    },
    {
      "problem": "OOM with 81 frames on 12GB VRAM",
      "solution": "Use vace block to swap=15 and normal block swap=30, or reduce frame count",
      "from": "Ashtar"
    },
    {
      "problem": "WanVideo BlockSwap error with VACE workflow",
      "solution": "Set VACE blocks to more than 0 when using block swap, can be 1-15",
      "from": "Kijai"
    },
    {
      "problem": "Shape error with custom heights",
      "solution": "Height and width must be divisible by 16, not just 8",
      "from": "Kijai"
    },
    {
      "problem": "Two different devices cuda:0 error",
      "solution": "Add 1 VACE block to swap or disconnect swap blocks node",
      "from": "JohnDopamine"
    },
    {
      "problem": "Lotus model error",
      "solution": "Wrong VAE - needs to use the 1.5 VAE, not other VAEs",
      "from": "JmySff"
    },
    {
      "problem": "Getting only 4 images in output",
      "solution": "Check number of control images - output matches number of control images provided",
      "from": "mamad8"
    },
    {
      "problem": "Bone artifacts appearing in outputs",
      "solution": "Use base model instead of DG, or reduce VACE strength",
      "from": "IllumiReptilien"
    },
    {
      "problem": "Memory allocation errors when canceling workflows",
      "solution": "Use unload models button at the top, though it takes time",
      "from": "David Snow"
    },
    {
      "problem": "Orange tinted outputs in VACE",
      "solution": "Use correct text encoder - fp8 scaled with upper group, bf16 from KJ with lower group",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Tensor error with negative dimension when doubling VACE embeds",
      "solution": "Need reference image in both VACE nodes, can use blank grey image",
      "from": "DawnII"
    },
    {
      "problem": "Poor likeness with VACE inpainting",
      "solution": "Remove background fully from reference, check resizing settings to avoid stretching",
      "from": "Kijai"
    },
    {
      "problem": "Pose dots appearing in final output",
      "solution": "Issue occurs when overlaying DWPose on top of Depthmap in black",
      "from": "burgstall"
    },
    {
      "problem": "Black output with I2V and SageAttention on auto mode",
      "solution": "Set SageAttention to disabled or manual mode instead of auto",
      "from": "Kijai"
    },
    {
      "problem": "Expected all tensors to be on the same device CUDA error with Grounding Dino",
      "solution": "Replace the problematic node with the lower/alternative version",
      "from": "\u2727\u0e05\u0e42\u0e51\u2180\u11ba\u2180 \u0e51\u0e43\u0e05\u2727PookieNumnums"
    },
    {
      "problem": "Tight shaped mask forcing unwanted shapes in output",
      "solution": "Try using box mask instead of character cutout shapes",
      "from": "Kijai"
    },
    {
      "problem": "TeaCache causing mosaic/noisy output with low step counts",
      "solution": "Use higher step counts (20+ steps) or lower threshold values, avoid with ancestral/SDE samplers",
      "from": "Kijai"
    },
    {
      "problem": "Ming nodes brightness/contrast not working with multiple frames",
      "solution": "Use dzNodes brightness adjustment instead",
      "from": "V\u00e9role"
    },
    {
      "problem": "Pose dots bleeding through in VACE output",
      "solution": "Issue with overlayed controlnets - VACE doesn't play well with them",
      "from": "traxxas25"
    },
    {
      "problem": "Depth video showing outside mask area",
      "solution": "Mask the depth map before feeding into VACE, or use original video with mask overlay",
      "from": "traxxas25"
    },
    {
      "problem": "ComfyUI loading wheel spinning indefinitely after update",
      "solution": "Reinstall ComfyUI from scratch, then install custom nodes in small batches of 2 at a time through manager",
      "from": "David Snow"
    },
    {
      "problem": "Video-depth-anything preprocessor giving OOM after install",
      "solution": "Switch to depthcrafter preprocessor, or ensure xformers is working properly as depth anything relies on it",
      "from": "David Snow"
    },
    {
      "problem": "Control networks destroying character consistency",
      "solution": "Try lower control strength, or mask out faces from control maps if possible. Issue affects both face and clothing consistency",
      "from": "wange1002"
    },
    {
      "problem": "SkyreelsA2 VACE node error",
      "solution": "Use normal I2V workflow instead of VACE nodes - SkyreelsA2 has nothing to do with VACE",
      "from": "Kijai"
    },
    {
      "problem": "VACE first/last frame not smooth interpolation",
      "solution": "Try higher shift values (up to 35), interpolate more frames between start/end if too much difference, or create character LoRA for better likeness",
      "from": "DawnII"
    },
    {
      "problem": "Blender depth maps don't work with VACE",
      "solution": "Add 3 pixel blur to the depth map - even minimal blur fixes the issue",
      "from": "Kijai"
    },
    {
      "problem": "CLIP images must be squares and center cropped by default",
      "solution": "Make sure reference image fits the offset position when using bear offset",
      "from": "Kijai"
    },
    {
      "problem": "Lineart showing white lines in final render",
      "solution": "Invert colors for lineart control input",
      "from": "JmySff"
    },
    {
      "problem": "Target width/height padding not working",
      "solution": "Disconnect the target_width and height inputs completely, don't use 0 or -1",
      "from": "Kijai"
    },
    {
      "problem": "Context window transitions visible in background",
      "solution": "Use control for background or accept that uncontrolled areas will change between windows",
      "from": "Kijai"
    },
    {
      "problem": "OOM errors with SAM2 nodes",
      "solution": "No specific solution provided, user seeking help",
      "from": "PirateWolf"
    },
    {
      "problem": "Torch weights loading error with newer PyTorch versions",
      "solution": "Set weights_only=False in torch.load or use safe_globals context manager",
      "from": "Ashtar"
    },
    {
      "problem": "VACE doesn't work well with blurred/faded masks",
      "solution": "Use solid masks without blur for better results, blurred masks leave artifacts on edges",
      "from": "A.I.Warper"
    },
    {
      "problem": "Image dimensions causing broadcasting errors in i2v",
      "solution": "Image dimensions need to be divisible by 16, round to nearest multiple of 16",
      "from": "traxxas25"
    },
    {
      "problem": "VACE inserting unwanted people in masked areas",
      "solution": "Ensure bounding box completely covers subject, mask shadows/reflections, and tint mask color to match background",
      "from": "traxxas25"
    },
    {
      "problem": "TeaCache not showing status and running slower after ComfyUI update",
      "solution": "Issue with automated memory management and patches, use --reserve-vram startup argument",
      "from": "Scrap"
    },
    {
      "problem": "Wireframe control not working well",
      "solution": "Use line extraction instead of wireframe for better results",
      "from": "Juan Gea"
    },
    {
      "problem": "FP8_E5 compilation not working on 30-series",
      "solution": "Works with torch 2.7 and triton 3.1.0, need to delete .triton cache",
      "from": "Hashu"
    },
    {
      "problem": "Getting fewer output frames than input control frames",
      "solution": "Issue observed with VACE start/end frame - 24 frames in, 21 frames out",
      "from": "notid"
    },
    {
      "problem": "Flashing and artifacts in vid2vid refinement",
      "solution": "Keep denoise above 0.5, control with latent strength, end percent and cfg. Low denoise causes weird flashing",
      "from": "HeadOfOliver"
    },
    {
      "problem": "Native T2V not working",
      "solution": "Must provide start_image and clip vision for native T2V to work",
      "from": "BondoMan"
    },
    {
      "problem": "Last 3 frames always black in native I2V",
      "solution": "Issue was using VAE decode (tiles) - using normal VAE decode solved it",
      "from": "Ka\u00efros"
    },
    {
      "problem": "OOM on RTX 2080 with bf16",
      "solution": "RTX 2080 doesn't fully support bf16 - switch to fp16 precision for VAE loader and model loader",
      "from": "Kijai"
    },
    {
      "problem": "T5 text encoder fp16 issues",
      "solution": "Original T5 text encoder won't work in fp16 - use bf16 or native text encoding setup with fp8 scaled",
      "from": "Kijai"
    },
    {
      "problem": "Spotty results with weird contrast using WanSampler",
      "solution": "Issue was text encoder precision - don't use T5 wrapper in fp16, use bf16 or comfy native loader",
      "from": "Kijai"
    },
    {
      "problem": "Fun Control workflow loading with wrong connections",
      "solution": "Control Embeds connected to Extra Latents needs to be manually changed after loading example workflow",
      "from": "The Shadow (NYC)"
    },
    {
      "problem": "OOM errors on A4500 system",
      "solution": "Kijai fixed something that resolved strange OOM error",
      "from": "The Shadow (NYC)"
    },
    {
      "problem": "Tiled encoding was broken in original code",
      "solution": "Was being set in latent space then doing in pixel space, fixed to work properly",
      "from": "Kijai"
    },
    {
      "problem": "Video-depth-anything instant OOMs on fresh ComfyUI install",
      "solution": "Install xformers to resolve the issue",
      "from": "David Snow"
    },
    {
      "problem": "VACE tensor shape mismatch with denoise less than 1",
      "solution": "Need to pad input frames to match, duplicate first frame x4 and add to front",
      "from": "Kijai"
    },
    {
      "problem": "Triton recompile errors after model code changes",
      "solution": "Clear Triton cache by deleting temp folder contents",
      "from": "zelgo_"
    },
    {
      "problem": "ComfyUI freezing when loading VACE workflow during generation",
      "solution": "Only load workflows when not running generation",
      "from": "DawnII"
    },
    {
      "problem": "Missing WanVACEFaceSwap node",
      "solution": "Reinstall WanVideo wrapper, using older version",
      "from": "xwsswww"
    },
    {
      "problem": "Block swap can't be set to 0",
      "solution": "Set block swap to non-zero value",
      "from": "Faust-SiN"
    },
    {
      "problem": "Teacache skips all steps with conditioning combine nodes",
      "solution": "Play with start and end settings of teacache to make it work",
      "from": "JmySff"
    },
    {
      "problem": "Key errors when using reward loras with 1.3B model",
      "solution": "Expected behavior - normal 1.3B doesn't have img cross attention keys, only I2V models do. Everything else still loads and has effect",
      "from": "Kijai"
    },
    {
      "problem": "Freezing issues during generation",
      "solution": "Update dependencies for custom nodes - exact cause unclear but dependency updates resolved the issue",
      "from": "xwsswww"
    },
    {
      "problem": "Core dumped crashes on Ubuntu with 5090",
      "solution": "Check dmesg for GPU-related issues, verify correct clip model is being used",
      "from": "MilesCorban"
    },
    {
      "problem": "Tensor size mismatch error (16320 vs 57120)",
      "solution": "Update to latest wrapper - Kijai fixed it to automatically pad the encoded video",
      "from": "Kijai"
    },
    {
      "problem": "Torch compile recompile errors",
      "solution": "Fixed (hopefully) at least some of the torch compile recompile errors last night",
      "from": "Kijai"
    },
    {
      "problem": "Black frames in normal vid2vid when using context options",
      "solution": "Known issue with context options on 14b model normal wrapper vid2vid",
      "from": "Flipping Sigmas"
    },
    {
      "problem": "Mask showing as fog in vid2vid with low denoise",
      "solution": "Use proper control model like VACE or Fun Inpaint instead of latent masking with base t2v model. Apply gaussian noise to the mask",
      "from": "DawnII"
    },
    {
      "problem": "Tensor mismatch with pose estimator",
      "solution": "Make sure pose estimator is set to correct resolution matching your video",
      "from": "Roman_S"
    },
    {
      "problem": "Reference image flashing at beginning of video",
      "solution": "Caused by using saved latent node from ComfyUI core - use load video node instead",
      "from": "Johnjohn7855"
    },
    {
      "problem": "Xformers AttributeError after node updates",
      "solution": "Disable quantization on the model - xformers not used by Wan nodes but triggered by diffusers",
      "from": "Impactframes."
    },
    {
      "problem": "OOM issues with video depth",
      "solution": "Had to reduce video depth to 512, potential memory leak in depth processing",
      "from": "traxxas25"
    },
    {
      "problem": "Control embeds not affecting results at 100% denoise",
      "solution": "Control lora not strong enough for 1.0 denoise setting - reduce denoise or use stronger control",
      "from": "David Snow"
    },
    {
      "problem": "Color shift in VACE results",
      "solution": "Better prompting - removed 'warm' from 'warm dimly lit hotel lobby' prompt",
      "from": "traxxas25"
    },
    {
      "problem": "Invalid WanVideo model selected error",
      "solution": "VACE models need to be used in VACE nodes, not regular WAN nodes. Use the correct model type for each node",
      "from": "\u2727\u0e05\u0e42\u0e51\u2180\u11ba\u2180 \u0e51\u0e43\u0e05\u2727PookieNumnums"
    },
    {
      "problem": "Need both VACE model and base WAN model",
      "solution": "If using the basic VACE-only model, you need to use it WITH the original WAN model. The merged VACE models only need one file",
      "from": "\u2727\u0e05\u0e42\u0e51\u2180\u11ba\u2180 \u0e51\u0e43\u0e05\u2727PookieNumnums"
    },
    {
      "problem": "VACE doesn't like input masks with holes",
      "solution": "VACE has issues with masks that contain holes in them",
      "from": "IllumiReptilien"
    },
    {
      "problem": "Style drifting in vid2vid generations",
      "solution": "More reference frames are needed, and reward/high res fix loras have bias towards realism so process without them first then add on second pass",
      "from": "Hashu"
    },
    {
      "problem": "First couple frames have reference image burned in",
      "solution": "Try using neutral gray or white image as ref in embed with control, or pad the front of the video",
      "from": "Kijai"
    },
    {
      "problem": "Frame count mismatch in outputs",
      "solution": "VHS has to round frame counts based on FPS settings. 81 frames at 16 FPS doesn't fit evenly into 5 seconds",
      "from": "Kijai"
    },
    {
      "problem": "Tensor size mismatch when using reference image in VACE",
      "solution": "Update ComfyUI-WanVideoWrapper - reference images are now automatically padded",
      "from": "Kijai"
    },
    {
      "problem": "Blue bar on right side of video with Skip Layer Guidance",
      "solution": "Reduce CFG by half (use CFG 3 instead of 6) and set SLG blocks to 8 or (8,9) instead of 10 for 14B models",
      "from": "N0NSens"
    },
    {
      "problem": "Model loading error when model name doesn't contain '720' or '480'",
      "solution": "Model names must include numbers '720' or '480' to avoid 'model_variant referenced before assignment' error",
      "from": "Kytra"
    },
    {
      "problem": "TeaCache conflicts with certain step counts",
      "solution": "If video output becomes noisy or strange with Tea Cache, try disabling it",
      "from": "David Snow"
    },
    {
      "problem": "Context windows don't work well with TeaCache",
      "solution": "TeaCache too strong for context windows, reduce TeaCache strength or disable",
      "from": "Kijai"
    },
    {
      "problem": "Darkening in middle of video with Fun InP first-end frame workflow",
      "solution": "Identified as style drift issue, happens consistently across different content types",
      "from": "Blink"
    },
    {
      "problem": "OptimalStepsSchedule produces black output",
      "solution": "Doesn't work with unipc, only works with Euler and dpm samplers",
      "from": "Kijai"
    },
    {
      "problem": "Can't right click VideoHelperSuite video combine node to save video",
      "solution": "Issue related to new UI update",
      "from": "David Snow"
    },
    {
      "problem": "ComfyUI won't restart after update with AIOHTTP error",
      "solution": "Problem was caused by Jovimetrix custom node - removing it fixed the issue",
      "from": "AJO"
    },
    {
      "problem": "Tensor size mismatch when using mask with WanVideoEncode in VACE v2v",
      "solution": "Remove mask from encode since mask is already provided in VACE encode",
      "from": "hablaba"
    },
    {
      "problem": "Context options changing style completely",
      "solution": "Start frames won't be part of windows that aren't the first, but reference images work because they're added to each context manually and aren't positional",
      "from": "Kijai"
    },
    {
      "problem": "Black frame outputs",
      "solution": "Update ComfyUI, frontend, Kijai nodes, and WanWrapper. Also refresh and try duplicating nodes then relinking",
      "from": "Flipping Sigmas"
    },
    {
      "problem": "Cannot use euler/beta scheduler",
      "solution": "Need to update WanVideoWrapper to latest version",
      "from": "Jas"
    },
    {
      "problem": "Washed out colors at start of generation",
      "solution": "Try switching off enhance-a-video node",
      "from": "N0NSens"
    },
    {
      "problem": "VACE producing distorted and dark first frames",
      "solution": "Issue with VACE inpainting causing gradual color recovery",
      "from": "PirateWolf"
    },
    {
      "problem": "Results degrading until cache cleared",
      "solution": "Use 'Free model and node cache' to prevent degradation",
      "from": "Ro"
    },
    {
      "problem": "SageAttention 2.0 installation issues on Windows",
      "solution": "Windows dev environment weirdness with CUDA paths - need CUDA 12.8 but PATH may point to 11.8",
      "from": "lostintranslation"
    },
    {
      "problem": "CUDA 12.8 not being detected despite installation",
      "solution": "Purge all old CUDA installations and reinstall with PATH configuration",
      "from": "Kijai"
    },
    {
      "problem": "Quality degrades after 1-2 generations with native + kjnodes",
      "solution": "Manually clear VRAM after each generation - bug with patch nodes and ComfyUI lowvram mode",
      "from": "Kijai"
    },
    {
      "problem": "Blurry outputs in Wan generations",
      "solution": "Increase steps to at least 20, adjust TeaCache start_step to 2 or 3",
      "from": "MilesCorban"
    },
    {
      "problem": "Fun Control breaks with end image",
      "solution": "Both Fun Control models don't support end image, only VACE supports start/end images with controlnet",
      "from": "DawnII"
    },
    {
      "problem": "Color flash at loop reset",
      "solution": "Set an end image and try color matching, though color matching isn't perfect",
      "from": "DawnII"
    },
    {
      "problem": "LoRAs don't work with torchcompile",
      "solution": "Use ModelPatcher node as workaround - compile applies first, then loras (weight patch)",
      "from": "Kijai"
    },
    {
      "problem": "GGUF and LoRAs use more VRAM and slow down significantly",
      "solution": "Use fp8 scaled models instead, or drop to Q5 GGUF. Every lora added slows GGUF down more",
      "from": "Kijai"
    },
    {
      "problem": "TeaCache bug with end steps causing generation failures",
      "solution": "Fixed in latest update, was aware of bug but kept forgetting to fix it",
      "from": "Kijai"
    },
    {
      "problem": "Deep fried/corrupted images in later workflows",
      "solution": "Something getting cached wrong affecting later workflows",
      "from": "lostintranslation"
    },
    {
      "problem": "Resolution error when changing from lowres to 832x480",
      "solution": "Need to change other matching parameters besides just resolution",
      "from": "N0NSens"
    },
    {
      "problem": "VACE depth map results in black and white outputs",
      "solution": "Reduce VACE strength significantly, but then it doesn't follow starting/ending frames as well",
      "from": "Sway"
    },
    {
      "problem": "14B LoRAs don't work on 14B Fun model but 1.3B LoRAs work on 1.3B Fun",
      "solution": "Fun models have more keys due to img cross attention, ComfyUI reports keys NOT applied but applies the rest",
      "from": "Kijai"
    },
    {
      "problem": "Quality degradation over multiple i2v generations in queue",
      "solution": "Clear VRAM cache after each generation to prevent permanent degradation",
      "from": "lostintranslation"
    },
    {
      "problem": "Black frames output in multiple LoRA workflow",
      "solution": "Related to optimizations like sageattn or torch.compile casting",
      "from": "MilesCorban"
    },
    {
      "problem": "FramePack showing black videos in Gradio",
      "solution": "Gradio glitch - check output folder for correct saved video",
      "from": "Benjimon"
    },
    {
      "problem": "CLIP encode node error in new nodes",
      "solution": "Need to concat (not average) the 2 image embeds in clip encode node",
      "from": "Kijai"
    },
    {
      "problem": "CUDA out of memory error",
      "solution": "Check if it's actually RAM issue not VRAM - allocation error = VRAM, other errors often mean out of RAM",
      "from": "Kijai"
    },
    {
      "problem": "FLF2V model producing still results with same start/end image",
      "solution": "Try higher CFG (7-10), full 1.0 motion lora, but higher CFG introduces artifacts",
      "from": "Eclipse"
    },
    {
      "problem": "VACE blur issues with depth control on anime style",
      "solution": "Try lineart instead of depth control, depth maps being too flat can cause camera distance confusion",
      "from": "Kijai"
    },
    {
      "problem": "VACE not following reference image",
      "solution": "Use first frame as encoded latent with proper temporal masking instead of just reference",
      "from": "Kijai"
    },
    {
      "problem": "Bad reference adherence in VACE",
      "solution": "Bump shift to double digits like 20-30, counter to base models",
      "from": "DawnII"
    },
    {
      "problem": "Quality degradation when stitching multiple video runs together",
      "solution": "Lower CFG and use color match node on the transition frame to maintain consistency",
      "from": "A.I.Warper"
    },
    {
      "problem": "Multiple VAE passes causing degradation",
      "solution": "Every decode/encode cycle degrades quality - minimize the number of VAE passes",
      "from": "DawnII"
    },
    {
      "problem": "LoRA-induced degradation between batches",
      "solution": "Clear VRAM between batches with different LoRAs using a dedicated clear VRAM workflow",
      "from": "lostintranslation"
    },
    {
      "problem": "VACE producing distorted results when control and reference don't align",
      "solution": "Start image should be close to the control, reference image doesn't necessarily need to mimic control",
      "from": "DawnII"
    },
    {
      "problem": "FramePack generating fixed backgrounds without movement",
      "solution": "Increase tile size for better quality, try higher resolution generation like 720x1280",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "problem": "3090 fp8 compatibility issues with torch inductor",
      "solution": "Use fp8_e5m2 weights with torch compile, or use SageAttention 2.0+ with direct cuda mode instead of Triton",
      "from": "Kijai"
    },
    {
      "problem": "Cannot import cached_download from huggingface_hub",
      "solution": "Updated the module so imports are in the nodes, shouldn't try to import huggingface_hub anymore",
      "from": "Kijai"
    },
    {
      "problem": "Model variant error when loading merged I2V model",
      "solution": "Rename the model to include 480 or 720 in filename, or update wrapper which defaults to 14B detection",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI freezing after producing output",
      "solution": "Multiple users experiencing this after latest update, suggested solution is to reinstall from scratch",
      "from": "lomerio"
    },
    {
      "problem": "High quality depth maps not working with VACE",
      "solution": "Blur the depth maps - VACE sees high quality depth as grayscale RGB and goes into colorize mode",
      "from": "Kijai"
    },
    {
      "problem": "Out of memory on VideoDepthAnythingProcess",
      "solution": "Use DepthAnythingV2 which is less resource intensive, or lower input size",
      "from": "David Snow"
    },
    {
      "problem": "Out of memory on WanVideoDecode",
      "solution": "Enable VAE tiling",
      "from": "David Snow"
    },
    {
      "problem": "UniAnimate tensor size mismatch error",
      "solution": "Match all sizes - resolution and frame count between inputs",
      "from": "Kijai"
    },
    {
      "problem": "White particles appearing in generations",
      "solution": "Lower the extra noise detail motion LoRA strength - too high causes this",
      "from": "David Snow"
    },
    {
      "problem": "ConnectionResetError at end of generation",
      "solution": "Can be ignored unless something actually breaks",
      "from": "David Snow"
    },
    {
      "problem": "Fast motion and hands getting blurry",
      "solution": "Turn off optimizations like SageAttention for better quality",
      "from": "Benjimon"
    },
    {
      "problem": "use_fresca toggle not working properly",
      "solution": "Bug reported - toggle does nothing, always activates if scale is set",
      "from": "Juampab12"
    },
    {
      "problem": "OOM errors in depth preprocessing",
      "solution": "Decrease input size and max res in the node, or use DepthAnything V2 which is less resource intensive",
      "from": "David Snow"
    },
    {
      "problem": "Reference images cause artifacts in VACE",
      "solution": "Disabling reference image eliminates flashes at start and glitches - no known fix for using reference images",
      "from": "David Snow"
    },
    {
      "problem": "Video distortion with 1.3B model",
      "solution": "Try 200 steps of DPM++ 2SA with eta=2.0 to eliminate temporal attention issues",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "problem": "Preprocessors pegging VRAM after update",
      "solution": "Full restart resolved the issue",
      "from": "David Snow"
    },
    {
      "problem": "Oversharpened upscaling artifacts",
      "solution": "Use RealESRGAN_x4plus_anime_6B instead of AnimeSharp for cleaner results",
      "from": "gshawn"
    },
    {
      "problem": "TeaCache node not properly disabling",
      "solution": "Node uses global patch, won't disable unless it runs again. When set to disabled it restores default attention mode. Remove launch parameter and restart.",
      "from": "Kijai"
    },
    {
      "problem": "UniAnimate tensor size mismatch error",
      "solution": "UniAnimate is for 14B only, won't work with other models or when combined with VACE",
      "from": "Kijai"
    },
    {
      "problem": "DWPose detection failures causing workflow crashes",
      "solution": "DWPose is not perfect and will fail on some videos. Added workaround to handle failed detections.",
      "from": "Kijai"
    },
    {
      "problem": "VACE oversaturated outputs with tile control",
      "solution": "Second pass processing, no combination of settings seems to eliminate it completely",
      "from": "David Snow"
    },
    {
      "problem": "Flash at start of vid2vid style transfer",
      "solution": "Changing control embed start to 1 from 0 gets rid of flash but introduces more movement",
      "from": "Gavmakes"
    },
    {
      "problem": "Wan wrapper update breaking workflows",
      "solution": "Update nodes after updating wrapper, issue was already fixed",
      "from": "Kijai"
    },
    {
      "problem": "SkyReels 1.3B I2V not working in wrapper",
      "solution": "Use penultimate hidden states from clip vision instead of last, or use wrapper clip vision node instead of native loader",
      "from": "Kijai"
    },
    {
      "problem": "Torch update needed for new model",
      "solution": "Update torch or use wrapper clip vision node",
      "from": "Kijai"
    },
    {
      "problem": "FLF jumping to last frame instead of morphing",
      "solution": "Use camera movement prompts instead of scene details",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "problem": "Second video in naive extension looks 'squished'",
      "solution": "Requires color correction, use VACE or Fun inp instead",
      "from": "Parker"
    },
    {
      "problem": "WanVideoDiffusionForcingSampler error about latent preview module",
      "solution": "Turn ON latent preview - error comes from having it off, fallback fails due to relative path issue",
      "from": "Kijai"
    },
    {
      "problem": "LoRA degradation issue with repeated generations",
      "solution": "Clear VRAM between generations, issue may be related to rgthree power lora loader",
      "from": "lostintranslation"
    },
    {
      "problem": "VACE strength control confusion",
      "solution": "Use two separate VACE encode nodes with different strength settings for different controls",
      "from": "Hashu"
    },
    {
      "problem": "VACE bad results in native",
      "solution": "Should be fixed now (Python whitespace issue)",
      "from": "comfy"
    },
    {
      "problem": "System memory fallback causing slowdown",
      "solution": "Activate 'prefer no system fallback' in Nvidia control panel",
      "from": "zelgo_"
    },
    {
      "problem": "Strong edge results with VACE + canny",
      "solution": "Use black lines on white background, anyline is better choice than canny",
      "from": "David Snow"
    },
    {
      "problem": "Swimming noise on rock textures",
      "solution": "Try lower CFG around 3.5-4.0 and shift around 4.0, avoid word 'wiggle' in prompt",
      "from": "Kytra"
    },
    {
      "problem": "Expected all tensors to be on same device error",
      "solution": "Issue was VRAM management, fixed in latest update",
      "from": "Dream Making"
    },
    {
      "problem": "TeaCache console message when disabled",
      "solution": "Generic console message when set to offload device",
      "from": "David Snow"
    },
    {
      "problem": "CUDA error with VACE when using first frame as ref_images",
      "solution": "Try setting non-blocking to False when offloading blocks, or reboot system as error may be random",
      "from": "Stad"
    },
    {
      "problem": "Torch compile eating all memory with DF models",
      "solution": "Issue with torch compile on DF models currently, avoid using compile with DF sampling",
      "from": "Kijai"
    },
    {
      "problem": "Video depth flipping in output",
      "solution": "Re-encode the input video file to fix format issues",
      "from": "David Snow"
    },
    {
      "problem": "TeaCache not clearing between sampler nodes causing OOM",
      "solution": "TeaCache doesn't clear cache between sampler nodes on 2nd sampler if first had entries",
      "from": "jellybean5361"
    },
    {
      "problem": "Missing workflow connections",
      "solution": "Check all node connections as workflows can have many missing inputs",
      "from": "David Snow"
    },
    {
      "problem": "First frame corruption/artifacts in generated videos",
      "solution": "Try toggling 'use_coefficients' on TeaCache or disconnect TeaCache completely. May be related to low step counts.",
      "from": "JohnDopamine"
    },
    {
      "problem": "MultiGPU setup causing OOM and device mismatch errors",
      "solution": "Put model on 4090 and VAE+text on 3090. There's a known bug with MultiGPU nodes causing 'Expected all tensors to be on the same device' error.",
      "from": "MilesCorban"
    },
    {
      "problem": "Second pass in split workflows producing completely different slow motion video",
      "solution": "Use lower denoise value (not 1.0) for second pass, as denoise 1.0 creates completely new video. Denoise is the ratio of new vs old content.",
      "from": "Kijai"
    },
    {
      "problem": "TAESD preview warning about missing models/vae_approx/None",
      "solution": "Download taew2_1.safetensors and place in vae_approx folder. The warning can be ignored but proper file fixes preview functionality.",
      "from": "David Snow"
    },
    {
      "problem": "First frame corruption in I2V",
      "solution": "Use longer videos (50+ frames), avoid very short clips like 1 second generations for testing",
      "from": "TK_999"
    },
    {
      "problem": "VACE line art issues",
      "solution": "Making edge maps binary helps improve results",
      "from": "Rishi Pandey"
    },
    {
      "problem": "TeaCache causing slowdowns at low threshold",
      "solution": "Use higher threshold like 0.25 for 14B models instead of 0.13, which is too low",
      "from": "Kijai"
    },
    {
      "problem": "DF model works poorly with torch compile",
      "solution": "Disable torch compile when using DF model",
      "from": "Kijai"
    },
    {
      "problem": "Block swap causing OOM on 81 frames",
      "solution": "Use 20 blocks for 81 frames on 16GB VRAM instead of 10",
      "from": "lostintranslation"
    },
    {
      "problem": "Burning/oversaturation with CFG",
      "solution": "Left image burning indicates CFG too high or layer skipping issues",
      "from": "Piblarg"
    },
    {
      "problem": "Weird colored output in VACE workflow",
      "solution": "Works with depth control but not with blend of depth and lineart",
      "from": "TheSwoosh"
    },
    {
      "problem": "mat1 and mat2 dtype error",
      "solution": "Set base_precision to fp32, then use fp8 scaled clip",
      "from": "Ablejones"
    },
    {
      "problem": "CUDA out of memory on RTX 3090 with 14B model",
      "solution": "Disable teacache or set mode to 'e' instead of 'e0', also try disabling compile",
      "from": "Ablejones"
    },
    {
      "problem": "Optimizations causing artifacts in chained videos",
      "solution": "Disable teacache and switch from fp8e4m3_fast to fp8e4m3 to fix artifacts that compound over time",
      "from": "MilesCorban"
    },
    {
      "problem": "Character giving up on motion in extensions",
      "solution": "Try bigger frame overlap and more specific prompts",
      "from": "seitanism"
    },
    {
      "problem": "First frame corruption",
      "solution": "Likely caused by too few frames generated, doesn't happen when running 81 frames",
      "from": "lostintranslation"
    },
    {
      "problem": "VRAM leak when canceling workflow during sampling",
      "solution": "Use clear models and clear cache buttons when canceling mid-way",
      "from": "seitanism"
    },
    {
      "problem": "OOM at second sampler in DF workflow",
      "solution": "Turn off teacache to prevent memory buildup between samplers",
      "from": "DawnII"
    },
    {
      "problem": "Color brightness increase after multiple DF generations",
      "solution": "Use color matching node to maintain consistent colors",
      "from": "seitanism"
    },
    {
      "problem": "Width and height are flipped causing generation issues",
      "solution": "Check aspect ratios of images (height/width)",
      "from": "DawnII"
    },
    {
      "problem": "WanModel object has no attribute 'dwpose_embedding' error",
      "solution": "This occurs when trying to use UniAnimate with 1.3B models - the LoRA only exists for 14B models",
      "from": "Nokai"
    },
    {
      "problem": "Input video length causing tensor size mismatch in DF",
      "solution": "Can't input more frames than the generation frame count is set to. Need to extract last x frames from input, not the whole video",
      "from": "TK_999"
    },
    {
      "problem": "WAN forces frame counts to 4n+1 rule",
      "solution": "Models require frame counts following 4n+1 pattern (65, 69, 81, etc). Can't generate 67 frames - will output 65 instead",
      "from": "DeZoomer"
    },
    {
      "problem": "OOM issues with DF models",
      "solution": "Need to unload blocks - user reports needing to unload all 40 blocks for 1280x720x81 generation",
      "from": "slmonker"
    },
    {
      "problem": "torch.compile issues on 3000 series GPUs",
      "solution": "Use e5m2 precision instead of e4 for 3000 series GPUs when wanting to use torch.compile",
      "from": "Kijai"
    },
    {
      "problem": "TeaCache OOM on second sampler",
      "solution": "Lower frame count on subsequent samplers to match first sampler, or turn off teacache entirely",
      "from": "Cubey"
    },
    {
      "problem": "OOM at specific step with torch compile",
      "solution": "Check dev branch of WanVideoWrapper repo for fix with torch compile and DF sampler",
      "from": "Ablejones"
    },
    {
      "problem": "IndexError with shift parameter",
      "solution": "Don't use shift value of 50 in DF sampler node as it messes up the sigmas list",
      "from": "Ablejones"
    },
    {
      "problem": "Wrong text encoder format",
      "solution": "Use text encoder from comfy org or kijai's huggingface, not other sources as they're in wrong format",
      "from": "Piblarg"
    },
    {
      "problem": "unianim_data referenced before assignment",
      "solution": "Update the node",
      "from": "Kijai"
    },
    {
      "problem": "phantom_end_percent referenced before assignment error",
      "solution": "Error occurs when using Fun models - Kijai fixed this issue",
      "from": "DawnII"
    },
    {
      "problem": "TypeError: convert_fp8_linear() got an unexpected keyword argument 'sd'",
      "solution": "Switch back to main branch as this was an unfinished feature related to fp8_fast",
      "from": "Kijai"
    },
    {
      "problem": "Diffusion forcing sampler crashes with feta node not connected",
      "solution": "Add disable_enhance() line to diffusion forcing sampler, as it was missing unlike the normal wanvideo wrapper",
      "from": "seitanism"
    },
    {
      "problem": "Model not fully offloading when cancelling mid-sampling causes OOM",
      "solution": "Set model loader to offload_device instead of main_device, and use 'force model offload' option on text encoder",
      "from": "Kijai"
    },
    {
      "problem": "Fun Control 14b not following depth map, immediately starts gesturing",
      "solution": "Use Fun 1.1 instead of 1.0, add noise_aug_strength of 0.03, and try different control embed settings",
      "from": "boorayjenkins"
    },
    {
      "problem": "Feta enhance broken on Fun 1.1",
      "solution": "Issue occurs when using reference image as it adds +1 to length but feta is not set for this",
      "from": "Kijai"
    },
    {
      "problem": "UnicodeDecodeError after PC crash during WAN2.1 generation",
      "solution": "Reinstalling torch fixed the issue",
      "from": "Ashtar"
    },
    {
      "problem": "AssertionError: Input tensors must be in dtype of torch.float16 or torch.bfloat16",
      "solution": "Switch from sageattn to sdpa in attention settings",
      "from": "shreyams."
    },
    {
      "problem": "Expected query, key, and value to have the same dtype error",
      "solution": "Select fp8_e4 quantization to fix dtype mismatch",
      "from": "David Snow"
    },
    {
      "problem": "Dtype errors in WAN sampler",
      "solution": "Change rope function from 'default' to 'comfy' in sampler settings",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Video preview disappearing when switching tabs",
      "solution": "Issue affects both native and wrapper implementations, no solution provided",
      "from": "lostintranslation"
    },
    {
      "problem": "Video degradation with DF extension, especially with 1.3B version",
      "solution": "Add colormatch node to each clip segment referring to initial frame, add refining stage with different model at denoise 0.6-0.7, or use unsampling/resampling",
      "from": "Ablejones"
    },
    {
      "problem": "FantasyTalking doesn't work with DF because DF isn't I2V model",
      "solution": "DF doesn't use image cross attention at all, so audio conditioning won't work",
      "from": "Kijai"
    },
    {
      "problem": "Context window with FantasyTalking loses the image",
      "solution": "Not fully resolved, but noted as limitation",
      "from": "Kijai"
    },
    {
      "problem": "T2V with FantasyTalking audio cfg gives poor results",
      "solution": "Model is designed for I2V, T2V compatibility is limited",
      "from": "Kijai"
    },
    {
      "problem": "Fantasy Talking audio sync completely off",
      "solution": "Update wrapper - audio CFG calculation was broken due to missing brackets",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI connection issues with new frontend",
      "solution": "Hit Esc key to dismiss connection overlay",
      "from": "MilesCorban"
    },
    {
      "problem": "First frame corruption with incompatible settings",
      "solution": "Use compatible model/latent size combinations",
      "from": "Kijai"
    },
    {
      "problem": "VACE start/end frame node error",
      "solution": "Check rope setting is 'comfy' not 'default' on sampler node",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "problem": "Fantasy Talking lip sync slightly off",
      "solution": "Set audio CFG to 1.0, or shift audio 5 frames later if silence at start",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "TeaCache causing early skip issues",
      "solution": "Set TeaCache start step to 6 or higher to avoid skipping too early",
      "from": "Kijai"
    },
    {
      "problem": "Tile LoRA producing noise output",
      "solution": "Cannot use tile control LoRA with VACE - they're incompatible",
      "from": "Kijai"
    },
    {
      "problem": "14B model OOM on 3090",
      "solution": "Use block swapping and quantization for large resolutions with 14B models",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "VRAM issues with large models",
      "solution": "Increase blocks to swap to 20, reduce clip vision precision to FP16 or BF16, use smaller resolution",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "torch._scaled_mm error on older GPUs",
      "solution": "Change from bf16 to fp16 and base_precision to fp8_e4m3fn instead of fp8_fast",
      "from": "J_Pyxal"
    },
    {
      "problem": "Negative dimension tensor error",
      "solution": "Check input frame dimensions and frame count - something off with input frames",
      "from": "Kijai"
    },
    {
      "problem": "Invalid WanVideo model selected for VACE",
      "solution": "VACE module needs to be loaded alongside normal Wan 1.3B model, not as main model",
      "from": "Kijai"
    },
    {
      "problem": "SAM2 error on video input",
      "solution": "Error occurs when mask indices aren't consistent across frames - if object not detected for 1 frame",
      "from": "ArtOfficial"
    },
    {
      "problem": "Wan video generations become progressively corrupted",
      "solution": "Issue with Wan 2.1 reward LoRA - manually unload models and free cache",
      "from": "gshawn"
    },
    {
      "problem": "Black frames with context uniform looped",
      "solution": "Use correct settings: 169 frames with context size 81 and 16 overlap",
      "from": "Kijai"
    },
    {
      "problem": "Reference image causes blink/flash at start of video",
      "solution": "Use fun ref image latent with empty embed instead of feeding in the ref image directly",
      "from": "Gavmakes"
    },
    {
      "problem": "First frame corruption when using high step values",
      "solution": "In wrapper TC node, values are exact steps not percentages - use actual step count (e.g., 20 for 20 steps)",
      "from": "JohnDopamine"
    },
    {
      "problem": "SystemError: PY_SSIZE_T_CLEAN macro error with RTX 5090 fresh install",
      "solution": "Issue appears related to recent ComfyUI update with triton and sage installation",
      "from": "Baku"
    }
  ],
  "comparisons": [
    {
      "comparison": "1.3B vs 14B for vid2vid",
      "verdict": "1.3B model is better for vid2vid without a doubt - much faster and with depth, highres, aesthetics loras reaches new quality level",
      "from": "David Snow"
    },
    {
      "comparison": "Wrapper vs Native workflow",
      "verdict": "Wrapper produces far more stable video outputs than native, native is a massive pain to use",
      "from": "David Snow"
    },
    {
      "comparison": "DepthCrafter vs Video Depth Anything",
      "verdict": "DepthCrafter appears better for static shots, left (DepthCrafter) looks better by a decent bit",
      "from": "Juampab12"
    },
    {
      "comparison": "VACE vs InP model",
      "verdict": "VACE already better than InP model - InP doesn't know what red panda is",
      "from": "Kijai"
    },
    {
      "comparison": "TeaCache performance between versions",
      "verdict": "Old revision with both old RoPE & comfy RoPE worked better - newer version skips less steps for same thresholds",
      "from": "DevouredBeef"
    },
    {
      "comparison": "VACE vs Fun model",
      "verdict": "VACE feels slower but better quality than Fun model",
      "from": "Cseti"
    },
    {
      "comparison": "VACE vs standard Wan speed",
      "verdict": "VACE is approximately 2x slower - 3 minute generations become 7 minutes",
      "from": "multiple users"
    },
    {
      "comparison": "VACE vs FunInP model",
      "verdict": "VACE makes FunInP model redundant due to superior capabilities",
      "from": "Kijai"
    },
    {
      "comparison": "LoRAs with VACE vs standard Wan",
      "verdict": "LoRAs are kinda weird and not quite as good as standard Wan but some aspects work",
      "from": "Kytra"
    },
    {
      "comparison": "FP16 vs FP32 on VACE",
      "verdict": "FP16 is 3 times faster but FP32 has much better quality",
      "from": "VK (5080 128gb)"
    },
    {
      "comparison": "With reference vs without reference on VACE",
      "verdict": "Without reference image quality is 100 times better, almost photorealistic vs oversaturated colors and plastic skin with reference",
      "from": "seitanism"
    },
    {
      "comparison": "VACE vs Fun models",
      "verdict": "VACE might make Fun models redundant",
      "from": "Kijai"
    },
    {
      "comparison": "10 steps vs 30 steps VACE",
      "verdict": "30 steps is a bit better quality but takes 3x longer (1 min vs 3 min for 41 frames)",
      "from": "VK (5080 128gb)"
    },
    {
      "comparison": "VACE vs Fun-Control",
      "verdict": "VACE is almost objectively better - works with original Wan LoRAs and has more control options",
      "from": "Kytra"
    },
    {
      "comparison": "Euler vs DPM++_SDE after timestep fix",
      "verdict": "Euler now produces same quality as UniPC after fix, while SDE variant has convergence issues",
      "from": "Kijai"
    },
    {
      "comparison": "VACE vs regular I2V",
      "verdict": "VACE offers more control options but setup is more complex",
      "from": "HeadOfOliver"
    },
    {
      "comparison": "SkyReels A2 vs VACE",
      "verdict": "SkyReels looks better but likely based on HYV so unfair comparison since VACE is 1.3B",
      "from": "Juampab12"
    },
    {
      "comparison": "Hunyuan vs SkyReels",
      "verdict": "SkyReels was bigger and slower than hunyuan but a good bit better",
      "from": "Benjimon"
    },
    {
      "comparison": "Base Wan 1.3B vs DG_Wan models for V2V",
      "verdict": "Base model better at following depth LoRA and closer to original video motion, DG_Wan models don't respond to LoRAs as well",
      "from": "David Snow"
    },
    {
      "comparison": "Sapiens vs DWPose for pose detection",
      "verdict": "Sapiens provides better temporal consistency and more detailed human preprocessing",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Depth vs Pose control in VACE",
      "verdict": "Pose gives better facial performance, depth provides different motion characteristics - can blend both for optimal results",
      "from": "A.I.Warper"
    },
    {
      "comparison": "WAN 1.3B vs other 1.4B models",
      "verdict": "Faster than 14B, slower than other 1.4B models due to more blocks",
      "from": "Zuko"
    },
    {
      "comparison": "WAN 1.3B vs other similar sized image models",
      "verdict": "Considerably better than other similar sized image models",
      "from": "YatharthSharma"
    },
    {
      "comparison": "WAN cost vs Kling",
      "verdict": "Would cost over $100 on Kling for the same effect achievable with WAN",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "FP16 vs FP32 for WAN 1.3B",
      "verdict": "FP32 is better quality but takes almost 2x time, marginal improvement not worth extra time",
      "from": "David Snow"
    },
    {
      "comparison": "UniPC vs Euler schedulers",
      "verdict": "UniPC used to be best option until Kijai fixed Euler, now slightly worse than Euler",
      "from": "David Snow"
    },
    {
      "comparison": "DG model versions comparison",
      "verdict": "Multiple DG variants available (High, Light, V3, V4) with different characteristics - High changes 2D/animation to realism",
      "from": "Hashu"
    },
    {
      "comparison": "4 steps vs 8 steps DG model",
      "verdict": "4 steps: 7 seconds generation time, 8 steps: longer but better quality",
      "from": "Kijai"
    },
    {
      "comparison": "VACE applied to different block counts",
      "verdict": "Applying VACE to only 5 blocks or half the blocks works well, keeps part of bounding box in result",
      "from": "Kijai"
    },
    {
      "comparison": "DG models vs base Wan",
      "verdict": "DG more creative but follows reference image much less, much faster (38sec vs 10min)",
      "from": "IllumiReptilien"
    },
    {
      "comparison": "VACE vs Fun Control models",
      "verdict": "VACE is all fun and more since you can combine them plus use reference",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "5 steps base Wan vs 5 steps DG Wan",
      "verdict": "DG significantly better at low step counts",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "DG at 60 steps vs base",
      "verdict": "DG at 60 steps is actually pretty good, 2s/it for 960x608x61",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "1.3B vs 14B model quality",
      "verdict": "14B produces significantly better results than 1.3B preview model",
      "from": "Kijai"
    },
    {
      "comparison": "I2V vs T2V task difficulty",
      "verdict": "Having actual starting frames (I2V) is not comparable to T2V generation - completely different tasks",
      "from": "Kijai"
    },
    {
      "comparison": "SkyreelsA2 vs VACE",
      "verdict": "SkyreelsA2 uses 14B I2V model and clip embeds, better for I2V. VACE uses different approach trained on T2V model without clip embeds. SkyreelsA2 better for VRAM as no extra control inputs",
      "from": "Kijai"
    },
    {
      "comparison": "14B VACE vs current models",
      "verdict": "14B VACE will be tough to run, probably unusable even on 4090 due to double compute requirement",
      "from": "Draken"
    },
    {
      "comparison": "Fun models vs VACE/SkyreelsA2",
      "verdict": "Fun models are mostly outclassed by VACE and SkyreelsA2, much worse than base 1.3B model",
      "from": "DawnII"
    },
    {
      "comparison": "VACE vs Fun Control for frame continuation",
      "verdict": "Fun Control 14B works better for last latent/first latent technique",
      "from": "JmySff"
    },
    {
      "comparison": "Base Wan vs distilled models",
      "verdict": "User feels like they prefer base Wan more, distilled models hard to tell apart",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Realistic lineart vs anyline",
      "verdict": "Realistic lineart works better than anyline",
      "from": "Kijai"
    },
    {
      "comparison": "Inverted anyline vs depth",
      "verdict": "Inverted anyline wins over depth for control",
      "from": "Kijai"
    },
    {
      "comparison": "VACE memory usage vs 14B Fun models",
      "verdict": "VACE competes with nearly as much VRAM usage as 14B fun models",
      "from": "DawnII"
    },
    {
      "comparison": "14B vs 1.3B vid2vid quality",
      "verdict": "14B substantially better for vid2vid, though much slower",
      "from": "David Snow"
    },
    {
      "comparison": "Wan vs AnimatedDiff",
      "verdict": "Wan is replacement for AnimatedDiff with better control and no background distortion on camera movement",
      "from": "Kijai"
    },
    {
      "comparison": "VACE vs Fun Control",
      "verdict": "Fully different models, both serve similar control purposes",
      "from": "Kijai"
    },
    {
      "comparison": "Fun Control 14B vs VACE quality",
      "verdict": "Fun Control 14B produces much better quality than VACE 1.3B, but loses reference subject/object feature",
      "from": "JmySff"
    },
    {
      "comparison": "1.3B skin detail capability",
      "verdict": "1.3B is terrible at skin detail",
      "from": "David Snow"
    },
    {
      "comparison": "Fun 1.3B vs VACE 1.3B",
      "verdict": "Fun 1.3B feels terrible, so much worse in general compared to VACE",
      "from": "Kijai"
    },
    {
      "comparison": "Fun 14B vs VACE for pose control",
      "verdict": "Fun 14B seems well trained for pose control, better than VACE",
      "from": "Kijai"
    },
    {
      "comparison": "VACE vs Fun Control versatility",
      "verdict": "VACE is most versatile and supports old LoRAs and different models",
      "from": "Kijai"
    },
    {
      "comparison": "Original vs DiffSynth reward LoRAs",
      "verdict": "DiffSynth converted version works much better, has bigger impact",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "comparison": "VACE vs Runway first frame style",
      "verdict": "VACE with first frame and control works as good if not better than Runway first frame style for the task, though maybe not quality",
      "from": "Draken"
    },
    {
      "comparison": "CFG1 distilled lora vs non-distilled",
      "verdict": "Distilled version actually slower despite lower CFG, not faster as expected",
      "from": "CJ"
    },
    {
      "comparison": "4o vs Gemini for image editing",
      "verdict": "4o better quality but changes faces when asked to change hair color. Gemini excellent at holding original image while changing it but lacks quality",
      "from": "Fill"
    },
    {
      "comparison": "5080 16GB vs older gen 24GB for Wan",
      "verdict": "Pretty clear choice if you want to run 14B, you'll lose any speed benefit to offloading with 16GB",
      "from": "Kijai"
    },
    {
      "comparison": "GGUF vs swapping performance",
      "verdict": "GGUF is just so slow compared to swapping",
      "from": "Kijai"
    },
    {
      "comparison": "Multiple VACE encodes vs single encode",
      "verdict": "Splitting encodes made results much worse",
      "from": "David Snow"
    },
    {
      "comparison": "Base 1.3B vs Fun-Control",
      "verdict": "Base 1.3B with control lora is much better than Fun-Control, which has glitches and artifacts. Fun-Control better for small details but worse overall",
      "from": "David Snow"
    },
    {
      "comparison": "DiffSynth LoRAs vs original",
      "verdict": "DiffSynth converted LoRAs have double strength due to removed alpha keys - using at 0.5 equals original at 1.0",
      "from": "Kijai"
    },
    {
      "comparison": "Fun-Control vs Base with depth lora",
      "verdict": "Fun-Control adds noise and artifacts, base with depth lora more stable",
      "from": "David Snow"
    },
    {
      "comparison": "VACE 1.3B vs regular 14B I2V",
      "verdict": "VACE 1.3B gets results as good as classic I2V with 14B model",
      "from": "V\u00e9role"
    },
    {
      "comparison": "Pika swap vs WAN VACE object replacement",
      "verdict": "Pika kept the shape of original object (beer bottle) when swapping for banana, wasn't great",
      "from": "Jas"
    },
    {
      "comparison": "SAM2 vs referring expression segmentation",
      "verdict": "Referring expression segmentation seems better but takes 15 minutes on 81 frames vs SAM2 being faster",
      "from": "StableVibrations"
    },
    {
      "comparison": "WAN vs Hunyuan Video overall",
      "verdict": "WAN better for image quality, physics, prompt adherence, control options, and flexibility. Hunyuan better for speed and NSFW content",
      "from": "Screeb"
    },
    {
      "comparison": "WAN vs Hunyuan I2V",
      "verdict": "WAN I2V much better - cleaner output without noise, camera stays still unless prompted vs Hunyuan's random camera movement",
      "from": "Blink"
    },
    {
      "comparison": "Hunyuan vs WAN for likeness LoRAs",
      "verdict": "Hunyuan T2V better for training human likeness LoRAs, but WAN better for I2V",
      "from": "JohnDopamine"
    },
    {
      "comparison": "Fun Control vs VACE for combined controls",
      "verdict": "Combined controls work relatively well with Fun Control but not with VACE - separate controlnets render much better with VACE",
      "from": "JmySff"
    },
    {
      "comparison": "DG_Boost models vs base 1.3B",
      "verdict": "DG_Boost models better for character quality and certain details, especially interiors, but base closer to original rendering style",
      "from": "David Snow"
    },
    {
      "comparison": "Fun Control vs VACE Control with Reference",
      "verdict": "VACE better for ease of use. Fun-Control is inconsistent - sometimes far better than base, sometimes far worse",
      "from": "David Snow"
    },
    {
      "comparison": "1.3B model vs optimal steps",
      "verdict": "Even with Flux get better results using euler/beta than optimal steps",
      "from": "Kijai"
    },
    {
      "comparison": "480p model vs 720p model for motion",
      "verdict": "480p model seemed to produce better motion anyway, 480p gives better result than 720p model",
      "from": "David Snow"
    },
    {
      "comparison": "Q8 vs FP8 models",
      "verdict": "Q8 model gives better quality than fp8. GGUF is slower than FP8 with optimizations but not by much",
      "from": "Johnjohn7855"
    },
    {
      "comparison": "VACE stability vs regular generation",
      "verdict": "VACE is very, very stable",
      "from": "David Snow"
    },
    {
      "comparison": "Fun vs VACE speed",
      "verdict": "Fun models are significantly faster than VACE for similar tasks",
      "from": "Pol"
    },
    {
      "comparison": "Fun 1.3B vs VACE for V2V",
      "verdict": "Fun 1.3B respects reference images better than VACE for video-to-video",
      "from": "Pol"
    },
    {
      "comparison": "Lotus Depth vs Depth Anything",
      "verdict": "Lotus depth gives more details in image generation, but causes flickering in video - not good for video use",
      "from": "David Snow"
    },
    {
      "comparison": "8 steps vs 60 steps with Fun model",
      "verdict": "8 steps produces remarkably good results, making it very efficient",
      "from": "A.I.Warper"
    },
    {
      "comparison": "Fun vs VACE VRAM usage",
      "verdict": "Fun uses significantly less VRAM than VACE due to VACE's 15 additional blocks",
      "from": "Kijai"
    },
    {
      "comparison": "WSL vs dual boot for development",
      "verdict": "Dual boot better for performance, WSL has more complex file system and memory management",
      "from": "Kijai"
    },
    {
      "comparison": "14B vs 1.3B model training",
      "verdict": "14B is much easier to train overall, 1.3B more challenging",
      "from": "Piblarg"
    },
    {
      "comparison": "Topaz vs other interpolators",
      "verdict": "Topaz significantly better than 5 other tested interpolators",
      "from": "Benjimon"
    },
    {
      "comparison": "RTX 4090 vs 3090 vs RTX 8000 speed",
      "verdict": "1hr on 4090, 2hrs on 3090, 2.5-3hrs on RTX 8000 for same generation",
      "from": "Benjimon"
    },
    {
      "comparison": "BF16 vs FP16 conversion",
      "verdict": "BF16 to FP16 conversion is lossy and produces worse quality than either format",
      "from": "Kijai"
    },
    {
      "comparison": "Kling 2 vs older Kling",
      "verdict": "New Kling 2 looks very good and dynamic but heavily censored, rejected 19 of 26 prompts that worked in older model",
      "from": "Fabricatedgirls"
    },
    {
      "comparison": "FLF2V vs Fun 14B",
      "verdict": "FLF2V probably better if comparing only first/last frame, but unsure if FLF2V can do more than 2 input frames like Fun can",
      "from": "Kijai"
    },
    {
      "comparison": "Q8 GGUF vs fp8",
      "verdict": "GGUF about 20% slower than fp8 when fp8 fits in VRAM, Q8 uses more memory than fp8",
      "from": "Kijai"
    },
    {
      "comparison": "bf16 vs fp16 vs fp16 w/ sage on Fun Control 14B",
      "verdict": "All produce cursed/body horror results in test case",
      "from": "Benjimon"
    },
    {
      "comparison": "480p vs 720p model performance",
      "verdict": "720p model may not be as good as 480p model, possibly trained on much less data",
      "from": "MilesCorban"
    },
    {
      "comparison": "Wan vs enhanced HYV in FramePack",
      "verdict": "Enhanced HYV shows similar performance to Wan with better human anatomy and slightly faster speed",
      "from": "Johnjohn7855"
    },
    {
      "comparison": "Wan vs HYV prompt following",
      "verdict": "Wan prompt following is 5x better literally, HYV never followed prompts great",
      "from": "yi"
    },
    {
      "comparison": "Using Wan vs going back to HYV",
      "verdict": "Like going back to play Atari when you have Steam - can throw 3 paragraphs at Wan and it figures everything out",
      "from": "crinklypaper"
    },
    {
      "comparison": "HYV vs Wan for celebrity/human likeness training",
      "verdict": "HYV is amazing for deepfake generations and celebrity likenesses, Wan trials didn't work out as well",
      "from": "JohnDopamine"
    },
    {
      "comparison": "VACE vs Fun Control",
      "verdict": "VACE better for full character replacement, Fun significantly faster for style transfer iteration. Fun uses 1/3 memory and time of VACE",
      "from": "A.I.Warper"
    },
    {
      "comparison": "FLF2V vs regular Wan 14B I2V",
      "verdict": "FLF2V has better source image adherence with loras than regular 14B 720 i2v model",
      "from": "Eclipse"
    },
    {
      "comparison": "FramePack vs 14B WAN quality",
      "verdict": "FramePack is efficiency upgrade but at small cost to quality. Shows weird smoothing on textures",
      "from": "Kytra"
    },
    {
      "comparison": "Fun vs VACE for stylized first frame + control",
      "verdict": "Fun is better for stylized 1st frame and control video ref scenario. VACE better for t2v prompting with control video ref",
      "from": "N0NSens"
    },
    {
      "comparison": "UniAnimate vs VACE",
      "verdict": "UniAnimate will beat VACE because it's for the proper I2V model that's already really good, uses LoRA weights and pose embeds, only 1GB in fp16",
      "from": "Kijai"
    },
    {
      "comparison": "Fun-control vs VACE for eye movement",
      "verdict": "Fun-control can capture eye movements better but doesn't look as good overall",
      "from": "David Snow"
    },
    {
      "comparison": "Inner's 1.3B model vs 14B model",
      "verdict": "Inner's 1.3B model is better than 14B, uses synthetic data in addition to original dataset",
      "from": "Flipping Sigmas"
    },
    {
      "comparison": "DepthAnythingV2 vs original DepthAnything",
      "verdict": "V2 is much faster and less resource intensive while maintaining quality",
      "from": "David Snow"
    },
    {
      "comparison": "SageAttention vs SDPA attention",
      "verdict": "SDPA gives higher quality output, SageAttention hits hands badly but provides speed",
      "from": "Benjimon"
    },
    {
      "comparison": "1.3B model skin rendering",
      "verdict": "Human skin is a weakness for 1.3B model compared to larger variants",
      "from": "David Snow"
    },
    {
      "comparison": "Benjimon's SkyReels vs stock i2v",
      "verdict": "Very close to the stock i2v model based on full weight analysis",
      "from": "Benjimon"
    },
    {
      "comparison": "VACE vs traditional control methods",
      "verdict": "VACE output is superior - essentially expands base model capability and looks better",
      "from": "David Snow"
    },
    {
      "comparison": "UniAnimate vs other pose methods",
      "verdict": "UniAnimate produces better quality than VACE preview model at 25 steps",
      "from": "V\u00e9role"
    },
    {
      "comparison": "SD 1.5 vs SDXL",
      "verdict": "SD 1.5 has stronger controlnets and IP adapters, better T5/IC light support, but harder to use (300+ nodes vs simple workflow)",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "comparison": "Wan vs AnimateDiff for style transfer",
      "verdict": "Wan much faster - 345 seconds on 4090 vs hours with AnimateDiff pipeline",
      "from": "Gavmakes"
    },
    {
      "comparison": "Different step counts for generation",
      "verdict": "20 steps vs 30 steps shows significant difference, 35 steps is sweet spot",
      "from": "ezMan"
    },
    {
      "comparison": "SkyReels 1.3B vs 14B speed",
      "verdict": "1.3B is four times faster than 14B",
      "from": "ezMan"
    },
    {
      "comparison": "Fun inp vs VACE for extension",
      "verdict": "VACE 1.3b gives better results than Fun inp",
      "from": "PirateWolf"
    },
    {
      "comparison": "SkyReels 540p-i2v-14B vs regular Wan i2v 14B",
      "verdict": "SkyReels 14B is closer to Wan i2v 14B than other variants based on weight analysis",
      "from": "Benjimon"
    },
    {
      "comparison": "4090 vs 4070Ti performance",
      "verdict": "4090 is 2x faster - 1008GB/s vs 505GB/s bandwidth plus >2x CUDA cores",
      "from": "MilesCorban"
    },
    {
      "comparison": "LTX 0.9.6 distilled vs Wan speed",
      "verdict": "LTX much faster (almost SDXL speed) but Wan quality is much better - 'comparison is not even close'",
      "from": "Colin"
    },
    {
      "comparison": "Wan I2V vs Hunyuan motion",
      "verdict": "Wan I2V is 'just on another level with the amount of motion it can do' even compared to Hunyuan",
      "from": "Kijai"
    },
    {
      "comparison": "VACE vs 14B models",
      "verdict": "VACE is 'much better' and quicker than 14B models",
      "from": "Colin"
    },
    {
      "comparison": "1.3B vs 14B model differences",
      "verdict": "Work about the same but 14B lacks fps embeds, 1.3B starts to lose quality in long generations",
      "from": "Kijai"
    },
    {
      "comparison": "MAGI-1 vs other models",
      "verdict": "Claims to beat everything, but only wins by tiny bit in user preference benchmark",
      "from": "Draken"
    },
    {
      "comparison": "VACE vs DG models",
      "verdict": "VACE is slower, DG very fast at 6 steps but not good for i2v",
      "from": "N0NSens"
    },
    {
      "comparison": "SageAttention vs SDPA",
      "verdict": "SageAttention gets 99% quality while being twice as fast",
      "from": "Kijai"
    },
    {
      "comparison": "Two-pass upscaling vs regular upscaling",
      "verdict": "Vastly prefer two-pass over regular upscaling",
      "from": "Davidodave"
    },
    {
      "comparison": "Wan vs SkyReels sampling behavior",
      "verdict": "Wan determines result in first few steps, SkyReels starts with big motion and refines details throughout",
      "from": "Zuko"
    },
    {
      "comparison": "LatentSync vs LivePortrait",
      "verdict": "Different uses - LivePortrait copies facial expressions, LatentSync generates lip motion from audio",
      "from": "Zuko"
    },
    {
      "comparison": "14B vs 1.3B Wan models",
      "verdict": "14B can handle 720p where 1.3B breaks down, but 14B is much slower (8 mins vs 30 secs per segment)",
      "from": "Draken"
    },
    {
      "comparison": "Base 1.3B vs VACE for second passing",
      "verdict": "Base 1.3B better for second passing due to VACE being too sensitive to control input artifacts",
      "from": "David Snow"
    },
    {
      "comparison": "LTX vs other models for iteration speed",
      "verdict": "LTX incredible for iteration speed - 20 seconds for 97 frames at 1280x720",
      "from": "David Snow"
    },
    {
      "comparison": "Cog 5B vs Wan 1.3B",
      "verdict": "Wan 1.3B definitely better than Cog 5B despite smaller size",
      "from": "Kijai"
    },
    {
      "comparison": "Phantom vs VACE quality for facial fidelity",
      "verdict": "1.4b models (Phantom, 1.4b VACE) are too weak to capture true facial fidelity from reference pics. Gets elements down but doesn't achieve celebrity-level accuracy like Taylor Swift/RDJ.",
      "from": "Zuko"
    },
    {
      "comparison": "CFG scheduling vs TeaCache for speed optimization",
      "verdict": "Quality hit from using CFG 1.0 is higher than just using more aggressive TeaCache",
      "from": "Kijai"
    },
    {
      "comparison": "SkyReels V2 vs other models prompting approach",
      "verdict": "Other models haven't done dynamics-only prompting that SkyReels V2 paper recommends. Everything else is image conditioned, similar to how Google's Whisk animate works.",
      "from": "fredbliss"
    },
    {
      "comparison": "SkyReelV2-i2v-540p vs base Wan i2v",
      "verdict": "SkyReelV2 is way better with improved movements, overall quality and subject fidelity",
      "from": "mamad8"
    },
    {
      "comparison": "Phantom vs SkyReels-A2",
      "verdict": "Phantom seems better and has 1.3B version available",
      "from": "Kijai"
    },
    {
      "comparison": "DF vs framepack",
      "verdict": "DF model is really miles ahead in quality and motion, allows prompt scheduling/travel",
      "from": "seitanism"
    },
    {
      "comparison": "Native vs wrapper VRAM usage",
      "verdict": "Native has automatic offloading when VRAM gets full, wrapper needs manual block swap configuration",
      "from": "Kijai"
    },
    {
      "comparison": "Phantom vs VACE for character identity",
      "verdict": "Phantom seems better at keeping character consistent",
      "from": "NebSH"
    },
    {
      "comparison": "Torch compile vs no compile on 4090",
      "verdict": "Compile is faster: 2.94s/it vs 4s/it",
      "from": "MilesCorban"
    },
    {
      "comparison": "fp8e4m3_fast vs fp8e4m3 for chaining",
      "verdict": "fp8e4m3 better for chaining as fp8e4m3_fast causes artifacts",
      "from": "MilesCorban"
    },
    {
      "comparison": "SkyReels V2 vs Wan 14b",
      "verdict": "SkyReels quality is really great, even slightly better than wan 14b",
      "from": "seitanism"
    },
    {
      "comparison": "SkyReels DF vs framepack",
      "verdict": "Much better than framepack",
      "from": "boorayjenkins"
    },
    {
      "comparison": "Wan vs Veo 2 color quality",
      "verdict": "SkyReels looks more cinematic with mood, Veo looks documentary-like but has better natural colors",
      "from": "seitanism"
    },
    {
      "comparison": "Skyreels vs HYV (HunyuanVideo)",
      "verdict": "HYV better for identity retention, facial behavior, fine hair, and cinematic look. Skyreels better for functionality and has ControlNet support. HYV is faster and native 24fps with correct motion blur",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "comparison": "Base WAN vs Skyreels V2",
      "verdict": "Base model performed better in user tests - Skyreels V2 results described as 'not good' across several images tested",
      "from": "N0NSens"
    },
    {
      "comparison": "Skyreels V2 I2V vs base 480p speed",
      "verdict": "V2 I2V 14B 540p faster than base 480p (2:50 vs 3:20) but result quality was bad",
      "from": "N0NSens"
    },
    {
      "comparison": "Base model vs Skyreels for anime/drawings",
      "verdict": "Base model works way better for anime stuff and drawings",
      "from": "Miku"
    },
    {
      "comparison": "20 steps vs 30 steps",
      "verdict": "20 steps sometimes looks better for simple scenes, but 30+ steps needed for complex scenes with lots of details",
      "from": "ezMan"
    },
    {
      "comparison": "fp8_fast vs fp16_fast on 3090",
      "verdict": "fp16_fast works on 3090, fp8_fast does not work on 3090",
      "from": "Kijai"
    },
    {
      "comparison": "Hunyuan vs Wan fp8 quality",
      "verdict": "Hunyuan quality doesn't degrade from fp8_fast, but Wan has something special that breaks with fp8",
      "from": "Kijai"
    },
    {
      "comparison": "Fun 1.1 vs VACE",
      "verdict": "Pretty similar to VACE but much faster",
      "from": "Kijai"
    },
    {
      "comparison": "Control 1.3B vs 14B",
      "verdict": "1.3B gives better results than 14B and is much faster",
      "from": "Nokai"
    },
    {
      "comparison": "Skyreels vs original base Wan",
      "verdict": "Skyreels i2v and t2v are better than original base wan in quality and prompt following",
      "from": "mamad8"
    },
    {
      "comparison": "14B 480p vs VACE i2v",
      "verdict": "14B has better motion but takes 1753 seconds vs VACE's 443 seconds. VACE is sharper but has motion artifacts. Multiple VACE passes in same time as one 14B run could eliminate artifacts",
      "from": "David Snow"
    },
    {
      "comparison": "WAN base vs Skyreels V2",
      "verdict": "V2 is better for videos with people, base WAN better for abstract ideas and anime/cartoon content",
      "from": "MilesCorban"
    },
    {
      "comparison": "Lower resolution (544x544) vs higher resolution (720x720+) with DF",
      "verdict": "Lower resolution has less noticeable degradation when using DF to extend clips",
      "from": "seitanism"
    },
    {
      "comparison": "Diffusion Forcing vs Framepack",
      "verdict": "Framepack is faster and does everything at once, DF is slower and more VRAM intensive but allows prompt scheduling like AnimateDiff",
      "from": "MilesCorban"
    },
    {
      "comparison": "HiDream vs Flux",
      "verdict": "HiDream better - no grid pattern, no 2nd pass needed",
      "from": "Colin"
    },
    {
      "comparison": "Fun_inp vs VACE for start/end images",
      "verdict": "Fun_inp way better according to testing",
      "from": "David Snow"
    },
    {
      "comparison": "VACE vs DG Fun for base Wan compatibility",
      "verdict": "Base Wan DG doesn't work well with VACE, contradictory reports",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Skyreels V2 I2V vs base Wan2.1",
      "verdict": "V2 I2V is much improved and current favorite",
      "from": "Colin"
    },
    {
      "comparison": "FantasyTalking vs Sonic",
      "verdict": "FantasyTalking much better than Sonic, higher quality and listens to prompts",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "comparison": "Fantasy Talking vs LatentSync + Wan",
      "verdict": "Comparable quality but Fantasy Talking offers more flexibility and works during transformations",
      "from": "Stad"
    },
    {
      "comparison": "DG models vs standard Fun_Inp",
      "verdict": "DG models are stronger for inpainting tasks",
      "from": "David Snow"
    },
    {
      "comparison": "Skyreels vs Phantom for T2V",
      "verdict": "Discussion initiated but no clear verdict provided",
      "from": "N0NSens"
    },
    {
      "comparison": "14B vs 1.3B SkyReels",
      "verdict": "14B is leagues better quality but much slower",
      "from": "Colin"
    },
    {
      "comparison": "Fantasy Talking vs Sonic vs Latent Sync",
      "verdict": "Sonic more direct but less flexible, Latent Sync more convoluted but more possibilities, Fantasy Talking most flexible",
      "from": "Stad"
    },
    {
      "comparison": "Fun Camera Control vs ReCamMaster",
      "verdict": "Fun is for T2V/I2V, ReCamMaster is for vid2vid",
      "from": "Kijai"
    },
    {
      "comparison": "14B vs 1.3B for control tasks",
      "verdict": "14B is much better but sometimes overkill, 1.3B can be underwhelming - a 3B model would be ideal",
      "from": "Draken"
    },
    {
      "comparison": "Text generation T2V vs I2V",
      "verdict": "T2V works better for text generation than I2V due to low resolution vision encoder (326x326)",
      "from": "pom"
    }
  ],
  "tips": [
    {
      "tip": "Use input file quality matters significantly",
      "context": "For vid2vid and style transfer, always use high quality source files and depthcrafter for depth inputs",
      "from": "David Snow"
    },
    {
      "tip": "Try second-passing old generations with 1.3B",
      "context": "Can significantly improve thousands of 'almost good enough' videos using depth, highres and aesthetics loras",
      "from": "David Snow"
    },
    {
      "tip": "Use context windows for longer v2v sequences",
      "context": "Especially with control, works great for longer video-to-video sequences",
      "from": "Kijai"
    },
    {
      "tip": "Mix depth and realistic lineart as control embeds",
      "context": "For v2v to stay close to original while improving quality",
      "from": "David Snow"
    },
    {
      "tip": "Avoid transferring videos through messaging apps",
      "context": "WhatsApp and similar apps compress and destroy video quality, affecting AI output",
      "from": "3Dmindscaper2000"
    },
    {
      "tip": "Regularly backup ComfyUI before updates",
      "context": "Zip portable ComfyUI before pressing update to avoid breaking workflows",
      "from": "Lumi"
    },
    {
      "tip": "Use Docker with pinned commits for ComfyUI",
      "context": "Pin every custom node to specific commit for consistent installs across machines",
      "from": "pixelperfecter"
    },
    {
      "tip": "Use max 77 frames with reference images",
      "context": "Because it ends up as 81 latents total",
      "from": "Kijai"
    },
    {
      "tip": "Remove background from reference images",
      "context": "Place subject on white background for better likeness",
      "from": "Kijai"
    },
    {
      "tip": "Use SLG and zero CFG for better results",
      "context": "When working with reference images",
      "from": "Kijai"
    },
    {
      "tip": "Use fp16 or bf16 with fp8_e4m3fn quantization",
      "context": "For running VACE efficiently",
      "from": "zelgo_"
    },
    {
      "tip": "Don't use block swapping with VACE",
      "context": "Won't help much and can cause errors",
      "from": "Kijai"
    },
    {
      "tip": "Remove background completely from reference images",
      "context": "When using VACE style transfer",
      "from": "ingi // SYSTMS"
    },
    {
      "tip": "Use compositor node to composite reference images",
      "context": "For better results when combining multiple references",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "tip": "Position reference face where face will be in video",
      "context": "When trying to transfer both face and clothing",
      "from": "ingi // SYSTMS"
    },
    {
      "tip": "Use second pass for cleanup",
      "context": "Model is better at denoising its own outputs than straight vid2vid",
      "from": "Piblarg"
    },
    {
      "tip": "Keep prompts simple",
      "context": "Complex prompts like adding rain can make output blurry",
      "from": "PookieNumnums"
    },
    {
      "tip": "Disable block 1 and 2 on quality degradation LoRAs",
      "context": "Often improves quality by quite a margin without losing what lora was trying to achieve",
      "from": "Juampab12"
    },
    {
      "tip": "Use 720p directly instead of upscaling from 480p",
      "context": "For quality generation",
      "from": "Mint"
    },
    {
      "tip": "Second denoise pass cleans up results when using control",
      "context": "When control reduces coherence",
      "from": "Piblarg"
    },
    {
      "tip": "Wan is super sensitive to style prompts",
      "context": "Include style specifications in prompts",
      "from": "Mint"
    },
    {
      "tip": "Composite mask directly on input video works best",
      "context": "For VACE workflows",
      "from": "Kijai"
    },
    {
      "tip": "Don't even use mask input for VACE",
      "context": "Mask input purpose unclear, works better without",
      "from": "Kijai"
    },
    {
      "tip": "Zoom in on reference face for clearer likeness",
      "context": "When using reference images",
      "from": "AJO"
    },
    {
      "tip": "Use mask as the key for better VACE results",
      "context": "When doing reference-based generation",
      "from": "Kijai"
    },
    {
      "tip": "Try first with each control on their own to see they work",
      "context": "Before combining reference and control",
      "from": "Kijai"
    },
    {
      "tip": "It's best to run new models with their repos first before introducing 20 layers of shlop",
      "context": "When testing new models",
      "from": "Benjimon"
    },
    {
      "tip": "Best to manually pad reference images to match aspect ratio",
      "context": "When using reference images with VACE",
      "from": "Kijai"
    },
    {
      "tip": "Set first frame 0, second frame 1 and last frame 1 for fade mask",
      "context": "When using create fade mask advanced node",
      "from": "Kijai"
    },
    {
      "tip": "Use low CFG for DiffSynth models",
      "context": "CFG 1-3 prevents overbaked/overcooked output, light_v1 works best with character LoRAs",
      "from": "BondoMan"
    },
    {
      "tip": "Structure prompts like training captions",
      "context": "Begin with video style, followed by content abstract, then detailed description for better prompt alignment",
      "from": "fearnworks"
    },
    {
      "tip": "Use high shift values for DiffSynth models",
      "context": "Need shift 11+ for proper prompt following like hand waving motions",
      "from": "BondoMan"
    },
    {
      "tip": "Combine reference frame with preprocessors",
      "context": "Put reference image as frame 0, preprocessor frames as rest for controlled starting point",
      "from": "Hashu"
    },
    {
      "tip": "Fade control images to make them darker for lower control strength while maintaining reference strength",
      "context": "When you want general control rather than specific control",
      "from": "Kijai"
    },
    {
      "tip": "Use Sapiens instead of OpenPose for face detection",
      "context": "OpenPose is barely usable for faces",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Align first frame exactly with first frame of control reference",
      "context": "For proper motion tracking when using control inputs",
      "from": "Zuko"
    },
    {
      "tip": "Use start frame instead of reference when you don't want white background",
      "context": "When animating with depth control and specific backgrounds",
      "from": "Kijai"
    },
    {
      "tip": "Set TinkerWAN Alpha lora to positive values with v1.0 version",
      "context": "New version works with positive instead of negative values",
      "from": "David Snow"
    },
    {
      "tip": "Try drawing hands manually for better hand control",
      "context": "When pose control isn't giving good hand results",
      "from": "Kijai"
    },
    {
      "tip": "Use hue shift on Sapiens bones for better results",
      "context": "To improve bone-based control quality",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use Image Composite Masked for video inpainting",
      "context": "When inpainting specific zones in video, use grey on input video and reference image",
      "from": "mamad8"
    },
    {
      "tip": "Masks help maintain surrounding consistency",
      "context": "Without masks, areas around inpaint regions will change too",
      "from": "Hashu"
    },
    {
      "tip": "Tab to Krita for complex compositing",
      "context": "For complex image composition tasks, external tools like Krita or Photopea are more efficient than ComfyUI nodes",
      "from": "Kijai"
    },
    {
      "tip": "Use CFG 2.0 with DG models",
      "context": "DG models are essentially CFG distilled, CFG 1.0 kills quality",
      "from": "Kijai"
    },
    {
      "tip": "Combine multiple depth estimation methods",
      "context": "Using depth, lotus normals, and realistic lineart together provides more detail for better video quality",
      "from": "David Snow"
    },
    {
      "tip": "For better likeness: bigger reference on screen, use closeup of character face on white bg, higher res reference, describe character in prompt, can set reference strength higher than 1",
      "context": "When using VACE for character consistency",
      "from": "Hashu"
    },
    {
      "tip": "Reference resolution and preprocessing doesn't have to match output video resolution",
      "context": "VACE reference images",
      "from": "Hashu"
    },
    {
      "tip": "Cartoon/anime characters are much harder to get good likeness than realistic characters",
      "context": "VACE character consistency",
      "from": "Hashu"
    },
    {
      "tip": "Do preprocessing beforehand so everything gets offloaded to avoid memory issues",
      "context": "Managing VRAM with VACE",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use TeaCache with VACE for better performance",
      "context": "Optimizing VACE workflows",
      "from": "JmySff"
    },
    {
      "tip": "Lower VACE embed strength for better results",
      "context": "User settled on slightly lower than 0.925 embed strength",
      "from": "Kytra"
    },
    {
      "tip": "Convert input to 16fps for better lipsync",
      "context": "Though this didn't fully solve lipsync issues in testing",
      "from": "burgstall"
    },
    {
      "tip": "Use Image Composite Masked node for transparency overlays",
      "context": "Create mask from grayscale image for proper compositing",
      "from": "traxxas25"
    },
    {
      "tip": "Avoid --fast startup argument with certain precision settings",
      "context": "Can cause issues with float16 accumulate weights",
      "from": "AJO"
    },
    {
      "tip": "Use box mask instead of tight character cutouts",
      "context": "Prevents shape forcing in VACE output",
      "from": "Kijai"
    },
    {
      "tip": "Use solid background for better SkyreelsA2 reference results",
      "context": "When using SkyreelsA2 reference mode",
      "from": "DawnII"
    },
    {
      "tip": "Create character LoRA for better likeness consistency",
      "context": "When having trouble with character consistency in VACE, use 10 images to train small LoRA",
      "from": "notid"
    },
    {
      "tip": "Pad first frame image for SkyreelsA2",
      "context": "To match their training approach and get proper results",
      "from": "Kijai"
    },
    {
      "tip": "Use individual clip embeds even when compositing images",
      "context": "For SkyreelsA2 workflows with multiple subjects",
      "from": "Kijai"
    },
    {
      "tip": "Plan layout and size of person/subject/background when uploading reference images",
      "context": "For better VACE results",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "tip": "Describe reference properly to help with prompt-to-reference mapping",
      "context": "VACE uses prompt to reference matching",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use control with VACE reference or the reference overtakes the generation",
      "context": "When using VACE with reference images",
      "from": "Kijai"
    },
    {
      "tip": "Include clothing details in prompts or reference to maintain consistency between context windows",
      "context": "When using VACE context windows",
      "from": "Kijai"
    },
    {
      "tip": "Prompt for correct environment/lighting when generating characters",
      "context": "For character replacement workflows",
      "from": "traxxas25"
    },
    {
      "tip": "Try mediapipe face instead of pose estimator for lip movements in closeups",
      "context": "When doing facial control with VACE",
      "from": "Piblarg"
    },
    {
      "tip": "Use Hunyuan at low denoise as refiner for VACE outputs",
      "context": "To refine results without losing character traits",
      "from": "traxxas25"
    },
    {
      "tip": "Gray works best for VACE masking",
      "context": "When doing subject removal or replacement",
      "from": "traxxas25"
    },
    {
      "tip": "Tint mask color to background for better subject removal",
      "context": "When using VACE for removing subjects from video",
      "from": "ArtOfficial"
    },
    {
      "tip": "Blur depth maps and make them darker when combining with pose",
      "context": "When blending depth and pose controls in VACE",
      "from": "Kijai"
    },
    {
      "tip": "Use first steps with pose only for more character shape freedom",
      "context": "When combining multiple control inputs in VACE",
      "from": "Kijai"
    },
    {
      "tip": "Use mask to composite back into original video",
      "context": "To avoid background artifacts when doing character transformations",
      "from": "David Snow"
    },
    {
      "tip": "Don't use blurred masks",
      "context": "When working with segmentation masks for control",
      "from": "Zuko"
    },
    {
      "tip": "Add 'rotating' in prompt for better rotation control",
      "context": "When trying to get objects to rotate properly",
      "from": "Kijai"
    },
    {
      "tip": "Use reference images with VACE for better control",
      "context": "When trying to get specific character transformations",
      "from": "Kijai"
    },
    {
      "tip": "Don't use depth LoRA for final upscale refinement",
      "context": "Better results without LoRA in refinement step",
      "from": "Piblarg"
    },
    {
      "tip": "Use black images as ignored control frames",
      "context": "For Fun Control - send black image and it's ignored as control, allows multiple controls at different time steps without overlap",
      "from": "Kijai"
    },
    {
      "tip": "Duplicate input 4 times for better frame preservation",
      "context": "When you want to keep specific frames intact in VACE, duplicating input 4 times may help reduce temporal leakage",
      "from": "Kijai"
    },
    {
      "tip": "Use block swap for memory optimization",
      "context": "Use at least 1 VACE block swap for almost no VRAM usage with offloading",
      "from": "Kijai"
    },
    {
      "tip": "Desaturate normal map outputs",
      "context": "When using depth/normal preprocessing, ensure no color remains - should be grayscale only",
      "from": "David Snow"
    },
    {
      "tip": "Match reference image pose to first frame",
      "context": "For best VACE results, reference image should match the pose of the first frame",
      "from": "HeadOfOliver"
    },
    {
      "tip": "Use 4 frames for single image generation",
      "context": "When doing single frame generation with Wan, generate 4 frames and extract the first to avoid noise",
      "from": "Colin"
    },
    {
      "tip": "Use dark line preprocessor and multiply over top",
      "context": "Better approach for line art preprocessing",
      "from": "David Snow"
    },
    {
      "tip": "Aesthetic LoRA obliterates motion",
      "context": "When using reward LoRAs for quality improvement",
      "from": "David Snow"
    },
    {
      "tip": "Use full white for full effect in VACE outpainting",
      "context": "For outpainting masks, though gray might work better for inpainting",
      "from": "Kijai"
    },
    {
      "tip": "DiffSynth LoRAs should be used at 0.5 strength instead of 1.0",
      "context": "Since they're 2x stronger than originals without alpha keys",
      "from": "Kijai"
    },
    {
      "tip": "Use reward LoRAs at CFG 1 for quality improvement",
      "context": "Even at CFG 1 it improves quality",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "tip": "Lower strength when doing frame extension with first frame as reference",
      "context": "When using VACE for frame extension with reference frame to help fidelity",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use standard 1.3B with aesthetics lora instead of DG if using loras",
      "context": "When wanting to use loras, standard model works better than distilled",
      "from": "DawnII"
    },
    {
      "tip": "Be detailed with prompts when doing character replacement",
      "context": "When using VACE for character replacement with reference images",
      "from": "traxxas25"
    },
    {
      "tip": "Use animated diff masks for advanced control",
      "context": "For more sophisticated video manipulation workflows",
      "from": "David Snow"
    },
    {
      "tip": "LoRAs help with likeness consistency on unseen angles",
      "context": "For I2V when character needs to turn head or bow, T2V lora applied to I2V model improves results",
      "from": "Faux"
    },
    {
      "tip": "Use lower shift values for more motion",
      "context": "Lower values = more motion, 5 is more stable value, try 3 and 7 shift",
      "from": "Benjimon"
    },
    {
      "tip": "Start prompts with action description",
      "context": "Start prompt with something like 'A mid shot action video sequence' for better motion",
      "from": "Benjimon"
    },
    {
      "tip": "Use frame generation for quick testing",
      "context": "Set number of frames to 1 to generate images from Wan for quick testing, then use 'run (instant)' to iterate settings",
      "from": "Jas"
    },
    {
      "tip": "Use AI for prompt generation",
      "context": "Use Claude, ChatGPT or Gemini to create prompts, then use node to incrementally go through them",
      "from": "Jas"
    },
    {
      "tip": "Add 'mute' or 'mouth closed' to prevent lip movement",
      "context": "For animations without lip movement",
      "from": "Cubey"
    },
    {
      "tip": "Pad reference images with white background",
      "context": "When using VACE reference images, pad with white background for better results",
      "from": "A.I.Warper"
    },
    {
      "tip": "Use higher frame rate from source video for better motion",
      "context": "Big motion causes issues even with pose preprocessor",
      "from": "notid"
    },
    {
      "tip": "Skip every second frame degrades motion significantly",
      "context": "When doing v2v, maintain full frame rate",
      "from": "David Snow"
    },
    {
      "tip": "Depth is more stable input than pose",
      "context": "Pose preprocessors available are not great",
      "from": "David Snow"
    },
    {
      "tip": "Use bbox mask for better VACE results",
      "context": "When doing face-only animation with VACE",
      "from": "IllumiReptilien"
    },
    {
      "tip": "Lower control lora strength for reward loras",
      "context": "Reward loras should be used at 0.5-0.7 strength and introduce fireflies at higher strengths",
      "from": "Kijai"
    },
    {
      "tip": "Use multiple controls by chaining VACE encodes",
      "context": "When you want to combine canny and depth or other controls",
      "from": "David Snow"
    },
    {
      "tip": "Invert your lineart for better results",
      "context": "When using lineart controls in VACE",
      "from": "David Snow"
    },
    {
      "tip": "Black areas in mask preserve original, white areas generate new content",
      "context": "When using input masks in VACE - black = original video, white = new generation",
      "from": "Hashu"
    },
    {
      "tip": "Process without reward/high res loras first, add on second pass",
      "context": "When doing style transfer to avoid bias towards realism",
      "from": "Hashu"
    },
    {
      "tip": "Use 2D, flat colour, cel-shading prompts to maintain style",
      "context": "When trying to prevent 2D to 3D style drift",
      "from": "StableVibrations"
    },
    {
      "tip": "Increase overlap to reduce context window switching artifacts",
      "context": "When generating long videos with context windows",
      "from": "David Snow"
    },
    {
      "tip": "Use CFG 2 with DG_Boost models",
      "context": "For better results with the new boosted models",
      "from": "David Snow"
    },
    {
      "tip": "Use 50px padding for VACE reference images",
      "context": "Works fine, don't need the 256px shown in examples",
      "from": "A.I.Warper"
    },
    {
      "tip": "Don't blur edges for VACE masks",
      "context": "Keep mask edges sharp for better results",
      "from": "Kijai"
    },
    {
      "tip": "Use 'arms cropped, hands cropped out of view' to avoid drawing hands",
      "context": "When you don't want hands visible in the generation",
      "from": "Kytra"
    },
    {
      "tip": "Tweak control blending percentages when combining multiple controls",
      "context": "100% often gives bad results, 60% worked better for anime style",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Separate reference image and input frames with different strengths in VACE",
      "context": "Reference at 100% strength, input frames at 60% gave cleaner results",
      "from": "Johnjohn7855"
    },
    {
      "tip": "For distant character details, crop out character and do stylization, then paste back to video",
      "context": "When character loses important details like robotic parts when far away",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Lower resolution to get more motion, use video to get control video, resize and run with VACE",
      "context": "When motion is required",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Too many controls tend to leave noise behind, increasing steps don't help",
      "context": "When combining multiple control methods",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Just stylize the first frame rather than using separate reference image",
      "context": "When using stylized first frame + ref-image seems redundant",
      "from": "David Snow"
    },
    {
      "tip": "Use 20 blocks instead of 10 for block swap with 720 model",
      "context": "When dealing with VRAM issues on hefty 720 model",
      "from": "David Snow"
    },
    {
      "tip": "Use last frame from previous generation as first frame + reference for continuity",
      "context": "For creating longer continuous videos by chaining generations",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Disable context windows for style preservation",
      "context": "When you want to maintain consistent style across generation",
      "from": "David Snow"
    },
    {
      "tip": "Add contrast node to normal output for mouth movements",
      "context": "Helps model pick up on small facial movements and mouth details",
      "from": "David Snow"
    },
    {
      "tip": "Use control video strength of 0.5-0.6 with reference at 1.0",
      "context": "Good balance for clean results while maintaining reference image influence",
      "from": "Johnjohn7855"
    },
    {
      "tip": "Use simple car prompts like 'Car speeds over a snowy landscape'",
      "context": "Complex motion prompts don't help with car direction issues",
      "from": "Verevolf"
    },
    {
      "tip": "Use end image to prevent color shifting in loops",
      "context": "When creating seamless video loops",
      "from": "DawnII"
    },
    {
      "tip": "Start TeaCache at step 2 or 3 for better quality",
      "context": "When using TeaCache optimization",
      "from": "MilesCorban"
    },
    {
      "tip": "Use last frame to start new video for extension",
      "context": "When extending Wan 14B videos with LoRAs",
      "from": "Benjimon"
    },
    {
      "tip": "Train on videos only instead of mixed video/image datasets",
      "context": "When training 1.3B models using synthetic 14B data",
      "from": "Piblarg"
    },
    {
      "tip": "Use SDPA attention for older GPUs like RTX 8000",
      "context": "RTX 8000 and similar older cards don't support BF16",
      "from": "Benjimon"
    },
    {
      "tip": "Use pose control for butter smooth motion",
      "context": "Simple test with just pose input produces very smooth results",
      "from": "David Snow"
    },
    {
      "tip": "Crank up LoRA strength to 2.0 for non-Fun LoRAs on Fun models",
      "context": "With 1.3B models, need higher strength to have any effect",
      "from": "Kijai"
    },
    {
      "tip": "Use defaults for shift and cfg with Wan",
      "context": "Defaults work best, more important than with other models like HunyuanVideo",
      "from": "Draken"
    },
    {
      "tip": "Try empty prompt or minimal prompts with FLF2V",
      "context": "Even empty prompt seems better sometimes, or model hallucinates too much between frames",
      "from": "Kijai"
    },
    {
      "tip": "Disable force_offload for multiple consecutive runs",
      "context": "Will speed up subsequent runs on same workflow",
      "from": "Kijai"
    },
    {
      "tip": "Use 768x768 or higher for 720p model",
      "context": "720p model works better at 720x720 and up, closer to 1 megapixel",
      "from": "Draken"
    },
    {
      "tip": "Use low denoise 0.38 with distilled 1.3B model",
      "context": "For fast rendering with good quality - 1.11 minutes for 161 frames",
      "from": "Flipping Sigmas"
    },
    {
      "tip": "Set clipvision strength around 0.3 for more dynamic i2v",
      "context": "To make i2v less static, combine with specific motion prompting",
      "from": "DevouredBeef"
    },
    {
      "tip": "Use Chinese prompts for FLF2V",
      "context": "Model was primarily trained on Chinese text-video pairs for better results",
      "from": "DawnII"
    },
    {
      "tip": "Crank up shift a lot in HYV to get motion",
      "context": "HYV requires high shift values for dynamic movement",
      "from": "Benjimon"
    },
    {
      "tip": "Use Chinese prompts for better FLF2V results",
      "context": "When using the new first frame last frame model",
      "from": "DawnII"
    },
    {
      "tip": "Increase resolution for lip and eye direction following",
      "context": "When using Fun Control, use 576x1024 or higher",
      "from": "A.I.Warper"
    },
    {
      "tip": "Use shift values of 20-30 for better VACE reference adherence",
      "context": "Counter to base models but improves style adherence",
      "from": "DawnII"
    },
    {
      "tip": "Use lineart instead of depth for anime style content",
      "context": "When getting blur issues with VACE depth control",
      "from": "Kijai"
    },
    {
      "tip": "Mix 3 control nets for better face control",
      "context": "Use depth (overall composition) + pose + line (face mimics)",
      "from": "N0NSens"
    },
    {
      "tip": "Upscale original footage for vid2vid workflows",
      "details": "Use AI upscaling like Topaz Starlight to make details much clearer for better processing",
      "from": "chrisd0073"
    },
    {
      "tip": "Overdo AI upscaling for toonish style conversion",
      "details": "When converting realistic footage to cartoon style, push upscaling until edges look artificial for better results",
      "from": "David Snow"
    },
    {
      "tip": "Use smaller frame ranges for more stability",
      "details": "33 frame batches provide more stability and allow for finer text prompting in each batch",
      "from": "Hashu"
    },
    {
      "tip": "Force 16fps in video loader for better motion handling",
      "details": "Setting 16fps can help with motion issues, consider interpolating to 60fps beforehand",
      "from": "Flipping Sigmas"
    },
    {
      "tip": "Consider tile size as quality control in FramePack",
      "details": "Larger tile sizes provide better quality, avoid tiny tiles in corners",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "tip": "Use threshold to force consistency with mediapipe landmarks",
      "context": "When working with eye detection and control",
      "from": "David Snow"
    },
    {
      "tip": "Use timestep scheduling instead of stacking VACE encodes",
      "context": "When wanting different strengths for different controls",
      "from": "Kijai"
    },
    {
      "tip": "Blur depth maps for VACE compatibility",
      "context": "When VACE treats high quality depth maps as RGB grayscale inputs",
      "from": "Kijai"
    },
    {
      "tip": "Always copy entire ComfyUI folder as backup before updating",
      "context": "To avoid reinstalling from scratch when updates break",
      "from": "Thom293"
    },
    {
      "tip": "Use symlinked models folder with multiple ComfyUI installs",
      "context": "Allows sharing models across different ComfyUI versions while maintaining compatibility",
      "from": "Nathan Shipley"
    },
    {
      "tip": "Force rate 24 instead of 16 for WAN helps with getting correct previews",
      "context": "When using VHS loader for output",
      "from": "Flipping Sigmas"
    },
    {
      "tip": "Never install requirements.txt files",
      "context": "They're often full of unnecessary dependencies",
      "from": "Kijai"
    },
    {
      "tip": "Enable animated previews during generation",
      "context": "Saves hours by allowing you to cancel bad runs early",
      "from": "David Snow"
    },
    {
      "tip": "Check ComfyUI input folder regularly",
      "context": "It keeps copies of every media file loaded and can cause launch failures",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use specific camera movement prompts for better motion",
      "context": "Prompting 'tracking shot' gives more lively character outputs",
      "from": "Jas"
    },
    {
      "tip": "For fast motion scenes, turn off all optimizations",
      "context": "Better quality for action sequences at cost of speed",
      "from": "Benjimon"
    },
    {
      "tip": "Use shift up to 17 for SkyReels",
      "context": "When running SkyReels models",
      "from": "Benjimon"
    },
    {
      "tip": "Run source footage through Topaz first for better results",
      "context": "When doing v2v processing - garbage in, garbage out principle",
      "from": "David Snow"
    },
    {
      "tip": "Use Florence2 box detection instead of referring expression segmentation",
      "context": "For better mask tracking results",
      "from": "Kijai"
    },
    {
      "tip": "Adding 'anime' to prompt improves overall quality",
      "context": "When using UniAnimate for anime-style content",
      "from": "Kijai"
    },
    {
      "tip": "Use CFG of 2.0 with VACE",
      "context": "High CFG causes artifacts with VACE",
      "from": "David Snow"
    },
    {
      "tip": "Use DPM++ or DEIS samplers to avoid blocky glitches",
      "context": "UniPC sampler causes weird issues, Euler with same settings won't",
      "from": "David Snow"
    },
    {
      "tip": "Make all resolutions/frame lengths match for UniAnimate",
      "context": "Node will cut if too long and pad with last pose if too short",
      "from": "Kijai"
    },
    {
      "tip": "Ignore ref pose for easiest UniAnimate setup",
      "context": "Load lora, use default I2V workflow, add pose inputs",
      "from": "Kijai"
    },
    {
      "tip": "Style lora would probably help a lot with UniAnimate",
      "context": "When working with pose-driven animation",
      "from": "Kijai"
    },
    {
      "tip": "Try different seeds to fix blocky artifacts",
      "context": "Wan models have similar blocky glitches that depend on seed, prompt, and CFG",
      "from": "Screeb"
    },
    {
      "tip": "Lower CFG generally makes blocky artifacts better or go away",
      "context": "All Wan models can have blocky glitches",
      "from": "Screeb"
    },
    {
      "tip": "Use camera movement prompts for FLF instead of scene details",
      "context": "When using First-Last-Frame morphing",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "tip": "Try UniPC FlowMatch scheduler for better results",
      "context": "When getting broken outputs",
      "from": "yi"
    },
    {
      "tip": "Use fp32 for better quality but slower speed",
      "context": "When quality is more important than speed",
      "from": "Kijai"
    },
    {
      "tip": "Update nodes before using new workflows",
      "context": "When using SkyReels workflows",
      "from": "Kijai"
    },
    {
      "tip": "Enable animated preview to save time",
      "context": "Will save days worth of wasted generations in the long run - it's a gamechanger",
      "from": "David Snow"
    },
    {
      "tip": "Use different prompts for each DF window",
      "context": "DF allows using different prompts etc. for each continuation window",
      "from": "Kijai"
    },
    {
      "tip": "Use LoRAs instead of reference images for style transfer",
      "context": "Reference images cause flash at start and glitches, LoRAs work better despite limited availability",
      "from": "David Snow"
    },
    {
      "tip": "Use VACE start/end frame node for automation",
      "context": "Auto fills grey frames and masks for you instead of manual setup",
      "from": "Hashu"
    },
    {
      "tip": "Add SLG later rather than from the getgo",
      "context": "When optimizing workflow",
      "from": "Kijai"
    },
    {
      "tip": "Use separate VACE encodes for multiple controls",
      "context": "Better than combining into one for reference image + line art",
      "from": "David Snow"
    },
    {
      "tip": "Use anyline instead of canny for VACE",
      "context": "Much better choice for line art control",
      "from": "David Snow"
    },
    {
      "tip": "Half-body pictures produce better faces",
      "context": "Larger face proportion in frame leads to better generated faces",
      "from": "wange1002"
    },
    {
      "tip": "Never use precision higher than model weights",
      "context": "No point in using higher precision than what weights are stored at",
      "from": "Kijai"
    },
    {
      "tip": "Use as much RAM as possible for speed",
      "context": "If you have 128GB RAM, use it all for faster performance",
      "from": "lostintranslation"
    },
    {
      "tip": "Use empty frame (black or gray) for missing VACE components",
      "context": "When trying to separate VACE controls",
      "from": "Kijai"
    },
    {
      "tip": "Combine depth, normal and line controls with image blend",
      "context": "Can encode multiple controls into single embed for both VACE and base 1.3B",
      "from": "David Snow"
    },
    {
      "tip": "Use TeaCache 'e' mode for less memory usage",
      "context": "E mode uses ~600MB instead of ~4GB for cache",
      "from": "Kijai"
    },
    {
      "tip": "VACE works best with cleanup and reruns",
      "context": "Can get amazing results with additional passes",
      "from": "Draken"
    },
    {
      "tip": "Better preprocessors needed for VACE",
      "context": "VACE sensitivity to artifacts means preprocessing quality is crucial",
      "from": "David Snow"
    },
    {
      "tip": "For I2V, only prompt the dynamics and let the image provide static context",
      "context": "When using SkyReels V2 I2V models, focus prompts on action/expression/camera motion rather than describing visual elements already in the image",
      "from": "fredbliss"
    },
    {
      "tip": "Adjust TeaCache threshold to balance speed vs quality",
      "context": "TeaCache can degrade hands at higher settings, so find the right threshold balance. You can see how many steps it skips after sampling.",
      "from": "Kijai"
    },
    {
      "tip": "Use spline editor for smooth CFG scheduling transitions",
      "context": "Instead of abrupt CFG changes, spline editor allows smooth transitions between CFG values across steps",
      "from": "Kijai"
    },
    {
      "tip": "Clear VRAM and cache between runs to avoid OOM",
      "context": "Especially important when canceling runs midway to prevent memory issues",
      "from": "MilesCorban"
    },
    {
      "tip": "Push start percent up from 1 to 10-30 for TeaCache",
      "context": "When experiencing first frame corruption",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use Resize To Closest node for better stride",
      "context": "Better at ensuring image has right stride leading to better videos",
      "from": "jellybean5361"
    },
    {
      "tip": "Test at 33 frames minimum for quality assessment",
      "context": "Anything less than 33 frames doesn't work well for testing enhancements and settings",
      "from": "Kijai"
    },
    {
      "tip": "Always keep cfgzerostar and Fresca enabled",
      "context": "Safe to always keep on, generally better output",
      "from": "Kijai"
    },
    {
      "tip": "Use separate VACE embed for each sampler node",
      "context": "When combining VACE with DF model",
      "from": "DawnII"
    },
    {
      "tip": "Use color match to fix brightness drift in extended videos",
      "context": "When chaining DF model videos together",
      "from": "MilesCorban"
    },
    {
      "tip": "Check outputs and do seed hunting with each sampler for infinite generation",
      "context": "When creating long video sequences",
      "from": "seitanism"
    },
    {
      "tip": "Adjust SLG start to 0.3 and end to 0.8 for better results",
      "context": "When experiencing discoloration or artifacts with default SLG settings",
      "from": "zelgo_"
    },
    {
      "tip": "Don't go under 33 frames or over 97 frames",
      "context": "When using DF models, too low frame count needs different settings",
      "from": "MilesCorban"
    },
    {
      "tip": "Use bigger frame overlap for better motion consistency",
      "context": "When character motion degrades in video extensions",
      "from": "seitanism"
    },
    {
      "tip": "Use 27 frame overlap instead of default 17 for better extension quality",
      "context": "When doing video extension with DF",
      "from": "boorayjenkins"
    },
    {
      "tip": "Use simpler images/scenes for better DF results",
      "context": "Complex scenes with lots of detail cause more degradation",
      "from": "seitanism"
    },
    {
      "tip": "Color matching helps but be careful with scene changes",
      "context": "Can cause weird colors if scene changes too much",
      "from": "Ablejones"
    },
    {
      "tip": "Use ID preprocessing for better identity retention with HYV",
      "context": "When doing identity-focused generation",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "tip": "Stabilize content for V2V then matchmove back after generation",
      "context": "When dealing with temporal downsampling issues in HYV",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "tip": "Try different overlap values for DF stitching",
      "context": "Default is 17 but 9 also works, can experiment with lower values for smaller inferences",
      "from": "Kijai"
    },
    {
      "tip": "Avoid using same seed on all DF samplers",
      "context": "Sometimes causes repetition in diffusion forcing workflows",
      "from": "Kijai"
    },
    {
      "tip": "Use WinDirStat to manage storage",
      "context": "For Windows users to understand which folders use most storage, especially cache folders that grow to absurd sizes",
      "from": "mamad8"
    },
    {
      "tip": "Use cfg scheduling for speed",
      "context": "1/3rd of steps with cfg 1 increases speed",
      "from": "seitanism"
    },
    {
      "tip": "Prompt input frames carefully",
      "context": "Input frames are very strong and often override prompts - need to hold the model's hand and only write prompts for next sampler after being happy with previous output",
      "from": "seitanism"
    },
    {
      "tip": "Use Keep Proportion for ultrawides",
      "context": "For ultrawide videos, use 1600x1600 with Keep Proportion in Image Resize node",
      "from": "ezMan"
    },
    {
      "tip": "20 steps for testing, 50+ for quality",
      "context": "General guideline for step counts",
      "from": "MilesCorban"
    },
    {
      "tip": "Start control embeds at 0.0 and end early rather than the other way around",
      "context": "When using control embeds, often start at 0.0 and end at 0.5",
      "from": "David Snow"
    },
    {
      "tip": "Use blur on depth maps to allow more natural interpretation",
      "context": "Add blur node to depth map and turn down latent strength to let model interpret motion more naturally",
      "from": "David Snow"
    },
    {
      "tip": "Use main_device only for cloud GPUs with 80GB+ VRAM",
      "context": "main_device should only be used for high VRAM setups or when using 1.3B models",
      "from": "Kijai"
    },
    {
      "tip": "Mix depth with line/canny/pose to fix faces",
      "context": "Using multiple control types can improve face quality",
      "from": "N0NSens"
    },
    {
      "tip": "For long vid2vid with VACE, feed it 5-6 frames from previous rendered piece",
      "context": "Don't do masking until after those frames, keep segments close to same length for consistent look",
      "from": "TimHannan"
    },
    {
      "tip": "Use first frame instead of just reference image for better VACE results",
      "context": "First frame gets incorporated into actual frames, while ref image gets turned into data and fed to model",
      "from": "Draken"
    },
    {
      "tip": "Can combine both first frame and reference image in VACE",
      "context": "Useful when first frame is close-up but you want model to know details that are off-screen",
      "from": "Draken"
    },
    {
      "tip": "Don't update ComfyUI unless you have to",
      "context": "Only update when new model needs it or required node needs it to avoid breaking things",
      "from": "Draken"
    },
    {
      "tip": "Use ComfyUI rope function instead of default",
      "context": "Comfy's rope is more efficient and avoids dtype errors",
      "from": "Kijai"
    },
    {
      "tip": "Choose LOW versions for many steps rendering, HIGH models for few steps (6-8)",
      "context": "DG models usage, modify CFG to avoid over-saturated colors",
      "from": "JmySff"
    },
    {
      "tip": "Padding beginning and end with first/last frame helps with degradation",
      "context": "DF extension workflows",
      "from": "TimHannan"
    },
    {
      "tip": "Use same prompt that generated initial t2v/i2v clip in DF stage",
      "context": "When using t2v/i2v fed into DF approach",
      "from": "anever"
    },
    {
      "tip": "Match 1.3B/14B LoRAs with right model when using DF",
      "context": "LoRA compatibility across model sizes",
      "from": "anever"
    },
    {
      "tip": "For FantasyTalking, adjust audio scale - 0.5 works well for t2v",
      "context": "Audio conditioning strength",
      "from": "Kijai"
    },
    {
      "tip": "With Wan LoRAs on Skyreels, consider using 16fps as LoRAs may be trained at that fps",
      "context": "Frame rate optimization",
      "from": "hablaba"
    },
    {
      "tip": "Use CFG balance for Fantasy Talking",
      "context": "Finding balance between audio CFG and sampler CFG is key to good results",
      "from": "Stad"
    },
    {
      "tip": "Use variable CFG scheduling",
      "context": "First 3 steps with audio CFG 5.0, rest at 1.0 for better quality",
      "from": "Kijai"
    },
    {
      "tip": "Clip silence from audio",
      "context": "Remove empty audio at beginning to avoid sync issues with Fantasy Talking",
      "from": "andrewrasmussen."
    },
    {
      "tip": "Use swap memory for larger models",
      "context": "Can run 14B with only 8GB VRAM using SSD as additional RAM (slower but works)",
      "from": "Stad"
    },
    {
      "tip": "Fantasy Talking works better with shorter prompts",
      "context": "Long prompts cause model to stop moving lips",
      "from": "ingi // SYSTMS"
    },
    {
      "tip": "Use frame numbers that are multiples of 16+1",
      "context": "Model likes these frame counts - try 81 instead of 77",
      "from": "ingi // SYSTMS"
    },
    {
      "tip": "Give VACE model room with single control",
      "context": "Often 1 control is enough, model needs space to work",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Use vocal separation for Fantasy Talking",
      "context": "Clear speech works better, remove music backing for better results",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "tip": "Add duplicate frames at video start for LoRA flash issue",
      "context": "Cut them off in post to handle flash artifacts from LoRAs",
      "from": "Gavmakes"
    },
    {
      "tip": "Use points editor workflow for masking",
      "context": "Send single frame first to add points, then full animation",
      "from": "David Snow"
    },
    {
      "tip": "Keep empty frame level at 0.5",
      "context": "This is the default the model was trained with",
      "from": "Kijai"
    },
    {
      "tip": "Save first frame and drag it over inputs when working with video segmentation",
      "context": "When node doesn't load first frame automatically",
      "from": "ArtOfficial"
    },
    {
      "tip": "Use birme.net for quick batch image resizing",
      "context": "Free tool for preparing training datasets",
      "from": "JohnDopamine"
    }
  ],
  "news": [
    {
      "update": "VACE model released",
      "details": "Ali-vilab released VACE-Wan2.1-1.3B-Preview on HuggingFace - video control system with subject reference capabilities",
      "from": "Kijai"
    },
    {
      "update": "VACE also released for LTX",
      "details": "VACE is available for both Wan and LTX models",
      "from": "DawnII"
    },
    {
      "update": "MuseTalk 1.5 released",
      "details": "New version of MuseTalk released but requirements are problematic for ComfyUI integration",
      "from": "burgstall"
    },
    {
      "update": "FastVideo Wan support in development",
      "details": "New support being prepared for Wan version in FastVideo project",
      "from": "JohnDopamine"
    },
    {
      "update": "VACE preview model released",
      "details": "Video control system with style transfer, inpainting, subject-driven capabilities",
      "from": "community"
    },
    {
      "update": "Native ComfyUI support coming for VACE",
      "details": "Official developer confirmed implementation in progress from China",
      "from": "comfy"
    },
    {
      "update": "Kijai added start/end percent controls to VACE",
      "details": "Can end at halfway through steps for faster processing",
      "from": "Kijai"
    },
    {
      "update": "59 new Wan effects LoRAs released",
      "details": "Collection includes T2V and I2V LoRAs for 14B model at 480p",
      "from": "\u26a1"
    },
    {
      "update": "VACE planned for 14B model",
      "details": "VACE people are intending on putting it on the 14B",
      "from": "Piblarg"
    },
    {
      "update": "VACE support added to Kijai's WanVideoWrapper",
      "details": "New VACE Encode node available for comprehensive control similar to ACE++ for video",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "TeaCache memory optimization",
      "details": "Fixed memory leak that was more apparent with VACE due to larger tensor sizes",
      "from": "Kijai"
    },
    {
      "update": "Wan Fun models released",
      "details": "Wan fun (inpaint and control) models available a few days ago",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "New SkyReels A2 model released",
      "details": "New model from Skywork AI team",
      "from": "Juampab12"
    },
    {
      "update": "VACE-14B model expected to drop",
      "details": "Larger variant of VACE model coming",
      "from": "Benjimon"
    },
    {
      "update": "SkyReels-A2 released by SkyworkAI",
      "details": "New model but lacks control features, diffusers format only, less interesting than VACE according to community",
      "from": "Kijai"
    },
    {
      "update": "12 new DiffSynth Wan 2.1 model variants",
      "details": "Different levels of distilled + hires + refined editions, experimental versions supporting 4-10+ steps",
      "from": "Kijai"
    },
    {
      "update": "VACE compilation improvements",
      "details": "Now compiles VACE blocks when compile transformer blocks selected, ~15% speed increase",
      "from": "Kijai"
    },
    {
      "update": "New Start to End Frame node available",
      "details": "Kijai released helper node for managing start and end frames in VACE",
      "from": "Kijai"
    },
    {
      "update": "TinkerWAN Alpha lora updated to v1.0",
      "details": "New version uses positive instead of negative values",
      "from": "David Snow"
    },
    {
      "update": "New example workflow with start/end frame and control with reference",
      "details": "Kijai provided updated workflow examples in single file",
      "from": "Kijai"
    },
    {
      "update": "WanVideoWrapper nodes updated with error detection",
      "details": "Added proper error messages to prevent common configuration mistakes",
      "from": "Kijai"
    },
    {
      "update": "New Test Pattern node added",
      "details": "Helper node for creating test patterns and adding empties between frames",
      "from": "Kijai"
    },
    {
      "update": "Standalone VACE model released",
      "details": "Separate VACE loader available, saves 6GB disk space and works with any 1.3B model",
      "from": "Kijai"
    },
    {
      "update": "VACE model select node pushed",
      "details": "New node for loading standalone VACE models",
      "from": "Kijai"
    },
    {
      "update": "Multiple DG model variants available",
      "details": "Various DG models (High, Light, V3, V4) available on HuggingFace",
      "from": "Hashu"
    },
    {
      "update": "New prev_embeds parameter added to VACE",
      "details": "Allows multiple control inputs with different strengths and timestep ranges",
      "from": "Kijai"
    },
    {
      "update": "VACE context input made optional",
      "details": "Fixed workflow compatibility issue",
      "from": "Kijai"
    },
    {
      "update": "New DG distilled models available",
      "details": "Multiple variants (light/medium/high, v1-v4) for faster inference",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "Kosinkadink working on native ADE context implementation for Wan",
      "details": "Will make ADE contexts treat any tensor dimension as batch, not just first dimension",
      "from": "Kosinkadink"
    },
    {
      "update": "Kosinkadink back from China conference but recovering from illness",
      "details": "Development may be slower while recovering",
      "from": "Kosinkadink"
    },
    {
      "update": "New custom node released for text extraction",
      "details": "ComfyUI-BS-Textchop for extracting multiple text segments",
      "from": "burgstall"
    },
    {
      "update": "SkyreelsA2 model released",
      "details": "14B model that does same as VACE with reference and start frame, better for I2V using 14B I2V model",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI getting more unstable with recent updates",
      "details": "Users reporting more frequent issues with custom nodes and loading after updates",
      "from": "David Snow"
    },
    {
      "update": "VACE native ComfyUI support coming soon",
      "details": "Comfy will look into native VACE support once available",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "SkyReels v2 paper released",
      "details": "User excited about potential dataset release",
      "from": "fredbliss"
    },
    {
      "update": "New tiny autoencoder available",
      "details": "Mentioned in passing, no details provided",
      "from": "fredbliss"
    },
    {
      "update": "Distilled Wan model available in pinned messages",
      "details": "Community member points to pinned messages for distilled version",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "update": "Triton now easily installable on Windows",
      "details": "Can now just pip install triton-windows, works with Python 3.12/CUDA 12 on Windows 11",
      "from": "Lumi"
    },
    {
      "update": "Dream Making released Frutiger Lora for 14B model",
      "details": "Available on HuggingFace: dreamer8/FrutigerLora14B_Wan with sample generations",
      "from": "Dream Making"
    },
    {
      "update": "Major VACE memory optimization update pushed",
      "details": "Significant VRAM reduction with minimal speed loss, removes redundant tensor conversions",
      "from": "Kijai"
    },
    {
      "update": "Reward LoRAs released",
      "details": "HuggingFace repo alibaba-pai/Wan2.1-Fun-Reward-LoRAs now available with HPS2.1 and MPS variants",
      "from": "Lumi"
    },
    {
      "update": "VACE 14B final version in development",
      "details": "VACE author confirmed they are working on final 14B version and seeking developer feedback",
      "from": "HJ"
    },
    {
      "update": "Native ComfyUI support for reward LoRAs",
      "details": "ComfyUI added native support that converts the released reward LoRAs",
      "from": "Screeb"
    },
    {
      "update": "Tiled VACE encode added",
      "details": "Kijai added tiled encoding option for VACE (2x2 tile) to handle higher resolutions",
      "from": "Kijai"
    },
    {
      "update": "VACE 14B model coming soon",
      "details": "Kijai confirmed it looks like 14B VACE should be out pretty soon",
      "from": "Kijai"
    },
    {
      "update": "New reward LoRAs from DiffSynth-Studio released",
      "details": "Different files with different transformer structure, works better than originals",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "update": "Kijai updated WanVideoWrapper with fixes",
      "details": "Fixed tiled encoding and other improvements",
      "from": "Kijai"
    },
    {
      "update": "14B VACE model development mentioned",
      "details": "Developers said 14B VACE is coming but not yet uploaded",
      "from": "ameasure"
    },
    {
      "update": "CFG distilled LoRA uploaded",
      "details": "Distilled model LoRA now available for faster inference",
      "from": "mkt"
    },
    {
      "update": "Fixed batched CFG support",
      "details": "Kijai added batched CFG support in recent update",
      "from": "Kijai"
    },
    {
      "update": "New aesthetic and highres LoRAs uploaded",
      "details": "New HPS and MPS LoRAs uploaded yesterday, different file sizes from previous ones",
      "from": "David Snow"
    },
    {
      "update": "Pytorch starting to have nvfp4 support",
      "details": "Would give further edge to the 5000 series GPUs",
      "from": "Kijai"
    },
    {
      "update": "IP Adapter for Wan 1.3b announced",
      "details": "Ostris announced IP Adapter with impressive adherence, early stages but promising",
      "from": "pom"
    },
    {
      "update": "ReCamMaster model available",
      "details": "Camera control model for Wan 2.1, available in branch but not merged to main",
      "from": "Kijai"
    },
    {
      "update": "New detail enhancement LoRA released",
      "details": "dg_wan2_1_v1_3b_lora_extra_noise_detail_motion.safetensors - adds significant detail, max recommended 0.35",
      "from": "David Snow"
    },
    {
      "update": "Multiple new boost LoRAs released",
      "details": "Boost ares, Boost evol, Boost stock, Boost train - 20 different variants available",
      "from": "DawnII"
    },
    {
      "update": "SkyReels-A2 model released",
      "details": "New model that needs its own workflow, has example with wrapper",
      "from": "AI_Fan"
    },
    {
      "update": "WanWrapper updated with simple nodes",
      "details": "Kijai merged simple version nodes to main branch",
      "from": "Kijai"
    },
    {
      "update": "New DG_Boost model variants released",
      "details": "Multiple versions including Evol V3, configured for 5-6 steps, compatible with Diffusion Pipe training tool, final version from creator",
      "from": "David Snow"
    },
    {
      "update": "Kijai added mask padding to VACE",
      "details": "Fixed issue where mask edges become obvious by automatically padding masks",
      "from": "Kijai"
    },
    {
      "update": "New schedulers added to WanVideoWrapper",
      "details": "DEIS, DPM, UniPC, Beta schedulers added - DEIS seemed somewhat sharper, UniPC/Beta showed promise",
      "from": "Kijai"
    },
    {
      "update": "OptimalSteps implemented for WAN native",
      "details": "GitHub repo bebebe666/OptimalSteps provides custom sigmas for potentially faster generation",
      "from": "crinklypaper"
    },
    {
      "update": "Pusa-VidGen has Wan2.1 on roadmap",
      "details": "GitHub project planning Wan2.1 integration",
      "from": "yi"
    },
    {
      "update": "SynCamMaster-Wan2.1 released",
      "details": "New camera control system for Wan 2.1 available on HuggingFace",
      "from": "yi"
    },
    {
      "update": "First animated logo LoRA released",
      "details": "First version available on Civitai, result not perfect but ready to start",
      "from": "Alisson Pereira"
    },
    {
      "update": "Wan 2.1 Knowledge Base ported to Notion",
      "details": "Complete channel summary with videos and workflows now publicly accessible",
      "from": "Nathan Shipley"
    },
    {
      "update": "SkipLayerGuidanceDiT node available in ComfyUI",
      "details": "Same as SLG args for skipping layers in diffusion models",
      "from": "yi"
    },
    {
      "update": "CausVid released for faster Wan inference",
      "details": "Alternative sampling method that runs model differently instead of sampling all frames together",
      "from": "Cubey"
    },
    {
      "update": "Wan2.1-FLF2V-14B-720P model released",
      "details": "First-last frame to video generation model, 720p only, available on HuggingFace with fp16 and fp8 versions",
      "from": "JohnDopamine"
    },
    {
      "update": "CublasOps in ComfyUI speeds up fp16 inference by ~20%",
      "details": "New commit in ComfyUI for faster fp16 inference, works well with SDXL",
      "from": "yi"
    },
    {
      "update": "TheDirector workflow published",
      "details": "Custom node for 2-24 scene Wan generation in one go with consistent characters using Gemini",
      "from": "AJO"
    },
    {
      "update": "FramePack I2V released by lllyasviel",
      "details": "New system for generating long duration videos (up to 1 minute) with minimal VRAM. HYV weights available on HuggingFace",
      "from": "JohnDopamine"
    },
    {
      "update": "Wan FLF2V 14B 720P officially released",
      "details": "Now available on official Wan-AI repo with Chinese prompt recommendation",
      "from": "DawnII"
    },
    {
      "update": "Kijai working on FramePack ComfyUI implementation",
      "details": "Very early mockup version running with native models except transformer",
      "from": "Kijai"
    },
    {
      "update": "Union Pro 2.0 Flux ControlNets released",
      "details": "New insane Flux control nets just dropped",
      "from": "Draken"
    },
    {
      "update": "Wan FLF2V (First Frame Last Frame) model released",
      "details": "14B model for 720P with fp8 quantization available",
      "from": "\ud83e\udd99rishappi"
    },
    {
      "update": "FramePack model released",
      "details": "Currently only for Hunyuan, allows low VRAM video generation and longer videos",
      "from": "BondoMan"
    },
    {
      "update": "UniAnimate-DiT released",
      "details": "New model from ali-vilab",
      "from": "yo9o"
    },
    {
      "update": "FLUX.1-dev-ControlNet-Union-Pro-2.0 released",
      "details": "New ControlNet model from Shakker-Labs",
      "from": "PolygenNoa"
    },
    {
      "update": "SkyReels V2 models released",
      "details": "Multiple variants: I2V 14B at 540P and 720P, T2V 14B at 540P and 720P, DF 1.3B at 540P - all based on Wan 2.1 and finetuned on 10M clips",
      "from": "yi"
    },
    {
      "update": "UniAnimate-DiT repository available",
      "details": "New repository for UniAnimate-DiT, ComfyUI implementation expected soon",
      "from": "amli"
    },
    {
      "update": "Wan developer AMA on Twitter",
      "details": "Lead developer shared that they have faster models but don't have OK to open source yet, expects VACE for 14B to perform much better than 1.3B",
      "from": "JohnDopamine"
    },
    {
      "update": "FLF2V GGUFs uploaded",
      "details": "city96 uploaded GGUF versions of Wan2.1-FLF2V-14B-720P",
      "from": "gshawn"
    },
    {
      "update": "SkyReels models made private/deleted from HuggingFace",
      "details": "All SkyReels V2 model weights were removed, possibly uploaded early by mistake before code release",
      "from": "Screeb"
    },
    {
      "update": "UniAnimate-DiT released",
      "details": "Pose controlnet for 14B I2V models with LoRA weights and pose embeds, includes improved dwpose detector with feet",
      "from": "Kijai"
    },
    {
      "update": "I2V model and new scheduler need new inference code",
      "details": "Current I2V runs but results are broken, no code released yet",
      "from": "Kijai"
    },
    {
      "update": "FreSca integration added to WanVideoWrapper",
      "details": "Now available through the wrapper interface",
      "from": "Kijai"
    },
    {
      "update": "SkyReels Wan version was released then taken down",
      "details": "Released today but quickly removed, will be rereleased later",
      "from": "Piblarg"
    },
    {
      "update": "Skycaptioner will be released after a week",
      "details": "Was accidentally made public before intended release",
      "from": "yi"
    },
    {
      "update": "DMD (step distillation) coming to SkyReels v2",
      "details": "Will reduce step count from 50 to 5 steps for faster generation",
      "from": "yi"
    },
    {
      "update": "SeedVR upscaler potentially releasing in August",
      "details": "Developer commented on issues saying hopefully they can get approval to release open by August",
      "from": "JohnDopamine"
    },
    {
      "update": "SkyReels V2 models released",
      "details": "All models now available on HuggingFace including Diffusion Forcing version for infinite length videos",
      "from": "yi"
    },
    {
      "update": "SkyReels V2 code released",
      "details": "GitHub repository now available with implementation",
      "from": "yi"
    },
    {
      "update": "SkyCaptioner V1 released",
      "details": "Video captioning model now available on HuggingFace",
      "from": "Persoon"
    },
    {
      "update": "SkyReels V2 models released",
      "details": "1.3B and 14B I2V models at 540P, plus Diffusion Forcing variants",
      "from": "multiple users"
    },
    {
      "update": "5B SkyReels model spotted",
      "details": "Larger variant appears to be in development",
      "from": "Draken"
    },
    {
      "update": "VACE compatibility with SkyReels DF confirmed",
      "details": "Successfully tested with 161 frames",
      "from": "DawnII"
    },
    {
      "update": "MAGI-1 24B model released",
      "details": "Fully autoregressive with V2V and I2V, 1440p VAE, 4.5B variant coming soon",
      "from": "yi"
    },
    {
      "update": "SkyReels V2 DF models available",
      "details": "1.3B and 14B variants with Diffusion Forcing, only 1.3B has fps embeds",
      "from": "Kijai"
    },
    {
      "update": "Chipmunk embedding caching tool",
      "details": "New tool for embed caching to improve performance",
      "from": "yi"
    },
    {
      "update": "Native VACE bad results fixed",
      "details": "Python whitespace issue resolved",
      "from": "comfy"
    },
    {
      "update": "VRAM management issue fixed",
      "details": "Expected tensors device error resolved in latest WanWrapper",
      "from": "Kijai"
    },
    {
      "update": "Sigmas input added to WanSampler",
      "details": "New feature in latest wrapper version for experimenting with OptimalSteps scheduler",
      "from": "Kijai"
    },
    {
      "update": "Phantom model released",
      "details": "Phantom-Wan 1.3B available, appears to be module to add to base Wan similar to VACE",
      "from": "DawnII"
    },
    {
      "update": "New quantization method for video models",
      "details": "Paper on SOTA diffusion model quantization that works on video models",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "update": "SkyReels V2 structured prompting revealed",
      "details": "Paper shows detailed JSON schema for structured video prompting with subjects, camera, environment",
      "from": "fredbliss"
    },
    {
      "update": "Phantom model now available with ComfyUI support",
      "details": "Phantom code pushed to dev branch for testing. It's a 1.3B parameter model that uses 10.6GB VRAM for 1280x768x81 generation.",
      "from": "Kijai"
    },
    {
      "update": "SkyReels V2 models and methodology released",
      "details": "New approach focusing on dynamics-only prompting for I2V, with structured caption fusion methodology",
      "from": "fredbliss"
    },
    {
      "update": "New SkyReels V2 models released",
      "details": "T2V and I2V variants with improved quality for humans",
      "from": "mamad8"
    },
    {
      "update": "DF (endless generation) model available",
      "details": "Allows extending videos by taking last 17 frames and generating new 97-frame clips",
      "from": "seitanism"
    },
    {
      "update": "Phantom model now available",
      "details": "1.3B model for subject reference, works like SkyReels A2 but smaller",
      "from": "DawnII"
    },
    {
      "update": "VACE + DF combination working",
      "details": "Can now use VACE with DF model for better continuation",
      "from": "mamad8"
    },
    {
      "update": "Phantom model available in wrapper dev branch",
      "details": "New character consistency model now integrated into WanVideoWrapper",
      "from": "Kijai"
    },
    {
      "update": "TeaCache support added to Skyworks models",
      "details": "Default values added for Skyworks models in teacache",
      "from": "jellybean5361"
    },
    {
      "update": "Fix pushed for clip model compatibility",
      "details": "Fixed compatibility issues with different clip model formats",
      "from": "Kijai"
    },
    {
      "update": "SkyReels teases Camera Director model",
      "details": "Coming Soon on their HuggingFace",
      "from": "seitanism"
    },
    {
      "update": "SkyReels released 6 models recently",
      "details": "Multiple new models in recent days",
      "from": "seitanism"
    },
    {
      "update": "Diffusion Pipe now supports SkyReels training",
      "details": "Added support for training likenesses on SkyReels models",
      "from": "JohnDopamine"
    },
    {
      "update": "Skyreels V2 models released with 720p support",
      "details": "New 720p I2V and DF models uploaded to HuggingFace, native 24fps output",
      "from": "jellybean5361"
    },
    {
      "update": "WAN going commercial",
      "details": "Wan announced commercialization, suggesting potential changes to open source availability",
      "from": "JohnDopamine"
    },
    {
      "update": "RealisDance-DiT and Uni3C coming",
      "details": "Two new WAN 2.1 utilities for controlling humans in videos, weights releasing same day",
      "from": "yi"
    },
    {
      "update": "New Fun Control Camera model released",
      "details": "Wan2.1-Fun-V1.1-1.3B-Control-Camera available on HuggingFace",
      "from": "DawnII"
    },
    {
      "update": "Sparge attention model available",
      "details": "There is now a sparge attention model for wan2.1 1.3B variant for memory optimization",
      "from": "yi"
    },
    {
      "update": "Wan team promises low VRAM version",
      "details": "Someone from Wan team said they'd add 'low VRAM version' with their optimizations mentioned in the paper",
      "from": "Kijai"
    },
    {
      "update": "Fun 1.1 models released with improved inpaint and control capabilities",
      "details": "Inpaint model trained with larger batch size for more stable performance, Control model includes reference image functionality",
      "from": "DawnII"
    },
    {
      "update": "14B models are now available",
      "details": "14B versions of the models are being released",
      "from": "DawnII"
    },
    {
      "update": "Kijai merged dev to main branch",
      "details": "Most things seem to work so dev branch was merged to main",
      "from": "Kijai"
    },
    {
      "update": "Step1X-Edit model released with MIT license",
      "details": "New editing model that seems good for editing first frame of video then using Wan to modify whole video",
      "from": "mamad8"
    },
    {
      "update": "ComfyUI frontend 1.17.11 reported as glitchy",
      "details": "Users experiencing issues with latest frontend version, recommendation to wait before updating",
      "from": "David Snow"
    },
    {
      "update": "Kijai updated node to fix frontend compatibility issues",
      "details": "Old node made no sense with frontend update and caused errors between workflows saved with different frontend versions",
      "from": "Kijai"
    },
    {
      "update": "Fix pushed for default rope dtype issues",
      "details": "Kijai fixed default rope option but recommends using comfy rope as it's more efficient",
      "from": "Kijai"
    },
    {
      "update": "Fun 1.1 models released with better performance",
      "details": "Includes reference image support",
      "from": "Kijai"
    },
    {
      "update": "FantasyTalking model released - Wan-based lip sync",
      "details": "Audio + image to video, 81 frames, uses 720p model as base",
      "from": "Kijai"
    },
    {
      "update": "CausVid model published - 4-step distillation from 50-step model",
      "details": "T2V model for faster inference, extends DMD to videos",
      "from": "multiple users"
    },
    {
      "update": "TransPixeler added Wan support",
      "details": "Available in branch on GitHub",
      "from": "JohnDopamine"
    },
    {
      "update": "Fantasy Talking model released",
      "details": "New lip sync model from Alibaba using 14B I2V, outputs 23fps, English only",
      "from": "Multiple users"
    },
    {
      "update": "Multiple character LoRAs released on Civitai",
      "details": "Large batch of anime character models for Wan T2V just released",
      "from": "Kytra"
    },
    {
      "update": "Wan2.1-Fun-V1.1-1.3B-Control-Camera model released",
      "details": "New camera control model for Wan, usage unclear",
      "from": "N0NSens"
    },
    {
      "update": "Fantasy Talking ComfyUI implementation available",
      "details": "Kijai implemented ComfyUI nodes within hours of model release",
      "from": "Multiple users"
    },
    {
      "update": "Fun Camera Control released",
      "details": "Compatible with AnimateDiff camera control poses, supports pan movements",
      "from": "Kijai"
    },
    {
      "update": "New camera control for 3D movements",
      "details": "Based on Google's RealEstate10K dataset, supports controllable camera paths",
      "from": "Kijai"
    }
  ],
  "workflows": [
    {
      "workflow": "Second-pass enhancement with 1.3B",
      "use_case": "Improving old Hunyuan/Wan generations using v2v with depth, highres, aesthetics loras at 6-8 steps",
      "from": "David Snow"
    },
    {
      "workflow": "Long video extension with InP model",
      "use_case": "Creating longer videos by extending with the InP model, though context windows better for long vid2vid",
      "from": "Kijai"
    },
    {
      "workflow": "VACE with depth video input",
      "use_case": "Using depth video as control signal for VACE model, works better than InP for subject consistency",
      "from": "Kijai"
    },
    {
      "workflow": "Video extension using VACE",
      "use_case": "Extending videos by masking empty gray frames with video content",
      "from": "Kijai"
    },
    {
      "workflow": "Reference image + depth control",
      "use_case": "Character consistency with pose control",
      "from": "multiple users"
    },
    {
      "workflow": "Outpainting with VACE",
      "use_case": "Expanding video canvas dimensions",
      "from": "Kijai"
    },
    {
      "workflow": "VACE style transfer with pose control",
      "use_case": "Consistent style transfer with pose guidance using DWPose preprocessor",
      "from": "ingi // SYSTMS"
    },
    {
      "workflow": "Double passing old Wan output with 1.3B",
      "use_case": "Improving quality of previous generations",
      "from": "David Snow"
    },
    {
      "workflow": "Context window processing for long videos",
      "use_case": "Processing videos longer than 81 frames by splitting into overlapping windows",
      "from": "Kijai"
    },
    {
      "workflow": "480p generation then V2V upscale to 720p",
      "use_case": "Quick upscaling - V2V upscale only needs 8-10 steps",
      "from": "zelgo_"
    },
    {
      "workflow": "Reference image face swapping with VACE",
      "use_case": "Subject replacement in videos using reference image and video input",
      "from": "Kijai"
    },
    {
      "workflow": "Loop generation for lofi girl style videos",
      "use_case": "Seamless looping videos with consistent style using custom LoRAs",
      "from": "Mint"
    },
    {
      "workflow": "Character lora + pose rig into VACE",
      "use_case": "Character consistency with controlled poses using text2video",
      "from": "Kytra"
    },
    {
      "workflow": "Reference image with proper masking for VACE",
      "use_case": "Placing subjects with background removed or padded images",
      "from": "Kijai"
    },
    {
      "workflow": "VACE with depth control for motion transfer",
      "use_case": "Applying motion from driving video to static images",
      "from": "seitanism"
    },
    {
      "workflow": "Mixed frame input for VACE",
      "use_case": "First frame as reference image, remaining frames as control input for motion guidance",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Outfit swap with gray masking",
      "use_case": "Composite gray over clothing areas in input frames, use reference image to guide replacement",
      "from": "Zuko"
    },
    {
      "workflow": "Blended depth and pose control",
      "use_case": "Overlay pose on top of depth maps for combined facial performance and body motion",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Start to End Frame with VACE masking",
      "use_case": "Creating controlled animations between specific start and end frames while preserving facial identity",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "Multi-frame keyframe workflow",
      "use_case": "Creating smooth transitions through multiple keyframes (1st-mid-end frames)",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "Combined depth and pose control",
      "use_case": "Using both depth maps and pose detection for enhanced motion control",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Context window long video generation",
      "use_case": "Generating very long videos (1300+ frames) using context windows",
      "from": "Kijai"
    },
    {
      "workflow": "Loop video creation",
      "use_case": "Creating seamless loop videos by using first frame as last frame",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "VACE with reference and prompt",
      "use_case": "Character consistency in video generation using reference image with text prompts",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Bounding box control for T2V/I2V",
      "use_case": "Controlling specific regions in video generation, works better with I2V than T2V",
      "from": "Kijai"
    },
    {
      "workflow": "Combined depth/normals/lineart preprocessing",
      "use_case": "Enhanced detail in video generation by combining multiple depth estimation methods",
      "from": "David Snow"
    },
    {
      "workflow": "Multiple control inputs using prev_embeds",
      "use_case": "Use pose for half the steps, depth for the rest, or both at same time with reduced strength",
      "from": "Kijai"
    },
    {
      "workflow": "Separate VACE module loading",
      "use_case": "Load base 1.3B and VACE separately for more flexibility, works with DG models",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Depth + reference with prev_embeds",
      "use_case": "First encode 0-0.2 with depth map and ref, second encode 0.2-1 with only ref image",
      "from": "zelgo_"
    },
    {
      "workflow": "First frame extraction and restyling pipeline",
      "use_case": "Grab first frame, restyle with Stable Diffusion, use as reference for video restyling - similar to Runway restyle",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "3D model animation to ComfyUI pipeline",
      "use_case": "Animate 3D model with .anims in Unity, export video on black background, import to ComfyUI for processing",
      "from": "Kytra"
    },
    {
      "workflow": "VACE inpainting with start frame + trajectory + bbox + reference",
      "use_case": "Character replacement and object manipulation in videos",
      "from": "Kijai"
    },
    {
      "workflow": "SkyreelsA2 reference workflow",
      "use_case": "Using reference images with 14B model for better I2V results",
      "from": "Kijai"
    },
    {
      "workflow": "Bbox inpainting with yo9o method",
      "use_case": "Custom inpainting workflow developed for targeted object modification",
      "from": "yo9o"
    },
    {
      "workflow": "3D cube to video generation",
      "use_case": "Converting 3D animated shapes to realistic video using reference images",
      "from": "ingi // SYSTMS"
    },
    {
      "workflow": "Two-step character replacement",
      "use_case": "Generate clean background first, then character with proper lighting, composite together",
      "from": "traxxas25"
    },
    {
      "workflow": "VACE temporal inpainting for frame extension",
      "use_case": "Keep certain frames with black mask, generate new frames in white mask areas",
      "from": "Kijai"
    },
    {
      "workflow": "Context windows for long video generation",
      "use_case": "Generate longer videos by splitting into overlapping chunks",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Automated subject removal with Gemini and segmentation",
      "use_case": "Remove subjects and objects automatically for clean plates",
      "from": "yo9o"
    },
    {
      "workflow": "VACE video extension with 4-frame overlap",
      "use_case": "Creating longer videos by extending 33-frame chunks with 4-frame overlap, allows new prompts and control adjustments per segment",
      "from": "Hashu"
    },
    {
      "workflow": "Depth and pose control blending",
      "use_case": "Using separate VACE encode nodes for depth and pose with reduced strength or step switching",
      "from": "yo9o"
    },
    {
      "workflow": "Subject removal with VACE",
      "use_case": "Removing people/objects from video using proper masking techniques",
      "from": "ArtOfficial"
    },
    {
      "workflow": "Two-step refinement: 1.3B initial generation then 14B refinement",
      "use_case": "Getting quality while managing speed - generate with 1.3B then refine with 14B at low denoise",
      "from": "Piblarg"
    },
    {
      "workflow": "Character mask workflow for clean backgrounds",
      "use_case": "Generate character transformation then composite back to avoid background artifacts",
      "from": "David Snow"
    },
    {
      "workflow": "Multi-frame VACE with intermediate frames",
      "use_case": "Create video in editor with white frames between keyframes for more control points",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Infinite video generation with VACE",
      "use_case": "Create long videos by keeping last 4 frames of each render as beginning frames of next batch",
      "from": "JmySff"
    },
    {
      "workflow": "VACE inpainting for watermark removal",
      "use_case": "Remove watermarks by compositing mask to input and using reference image",
      "from": "Alisson Pereira"
    },
    {
      "workflow": "VACE outpainting",
      "use_case": "Expand video frames in any direction - very simple setup",
      "from": "Kijai"
    },
    {
      "workflow": "Triple preprocessor setup for control",
      "use_case": "Using video-depth-anything, lotus normals, anyline lineart for comprehensive control",
      "from": "David Snow"
    },
    {
      "workflow": "VACE with dual encode nodes for different strengths",
      "use_case": "Using 2 different VACE encode nodes with prev_embeds for blending lineart and depth",
      "from": "Hashu"
    },
    {
      "workflow": "5-extension workflow for long videos",
      "use_case": "81 frame chunks extended 5 times, 30 minutes total generation time",
      "from": "Hashu"
    },
    {
      "workflow": "Upscale/refine workflow with reward LoRAs",
      "use_case": "Using 1.3b-control with reward LoRAs for upscaling",
      "from": "HeadOfOliver"
    },
    {
      "workflow": "VACE infinite loop using frame extension and Flux",
      "use_case": "Mix frame extension with i2i of last frame through Flux to use as ref in VACE in a loop",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Character replacement with VACE",
      "use_case": "White padding on reference, openpose for movement, comp background with mask, optional vid2vid with low denoise",
      "from": "traxxas25"
    },
    {
      "workflow": "Using conditioning combine nodes for better results",
      "use_case": "Runs multiple predictions and combines them, takes longer but better results than simply mixing controlnet images",
      "from": "JmySff"
    },
    {
      "workflow": "VACE subject removal",
      "use_case": "Remove any subject (cars, birds, people) from videos using BBOX detection",
      "from": "ArtOfficial"
    },
    {
      "workflow": "Multi-pass refinement",
      "use_case": "First pass VACE generation, then vanilla 1.3B with enhancement LoRAs for 8 steps to improve quality",
      "from": "David Snow"
    },
    {
      "workflow": "V2V with VACE start frame and control",
      "use_case": "Video-to-video with around 0.8 denoise, start image, depth and lineart control",
      "from": "David Snow"
    },
    {
      "workflow": "VACE with DWpose face only",
      "use_case": "Character animation with lip sync using reference image and grey background masking",
      "from": "IllumiReptilien"
    },
    {
      "workflow": "Two-pass VACE workflow",
      "use_case": "60 steps VACE with pose + ref frame, then 8 steps second pass",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Crop and paste back workflow",
      "use_case": "Cropping character at full resolution, processing through VACE, then pasting back in place",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Context window looping",
      "use_case": "169 frames, uniform looped context for better loops but slower processing",
      "from": "Kijai"
    },
    {
      "workflow": "Multiple VACE controls",
      "use_case": "Using canny and depth simultaneously by chaining VACE encodes",
      "from": "StableVibrations"
    },
    {
      "workflow": "Object replacement with masking",
      "use_case": "Replacing objects in videos using input masks - black preserves original, white generates new",
      "from": "IllumiReptilien"
    },
    {
      "workflow": "Two-pass style processing",
      "use_case": "First pass without enhancement loras, second pass adds them to avoid realism bias",
      "from": "Hashu"
    },
    {
      "workflow": "Looping with AnimateDiff upscaler",
      "use_case": "Running flashing loops through AnimateDiff upscaler to make more seamless and increase color contrast",
      "from": "Jas"
    },
    {
      "workflow": "VACE with reference image for style transfer",
      "use_case": "Getting specific art styles that are difficult to achieve with LoRAs alone",
      "from": "David Snow"
    },
    {
      "workflow": "Two-stage VACE generation",
      "use_case": "Save latents in first stage, decode in second stage to avoid reference image being merged into first frames",
      "from": "Johnjohn7855"
    },
    {
      "workflow": "Multi-pass generation for camera movement",
      "use_case": "Generate video in first pass, then use Recam in second pass for single image input",
      "from": "David Snow"
    },
    {
      "workflow": "VACE with start/end frames and control preprocessor",
      "use_case": "Direct i2v with control frames for stylization while maintaining control",
      "from": "David Snow"
    },
    {
      "workflow": "Crop and paste system for distant characters",
      "use_case": "Running distant character at full resolution by cropping subject with masking then pasting back to source video location",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Fun control for vid2vid",
      "use_case": "Works well for video to video generation",
      "from": "boorayjenkins"
    },
    {
      "workflow": "Mixing depth input with sporadic lineart frames",
      "use_case": "Give helper frames on ones model doesn't read well - works better than putting actual reference frames",
      "from": "TimHannan"
    },
    {
      "workflow": "Looping video generation with last frame continuation",
      "use_case": "Creating extended video sequences by feeding last frame of previous generation as input to next",
      "from": "AJO"
    },
    {
      "workflow": "Reference + control for first gen, then last frame + reference + control for subsequent",
      "use_case": "Maintaining consistency across multiple video generations",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Triple control setup with depth, normal, and reference",
      "use_case": "Maximum temporal consistency and detail preservation",
      "from": "David Snow"
    },
    {
      "workflow": "3x RIFE interpolation then drop every other frame",
      "use_case": "Converting 16fps Wan output to 24fps for film",
      "from": "Kijai"
    },
    {
      "workflow": "Multiple clip generation with frame dropping",
      "use_case": "Create 3-5 clips in one workflow, drop duplicate frames, interpolate 3x with RIFE",
      "from": "seitanism"
    },
    {
      "workflow": "Seamless loop creation",
      "use_case": "Best system for creating seamless video loops with color matching",
      "from": "gshawn"
    },
    {
      "workflow": "TheDirector custom node",
      "use_case": "Create 2-24 scene wan generation in one go with consistent character images",
      "from": "AJO"
    },
    {
      "workflow": "First-last frame generation with FLF2V model",
      "use_case": "Generate video between two specific frames with better coherence",
      "from": "Kijai"
    },
    {
      "workflow": "Combining depth and normals for control",
      "use_case": "Better stability in controlled video generation",
      "from": "David Snow"
    },
    {
      "workflow": "Multiple LoRA mixing with distilled 1.3B",
      "use_case": "Fast rendering: Ghibli LoRA + ex LoRA + depth control, 8 steps, CFG 1.5, 161 frames in 1.11 minutes",
      "from": "Flipping Sigmas"
    },
    {
      "workflow": "VACE arbitrary frame interpolation",
      "use_case": "True interpolation by placing input frames on every other frame, having VACE interpolate missing ones",
      "from": "Screeb"
    },
    {
      "workflow": "Perspective change with VACE + DWPose",
      "use_case": "Use Blender to move camera with dwpose, then img2vid with dwpose control for perspective changes",
      "from": "hablaba"
    },
    {
      "workflow": "VACE temporal masking with first frame",
      "use_case": "Using stylized first frame as keyframe with control frames for rest of sequence",
      "from": "Kijai"
    },
    {
      "workflow": "Daisy chaining VACE runs for long sequences",
      "use_case": "Extract last frame from first run, use as start for next run with shifted control video",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Fun Control with multiple control nets",
      "use_case": "Combining depth + pose + lineart for better face control",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Two-pass VACE processing with stitching",
      "details": "Run 81 frames with DWPose + ref image, extract frame 81, run new 81 frames starting from that frame, stitch together with batch combine node",
      "use_case": "Creating longer videos while maintaining character consistency",
      "from": "A.I.Warper"
    },
    {
      "workflow": "VACE with first pass and FUN cleanup",
      "details": "Use VACE for initial generation, then do second pass with FUN to clean up results",
      "use_case": "Faster processing while maintaining quality",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Flux i2i + LoRAs + Redux for first frame restyling",
      "details": "Use Flux with i2i, LoRAs and Redux for restyling the first frame before video generation",
      "use_case": "Preparing stylized starting frames",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Use original video as latents with mask for selective face preservation",
      "use_case": "Keep face from start image while using original video for the rest",
      "from": "DeZoomer"
    },
    {
      "workflow": "Style transfer using V2V low denoise with DG models plus ExVideo LoRA",
      "use_case": "Simple style transfer for long videos, used for 1000 frame generations",
      "from": "Flipping Sigmas"
    },
    {
      "workflow": "Redux for image restyling with 0.6-0.8 denoise + LoRA",
      "use_case": "Image-to-image style transfer and character modification",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Wan 1.3B upscaling with controlnets",
      "use_case": "Video upscaling while preserving structure using depth and lineart guidance",
      "from": "David Snow"
    },
    {
      "workflow": "VACE ref2vid for restyling",
      "use_case": "Style transfer on videos with depth and line art controls",
      "from": "sneako1234"
    },
    {
      "workflow": "VACE v2v with 5 LoRAs and prompting",
      "use_case": "High-quality video transformation at 1280x544 ultrawide resolution with denoise at 0.8",
      "from": "David Snow"
    },
    {
      "workflow": "Generate at 480p then upscale to 720p",
      "use_case": "Using 14B for animation then upscaling with ultrasharp",
      "from": "PirateWolf"
    },
    {
      "workflow": "Using UniAnimate for pose-driven animation",
      "use_case": "1GB LoRA weights with pose embeds, works with both 14B and 720p models",
      "from": "Kijai"
    },
    {
      "workflow": "UniAnimate with basic 14B I2V",
      "use_case": "Pose-driven animation using dwpose detection",
      "from": "Kijai"
    },
    {
      "workflow": "VACE with background removal",
      "use_case": "Style transfer while preserving green screen background",
      "from": "David Snow"
    },
    {
      "workflow": "Context window for video extension",
      "use_case": "Extending video length with Wan 14B",
      "from": "hablaba"
    },
    {
      "workflow": "Second pass enhancement",
      "use_case": "Improving VACE oversaturated outputs",
      "from": "Gavmakes"
    },
    {
      "workflow": "FLF morphing with camera movement prompts",
      "use_case": "Creating smooth transitions between different frames",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "workflow": "Video extension with Fun inp for 14B",
      "use_case": "Extending videos when VACE not available for 14B",
      "from": "Kijai"
    },
    {
      "workflow": "VACE with SkyReels DF for long generation",
      "use_case": "161 frame generation with control",
      "from": "DawnII"
    },
    {
      "workflow": "LTX + 1.3B cleanup pass",
      "use_case": "Use LTX for initial generation, then cleanup with 1.3B using depth/normal/line controls and Florence2 prompts",
      "from": "David Snow"
    },
    {
      "workflow": "DF + VACE combination",
      "use_case": "Combine Diffusion Forcing with VACE controls for extended video generation with style control",
      "from": "Kijai"
    },
    {
      "workflow": "Chained DF nodes for infinite video",
      "use_case": "Chain DF sampler nodes with decode/encode between each to continue video indefinitely",
      "from": "Kijai"
    },
    {
      "workflow": "Two-pass video generation",
      "use_case": "Higher quality output by feeding sampler output into another sampler",
      "from": "lostintranslation"
    },
    {
      "workflow": "Face restoration workflow",
      "use_case": "Crop, inpaint and stitch workflow targeting faces using CropAndStitch nodes",
      "from": "David Snow"
    },
    {
      "workflow": "VACE upscaling workflow",
      "use_case": "Using VACE for upscaling instead of traditional methods",
      "from": "Cseti"
    },
    {
      "workflow": "Multi-stage sampling with sigma split",
      "use_case": "Better quality and faster generation by splitting steps into two parts with different CFG",
      "from": "lostintranslation"
    },
    {
      "workflow": "VACE upscale workflow",
      "use_case": "Upscaling videos using VACE with reference images",
      "from": "Cseti"
    },
    {
      "workflow": "480p to 720p upscaling",
      "use_case": "Video resolution upscaling workflow",
      "from": "Cseti"
    },
    {
      "workflow": "SkyReels V2 I2V with dynamics-only prompting",
      "use_case": "Generate videos from images using only motion/action descriptions, letting the image provide static visual context",
      "from": "fredbliss"
    },
    {
      "workflow": "Multi-reference Phantom workflow",
      "use_case": "Generate videos with multiple reference images (up to 4) for consistent character/object generation",
      "from": "Kijai"
    },
    {
      "workflow": "CFG scheduling in single sampler",
      "use_case": "Replace two-pass workflows with single sampler using scheduled CFG values per step for speed optimization",
      "from": "TK_999"
    },
    {
      "workflow": "Multi-viewpoint Phantom generation",
      "use_case": "Improving character fidelity by using 4 different viewpoints as separate latents",
      "from": "mamad8"
    },
    {
      "workflow": "DF endless generation",
      "use_case": "Take last 17 frames of clip, extend to 97 frames, stitch together removing first 17 frames of new clip for smooth transitions",
      "from": "seitanism"
    },
    {
      "workflow": "VACE + DF combination",
      "use_case": "Using VACE with DF for temporal inpainting and better video continuation",
      "from": "mamad8"
    },
    {
      "workflow": "Infinite video generation with multiple samplers",
      "use_case": "Creating long seamless video sequences by chaining 4-second clips with context overlap",
      "from": "seitanism"
    },
    {
      "workflow": "iClone animation to SDXL to Wan Fun with depth",
      "use_case": "Using 3D animation as base for AI video generation with depth control",
      "from": "boorayjenkins"
    },
    {
      "workflow": "Color correction for extended videos",
      "use_case": "Maintaining consistent brightness across chained video segments",
      "from": "MilesCorban"
    },
    {
      "workflow": "DF extension with overlap frames",
      "use_case": "Creating long videos (30 seconds to 2+ minutes) without visible seams, allows prompt traveling and different LoRAs per section",
      "from": "seitanism"
    },
    {
      "workflow": "81 frame gen with -27 frame reference for 97 frame extension",
      "use_case": "Each loop generates 70 new frames (97 max - 27 overlap)",
      "from": "boorayjenkins"
    },
    {
      "workflow": "Unsample/resample degraded segments",
      "use_case": "Improving quality of individual clips when degradation occurs",
      "from": "Ablejones"
    },
    {
      "workflow": "Dreambooth training with Skyreels T2V",
      "use_case": "Human likeness training with simple token/class captions like 'ohwx man' - works well with just 16 images in about an hour at 1e-4",
      "from": "JohnDopamine"
    },
    {
      "workflow": "Diffusion Forcing for video extension",
      "use_case": "Autoregressive model that can continue videos endlessly from any number of frames, even just 1 frame",
      "from": "Kijai"
    },
    {
      "workflow": "VACE native ComfyUI workflow",
      "use_case": "Simple workflow for VACE controlnet using native nodes, adapted from wan fun workflow",
      "from": "V\u00e9role"
    },
    {
      "workflow": "FramePacking for 4n+1 frames",
      "use_case": "Getting proper frame ranges for Wan by padding image batch to X number of images by duplicating last frame",
      "from": "Rishi Pandey"
    },
    {
      "workflow": "Edit first frame then use Wan for whole video",
      "use_case": "Use Step1X-Edit to modify first frame of video, then use Wan to apply changes to entire video",
      "from": "mamad8"
    },
    {
      "workflow": "First and last frame modification with FLF2V",
      "use_case": "Modify both first and last frame of video using in-context technique, then use FLF2V",
      "from": "mamad8"
    },
    {
      "workflow": "Fun Control with reference image only",
      "use_case": "Using Fun 1.1 control model with reference image input for VACE-like functionality",
      "from": "Kijai"
    },
    {
      "workflow": "VACE native with auto masking using segment anything",
      "use_case": "Automated masking for VACE operations",
      "from": "V\u00e9role"
    },
    {
      "workflow": "Florence2 + Groq LLM for i2v prompting",
      "use_case": "Florence2 for basic image details, fed into Groq for descriptive video prompts",
      "from": "David Snow"
    },
    {
      "workflow": "Multi-stage DF workflow for long videos",
      "use_case": "Takes image/video input, guides 33-frame clip, feeds last 4-17 frames back for next stage",
      "from": "MilesCorban"
    },
    {
      "workflow": "Using DG Fun models for i2v with custom nodes",
      "use_case": "Start/end image workflows using native nodes",
      "from": "David Snow"
    },
    {
      "workflow": "T2V/I2V fed into DF for extension",
      "use_case": "Generate short video then extend with DF, works well with LoRAs",
      "from": "anever"
    },
    {
      "workflow": "I2V + DF extension with last frame as first frame of FLF2V",
      "use_case": "Looping video creation",
      "from": "daking999"
    },
    {
      "workflow": "Fantasy Talking lip sync",
      "use_case": "Adding realistic lip sync to static images with audio input",
      "from": "Multiple users"
    },
    {
      "workflow": "Two-stage Fantasy Talking refinement",
      "use_case": "Use Fantasy Talking as base, then run through Wan again with pose processor for better quality",
      "from": "ingi // SYSTMS"
    },
    {
      "workflow": "VACE with composited controls",
      "use_case": "Combine multiple control inputs (depth, pose, outlines) by compositing images rather than using multiple nodes",
      "from": "Kijai"
    },
    {
      "workflow": "VACE multi-control setup",
      "use_case": "Use two VACE nodes connected via prev_vace_embeds for multiple controls",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "workflow": "Fantasy Talking with variable CFG",
      "use_case": "Audio CFG 5, sampler CFG 5, works only in English, 81 frames max",
      "from": "Stad"
    },
    {
      "workflow": "Points editor for video masking",
      "use_case": "Select mask points on single frame, then process full video",
      "from": "David Snow"
    },
    {
      "workflow": "Batch I2V generation with multiple variants per image",
      "use_case": "Generate three videos per image overnight by duplicating custom sampler three times instead of using batch size",
      "from": "David Snow"
    },
    {
      "workflow": "Custom camera control using Blender paths",
      "use_case": "Creating complex camera movements by designing paths in Blender and importing to Wan",
      "from": "Kijai"
    }
  ],
  "settings": [
    {
      "setting": "SLG blocks",
      "value": "9 or 10",
      "reason": "Other blocks performed worse in tests",
      "from": "ezMan"
    },
    {
      "setting": "SLG range",
      "value": "0.2/0.8",
      "reason": "Recommended start and end values",
      "from": "ezMan"
    },
    {
      "setting": "CFG for SLG",
      "value": "4",
      "reason": "Avoids weird rainbow prismatic color weirdness, 3 is not enough",
      "from": "ezMan"
    },
    {
      "setting": "TeaCache threshold",
      "value": "0.2",
      "reason": "Works better with 20 steps, skips 6 conditional steps vs 8 with higher threshold",
      "from": "Kijai"
    },
    {
      "setting": "CFG for v2v enhancement",
      "value": "2",
      "reason": "Used with euler/beta scheduler for second-pass improvements",
      "from": "David Snow"
    },
    {
      "setting": "Denoise strength",
      "value": "0.8",
      "reason": "For second-pass v2v to retain more detail from original",
      "from": "David Snow"
    },
    {
      "setting": "Steps for second-pass",
      "value": "6-8",
      "reason": "Low step count effective for improving old generations",
      "from": "David Snow"
    },
    {
      "setting": "Zero_init for I2V",
      "value": "not recommended",
      "reason": "Zero_init not recommended for I2V according to community consensus",
      "from": "ameasure"
    },
    {
      "setting": "VACE Encode strength",
      "value": "0.4",
      "reason": "Maintains control while allowing creativity",
      "from": "Cseti"
    },
    {
      "setting": "Base precision",
      "value": "fp16 or bf16",
      "reason": "Optimal performance",
      "from": "zelgo_"
    },
    {
      "setting": "Quantization",
      "value": "fp8_e4m3fn",
      "reason": "Memory efficiency",
      "from": "zelgo_"
    },
    {
      "setting": "Gray frame value for extension",
      "value": "0.5",
      "reason": "Works best for empty frames in video extension",
      "from": "Kijai"
    },
    {
      "setting": "TeaCache threshold for VACE",
      "value": "0.1",
      "reason": "Standard 0.015 doesn't work well with VACE",
      "from": "Kijai"
    },
    {
      "setting": "Steps for VACE",
      "value": "10 steps",
      "reason": "Good balance of speed vs quality",
      "from": "VK (5080 128gb)"
    },
    {
      "setting": "SLG for VACE",
      "value": "Block 8, start 0.1, end 0.3-0.5",
      "reason": "Prevents oversaturation issues",
      "from": "IllumiReptilien"
    },
    {
      "setting": "DensePose strength",
      "value": "1",
      "reason": "Works well at full strength",
      "from": "VK (5080 128gb)"
    },
    {
      "setting": "Frame limit",
      "value": "81 frames",
      "reason": "Model can only do up to 81 frames properly by default",
      "from": "Kijai"
    },
    {
      "setting": "Block swap for 12GB VRAM",
      "value": "27+ for VACE workflows",
      "reason": "Prevents OOM errors",
      "from": "Ashtar"
    },
    {
      "setting": "V2V upscale steps",
      "value": "8-10 steps",
      "reason": "Sufficient for upscaling and quite quick",
      "from": "zelgo_"
    },
    {
      "setting": "SLG parameters for VACE",
      "value": "SLG 8, 0.1 start, 0.3 end",
      "reason": "Helps with image arrangement",
      "from": "IllumiReptilien"
    },
    {
      "setting": "Frame limit for default Wan",
      "value": "81 frames maximum",
      "reason": "Beyond 81 frames it loops",
      "from": "zelgo_"
    },
    {
      "setting": "VACE strength",
      "value": "1.5",
      "reason": "Really boosts the reference for better results",
      "from": "Kijai"
    },
    {
      "setting": "TeaCache with VACE",
      "value": "e with 0.1 threshold",
      "reason": "Skips about 30% of the steps",
      "from": "Kijai"
    },
    {
      "setting": "Model precision for 1.3B VACE",
      "value": "fp32 base with VACE blocks in bf16",
      "reason": "1.3B model is in fp32 but VACE blocks are bf16, using fp16 is a bit lossy",
      "from": "Kijai"
    },
    {
      "setting": "Frame count for VACE",
      "value": "81 frames",
      "reason": "More than 81 frames adds artifacts, at 81 it's crystal clear",
      "from": "seitanism"
    },
    {
      "setting": "Context frames and overlap",
      "value": "context_frames 81 & default context_overlap 16",
      "reason": "Default recommended settings",
      "from": "Kijai"
    },
    {
      "setting": "CFG for DiffSynth models",
      "value": "1-3",
      "reason": "Prevents overbaked output, light_v1 variant works best",
      "from": "BondoMan"
    },
    {
      "setting": "SLG parameters",
      "value": "8/0.2/0.8",
      "reason": "Works fine with WanLoraBlock8 set to false",
      "from": "BondoMan"
    },
    {
      "setting": "Shift value for DiffSynth",
      "value": "11+",
      "reason": "Required for proper prompt following with complex motions",
      "from": "BondoMan"
    },
    {
      "setting": "Context size for testing",
      "value": "81 frames with 16 overlap",
      "reason": "Recommended starting point for context window testing",
      "from": "Kijai"
    },
    {
      "setting": "VACE block to swap",
      "value": "15",
      "reason": "Helps manage VRAM usage on 12GB cards",
      "from": "Ashtar"
    },
    {
      "setting": "Normal block swap",
      "value": "30",
      "reason": "Additional VRAM management for larger frame counts",
      "from": "Kijai"
    },
    {
      "setting": "TinkerWAN Alpha lora strength",
      "value": "Positive values (v1.0)",
      "reason": "Negative values obliterate videos with new version",
      "from": "David Snow"
    },
    {
      "setting": "Overlap parameter",
      "value": "5 or higher",
      "reason": "Even small overlap of 5 doesn't significantly affect speed, so higher overlap can be used",
      "from": "A.I.Warper"
    },
    {
      "setting": "VACE blocks",
      "value": "1-15 (more than 0)",
      "reason": "Required when using block swap to avoid errors",
      "from": "Kijai"
    },
    {
      "setting": "Context options",
      "value": "81/16/32",
      "reason": "Sweet spot settings mentioned by experienced user",
      "from": "David Snow"
    },
    {
      "setting": "CFG for DG models",
      "value": "2.0",
      "reason": "DG models are CFG distilled, lower values kill quality",
      "from": "Kijai"
    },
    {
      "setting": "Hi-res LoRA strength",
      "value": "1.0",
      "reason": "Recommended strength when using hi-res LoRA with higher resolutions",
      "from": "VK (5080 128gb)"
    },
    {
      "setting": "DG models",
      "value": "4 steps, 1 CFG",
      "reason": "Works much better at low step counts",
      "from": "Hashu"
    },
    {
      "setting": "VACE depth control",
      "value": "0.5 strength, 0.75 end percent",
      "reason": "Good balance for depth control",
      "from": "Hashu"
    },
    {
      "setting": "Reference strength",
      "value": "Can use values higher than 1.0",
      "reason": "Sometimes helps with likeness",
      "from": "Hashu"
    },
    {
      "setting": "Prev_embeds early steps",
      "value": "Use lower percentages like 0.0-0.2",
      "reason": "Early steps are much stronger, 50/50 split won't do much",
      "from": "Kijai"
    },
    {
      "setting": "DG V4 High CFG",
      "value": "1.0",
      "reason": "Recommended setting for DG V4 High model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "SLG blocks and strength",
      "value": "8 blocks, 0.10 to 0.30",
      "reason": "User's VACE settings though results were disappointing",
      "from": "PirateWolf"
    },
    {
      "setting": "TeaCache threshold",
      "value": "0.1-0.25",
      "reason": "Lower values needed for low step counts, 0.25 works with 22 samples",
      "from": "zelgo_"
    },
    {
      "setting": "VACE embed strength",
      "value": "slightly lower than 0.925",
      "reason": "Better results than default 0.925",
      "from": "Kytra"
    },
    {
      "setting": "TeaCache threshold",
      "value": "30% skipped steps",
      "reason": "Good balance of quality and speed, more than 50% usually bad quality",
      "from": "Kijai"
    },
    {
      "setting": "VACE shift values",
      "value": "Double digit values, up to 35",
      "reason": "Better interpolation results for first/last frame workflows",
      "from": "DawnII"
    },
    {
      "setting": "Control network strength",
      "value": "Lower values",
      "reason": "To preserve character consistency when using controls",
      "from": "Draken"
    },
    {
      "setting": "Resolution for VACE",
      "value": "960x480x81 or less frames, 848x480 also works",
      "reason": "Recommended by experienced users",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Aspect ratio for Wan",
      "value": "856x480",
      "reason": "Best results",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Depth map preprocessing",
      "value": "Add 3 pixel blur to Blender depth maps",
      "reason": "Sharp depth maps don't work with VACE",
      "from": "Kijai"
    },
    {
      "setting": "VACE depth strength",
      "value": "0.4",
      "reason": "0.3 is too loose, 0.4 works better for depth control",
      "from": "Kijai"
    },
    {
      "setting": "Combined control strength",
      "value": "0.5 or less on each",
      "reason": "Full strength on multiple controls won't work, need to reduce strength when combining",
      "from": "Kijai"
    },
    {
      "setting": "Image dimensions",
      "value": "Divisible by 16",
      "reason": "Required to avoid broadcasting errors",
      "from": "traxxas25"
    },
    {
      "setting": "Denoise for vid2vid refinement",
      "value": "Above 0.5",
      "reason": "Prevents flashing and artifacts",
      "from": "HeadOfOliver"
    },
    {
      "setting": "Vid2vid denoise range",
      "value": "0.4-0.7",
      "reason": "Depends on resolution and video length",
      "from": "Piblarg"
    },
    {
      "setting": "VACE strength",
      "value": "0.5",
      "reason": "Controls both control and reference strength",
      "from": "Kijai"
    },
    {
      "setting": "Training learning rate",
      "value": "5e-5 for dim 128, 5e-4 for dim 32",
      "reason": "Optimal learning rates for different network dimensions",
      "from": "Benjimon"
    },
    {
      "setting": "Training batch size",
      "value": "1",
      "reason": "Unless using multi-GPU setup",
      "from": "Benjimon"
    },
    {
      "setting": "HPS LoRA strength",
      "value": "0.5",
      "reason": "Recommended by benchmark tests",
      "from": "DawnII"
    },
    {
      "setting": "MPS LoRA strength",
      "value": "0.7",
      "reason": "Recommended by benchmark tests",
      "from": "DawnII"
    },
    {
      "setting": "VACE frame formula",
      "value": "(4*x)+1",
      "reason": "VAE encodes 4 frames to one latent except for the first",
      "from": "Piblarg"
    },
    {
      "setting": "Maximum stable frames for I2V",
      "value": "81",
      "reason": "Standard limit for stable generation",
      "from": "DawnII"
    },
    {
      "setting": "CFG",
      "value": "1",
      "reason": "Works well with VACE medium V2 and reward LoRAs",
      "from": "Hashu"
    },
    {
      "setting": "Steps",
      "value": "10",
      "reason": "Sufficient for VACE with reward LoRAs",
      "from": "Hashu"
    },
    {
      "setting": "DiffSynth reward LoRA strength",
      "value": "0.4-0.5",
      "reason": "Sweet spot, above 0.5 causes artifacts even at CFG 1",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "setting": "14B model settings",
      "value": "Steps 25, CFG 6, shift 11, denoise 0.65, LoRA 1.12",
      "reason": "Good results with Ghibli LoRA",
      "from": "Flipping Sigmas"
    },
    {
      "setting": "Tiled encoding",
      "value": "Enable for 8GB VRAM or less",
      "reason": "Encoding seems heavier than decoding",
      "from": "Kijai"
    },
    {
      "setting": "VACE strength for frame extension",
      "value": "Lower than default",
      "reason": "Prevents stutter when using first frame as reference",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "CFG for distilled model",
      "value": "1.0",
      "reason": "Required setting when using CFG distilled LoRA",
      "from": "crinklypaper"
    },
    {
      "setting": "LoRA strength for distilled model",
      "value": "1.0",
      "reason": "Standard strength for CFG distilled LoRA",
      "from": "crinklypaper"
    },
    {
      "setting": "Riflex default value",
      "value": "6",
      "reason": "Allows for new frames to be generated after without looping",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "setting": "Shift parameter",
      "value": "3 or 7",
      "reason": "Lower values provide more motion, 5 is more stable",
      "from": "Benjimon"
    },
    {
      "setting": "Denoise for v2v",
      "value": "0.8",
      "reason": "Standard denoise level for video-to-video workflows",
      "from": "Kijai"
    },
    {
      "setting": "Steps for second pass refinement",
      "value": "8 steps",
      "reason": "Sufficient for refinement pass with enhancement LoRAs",
      "from": "David Snow"
    },
    {
      "setting": "Empty frame level with LoRA stack",
      "value": "1.0",
      "reason": "1 = white seems to work best in testing",
      "from": "The Shadow (NYC)"
    },
    {
      "setting": "VACE steps for quality",
      "value": "20+ steps",
      "reason": "Higher steps might help with ghosting artifacts",
      "from": "ArtOfficial"
    },
    {
      "setting": "Control lora strength",
      "value": "1.0 (not 0.55)",
      "reason": "Workflow accidentally had wrong setting",
      "from": "David Snow"
    },
    {
      "setting": "VACE grey background",
      "value": "50% grey with normal blend mode at 50%",
      "reason": "For masking in VACE workflows",
      "from": "IllumiReptilien"
    },
    {
      "setting": "Reward lora strength",
      "value": "0.5-0.7",
      "reason": "Higher strengths introduce fireflies",
      "from": "Kijai"
    },
    {
      "setting": "Shift skip for looping",
      "value": "12 for 81 frames",
      "reason": "Should be half of the latents",
      "from": "Kijai"
    },
    {
      "setting": "Loop generation",
      "value": "77 frames with slight jump",
      "reason": "Current method has issues, context window method better but slower",
      "from": "Jas"
    },
    {
      "setting": "Extra noise detail LoRA",
      "value": "0.2 recommended, max 0.35",
      "reason": "Higher values just add noise without benefit",
      "from": "David Snow"
    },
    {
      "setting": "Depth strength for 2D style",
      "value": "0.55 with end at 0.4",
      "reason": "Prevents too much 3D drift in 2D styles",
      "from": "StableVibrations"
    },
    {
      "setting": "Pose/ref combo strength",
      "value": "0.9 strength full blast",
      "reason": "For maintaining character consistency",
      "from": "StableVibrations"
    },
    {
      "setting": "New boost LoRAs",
      "value": "4-6 steps",
      "reason": "More aggressive models that require fewer steps",
      "from": "David Snow"
    },
    {
      "setting": "CFG",
      "value": "2",
      "reason": "Works well with DG_Boost models",
      "from": "David Snow"
    },
    {
      "setting": "CFG with Skip Layer Guidance",
      "value": "3",
      "reason": "Reduce CFG by half when using SLG to avoid blue bar artifact",
      "from": "N0NSens"
    },
    {
      "setting": "Skip Layer Guidance blocks",
      "value": "8 or (8,9)",
      "reason": "Avoid setting blocks to 10 on 14B models to prevent blue bar",
      "from": "JmySff"
    },
    {
      "setting": "Depth LoRA strength",
      "value": "0.05",
      "reason": "Very low strength still works effectively",
      "from": "David Snow"
    },
    {
      "setting": "DG_Boost models steps",
      "value": "5 or 6",
      "reason": "Models configured for these step counts by default",
      "from": "David Snow"
    },
    {
      "setting": "Control blending percentage",
      "value": "60%",
      "reason": "100% gave bad results, 60% worked better for combined controls",
      "from": "Johnjohn7855"
    },
    {
      "setting": "VACE control video + reference image strength",
      "value": "0.5 to 0.6",
      "reason": "Cleaner outputs compared to 1.0 which creates noise",
      "from": "Johnjohn7855"
    },
    {
      "setting": "CFG and shift with cfg distilled lora",
      "value": "cfg=1 and shift=10",
      "reason": "Used with cfg distilled lora + aesthetic lora",
      "from": "Johnjohn7855"
    },
    {
      "setting": "CFG and shift for general use",
      "value": "cfg=4.5 and shift=7",
      "reason": "Previous recommended settings",
      "from": "Johnjohn7855"
    },
    {
      "setting": "Block swap for 720 model",
      "value": "20 blocks",
      "reason": "Better VRAM management for hefty 720 model",
      "from": "David Snow"
    },
    {
      "setting": "Context window size",
      "value": "81 frames",
      "reason": "Model performs best at 81 frames, should be best context window size",
      "from": "Kijai"
    },
    {
      "setting": "Steps for Fun model",
      "value": "8 steps",
      "reason": "Produces remarkably good results with much faster generation",
      "from": "A.I.Warper"
    },
    {
      "setting": "Framerate for Wan models",
      "value": "16fps",
      "reason": "Fixed framerate that motion is supposed to look good at",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Control merge strength",
      "value": "0.6",
      "reason": "Good balance - 1.0 is too rigid for reference image application",
      "from": "Johnjohn7855"
    },
    {
      "setting": "RIFE interpolation for 24fps",
      "value": "RIFE x3 then take every 2nd frame",
      "reason": "Converts from 16fps to 24fps (16x3/2=24)",
      "from": "AJO"
    },
    {
      "setting": "Resolution for extended frames",
      "value": "368x640 or 368x680",
      "reason": "Allows 220+ frames without OOM on 4060 ti 16GB",
      "from": "Pol"
    },
    {
      "setting": "Steps",
      "value": "At least 20",
      "reason": "Prevents blurry outputs",
      "from": "MilesCorban"
    },
    {
      "setting": "TeaCache start_step",
      "value": "2 or 3",
      "reason": "Better quality before TeaCache kicks in",
      "from": "MilesCorban"
    },
    {
      "setting": "TeaCache skip percentage",
      "value": "30% of steps",
      "reason": "Acceptable quality tradeoff",
      "from": "Kijai"
    },
    {
      "setting": "Resolution for quality testing",
      "value": "960x720",
      "reason": "Good balance for testing outputs",
      "from": "Sway"
    },
    {
      "setting": "TeaCache",
      "value": "0.13 cutoff",
      "reason": "Cutoff for major quality degradation",
      "from": "lostintranslation"
    },
    {
      "setting": "LoRA strength on Fun models",
      "value": "2.0",
      "reason": "Need higher strength for non-Fun LoRAs to have any effect",
      "from": "Kijai"
    },
    {
      "setting": "CFG and shift for Wan",
      "value": "Use defaults",
      "reason": "Defaults work best, more critical than with other models",
      "from": "Draken"
    },
    {
      "setting": "Shift for FLF2V",
      "value": "7 vs 16",
      "reason": "Some users get better results with shift 7 instead of default 16",
      "from": "seitanism"
    },
    {
      "setting": "Steps for FLF2V",
      "value": "24 steps vs 6",
      "reason": "24 steps much better quality but slower (6 steps: 1min, 24 steps: 3:50)",
      "from": "N0NSens"
    },
    {
      "setting": "Denoise",
      "value": "0.38",
      "reason": "Works well with distilled 1.3B model for quality/speed balance",
      "from": "Flipping Sigmas"
    },
    {
      "setting": "CFG",
      "value": "1.5",
      "reason": "Used with distilled model and multiple LoRAs",
      "from": "Flipping Sigmas"
    },
    {
      "setting": "Steps",
      "value": "8",
      "reason": "Sufficient for distilled model with LoRAs",
      "from": "Flipping Sigmas"
    },
    {
      "setting": "Clipvision strength",
      "value": "0.3",
      "reason": "Reduces static behavior in i2v generations",
      "from": "DevouredBeef"
    },
    {
      "setting": "blocks_to_swap for 12GB VRAM",
      "value": "37-40",
      "reason": "Allows generation above 480p with 81 frames on FLF2V",
      "from": "DawnII"
    },
    {
      "setting": "CFG for FLF2V with same start/end image",
      "value": "7-10 with 1.0 motion lora",
      "reason": "Needed to get motion, but higher values introduce artifacts",
      "from": "Eclipse"
    },
    {
      "setting": "VACE shift for better reference adherence",
      "value": "20-30",
      "reason": "Double digits improve style adherence, counter to base models",
      "from": "DawnII"
    },
    {
      "setting": "gpu_memory_preservation for 4090-5090",
      "value": "3-6",
      "reason": "ChatGPT suggestion for these GPU ranges",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "setting": "VACE steps with DG model and extend lora",
      "value": "8 steps",
      "reason": "Allows longer generation but produces realistic results",
      "from": "Rishi Pandey"
    },
    {
      "setting": "CFG",
      "value": "Lower values",
      "reason": "Reduces overcooking/artifacts and maintains better character consistency across runs",
      "from": "A.I.Warper"
    },
    {
      "setting": "VACE steps",
      "value": "20 steps (potentially 8)",
      "reason": "Provides good quality results efficiently",
      "from": "A.I.Warper"
    },
    {
      "setting": "Frame batch size",
      "value": "33 frames",
      "reason": "Provides more stability and allows finer text prompting control",
      "from": "Hashu"
    },
    {
      "setting": "Video loader fps",
      "value": "16 fps",
      "reason": "Helps with motion handling, can interpolate to higher fps later",
      "from": "Flipping Sigmas"
    },
    {
      "setting": "Shift parameter for SkyReels 540p model",
      "value": "17",
      "reason": "To get good movement",
      "from": "Benjimon"
    },
    {
      "setting": "Depth control timestep scheduling",
      "value": "0.0 - 0.1 or 0.2 of the steps",
      "reason": "To control depth frame influence while maintaining other controls",
      "from": "Kijai"
    },
    {
      "setting": "Context window settings for long videos",
      "value": "Specific context options shown in screenshot",
      "reason": "For generating 1000+ frame videos without breaking",
      "from": "Flipping Sigmas"
    },
    {
      "setting": "Extra noise detail motion LoRA strength",
      "value": "0.1-0.2 maximum",
      "reason": "Higher values cause white particles",
      "from": "David Snow"
    },
    {
      "setting": "MPS/HPS LoRA strength",
      "value": "0.3 maximum",
      "reason": "Higher values change face appearance too much, diffsynth converts are 2x stronger",
      "from": "gshawn"
    },
    {
      "setting": "Denoise for upscaling",
      "value": "0.3-0.7 range",
      "reason": "0.7 gives detail reset, 0.3 minimal change, find sweet spot",
      "from": "lostintranslation"
    },
    {
      "setting": "FreSca scale_high",
      "value": "1.50",
      "reason": "Provides small but significant quality improvement",
      "from": "David Snow"
    },
    {
      "setting": "UniAnimate CFG",
      "value": "Higher than 1.5",
      "reason": "Demo uses 1.5 for weird reason, higher CFG with SLG gives better results",
      "from": "Kijai"
    },
    {
      "setting": "Steps for Wan",
      "value": "50 steps",
      "reason": "Preferred setting for quality",
      "from": "Benjimon"
    },
    {
      "setting": "SLG block settings",
      "value": "block 8, 0.10-1.00",
      "reason": "For VACE processing",
      "from": "David Snow"
    },
    {
      "setting": "UniAnimate CFG",
      "value": "1.0",
      "reason": "Works properly at this setting",
      "from": "Kijai"
    },
    {
      "setting": "VACE denoise",
      "value": "0.8",
      "reason": "Very little of original remains, relies on preprocessor influence",
      "from": "David Snow"
    },
    {
      "setting": "UniAnimate steps with teacache",
      "value": "50 steps with 22 skipped",
      "reason": "Maintains quality while using teacache acceleration",
      "from": "Kijai"
    },
    {
      "setting": "CFG",
      "value": "2.0",
      "reason": "Prevents artifacts with VACE",
      "from": "David Snow"
    },
    {
      "setting": "Steps",
      "value": "35",
      "reason": "Sweet spot for generation quality",
      "from": "Johnjohn7855"
    },
    {
      "setting": "Control embed start",
      "value": "1 instead of 0",
      "reason": "Gets rid of flash at start but introduces more movement",
      "from": "Gavmakes"
    },
    {
      "setting": "Reference threshold",
      "value": "0.3 (default)",
      "reason": "For DWPose detection in UniAnimate",
      "from": "wange1002"
    },
    {
      "setting": "FPS output",
      "value": "24fps",
      "reason": "Default for SkyReels but may appear too fast",
      "from": "Kijai"
    },
    {
      "setting": "Resolution for 1.3B",
      "value": "544x960",
      "reason": "Standard resolution for the model",
      "from": "Kijai"
    },
    {
      "setting": "Steps for DF",
      "value": "50 steps recommended",
      "reason": "Better quality with Diffusion Forcing models",
      "from": "V\u00e9role"
    },
    {
      "setting": "CFG for DF testing",
      "value": "6",
      "reason": "Good balance for testing",
      "from": "V\u00e9role"
    },
    {
      "setting": "prefix_samples in DF",
      "value": "17 frames commonly used",
      "reason": "For 97 frame generation with 17 input frames, generates 80 new frames",
      "from": "Kijai"
    },
    {
      "setting": "empty_frame_level in VACE",
      "value": "0.5",
      "reason": "Creates grey frames (127) that model recognizes for inpainting instead of black frames",
      "from": "Hashu"
    },
    {
      "setting": "CFG for DF",
      "value": "CFG 1 not great",
      "reason": "Low CFG values don't work well but content carries through generations",
      "from": "DawnII"
    },
    {
      "setting": "Base precision",
      "value": "Bf16",
      "reason": "Recommended for better performance",
      "from": "David Snow"
    },
    {
      "setting": "SLG block",
      "value": "8",
      "reason": "Good choice for 1.3B model",
      "from": "Kijai"
    },
    {
      "setting": "SLG parameters for 14B 480p",
      "value": "8, 9, 10 with CFG 3-4",
      "reason": "Reduce CFG to compensate for SLG boost",
      "from": "Piblarg"
    },
    {
      "setting": "SLG start/end percent",
      "value": "0.2, 0.8",
      "reason": "Weaker but less likely to burn, safer than 0,1",
      "from": "Piblarg"
    },
    {
      "setting": "CFG for Uni-PC sampler",
      "value": "3.5 to 4.0",
      "reason": "Good range with shift around 4.0 and 50 steps",
      "from": "Kytra"
    },
    {
      "setting": "TeaCache for 1.3B",
      "value": "Use retention/e0 mode",
      "reason": "Old settings work okay with alternative mode",
      "from": "Kijai"
    },
    {
      "setting": "TeaCache mode",
      "value": "e mode instead of e0",
      "reason": "Uses ~600MB instead of ~4GB memory",
      "from": "Kijai"
    },
    {
      "setting": "14B DF sampling steps",
      "value": "69 frames max on first gen",
      "reason": "Memory limitations on RTX 5090",
      "from": "jellybean5361"
    },
    {
      "setting": "LTX generation",
      "value": "97 frames at 1280x720",
      "reason": "20 second generation time",
      "from": "David Snow"
    },
    {
      "setting": "Wan 14B generation time",
      "value": "~3-5 minutes for 832x480",
      "reason": "With TeaCache and SAGE attention",
      "from": "jellybean5361"
    },
    {
      "setting": "SkyReels V2 default resolution",
      "value": "1280x768",
      "reason": "Official default resolution used in examples",
      "from": "Kijai"
    },
    {
      "setting": "TeaCache threshold for hands preservation",
      "value": "0.13",
      "reason": "Seems to not degrade hands too much while providing speed benefit",
      "from": "lostintranslation"
    },
    {
      "setting": "CFG scheduling example",
      "value": "6,6,6,6,6,1,1,1,1,1 for 10 steps",
      "reason": "Replaces two-pass workflow with single sampler using higher CFG for first half, CFG 1 for second half",
      "from": "MilesCorban"
    },
    {
      "setting": "Denoise for second pass",
      "value": "Less than 1.0",
      "reason": "Denoise 1.0 creates completely new video, lower values blend new with old content",
      "from": "Kijai"
    },
    {
      "setting": "TeaCache threshold for 14B models",
      "value": "0.25",
      "reason": "0.13 is too low and causes performance issues",
      "from": "Kijai"
    },
    {
      "setting": "TeaCache start step with e0",
      "value": "6",
      "reason": "When using e0 coefficient setting",
      "from": "Kijai"
    },
    {
      "setting": "Block swap for 16GB VRAM",
      "value": "20",
      "reason": "Prevents OOM on 81 frames, 10 blocks insufficient",
      "from": "lostintranslation"
    },
    {
      "setting": "SLG settings for quality",
      "value": "blocks: 10, start: 0.2, end: 0.8, cfg: 3.5",
      "reason": "Good balance for hand quality while using TeaCache",
      "from": "Kytra"
    },
    {
      "setting": "Minimum frame count for testing",
      "value": "33-50 frames",
      "reason": "Model tuned for 81 frames, shorter clips need different settings and don't test well",
      "from": "Kijai"
    },
    {
      "setting": "CFG for longer clips",
      "value": "Higher CFG",
      "reason": "With longer clips you can and should use higher CFG",
      "from": "Kijai"
    },
    {
      "setting": "Enhance value for full frame count",
      "value": "4",
      "reason": "Value of 4 on enhance is meant for full 81 frame count",
      "from": "Kijai"
    },
    {
      "setting": "CFG for DF model",
      "value": "7",
      "reason": "Higher CFG of 7 with shift of 4 produces better previews",
      "from": "boorayjenkins"
    },
    {
      "setting": "Samplers for quality",
      "value": "40",
      "reason": "Increased from 25 to 40 for better results",
      "from": "boorayjenkins"
    },
    {
      "setting": "Frame overlap",
      "value": "27 frames",
      "reason": "Bigger overlap helps maintain motion consistency in extensions",
      "from": "boorayjenkins"
    },
    {
      "setting": "SLG guidance",
      "value": "start 0.3, end 0.8",
      "reason": "Better than defaults which can cause discoloration",
      "from": "zelgo_"
    },
    {
      "setting": "TeaCache mode",
      "value": "e",
      "reason": "Uses only 600MB vs 4000MB with e0 mode",
      "from": "Kijai"
    },
    {
      "setting": "Overlap frames",
      "value": "27 frames (vs default 17)",
      "reason": "Better extension quality, lose 10 more frames but improves results",
      "from": "boorayjenkins"
    },
    {
      "setting": "VRAM usage for DF 14b",
      "value": "30GB at 544x544x97 with 15 block swap",
      "reason": "High VRAM requirements for diffusion forcing",
      "from": "seitanism"
    },
    {
      "setting": "VRAM usage for 1.3b",
      "value": "8GB at 1280x768",
      "reason": "More manageable requirements",
      "from": "DawnII"
    },
    {
      "setting": "Block swapping for VRAM management",
      "value": "30 blocks for 720x720x81 frames (overkill), user reports needing all 40 blocks for 1280x720x81",
      "reason": "Memory optimization for large generations",
      "from": "Kijai"
    },
    {
      "setting": "Phantom model resolution",
      "value": "Max 1280x768",
      "reason": "Takes more VRAM than 1.3B usually, can be lowered",
      "from": "Kijai"
    },
    {
      "setting": "TeaCache with DF models",
      "value": "TeaCache e0 uses more memory than e",
      "reason": "Memory optimization consideration",
      "from": "jellybean5361"
    },
    {
      "setting": "TeaCache weight and start",
      "value": "0.15 weight, start at step 5",
      "reason": "Works well for memory optimization",
      "from": "mamad8"
    },
    {
      "setting": "Blockswap for 720p I2V",
      "value": "10 blockswap",
      "reason": "Allows 81 frames at 480x480 on 3090 24GB VRAM",
      "from": "mamad8"
    },
    {
      "setting": "Steps for different use cases",
      "value": "20 steps for testing, 50+ for quality",
      "reason": "Balance between speed and quality",
      "from": "MilesCorban"
    },
    {
      "setting": "noise_aug_strength",
      "value": "0.03",
      "reason": "Helps with motion following in WanVideo ImageToVideo Encode",
      "from": "boorayjenkins"
    },
    {
      "setting": "control_embed_strength",
      "value": "start 0.0, end 0.5",
      "reason": "Better control embed performance",
      "from": "David Snow"
    },
    {
      "setting": "model_device",
      "value": "offload_device",
      "reason": "Prevents OOM issues when changing LoRA settings, main_device only for 80GB+ VRAM",
      "from": "Kijai"
    },
    {
      "setting": "Steps for Phantom model",
      "value": "20 steps",
      "reason": "Good balance of quality and speed for 14B 720p model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Resolution for Phantom",
      "value": "1024x576",
      "reason": "89 frames at this resolution with 20 steps completed in ~180 seconds",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "setting": "Minimum steps for quality",
      "value": "20 steps minimum",
      "reason": "Below 20 steps everything starts to change/fall apart",
      "from": "MilesCorban"
    },
    {
      "setting": "Skyreels i2v quantization for 4090",
      "value": "fp8",
      "reason": "Fits almost all on VRAM with minimal system RAM offload",
      "from": "MilesCorban"
    },
    {
      "setting": "Rope function",
      "value": "comfy",
      "reason": "More efficient than default and avoids dtype errors",
      "from": "Kijai"
    },
    {
      "setting": "CFG",
      "value": "1",
      "reason": "Can trade with very high shift (200) for 50% faster inference",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "setting": "Shift",
      "value": "10-17",
      "reason": "Works well for Skyreels, especially 14B model",
      "from": "Colin"
    },
    {
      "setting": "Audio scale",
      "value": "0.5",
      "reason": "Good balance for FantasyTalking with t2v",
      "from": "Kijai"
    },
    {
      "setting": "Denoise",
      "value": "0.6-0.7",
      "reason": "For refining stage in DF extension to combat degradation",
      "from": "Ablejones"
    },
    {
      "setting": "Fantasy Talking audio CFG",
      "value": "1.0",
      "reason": "Higher values can cause sync issues and quality degradation",
      "from": "Multiple users"
    },
    {
      "setting": "Fantasy Talking FPS",
      "value": "23",
      "reason": "Model is trained for 23fps output, other frame rates cause sync issues",
      "from": "Kijai"
    },
    {
      "setting": "TeaCache start step",
      "value": "6",
      "reason": "Prevents early skipping that degrades quality",
      "from": "Kijai"
    },
    {
      "setting": "Variable audio CFG",
      "value": "5.0 for first 3 steps, then 1.0",
      "reason": "Balances quality and sync accuracy",
      "from": "Kijai"
    },
    {
      "setting": "DF different seeds",
      "value": "Use different seed for each sampler",
      "reason": "Prevents overburning and overexposure",
      "from": "Kijai"
    },
    {
      "setting": "blocks_to_swap",
      "value": "20",
      "reason": "Reduces VRAM usage if you have decent RAM",
      "from": "ingi // SYSTMS"
    },
    {
      "setting": "clip_vision_precision",
      "value": "FP16 or BF16",
      "reason": "Reduces VRAM usage",
      "from": "ingi // SYSTMS"
    },
    {
      "setting": "base_precision",
      "value": "fp8_e4m3fn",
      "reason": "Use e5 in quant for older GPU compatibility",
      "from": "Jemmo"
    },
    {
      "setting": "addnoise_condition",
      "value": "10",
      "reason": "Prevents flickering, paper suggests 20 for 14B model",
      "from": "N0NSens"
    },
    {
      "setting": "CFG settings",
      "value": "cfg3/shift5/addnoise10",
      "reason": "No flickering configuration",
      "from": "N0NSens"
    },
    {
      "setting": "camera_control_strength",
      "value": "0.2",
      "reason": "Lower strength to prevent over-constraining",
      "from": "Kijai"
    },
    {
      "setting": "fantasy_talking_fps",
      "value": "23",
      "reason": "Only works at 23 frames per second",
      "from": "Stad"
    },
    {
      "setting": "Empty frame level",
      "value": "0.5",
      "reason": "Model is trained with this value (127,127,127 RGB)",
      "from": "Kijai"
    },
    {
      "setting": "TC node start/end values in wrapper",
      "value": "Use exact steps not percentages",
      "reason": "Wrapper uses steps while native node uses percentages",
      "from": "Kijai"
    }
  ],
  "concepts": [
    {
      "term": "SLG (Skip Layer Guidance)",
      "explanation": "Node that works on uncond blocks at set inference range, very good at low step count",
      "from": "ezMan"
    },
    {
      "term": "VACE",
      "explanation": "Video control system from Alibaba with style transfer, inpainting, subject-driven, and outpainting capabilities",
      "from": "Kijai"
    },
    {
      "term": "Second-pass enhancement",
      "explanation": "Running existing video generations through another model to improve quality and motion coherence",
      "from": "David Snow"
    },
    {
      "term": "TeaCache modulated time embeds",
      "explanation": "Optimization technique that doesn't work with dpmpp_sde scheduler, causes rainbow artifacts",
      "from": "ezMan"
    },
    {
      "term": "Context windows",
      "explanation": "Method for processing longer video sequences by breaking them into overlapping segments",
      "from": "Kijai"
    },
    {
      "term": "VACE blocks",
      "explanation": "15 additional processing blocks that run every sampling step, causing slower performance",
      "from": "Kijai"
    },
    {
      "term": "Mixed precision in VACE",
      "explanation": "VACE blocks use bf16 while rest of model uses fp32",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Splits long video processing into 81-frame chunks with overlap, allowing processing of longer videos without memory issues",
      "from": "Kijai"
    },
    {
      "term": "Reference compositing",
      "explanation": "Multiple reference images are combined into a single image rather than processed separately",
      "from": "Draken"
    },
    {
      "term": "First frame injection",
      "explanation": "VACE puts reference image as the first frame and is trained to pick up likeness from it",
      "from": "Draken"
    },
    {
      "term": "VACE",
      "explanation": "All-encompassing control model for video generation, similar to ACE++ but for video. Works with reference images, masks, and various control inputs",
      "from": "Kytra"
    },
    {
      "term": "TeaCache",
      "explanation": "Memory caching system that had a bug causing memory leaks, especially noticeable with VACE's larger tensors",
      "from": "Kijai"
    },
    {
      "term": "VACE task interpretation",
      "explanation": "Model interprets tasks automatically rather than requiring explicit task specification like in Python API",
      "from": "DawnII"
    },
    {
      "term": "VACE reference vs I2V reference",
      "explanation": "The reference image is not like you'd use with I2V model - needs specific preprocessing",
      "from": "Kijai"
    },
    {
      "term": "VACE masking system",
      "explanation": "Gray areas (127 values) represent missing video parts, white areas in mask represent parts to be generated, black areas represent parts to be retained",
      "from": "AJO"
    },
    {
      "term": "Prompt alignment/extension",
      "explanation": "LLM-based system that rewrites user prompts to match training caption distribution - adds details, motion attributes, and proper structure",
      "from": "fearnworks"
    },
    {
      "term": "VACE input frame interpretation",
      "explanation": "Model automatically distinguishes between reference frames (colored) and control frames (preprocessed) without explicit labeling",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "TeaCache memory bug",
      "explanation": "Memory usage incorrectly scaling with steps in VACE, now fixed in latest commits",
      "from": "Kijai"
    },
    {
      "term": "Reference vs Start Frame in VACE",
      "explanation": "Reference means reference object/subject (needs white background), start frame is for matching first frame of sequence",
      "from": "Kijai"
    },
    {
      "term": "VACE mask input",
      "explanation": "Controls where VACE effects are applied in both spatial (image areas) and temporal (time) dimensions",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Method for generating long videos by processing in overlapping segments",
      "from": "Kijai"
    },
    {
      "term": "VACE blocks",
      "explanation": "15 blocks that can be selectively applied, controls which parts of the model use VACE functionality",
      "from": "Kijai"
    },
    {
      "term": "Context Embedder and Context Blocks",
      "explanation": "In VACE training, DiT parameters are frozen and only Context Embedder and Context Blocks are trainable",
      "from": "mamad8"
    },
    {
      "term": "Block swap",
      "explanation": "Memory optimization technique that requires VACE blocks > 0 to function properly",
      "from": "Kijai"
    },
    {
      "term": "DG models",
      "explanation": "Distilled Guidance models - CFG distilled variants that work at lower CFG values",
      "from": "DawnII"
    },
    {
      "term": "prev_embeds",
      "explanation": "Allows multiple VACE control inputs with different strengths and timestep ranges, like multiple ControlNets",
      "from": "Kijai"
    },
    {
      "term": "DG models",
      "explanation": "Distilled Wan models for faster inference, available in light/medium/high variants",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "VACE module separation",
      "explanation": "VACE can be loaded separately from base model since it doesn't train base layers",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "TeaCache step skipping behavior",
      "explanation": "TeaCache skips 'useless steps' to speed up generation, but can cause issues with low step counts or certain samplers",
      "from": "TimHannan"
    },
    {
      "term": "SageAttention auto mode issue",
      "explanation": "Auto mode can cause NaN (black output) with I2V, manual mode works fine",
      "from": "Kijai"
    },
    {
      "term": "VACE mask input vs reference masking",
      "explanation": "Mask input is for the video, not the reference image - controls what parts of video get replaced",
      "from": "traxxas25"
    },
    {
      "term": "SkyreelsA2 reference padding",
      "explanation": "Model adds padding to first frame image and uses clip embeds for multiple reference images, different approach from VACE",
      "from": "Kijai"
    },
    {
      "term": "VACE block swap",
      "explanation": "VACE can be run separately and results used one by one on blocks for better memory efficiency",
      "from": "Kijai"
    },
    {
      "term": "TeaCache skip reporting",
      "explanation": "TeaCache reports how many steps were actually skipped after generation, helps optimize threshold settings",
      "from": "Kijai"
    },
    {
      "term": "VACE temporal inpainting",
      "explanation": "Black mask areas are kept from input, white areas are generated new. Can use any number of input frames",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Splits model prediction on each step, does chunks with overlap and combines them. Very slow but enables longer generations",
      "from": "Kijai"
    },
    {
      "term": "Prompt to reference matching",
      "explanation": "VACE uses prompt description to map to reference image features",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "VACE prev embeds",
      "explanation": "Feature for chaining multiple VACE encode nodes, feeding output of one as input to another for complex control combinations",
      "from": "yo9o"
    },
    {
      "term": "VACE block swap",
      "explanation": "Memory management technique that automatically enables when even 1 VACE block is swapped, moves intermediate results to RAM",
      "from": "Kijai"
    },
    {
      "term": "VACE strength",
      "explanation": "Controls both the control image and reference image strength - it's the overall strength of VACE itself",
      "from": "Kijai"
    },
    {
      "term": "Reward Backpropagation",
      "explanation": "Technique used to optimize generated videos for better alignment with human preferences",
      "from": "Lumi"
    },
    {
      "term": "Network dim in LoRA training",
      "explanation": "Higher dim = more parameters trained. Keep low for simple/single concepts, higher for complex training",
      "from": "Benjimon"
    },
    {
      "term": "Temporal leakage",
      "explanation": "When keeping 1 frame in VACE, it gets packed in a latent with 3 other frames causing some influence from adjacent frames, but this also enables smooth operation",
      "from": "Kijai"
    },
    {
      "term": "Inactive vs reactive frames",
      "explanation": "VACE refers to frames as inactive (preserved) and reactive (generated) - frames marked with mask are considered part of desired output, rest is inpainted",
      "from": "Kijai"
    },
    {
      "term": "VACE",
      "explanation": "Video control system with style transfer, inpainting, subject-driven, outpainting capabilities. Additional module that doesn't modify original model weights",
      "from": "Kijai"
    },
    {
      "term": "prev_embeds",
      "explanation": "Input for combining multiple control types in VACE, allows blending different preprocessors",
      "from": "Hashu"
    },
    {
      "term": "Alpha keys in LoRAs",
      "explanation": "Original LoRAs had alpha=64 with rank=128, DiffSynth removed alpha making them 2x stronger",
      "from": "Kijai"
    },
    {
      "term": "White canvas requirement",
      "explanation": "VACE is trained to detect subjects on white canvas background, can have multiple subjects",
      "from": "Kijai"
    },
    {
      "term": "Empty_frame_level",
      "explanation": "Setting in VACE that creates white, black or grey masks/frames that it will generate content in",
      "from": "BondoMan"
    },
    {
      "term": "RIFLEX",
      "explanation": "Frequency index that allows for new frames to be generated after without looping, disabled when set to 0",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "term": "Temporal mask",
      "explanation": "Input for controlling which parts of video are affected over time",
      "from": "Dream Making"
    },
    {
      "term": "VACE",
      "explanation": "Only an extra module used with the original model, they just included the original model in the initial file they shared",
      "from": "Kijai"
    },
    {
      "term": "Reference image vs start image",
      "explanation": "Reference image is different from start image - reference needs white canvas, start image is for exact first frame creation",
      "from": "Kijai"
    },
    {
      "term": "Temporal masking",
      "explanation": "The mask from the start/end frame node is temporal masking, it helps preserve your stylized first frame",
      "from": "DawnII"
    },
    {
      "term": "DiffSynth LoRA conversion",
      "explanation": "Removes alpha keys, effectively doubling strength - 0.5 strength equals original at 1.0",
      "from": "Kijai"
    },
    {
      "term": "VACE keyframes",
      "explanation": "Black mask areas are keyframes with strong influence, white areas are generated",
      "from": "Kijai"
    },
    {
      "term": "Grey background masking",
      "explanation": "Using grey color in masked areas instead of black to avoid filtering issues",
      "from": "IllumiReptilien"
    },
    {
      "term": "VACE model types",
      "explanation": "Two types: basic VACE-only model (needs original WAN model too) and merged model (standalone)",
      "from": "\u2727\u0e05\u0e42\u0e51\u2180\u11ba\u2180 \u0e51\u0e43\u0e05\u2727PookieNumnums"
    },
    {
      "term": "Temporal inpainting",
      "explanation": "VACE's method of selectively preserving some frames while generating others, not limited to just start/end frames",
      "from": "Kijai"
    },
    {
      "term": "Latent warm up",
      "explanation": "1 latent = 4 frames, identified as potential cause of flashing in loops",
      "from": "The Shadow (NYC)"
    },
    {
      "term": "Reference image positioning",
      "explanation": "Position matters in VACE reference images as they affect the whole generation regardless of temporal information",
      "from": "Kijai"
    },
    {
      "term": "Fun InP",
      "explanation": "WAN inpainting model for temporal inpainting only, while VACE can do both temporal and spatial inpainting",
      "from": "Kijai"
    },
    {
      "term": "Differential Diffusion in VACE",
      "explanation": "Built into WanVideo encode node via mask input, blends masked areas but can affect style consistency",
      "from": "David Snow"
    },
    {
      "term": "Reference image padding",
      "explanation": "Reference images automatically add 4 latent frames in VACE, now handled automatically",
      "from": "Kijai"
    },
    {
      "term": "OptimalSteps",
      "explanation": "Custom sigmas implementation similar to 'align your steps' but with different values for potentially faster generation",
      "from": "Kijai"
    },
    {
      "term": "VACE flexibility",
      "explanation": "Can use control only, or control + source video. Control frames generated from source video but source video itself doesn't need to be used in diffusion",
      "from": "David Snow"
    },
    {
      "term": "Context window overlap calculation",
      "explanation": "Context frames + overlap determines total frames rendered. 81 context + 24 overlap + 81 frames = 169 total for example",
      "from": "Kijai"
    },
    {
      "term": "Reference images vs start frames in context",
      "explanation": "Start frames won't be part of windows that aren't first, but reference images work because added to each context manually and aren't positional",
      "from": "Kijai"
    },
    {
      "term": "ExVideo LoRA",
      "explanation": "LoRA that doubles video length from 81 to 161 frames and can also act as detail enhancement",
      "from": "David Snow"
    },
    {
      "term": "Context windows",
      "explanation": "Feature for longer generations but can cause style shifting - disable for style preservation",
      "from": "David Snow"
    },
    {
      "term": "SageAttention dtype selection",
      "explanation": "Different precision modes (auto, fp16_cuda) for different GPUs - auto works for most, fp16_cuda needed for 4090 with I2V",
      "from": "Kijai"
    },
    {
      "term": "SLG (Skip Layer Guidance)",
      "explanation": "Same as SkipLayerGuidanceDiT node for skipping diffusion layers",
      "from": "Colin"
    },
    {
      "term": "BF16 to FP16 conversion",
      "explanation": "Lossy conversion that can't recover missing precision data, results in worse quality than either format",
      "from": "Kijai"
    },
    {
      "term": "FPS conditioning",
      "explanation": "Model conditioning based on frame rate - Wan doesn't seem to have this feature",
      "from": "Kijai"
    },
    {
      "term": "pos_embed in FLF2V",
      "explanation": "New positional embedding that affects how clip image embeds are applied, seems to make first frame influence stay longer",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Memory management technique that can improve generation speed by 60% on both 41 and 81 frame generations",
      "from": "lostintranslation"
    },
    {
      "term": "SLG/cfg zero star",
      "explanation": "Technique that works with FLF2V model, though optimal prompting strategy still unclear",
      "from": "Kijai"
    },
    {
      "term": "DiT patchify for temporal coherence",
      "explanation": "Technique where recent frames get more patches than distant frames - 3rd frame normal patches, 2nd fewer patches, 1st least patches. Not possible with UNet architecture",
      "from": "Fannovel16"
    },
    {
      "term": "Inverted sampling in FramePack",
      "explanation": "Model generates from end to beginning, so ending actions appear before starting actions during generation",
      "from": "JohnDopamine"
    },
    {
      "term": "FramePack architecture",
      "explanation": "Uses HunyuanVideoTransformer3DModelPacked class name, different from standard HYV architecture",
      "from": "JohnDopamine"
    },
    {
      "term": "Temporal masking in VACE",
      "explanation": "Black mask frames are keyframes that don't change, white mask frames are inpainted temporally. Allows mixing keyframes with generated content",
      "from": "Kijai"
    },
    {
      "term": "First frame vs reference in VACE",
      "explanation": "First frame as encoded latent maintains better coherence than sending stylized frame as reference",
      "from": "DawnII"
    },
    {
      "term": "FramePack resolution limitation",
      "explanation": "Gradio demo fixed at 600x600 resolution which impacts quality, not a fault of Gradio but intentional limitation",
      "from": "BNP4535353"
    },
    {
      "term": "Multiple VAE passes degradation",
      "explanation": "Every time an image is decoded and then re-encoded through a VAE, it loses quality - this compounds with each additional pass",
      "from": "DawnII"
    },
    {
      "term": "DF in model name",
      "explanation": "Likely stands for Diffusion-Forcing, a training technique, though exact meaning unclear",
      "from": "Screeb"
    },
    {
      "term": "Diffusion Forcing (DF)",
      "explanation": "A different category of model entirely, like 1 frame T2V that is fed as a reference",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "term": "VACE",
      "explanation": "A controlnet module that allows T2V models to use images, enables controls or reference passing",
      "from": "Kijai"
    },
    {
      "term": "TeaCache mode",
      "explanation": "Detection method used to differentiate between 480p and 720p models that are identical in keys",
      "from": "Kijai"
    },
    {
      "term": "Image embeds in Fun LoRAs",
      "explanation": "Special keys that normal 1.3B model lacks, causing incomplete LoRA loading",
      "from": "Kijai"
    },
    {
      "term": "VAE tiling",
      "explanation": "Memory optimization technique for video decoding to prevent OOM errors",
      "from": "David Snow"
    },
    {
      "term": "VACE blocks",
      "explanation": "Numbers that represent which blocks VACE is applied to, affects processing quality",
      "from": "David Snow"
    },
    {
      "term": "DMD vs CFG distillation",
      "explanation": "DMD is step distilled (reduces step count from 50 to 5), CFG distilled is different and doesn't work well with low steps",
      "from": "yi"
    },
    {
      "term": "UniAnimate architecture",
      "explanation": "1GB LoRA weights and pose embeds that work as control model for normal 14B I2V",
      "from": "Kijai"
    },
    {
      "term": "LoRA compatibility",
      "explanation": "LoRAs = change weights decomposed to two matrices A and B. Can add change weights to original model always - software issue rather than math issue when not possible",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "term": "Block swap bottleneck",
      "explanation": "When using block swap, those become more of a bottleneck and you won't feel attention speed up as much",
      "from": "Kijai"
    },
    {
      "term": "Diffusion Forcing",
      "explanation": "Method for generating infinite-length videos, supports both T2V and I2V with synchronous and asynchronous inference modes",
      "from": "Colin"
    },
    {
      "term": "Diffusion Forcing (DF)",
      "explanation": "New architecture requiring whole new sampling process, allows asynchronous usage",
      "from": "Kijai"
    },
    {
      "term": "Penultimate hidden states",
      "explanation": "Second-to-last layer outputs from clip vision, fixes compatibility issues",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Memory management technique needed for larger models on limited VRAM",
      "from": "Juampab12"
    },
    {
      "term": "Diffusion Forcing (DF)",
      "explanation": "Method that can continue from any number of frames, works like T2V with input latents where prefix_samples overwrite noise",
      "from": "Kijai"
    },
    {
      "term": "fps_embeds",
      "explanation": "FPS embedding layers only available in 1.3B DF model, missing from 14B version",
      "from": "Kijai"
    },
    {
      "term": "addnoise_condition",
      "explanation": "Hyperparameter in DF that adds noise and improves new frame generation",
      "from": "Kijai"
    },
    {
      "term": "SLG",
      "explanation": "Like a big boost to CFG but works better than just raising CFG, provides more detail and motion without artifacts",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "More blocks swapped = higher VRAM savings, trial and error system specific setting",
      "from": "David Snow"
    },
    {
      "term": "VACE",
      "explanation": "More like ControlNet, was released bundled with normal 1.3B model",
      "from": "Kijai"
    },
    {
      "term": "Diffusion Forcing",
      "explanation": "Uses different scheduling, creates new scheduler for each latent with per-token denoising",
      "from": "Kijai"
    },
    {
      "term": "TeaCache threshold",
      "explanation": "Works like step count consideration - too high skips too many steps with quality hit",
      "from": "Kijai"
    },
    {
      "term": "VACE",
      "explanation": "Almost like a single controlnet that can do multiple functions (ref, mask, control) but everything is bundled together",
      "from": "Draken"
    },
    {
      "term": "DF (Diffusion Forcing)",
      "explanation": "Sampling method that requires massive VRAM due to time step embedding size",
      "from": "Kijai"
    },
    {
      "term": "TeaCache",
      "explanation": "Caching system that clones timeembed, uses 4GB for cache in standard mode, 600MB in 'e' mode",
      "from": "Kijai"
    },
    {
      "term": "SkyReels structured prompting",
      "explanation": "JSON schema with subjects, shot_type, camera_motion, environment, lighting fields for better video generation",
      "from": "fredbliss"
    },
    {
      "term": "Dynamics-only prompting",
      "explanation": "SkyReels V2 methodology where I2V prompts only describe temporal changes (action/expression/camera motion) while the input image provides static visual context",
      "from": "fredbliss"
    },
    {
      "term": "Model parameters (14b vs 1.3b)",
      "explanation": "Refers to number of parameters in models - 14 billion vs 1.3 billion, basically the brain cells. 14b gives better quality but takes more time and memory to run.",
      "from": "Zuko"
    },
    {
      "term": "Denoise ratio",
      "explanation": "The ratio of new vs old content in diffusion sampling. 1.0 means completely new, lower values blend new generation with existing content.",
      "from": "Kijai"
    },
    {
      "term": "TeaCache",
      "explanation": "Speed optimization technique that can skip diffusion steps, but may degrade quality especially for hands at higher threshold settings",
      "from": "Kijai"
    },
    {
      "term": "FLF2V",
      "explanation": "First-Last-Frame to Video, only allows start and end frame input",
      "from": "Kijai"
    },
    {
      "term": "DF model",
      "explanation": "Endless generation model that extends videos by using previous frames as input for new generations",
      "from": "seitanism"
    },
    {
      "term": "fp8_e4m3fn vs e5m2",
      "explanation": "Different precision formats - GPUs prior to 4000 series don't work with e4m3fn and need e5m2, e4m3 has less precision than e4m8",
      "from": "Kijai"
    },
    {
      "term": "Block swap",
      "explanation": "Manual VRAM management in wrapper vs automatic offloading in native implementation",
      "from": "Kijai"
    },
    {
      "term": "Phantom model",
      "explanation": "Uses images as suggestion rather than straight i2v, not like ipadapter, good for subject reference",
      "from": "TK_999"
    },
    {
      "term": "Diffusion Forcing (DF)",
      "explanation": "New technique/architecture used in Skyreels V2 models",
      "from": "seitanism"
    },
    {
      "term": "VACE ending",
      "explanation": "Ability to end VACE conditioning at specific point (like 0.5) when combining with other techniques",
      "from": "Kijai"
    },
    {
      "term": "TeaCache modes",
      "explanation": "Different memory usage modes - 'e' uses 600MB, 'e0' uses 4000MB for time embeddings",
      "from": "Kijai"
    },
    {
      "term": "Diffusion Forcing (DF)",
      "explanation": "Technique for extending videos by using overlap frames from previous generation to guide next generation seamlessly",
      "from": "seitanism"
    },
    {
      "term": "Overlap frames",
      "explanation": "Frames from end of previous video used as reference for next generation - default 17, can use 27 for better quality",
      "from": "boorayjenkins"
    },
    {
      "term": "Diffusion Forcing (DF)",
      "explanation": "Autoregressive video model that can continue videos endlessly with proper motion continuation, works by extending from any number of input frames",
      "from": "Kijai"
    },
    {
      "term": "4n+1 frame rule",
      "explanation": "WAN models require frame counts following pattern of 4x+1 (like 65, 69, 81) because each unit the model generates is 4 frames plus 1 start frame",
      "from": "jellybean5361"
    },
    {
      "term": "Token/class captions",
      "explanation": "Dreambooth style training using simple 2-word captions like 'ohwx man' or 'joesmith person' instead of full descriptions",
      "from": "JohnDopamine"
    },
    {
      "term": "fp8_fast quality degradation",
      "explanation": "Previously fp8_fast cast inputs to different precision (fp8_e5m2) when weights were fp8_e4m3fn, causing quality loss. Now same dtype works without degradation",
      "from": "Kijai"
    },
    {
      "term": "TeaCache step activation",
      "explanation": "TeaCache activates at step 6, which is the 15% point when using default settings",
      "from": "Cubey"
    },
    {
      "term": "fp8_e4m3fn_fast_no_ffn",
      "explanation": "Skips the FFN layers which cause quality degradation, but is then not much faster than regular fp8_fast",
      "from": "Kijai"
    },
    {
      "term": "addnoise_condition",
      "explanation": "Parameter that supposedly improves consistency, though user noted minimal difference between settings",
      "from": "seitanism"
    },
    {
      "term": "Control embeds",
      "explanation": "Better term for ControlNet when used with these models",
      "from": "David Snow"
    },
    {
      "term": "TeaCache",
      "explanation": "Optimization technique that skips certain steps during generation - shown skipping steps 7, 9, 11, 13, 15 for conditional, unconditional, and prediction_2 passes",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "term": "Block swap",
      "explanation": "Technique for memory management, but current implementations may not work properly or run blocks on CPU",
      "from": "Kijai"
    },
    {
      "term": "Flow shift",
      "explanation": "Controls amount of change/movement throughout video - low shift = less change/movement, high shift = more change/movement",
      "from": "Ro"
    },
    {
      "term": "Hidream",
      "explanation": "Feature that generates very similar images at different seeds, can be used with prompts like 'close-up' to generate same picture with different angle/zoom for start/end frames",
      "from": "N0NSens"
    },
    {
      "term": "Image cross attention",
      "explanation": "Image conditioning or 'image prompting' - how models use image input for conditioning",
      "from": "Kijai"
    },
    {
      "term": "DF (Diffusion Forcing)",
      "explanation": "Model that can continue from any amount of frames, even just one, but doesn't use image cross attention",
      "from": "Kijai"
    },
    {
      "term": "DG models",
      "explanation": "Distilled models with LOW versions for many steps, HIGH versions for few steps",
      "from": "JmySff"
    },
    {
      "term": "Audio conditioning",
      "explanation": "Additional cross attention mechanism in FantasyTalking for audio-to-video sync",
      "from": "Kijai"
    },
    {
      "term": "Audio CFG",
      "explanation": "Classifier-free guidance specifically for audio conditioning in Fantasy Talking model",
      "from": "Kijai"
    },
    {
      "term": "Swap memory",
      "explanation": "Using SSD space as additional RAM when system RAM is insufficient for large models",
      "from": "Stad"
    },
    {
      "term": "Control LoRA input channel modification",
      "explanation": "Control LoRAs modify model input channels making them incompatible with other techniques like VACE",
      "from": "Kijai"
    },
    {
      "term": "Composited control inputs",
      "explanation": "Combining multiple control types by overlaying control images rather than using separate control nodes",
      "from": "Kijai"
    },
    {
      "term": "VACE",
      "explanation": "Video control system that works as modules alongside base models, like controlnet for video",
      "from": "Kijai"
    },
    {
      "term": "Fun Control reference vs start frame",
      "explanation": "Reference image doesn't need to match control closely, more flexible than start frame",
      "from": "Kijai"
    },
    {
      "term": "Camera control dataset",
      "explanation": "Uses Google's RealEstate10K dataset for 3D camera movement training",
      "from": "Kijai"
    },
    {
      "term": "Empty frame level",
      "explanation": "Controls what to replace blank frames with - 0 is black, 1 is white, 0.5 is mid gray (127,127,127 RGB)",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "term": "Fun ref image latent with empty embed",
      "explanation": "Method to use reference images without causing flash/blink artifacts at video start",
      "from": "Gavmakes"
    }
  ],
  "resources": [
    {
      "resource": "VACE-Wan2.1-1.3B-Preview",
      "url": "https://huggingface.co/ali-vilab/VACE-Wan2.1-1.3B-Preview/tree/main",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "DiffSynth-Studio-Lora-Wan2.1-ComfyUI",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI",
      "type": "lora",
      "from": "DawnII"
    },
    {
      "resource": "VACE GitHub Repository",
      "url": "https://github.com/ali-vilab/VACE",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "Wan2.1 end frame workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_480p_I2V_endframe_example_01.json",
      "type": "workflow",
      "from": "Faux"
    },
    {
      "resource": "Video Depth Anything",
      "url": "https://videodepthanything.github.io/",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Recognize Anything Model (RAM)",
      "url": "https://github.com/xinyu1205/recognize-anything",
      "type": "tool",
      "from": "TK_999"
    },
    {
      "resource": "MuseTalk 1.5",
      "url": "https://github.com/TMElyralab/MuseTalk",
      "type": "repo",
      "from": "burgstall"
    },
    {
      "resource": "Wan prompt extend system prompts",
      "url": "https://github.com/Wan-Video/Wan2.1/blob/main/wan/utils/prompt_extend.py",
      "type": "repo",
      "from": "Kagi"
    },
    {
      "resource": "VACE GitHub repository",
      "url": "https://github.com/ali-vilab/VACE",
      "type": "repo",
      "from": "zelgo_"
    },
    {
      "resource": "VACE project page with examples",
      "url": "https://ali-vilab.github.io/VACE-Page/",
      "type": "documentation",
      "from": "makeitrad"
    },
    {
      "resource": "Arcane Jinx LoRA for VACE testing",
      "url": "https://huggingface.co/Cseti/Wan-LoRA-Arcane-v1/blob/main/664463-csetiarcane-Nfj1nx-e15-e7-s5070-ipv.safetensors",
      "type": "model",
      "from": "Cseti"
    },
    {
      "resource": "VACE annotators",
      "url": "https://huggingface.co/ali-vilab/VACE-Annotators/tree/main",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "VACE-Wan2.1-1.3B-Preview model",
      "url": "https://huggingface.co/ali-vilab/VACE-Wan2.1-1.3B-Preview/tree/main",
      "type": "model",
      "from": "VK"
    },
    {
      "resource": "VACE workflow tutorial video",
      "url": "https://youtu.be/r3mDwPROC1k?si=aCo9BWShbL4uBPYJ",
      "type": "tutorial",
      "from": "Impactframes."
    },
    {
      "resource": "VACE official page",
      "url": "https://ali-vilab.github.io/VACE-Page/",
      "type": "documentation",
      "from": "Siraj"
    },
    {
      "resource": "ComfyUI-enricos-nodes",
      "url": "https://github.com/erosDiffusion/ComfyUI-enricos-nodes",
      "type": "repo",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "TinkerWan stability LoRA",
      "url": "https://civitai.com/models/1425406/tinkerwan-wan-13b-stability-and-quality-enhancement-lora?modelVersionId=1611139",
      "type": "lora",
      "from": "Juampab12"
    },
    {
      "resource": "DiffSynth-Studio LoRA",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI",
      "type": "lora",
      "from": "IllumiReptilien"
    },
    {
      "resource": "Wan2.1 14B T2V LoRAs collection",
      "url": "https://huggingface.co/collections/Remade-AI/wan21-14b-t2v-loras-67dc73d82f66cfac2b4eb253",
      "type": "lora collection",
      "from": "\u26a1"
    },
    {
      "resource": "Wan2.1 14B I2V LoRAs collection",
      "url": "https://huggingface.co/collections/Remade-AI/wan21-14b-480p-i2v-loras-67d0e26f08092436b585919b",
      "type": "lora collection",
      "from": "\u26a1"
    },
    {
      "resource": "Alvdansen LoRAs",
      "url": "https://civitai.com/user/alvdansen",
      "type": "lora collection",
      "from": "Mint"
    },
    {
      "resource": "VACE model weights",
      "url": "https://huggingface.co/ali-vilab/VACE-Wan2.1-1.3B-Preview/tree/main",
      "type": "model",
      "from": "Kytra"
    },
    {
      "resource": "VACE workflow examples",
      "url": "https://github.com/if-ai/ComfyUI-IF_VideoPrompts/tree/main/WF",
      "type": "workflow",
      "from": "Impactframes."
    },
    {
      "resource": "WanFunControlToVideo node update",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1354606222833353005",
      "type": "node",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Mediapipe face preprocessor",
      "url": "https://github.com/kijai/ComfyUI-FollowYourEmojiWrapper",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "VACE Annotators",
      "url": "https://huggingface.co/ali-vilab/VACE-Annotators",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "VACE UserGuide",
      "url": "https://github.com/ali-vilab/VACE/blob/main/UserGuide.md#32-single-tasks",
      "type": "documentation",
      "from": "AJO"
    },
    {
      "resource": "Hi-Res LoRA for Wan2.1",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "model",
      "from": "VK (5080 128gb)"
    },
    {
      "resource": "SkyReels A2 model",
      "url": "https://huggingface.co/Skywork/SkyReels-A2",
      "type": "model",
      "from": "Juampab12"
    },
    {
      "resource": "Sapiens models",
      "url": "https://huggingface.co/facebook/sapiens-pose-1b-torchscript",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "SkyReels-A2 code",
      "url": "https://github.com/SkyworkAI/SkyReels-A2",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "DiffSynth Wan models",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "model",
      "from": "BondoMan"
    },
    {
      "resource": "Wan prompt extension code",
      "url": "https://github.com/Wan-Video/Wan2.1/blob/679ccc6c68eee39ac9af0618dff904bf2c708283/wan/utils/prompt_extend.py",
      "type": "repo",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "TinkerWAN Alpha lora v1.0",
      "url": "https://civitai.com/models/1425406?modelVersionId=1614659",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "Sapiens ComfyUI nodes",
      "url": "https://github.com/smthemex/ComfyUI_Sapiens",
      "type": "nodes",
      "from": "ArtOfficial"
    },
    {
      "resource": "DreamyVibes model",
      "url": "https://huggingface.co/CCP6/dreamyvibes/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Multiple control stacking example",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1357385562256048369",
      "type": "workflow",
      "from": "ameasure"
    },
    {
      "resource": "Standalone VACE model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1_VACE_1_3B_preview_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "DG model collection",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SkyReels A2 model",
      "url": "https://huggingface.co/Skywork/SkyReels-A2",
      "type": "model",
      "from": "s2k"
    },
    {
      "resource": "VACE official code",
      "url": "https://github.com/ali-vilab/VACE",
      "type": "repo",
      "from": "yo9o"
    },
    {
      "resource": "ComfyUI TexturePacker",
      "url": "https://github.com/kijai/ComfyUI-TexturePacker",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "Enricos compositor nodes",
      "url": "https://github.com/erosDiffusion/ComfyUI-enricos-nodes/tree/master",
      "type": "tool",
      "from": "IllumiReptilien"
    },
    {
      "resource": "Triton for Windows",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "PyTorch 2.8 installation guide",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1jdfs6e/automatic_installation_of_pytorch_28_nightly/",
      "type": "guide",
      "from": "DeZoomer"
    },
    {
      "resource": "Video Depth Anything ComfyUI node",
      "url": "https://github.com/yuvraj108c/ComfyUI-Video-Depth-Anything",
      "type": "node",
      "from": "David Snow"
    },
    {
      "resource": "VACE module",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1_VACE_1_3B_preview_bf16.safetensors",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "DG distilled models",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "model",
      "from": "JmySff"
    },
    {
      "resource": "TAEW preview file",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/taew2_1.safetensors",
      "type": "tool",
      "from": "Zuko"
    },
    {
      "resource": "Wan 1.3B LoRA search",
      "url": "https://huggingface.co/models?other=base_model:adapter:Wan-AI/Wan2.1-T2V-1.3B",
      "type": "search",
      "from": "seruva19"
    },
    {
      "resource": "SOTS Art Wan LoRA",
      "url": "https://huggingface.co/AlekseyCalvin/SOTS_Art_Wan1.3B_LoRA_rank256_bySilverAgePoets",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "MMAudio safetensors",
      "url": "https://huggingface.co/Kijai/MMAudio_safetensors/tree/main",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "AudioX model",
      "url": "https://huggingface.co/HKUSTAudio/AudioX",
      "type": "model",
      "from": "YatharthSharma"
    },
    {
      "resource": "ComfyUI-BS-Textchop",
      "url": "https://github.com/Burgstall-labs/ComfyUI-BS-Textchop",
      "type": "node",
      "from": "burgstall"
    },
    {
      "resource": "Civitai Green (SFW version)",
      "url": "https://civitai.green/search/models?sortBy=models_v9&query=wan%20lora",
      "type": "model repository",
      "from": "AJO"
    },
    {
      "resource": "3D character rig resource",
      "url": "https://toyxyz.gumroad.com/l/ciojz",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "SkyreelsA2 fp8 model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1_SkyreelsA2_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "VACE native ComfyUI discussion",
      "url": "https://ptb.discord.com/channels/1076117621407223829/1342763350815277067/1356778493983330525",
      "type": "discussion",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "SkyreelsA2 example workflow",
      "url": "Included with node update",
      "type": "workflow",
      "from": "Kijai"
    },
    {
      "resource": "A2-Bench dataset",
      "url": "https://huggingface.co/datasets/Skywork/A2-Bench",
      "type": "dataset",
      "from": "fredbliss"
    },
    {
      "resource": "Triton for Windows",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "tool",
      "from": "Lumi"
    },
    {
      "resource": "RTX 5090 Triton setup guide",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1jle4re/how_to_run_a_rtx_5090_50xx_with_triton_and_sage/",
      "type": "guide",
      "from": "ChronoKnight"
    },
    {
      "resource": "MatAnyone ComfyUI implementation",
      "url": "https://github.com/KytraScript/ComfyUI_MatAnyone_Kytra",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Frutiger Lora 14B",
      "url": "https://huggingface.co/dreamer8/FrutigerLora14B_Wan",
      "type": "lora",
      "from": "Dream Making"
    },
    {
      "resource": "Complete list of Civitai WAN loras",
      "url": "",
      "type": "resource",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Motion segmentation research",
      "url": "https://motion-seg.github.io/",
      "type": "research",
      "from": "yo9o"
    },
    {
      "resource": "Higgsfield AI using WAN loras",
      "url": "https://higgsfield.ai",
      "type": "service",
      "from": "Draken"
    },
    {
      "resource": "Ghibli LoRA for 1.3B",
      "url": "",
      "type": "lora",
      "from": "Piblarg"
    },
    {
      "resource": "Ghibli LoRA for 14B",
      "url": "https://civitai.com/models/1404755/studio-ghibli-style-wan21-t2v-14b",
      "type": "lora",
      "from": "Piblarg"
    },
    {
      "resource": "Reward LoRAs",
      "url": "https://huggingface.co/alibaba-pai/Wan2.1-Fun-Reward-LoRAs",
      "type": "model",
      "from": "Lumi"
    },
    {
      "resource": "RTX 5090 ComfyUI setup guide",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1jk2tcm/step_by_step_from_fresh_windows_11_install_how_to/",
      "type": "guide",
      "from": "Piblarg"
    },
    {
      "resource": "Musubi trainer frontend",
      "url": "https://github.com/maybleMyers/H1111",
      "type": "tool",
      "from": "Benjimon"
    },
    {
      "resource": "Upscale workflow",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1358558275733553345",
      "type": "workflow",
      "from": "HeadOfOliver"
    },
    {
      "resource": "Converted Reward LoRAs for ComfyUI",
      "url": "https://huggingface.co/Kijai/Wan2.1-Fun-Reward-LoRAs-comfy/tree/main",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Fun Control 1.3B model",
      "url": "https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "ComfyUI repackaged text encoders",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/text_encoders",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Higres and Aesthetics LoRAs",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "WanTraining control LoRA trainer",
      "url": "https://github.com/spacepxl/WanTraining",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "VACE feedback and issues",
      "url": "https://github.com/ali-vilab/VACE/issues/27",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Audio reactive spline example",
      "url": "https://youtu.be/_ioElEFP8TY",
      "type": "workflow",
      "from": "Kijai"
    },
    {
      "resource": "MatAnyone ComfyUI node",
      "url": "https://github.com/KytraScript/ComfyUI_MatAnyone_Kytra",
      "type": "node",
      "from": "JohnDopamine"
    },
    {
      "resource": "MatAnyone main repo",
      "url": "https://github.com/pq-yang/MatAnyone",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "DiffSynth reward LoRAs",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI",
      "type": "model",
      "from": "Flipping Sigmas"
    },
    {
      "resource": "VACE preview model",
      "url": "https://huggingface.co/ali-vilab/VACE-Wan2.1-1.3B-Preview/tree/main",
      "type": "model",
      "from": "xwsswww"
    },
    {
      "resource": "Video batch loading node",
      "url": "https://github.com/alt-key-project/comfyui-dream-video-batches",
      "type": "node",
      "from": "Mikkle"
    },
    {
      "resource": "VACE GitHub issue about inpainting",
      "url": "https://github.com/ali-vilab/VACE/issues/30",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Wan ecosystem summary document",
      "url": "",
      "type": "workflow",
      "from": "Adrien Toupet"
    },
    {
      "resource": "Native ComfyUI T2V workflow",
      "url": "https://comfyanonymous.github.io/ComfyUI_examples/wan/text_to_video_wan.json",
      "type": "workflow",
      "from": "MilesCorban"
    },
    {
      "resource": "WanWeightedControlToVideo node",
      "url": "https://github.com/wordbrew/comfyui-wan-control-nodes",
      "type": "tool",
      "from": "CJ"
    },
    {
      "resource": "Face swap with VACE example",
      "url": "https://www.reddit.com/r/comfyui/comments/1jugop6/a_more_rigorous_vace_faceswap_vaceswap_example/?sort=new",
      "type": "workflow",
      "from": "xwsswww"
    },
    {
      "resource": "FL_gemini nodes for prompt generation",
      "url": "http://github.com/filliptm/ComfyUI_Fill-Nodes",
      "type": "tool",
      "from": "Flipping Sigmas"
    },
    {
      "resource": "Loop Sequential Integer node for testing",
      "url": "https://github.com/justUmen/Bjornulf_custom_nodes?tab=readme-ov-file#56----loop-sequential-integer",
      "type": "tool",
      "from": "AJO"
    },
    {
      "resource": "Perfect loop LoRA",
      "url": "https://civitai.com/models/1264662?modelVersionId=1528511",
      "type": "model",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "ExVideo LoRA",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "Depth LoRA",
      "url": "https://huggingface.co/spacepxl/Wan2.1-control-loras/tree/main/1.3b/depth",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "Speed Control LoRA",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/blob/main/Wan2.1-1.3b-lora-speedcontrol-v1_new.safetensors",
      "type": "model",
      "from": "Johnjohn7855"
    },
    {
      "resource": "ReCamMaster-Wan2.1",
      "url": "https://huggingface.co/KwaiVGI/ReCamMaster-Wan2.1",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "AutoCrop Faces ComfyUI node",
      "url": "https://github.com/liusida/ComfyUI-AutoCropFaces",
      "type": "tool",
      "from": "xwsswww"
    },
    {
      "resource": "DG_Wan variants",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "ReCamMaster model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1_kwai_recammaster_1_3B_step20000_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "ReCamMaster branch",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/recammaster",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "14B CFG distilled model",
      "url": "https://huggingface.co/mktn/Wan2.1-14B-cfgdistill_test/tree/main",
      "type": "model",
      "from": "CJ"
    },
    {
      "resource": "Pusa V0.5 model",
      "url": "https://huggingface.co/RaphaelLiu/Pusa-V0.5",
      "type": "model",
      "from": "const username = undefined;"
    },
    {
      "resource": "AutoCropFaces node",
      "url": "https://github.com/liusida/ComfyUI-AutoCropFaces",
      "type": "node",
      "from": "MilesCorban"
    },
    {
      "resource": "Yolo Cropper node",
      "url": "https://github.com/tooldigital/ComfyUI-Yolo-Cropper",
      "type": "node",
      "from": "MilesCorban"
    },
    {
      "resource": "Extra noise detail LoRA",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/blob/main/dg_wan2_1_v1_3b_lora_extra_noise_detail_motion.safetensors",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "Kijai WanVideo models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main",
      "type": "model",
      "from": "harryB"
    },
    {
      "resource": "LatentPop style LoRA for Flux",
      "url": "https://huggingface.co/jakedahn/flux-latentpop",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "Multiple new boost LoRAs",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/commit/3fec64897d603a9a0a4c1f656a818079ceec095b",
      "type": "lora",
      "from": "DawnII"
    },
    {
      "resource": "SkyReels-A2 model",
      "url": "https://github.com/SkyworkAI/SkyReels-A2",
      "type": "model",
      "from": "AI_Fan"
    },
    {
      "resource": "Flat color LoRA",
      "url": "https://civitai.com/models/1132089?modelVersionId=1525407",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "DG_Boost Evol V3 model",
      "url": "",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "OptimalSteps implementation",
      "url": "https://github.com/bebebe666/OptimalSteps",
      "type": "tool",
      "from": "crinklypaper"
    },
    {
      "resource": "Co-tracker for mask tracking",
      "url": "https://github.com/facebookresearch/co-tracker",
      "type": "tool",
      "from": "mamad8"
    },
    {
      "resource": "ComfyUI-BETA-Helpernodes with crop and stitch nodes",
      "url": "https://github.com/Burgstall-labs/ComfyUI-BETA-Helpernodes",
      "type": "node",
      "from": "burgstall"
    },
    {
      "resource": "MatAnyone Kytra custom nodes",
      "url": "https://github.com/KytraScript/ComfyUI_MatAnyone_Kytra",
      "type": "node",
      "from": "Kytra"
    },
    {
      "resource": "Sameface fix Flux LoRA",
      "url": "https://civitai.com/models/766608/sameface-fix-flux-lora",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "DG_Model_Wan2_1_v1_3b_t2v_Boost_Final",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main/DG_Model_Wan2_1_v1_3b_t2v_Boost_Final",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "Pusa-VidGen with Wan2.1 roadmap",
      "url": "https://github.com/Yaofang-Liu/Pusa-VidGen",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "ComfyUI-GGUF",
      "url": "https://github.com/city96/ComfyUI-GGUF",
      "type": "repo",
      "from": "MilesCorban"
    },
    {
      "resource": "Wan2.1-T2V-14B-gguf models",
      "url": "https://huggingface.co/city96/Wan2.1-T2V-14B-gguf",
      "type": "model",
      "from": "MilesCorban"
    },
    {
      "resource": "ComfyUI face parsing",
      "url": "https://github.com/Ryuukeisyou/comfyui_face_parsing",
      "type": "repo",
      "from": "Johnjohn7855"
    },
    {
      "resource": "ComfyUI-RMBG",
      "url": "https://github.com/1038lab/ComfyUI-RMBG",
      "type": "repo",
      "from": "Johnjohn7855"
    },
    {
      "resource": "Eyes detection ADetailer",
      "url": "https://civitai.com/models/150925/eyes-detection-adetailer",
      "type": "model",
      "from": "xwsswww"
    },
    {
      "resource": "ComfyUI-AIR-Nodes",
      "url": "https://github.com/jonnydolake/ComfyUI-AIR-Nodes",
      "type": "repo",
      "from": "A.I.Warper"
    },
    {
      "resource": "ComfyUI-Inpaint-CropAndStitch",
      "url": "https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "ExVideo LoRA",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "SynCamMaster-Wan2.1",
      "url": "https://huggingface.co/KwaiVGI/SynCamMaster-Wan2.1",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "SynCamMaster main repo",
      "url": "https://github.com/KwaiVGI/SynCamMaster",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Animated Logo LoRA",
      "url": "https://civitai.com/models/1468212?modelVersionId=1660567",
      "type": "lora",
      "from": "Alisson Pereira"
    },
    {
      "resource": "Fun 14B FP16 conversion",
      "url": "https://huggingface.co/maybleMyers/wan_files_for_h1111/tree/main",
      "type": "model",
      "from": "Benjimon"
    },
    {
      "resource": "Lotus Depth models",
      "url": "https://huggingface.co/Kijai/lotus-comfyui/tree/main",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "Wan 2.1 Knowledge Base",
      "url": "https://nathanshipley.notion.site/Wan-2-1-Knowledge-Base-1d691e115364814fa9d4e27694e9468f",
      "type": "knowledge base",
      "from": "Nathan Shipley"
    },
    {
      "resource": "Ghibli Wan 1.3B LoRA",
      "url": "https://civitai.com/models/1474964/ghibli-wan-13b?modelVersionId=1668351",
      "type": "model",
      "from": "Piblarg"
    },
    {
      "resource": "Seamless Loop Workflow",
      "url": "https://civitai.com/models/1426572/wan-21-seamless-loop-test-workflow",
      "type": "workflow",
      "from": "gshawn"
    },
    {
      "resource": "CausVid",
      "url": "https://github.com/tianweiy/CausVid?tab=readme-ov-file",
      "type": "repo",
      "from": "Cubey"
    },
    {
      "resource": "Eyes In Camera LoRA",
      "url": "https://drive.google.com/file/d/18Po5RJR_3x1jaUDK4_nksK41QtVNV3be/view?usp=sharing",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "Wan2.1-FLF2V-14B-720P",
      "url": "https://huggingface.co/ypyp/Wan2.1-FLF2V-14B-720P",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "NormalCrafter",
      "url": "https://huggingface.co/spaces/Yanrui95/NormalCrafter",
      "type": "tool",
      "from": "DawnII"
    },
    {
      "resource": "Kijai fp8 version of FLF2V",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-FLF2V-14B-720P_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "FLF2V inference code",
      "url": "https://github.com/yupeng1111/Wan2.1/tree/first_last_frame_transformation",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "TheDirector workflow",
      "url": "https://civitai.com/models/1476469?modelVersionId=1670194",
      "type": "workflow",
      "from": "AJO"
    },
    {
      "resource": "FramePack I2V HY weights",
      "url": "https://huggingface.co/lllyasviel/FramePackI2V_HY/tree/main",
      "type": "model",
      "from": "JulienDoku"
    },
    {
      "resource": "FramePack repository",
      "url": "https://github.com/lllyasviel/FramePack",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "CausVid alternative approach",
      "url": "https://github.com/tianweiy/CausVid",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "Wan FLF2V 14B official",
      "url": "https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "VACE user experience improvement thread",
      "url": "https://github.com/ali-vilab/VACE/issues/27",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "WanVideo FLF2V fp8 model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-FLF2V-14B-720P_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "FramePackWrapper ComfyUI node",
      "url": "https://github.com/kijai/ComfyUI-FramePackWrapper",
      "type": "repo",
      "from": "zelgo_"
    },
    {
      "resource": "WanVideo FLF2V example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_FLF2V_720P_example_01.json",
      "type": "workflow",
      "from": "mamad8"
    },
    {
      "resource": "HunyuanVideo repackaged model",
      "url": "https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "UniAnimate-DiT",
      "url": "https://github.com/ali-vilab/UniAnimate-DiT",
      "type": "repo",
      "from": "yo9o"
    },
    {
      "resource": "Discord chat analysis with NotebookLM",
      "url": "https://youtu.be/kG81Kkal5XM",
      "type": "tool",
      "from": "Nathan Shipley"
    },
    {
      "resource": "Chat data files for NotebookLM",
      "url": "https://www.dropbox.com/scl/fo/r4t10eeok40qqapnhnzc3/ALncPlceSgiPB3ZKTILazg4?rlkey=62o9du85uk9fumd1hmc33rj7g&dl=0",
      "type": "tool",
      "from": "Nathan Shipley"
    },
    {
      "resource": "Fal.ai coupon for $20 credit",
      "url": "https://fal.ai/dashboard?coupon=YEAROFVIDEO",
      "type": "tool",
      "from": "NebSH"
    },
    {
      "resource": "SkyReels V2 I2V 14B 540P",
      "url": "https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-540P",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "SkyReels V2 T2V 14B 720P",
      "url": "https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-720P",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "UniAnimate-DiT repository",
      "url": "https://github.com/ali-vilab/UniAnimate-DiT",
      "type": "repo",
      "from": "amli"
    },
    {
      "resource": "Wan2.1-FLF2V-14B-720P GGUFs",
      "url": "https://huggingface.co/city96/Wan2.1-FLF2V-14B-720P-gguf",
      "type": "model",
      "from": "gshawn"
    },
    {
      "resource": "FramePack Gradio demo",
      "url": "https://github.com/lllyasviel/FramePack",
      "type": "repo",
      "from": "Benjimon"
    },
    {
      "resource": "ComfyUI-WanVideoWrapper",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "Wan developer Twitter AMA",
      "url": "https://x.com/StevenZhang66/with_replies",
      "type": "resource",
      "from": "JohnDopamine"
    },
    {
      "resource": "SkyReels V2 fp8 conversion",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-SkyReels-V2-I2V-14B-540P_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "UniAnimate-DiT",
      "url": "https://github.com/ali-vilab/UniAnimate-DiT",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI-FollowYourEmojiWrapper",
      "url": "https://github.com/kijai/ComfyUI-FollowYourEmojiWrapper",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "UniAnimate LoRA weights",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/UniAnimate-Wan2.1-14B-Lora-12000-fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SkyReels V2 I2V 540p model backup",
      "url": "https://huggingface.co/maybleMyers/wan_files_for_h1111/tree/main",
      "type": "model",
      "from": "Benjimon"
    },
    {
      "resource": "FLUX Redux style shaping workflow",
      "url": "https://gist.github.com/nathanshipley/7a9ac1901adde76feebe58d558026f68",
      "type": "workflow",
      "from": "Nathan Shipley"
    },
    {
      "resource": "FLUX Redux HuggingFace Space",
      "url": "https://huggingface.co/spaces/multimodalart/flux-style-shaping",
      "type": "tool",
      "from": "Nathan Shipley"
    },
    {
      "resource": "Extra noise detail motion LoRA",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/blob/main/dg_wan2_1_v1_3b_lora_extra_noise_detail_motion.safetensors",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "Fun Reward LoRAs",
      "url": "https://huggingface.co/alibaba-pai/Wan2.1-Fun-Reward-LoRAs/tree/main",
      "type": "lora",
      "from": "David Snow"
    },
    {
      "resource": "DepthAnythingV2 models",
      "url": "https://huggingface.co/Kijai/DepthAnythingV2-safetensors/tree/main",
      "type": "model",
      "from": "lostintranslation"
    },
    {
      "resource": "ComfyUI-DepthAnythingV2 node",
      "url": "https://github.com/kijai/ComfyUI-DepthAnythingV2",
      "type": "node",
      "from": "David Snow"
    },
    {
      "resource": "FreSca implementation",
      "url": "https://github.com/WikiChao/FreSca",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Benjimon's Wan files",
      "url": "https://huggingface.co/maybleMyers/wan_files_for_h1111/tree/main",
      "type": "model",
      "from": "Benjimon"
    },
    {
      "resource": "DepthAnything V2",
      "url": "https://github.com/kijai/ComfyUI-DepthAnythingV2",
      "type": "repo",
      "from": "David Snow"
    },
    {
      "resource": "SeedVR upscaler",
      "url": "https://iceclear.github.io/projects/seedvr/",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "STAR upscaler (CogvideoX based)",
      "url": "https://github.com/NJU-PCALab/STAR",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "CFG distilled Wan model",
      "url": "https://huggingface.co/mktn/Wan2.1-14B-cfgdistill_test/tree/main",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "ComfyUI-Sapiens",
      "url": "https://github.com/smthemex/ComfyUI_Sapiens",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "UniAnimate-W for ComfyUI",
      "url": "https://github.com/Isi-dev/ComfyUI-UniAnimate-W",
      "type": "repo",
      "from": "N0NSens"
    },
    {
      "resource": "Perturbed attention guidance",
      "url": "https://github.com/pamparamm/sd-perturbed-attention",
      "type": "repo",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "resource": "Characteristic guidance",
      "url": "https://github.com/redhottensors/ComfyUI-Prediction",
      "type": "repo",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "resource": "SkyReels V2 models",
      "url": "https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "SkyReels V2 code",
      "url": "https://github.com/SkyworkAI/SkyReels-V2",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "BiRefNet background removal",
      "url": "https://github.com/lldacing/ComfyUI_BiRefNet_ll",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "Inspyrenet-Rembg",
      "url": "https://github.com/john-mnz/ComfyUI-Inspyrenet-Rembg",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "MTB nodes",
      "url": "https://github.com/melMass/comfy_mtb",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "SkyCaptioner V1",
      "url": "https://huggingface.co/Skywork/SkyCaptioner-V1",
      "type": "model",
      "from": "Persoon"
    },
    {
      "resource": "SkyReels V2 I2V 1.3B",
      "url": "https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P/blob/main/model.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SkyReels collection",
      "url": "https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9",
      "type": "model collection",
      "from": "Seb"
    },
    {
      "resource": "Weight comparison script",
      "url": "https://github.com/maybleMyers/H1111/blob/main/compare_safetensors_weights.py",
      "type": "tool",
      "from": "Benjimon"
    },
    {
      "resource": "TorchInfo for model analysis",
      "url": "https://github.com/TylerYep/torchinfo",
      "type": "tool",
      "from": "Kytra"
    },
    {
      "resource": "SkyReels 14B fp16",
      "url": "https://huggingface.co/maybleMyers/wan_files_for_h1111/tree/main",
      "type": "model",
      "from": "Benjimon"
    },
    {
      "resource": "Groq ComfyUI node",
      "url": "https://github.com/jurdnisglobby/ComfyUI-Jurdns-Groq-Node",
      "type": "node",
      "from": "Colin"
    },
    {
      "resource": "SkyReels models on HuggingFace",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Skyreels",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "MAGI-1 repository",
      "url": "https://github.com/SandAI-org/MAGI-1",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "WanVideoWrapper DF testing commit",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/commit/65f5505fca115c23f3eb60689a2cf793d11b90df",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Chipmunk embedding caching",
      "url": "https://github.com/sandyresearch/chipmunk",
      "type": "tool",
      "from": "yi"
    },
    {
      "resource": "Kijai GitHub sponsorship",
      "url": "https://github.com/kijai",
      "type": "repo",
      "from": "Zuko"
    },
    {
      "resource": "DG models for ComfyUI",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI",
      "type": "model",
      "from": "N0NSens"
    },
    {
      "resource": "In-Context LoRA",
      "url": "https://github.com/ali-vilab/In-Context-LoRA",
      "type": "repo",
      "from": "N0NSens"
    },
    {
      "resource": "CropAndStitch nodes",
      "url": "https://github.com/lquesada/ComfyUI-Inpaint-CropAndStitch",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "BETA Helper nodes",
      "url": "https://github.com/Burgstall-labs/ComfyUI-BETA-Helpernodes",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "SkyReels DF example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_skyreels_diffusion_forcing_extension_example_01.json",
      "type": "workflow",
      "from": "aiacsp"
    },
    {
      "resource": "MAGI-1 model",
      "url": "https://huggingface.co/sand-ai/MAGI-1",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "I2V simple workflow",
      "url": "https://civitai.com/models/1309369/img-to-video-simple-workflow-wan21-or-gguf-or-lora-or-upscale-or-teacache",
      "type": "workflow",
      "from": "lostintranslation"
    },
    {
      "resource": "MediaPipe for FollowYourEmoji",
      "url": "https://github.com/kijai/ComfyUI-FollowYourEmojiWrapper",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "Phantom-Wan models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Phantom-Wan-1_3B_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "File Converter tool",
      "url": "https://github.com/Tichau/FileConverter/releases",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "Phantom research",
      "url": "https://github.com/Phantom-video/Phantom",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "SkyReels V2 model",
      "url": "https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P/tree/main",
      "type": "model",
      "from": "CDS"
    },
    {
      "resource": "Quantization research paper",
      "url": "https://arxiv.org/html/2503.06564v1",
      "type": "research",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "resource": "SkyCaptioner V1 model",
      "url": "https://huggingface.co/Skywork/SkyCaptioner-V1",
      "type": "model",
      "from": "fredbliss"
    },
    {
      "resource": "A2-Bench dataset",
      "url": "https://huggingface.co/datasets/Skywork/A2-Bench/blob/main/sample_v1.json",
      "type": "dataset",
      "from": "fredbliss"
    },
    {
      "resource": "Phantom model",
      "url": "https://huggingface.co/bytedance-research/Phantom",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "TAEW2_1 VAE file",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/taew2_1.safetensors",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "SkyReels V2 repository",
      "url": "https://github.com/SkyworkAI/SkyReels-V2/tree/main/skycaptioner_v1",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "ComfyUI QwenVL PR for SkyCaptioner",
      "url": "https://github.com/alexcong/ComfyUI_QwenVL/pull/18",
      "type": "tool",
      "from": "TK_999"
    },
    {
      "resource": "WanVideo A2 example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_skyreels_a2_example_01.json",
      "type": "workflow",
      "from": "Zuko"
    },
    {
      "resource": "VACE project page",
      "url": "https://ali-vilab.github.io/VACE-Page/",
      "type": "documentation",
      "from": "Zuko"
    },
    {
      "resource": "SkyReelV2-i2v-540p",
      "url": "",
      "type": "model",
      "from": "mamad8"
    },
    {
      "resource": "Resize To Closest node",
      "url": "",
      "type": "node",
      "from": "jellybean5361"
    },
    {
      "resource": "WanVideoWrapper DF sampler",
      "url": "",
      "type": "node",
      "from": "seitanism"
    },
    {
      "resource": "PyTorch nightly",
      "url": "",
      "type": "tool",
      "from": "lostintranslation"
    },
    {
      "resource": "Wan2_1-SkyReels-V2-DF-14B-540P_fp8_e4m3fn.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Skyreels/Wan2_1-SkyReels-V2-DF-14B-540P_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "seitanism"
    },
    {
      "resource": "Phantom model",
      "url": "https://huggingface.co/bytedance-research/Phantom",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "Wan2_1-SkyReels-V2-DF-1_3B-540P_fp32.safetensors",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Skyreels/Wan2_1-SkyReels-V2-DF-1_3B-540P_fp32.safetensors",
      "type": "model",
      "from": "Jemmo"
    },
    {
      "resource": "RealisDance-DiT",
      "url": "https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index",
      "type": "research",
      "from": "yi"
    },
    {
      "resource": "ComfyUI-FreeMemory",
      "url": "",
      "type": "tool",
      "from": "Mngbg"
    },
    {
      "resource": "SkyReels V2 Project",
      "url": "https://github.com/SkyworkAI/SkyReels-V2",
      "type": "repo",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Kijai SkyReels Models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Skyreels",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "Example DF workflow on Kijai's git",
      "url": "",
      "type": "workflow",
      "from": "boorayjenkins"
    },
    {
      "resource": "Diffusion Pipe SkyReels support",
      "url": "https://github.com/tdrussell/diffusion-pipe/commit/ed947867f363b7fa39596561ed2054b919cff2f6",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Skyreels V2 720p models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Skyreels",
      "type": "model",
      "from": "ezMan"
    },
    {
      "resource": "RealisDance-DiT",
      "url": "https://thefoxofsky.github.io/project_pages/RealisDance-DiT/index",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "Uni3C",
      "url": "https://ewrfcas.github.io/Uni3C/",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "WAN VACE native workflow",
      "url": "https://civitai.com/articles/13951/wan-video-vace-native-comfyui",
      "type": "workflow",
      "from": "V\u00e9role"
    },
    {
      "resource": "WinDirStat storage management",
      "url": "",
      "type": "tool",
      "from": "mamad8"
    },
    {
      "resource": "ComfyUI-FramePacking",
      "url": "https://github.com/rishipandey125/ComfyUI-FramePacking",
      "type": "tool",
      "from": "Rishi Pandey"
    },
    {
      "resource": "Wan2.1-Fun-V1.1-1.3B-Control-Camera",
      "url": "https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-1.3B-Control-Camera",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Sparge attention model for wan2.1",
      "url": "https://huggingface.co/Xiang-cd/sparge-attention-model-zoo/tree/main/want2v-1.3B",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "SpargeAttn repository",
      "url": "https://github.com/thu-ml/SpargeAttn",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "Fun 1.1 14B Control model",
      "url": "https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-1.3B-Control/blob/main/diffusion_pytorch_model.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Quantized 14B model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-Fun-V1_1-14B-Control_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "VideoX-Fun update commit",
      "url": "https://github.com/aigc-apps/VideoX-Fun/commit/fa953a04e95edbd357d43128bec0eb03cf9372f8",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "Sparge attention model",
      "url": "https://huggingface.co/Xiang-cd/sparge-attention-model-zoo/tree/main/want2v-1.3B",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "Step1X-Edit model",
      "url": "https://github.com/stepfun-ai/Step1X-Edit",
      "type": "repo",
      "from": "mamad8"
    },
    {
      "resource": "Wan 2.1 Knowledge Base",
      "url": "https://nathanshipley.notion.site/Wan-2-1-Knowledge-Base-1d691e115364814fa9d4e27694e9468f?pvs=74",
      "type": "tool",
      "from": "Nathan Shipley"
    },
    {
      "resource": "NotebookLM chat scraping demo",
      "url": "https://www.youtube.com/watch?v=kG81Kkal5XM&feature=youtu.be",
      "type": "tool",
      "from": "Nathan Shipley"
    },
    {
      "resource": "ComfyUIMini",
      "url": "https://github.com/ImDarkTom/ComfyUIMini",
      "type": "tool",
      "from": "boorayjenkins"
    },
    {
      "resource": "Comfy Portal iOS app",
      "url": "https://apps.apple.com/us/app/comfy-portal/id6741044736",
      "type": "tool",
      "from": "boorayjenkins"
    },
    {
      "resource": "WAN VACE native workflow",
      "url": "https://civitai.com/models/1508309/wan-vace-native-workflow",
      "type": "workflow",
      "from": "V\u00e9role"
    },
    {
      "resource": "Civitai SFW version",
      "url": "https://civitai.green/",
      "type": "tool",
      "from": "yi"
    },
    {
      "resource": "Phantom 1.3B model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Phantom-Wan-1_3B_fp32.safetensors",
      "type": "model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "resource": "SkyReels V2 DF model",
      "url": "https://huggingface.co/Skywork/SkyReels-V2-DF-1.3B-540P",
      "type": "model",
      "from": "Nokai"
    },
    {
      "resource": "SkyReels V2 I2V model",
      "url": "https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P",
      "type": "model",
      "from": "Nokai"
    },
    {
      "resource": "ComfyUI-wanBlockswap",
      "url": "https://github.com/orssorbit/ComfyUI-wanBlockswap",
      "type": "node",
      "from": "Draken"
    },
    {
      "resource": "ViewComfy",
      "url": "https://github.com/ViewComfy/ViewComfy",
      "type": "tool",
      "from": "A.I.Warper"
    },
    {
      "resource": "DiffSynth Studio LoRA",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI",
      "type": "model",
      "from": "N0NSens"
    },
    {
      "resource": "DG Wan Fun models",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main/DG_Wan2_1_V1.3b_Fun_Inp",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "DG Custom Nodes",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main/DG_Wan2_1_V1.3b_Fun_Inp/DG_Custom_Node_For_Wan2_1_I2V_Double_Prompts",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "Skyreels V2 I2V",
      "url": "https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P",
      "type": "model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "FantasyTalking",
      "url": "https://github.com/Fantasy-AMAP/fantasy-talking",
      "type": "repo",
      "from": "Bingo"
    },
    {
      "resource": "CausVid model",
      "url": "https://huggingface.co/lightx2v/Wan2.1-T2V-14B-CausVid",
      "type": "model",
      "from": "mkt"
    },
    {
      "resource": "TransPixeler Wan support",
      "url": "https://github.com/wileewang/TransPixeler/tree/wan",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Throwaway 1.3 LoRAs",
      "url": "https://huggingface.co/CCP6/throwaway-1.3-lora-for-kicks/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Latentpop Flux LoRA",
      "url": "https://huggingface.co/jakedahn/flux-latentpop",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "DG Wan2.1 Fun_Inp models",
      "url": "https://huggingface.co/Evados/DiffSynth-Studio-Lora-Wan2.1-ComfyUI/tree/main/DG_Wan2_1_V1.3b_Fun_Inp",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "Wan2.1 Fun collection",
      "url": "https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17",
      "type": "model",
      "from": "\u7247\u30e8\u4ea1\u4ea1\u4e39\u7247"
    },
    {
      "resource": "Tile control LoRA",
      "url": "https://huggingface.co/spacepxl/Wan2.1-control-loras/blob/main/1.3b/tile/wan2.1-1.3b-control-lora-tile-v1.0_comfy.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.1 base model",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SkyReels 1.3B I2V",
      "url": "https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P",
      "type": "model",
      "from": "Colin"
    },
    {
      "resource": "Fun Camera Control 14B",
      "url": "https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-14B-Control-Camera",
      "type": "model",
      "from": "Zuko"
    },
    {
      "resource": "Google RealEstate10K dataset",
      "url": "https://google.github.io/realestate10k/download.html",
      "type": "dataset",
      "from": "Kijai"
    },
    {
      "resource": "Blender camera control addon",
      "url": "https://toyxyz.gumroad.com/l/ciojz",
      "type": "tool",
      "from": "toyxyz"
    },
    {
      "resource": "ComfyUI segment anything 2 video example",
      "url": "https://github.com/kijai/ComfyUI-segment-anything-2/blob/main/example_workflows/points_segment_video_example.json",
      "type": "workflow",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "AudioX repository",
      "url": "https://github.com/ZeyueT/AudioX",
      "type": "repo",
      "from": "Jemmo"
    },
    {
      "resource": "RTX 5090 setup guide",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1jle4re/how_to_run_a_rtx_5090_50xx_with_triton_and_sage/",
      "type": "guide",
      "from": "Baku"
    },
    {
      "resource": "BIRME image resizer",
      "url": "https://www.birme.net/?target_width=1024&target_height=1024",
      "type": "tool",
      "from": "JohnDopamine"
    }
  ],
  "limitations": [
    {
      "limitation": "Fun inpaint model terrible at T2V",
      "details": "Produces grey outputs and poor results when used for text-to-video generation",
      "from": "Kijai"
    },
    {
      "limitation": "Control model issues with out-of-frame objects",
      "details": "When character gets up from sofa, reveals gaping hole where they sat - unfun I2V was more consistent",
      "from": "miko"
    },
    {
      "limitation": "Enhanced-a-video too slow with native",
      "details": "Too big of a speed hit with native workflow, wrapper doesn't support fp8 scaled",
      "from": "Doctor Shotgun"
    },
    {
      "limitation": "Finger generation still poor",
      "details": "Even with improved 1.3B setup at 8 steps, still can't do fingers properly",
      "from": "David Snow"
    },
    {
      "limitation": "VACE only works with video input currently",
      "details": "Reference image functionality not yet implemented, only input frames work as control signal",
      "from": "Kijai"
    },
    {
      "limitation": "Inpaint model changes unmasked areas",
      "details": "When using fun inpaint model, unmasked areas also change during inpainting process",
      "from": "wayward_18"
    },
    {
      "limitation": "Video Depth Anything OOM issues",
      "details": "Need to reduce input size to avoid out of memory, can OOM quickly with longer sequences",
      "from": "David Snow"
    },
    {
      "limitation": "VACE is significantly slower than standard models",
      "details": "Approximately 2x slower due to 15 additional blocks running every step",
      "from": "multiple users"
    },
    {
      "limitation": "High VRAM consumption",
      "details": "832x480x45 frames takes 19GB VRAM, can use up to 31GB",
      "from": "Kijai"
    },
    {
      "limitation": "Reference image likeness inconsistent",
      "details": "Doesn't always maintain close likeness without proper preprocessing",
      "from": "Ro"
    },
    {
      "limitation": "LoRA compatibility issues",
      "details": "LoRAs work but not as well as with standard Wan models",
      "from": "Kytra"
    },
    {
      "limitation": "Block swapping ineffective",
      "details": "Won't help much with VRAM usage and can cause errors",
      "from": "Kijai"
    },
    {
      "limitation": "VACE much heavier than base 1.3B model",
      "details": "Surprisingly heavy despite being based on 1.3B, requires block swap on 4090 to avoid OOM",
      "from": "ingi // SYSTMS"
    },
    {
      "limitation": "Model limited to 81 frames by default",
      "details": "Can be forced to do more with control but doesn't work properly",
      "from": "Kijai"
    },
    {
      "limitation": "Reference images don't work with context windows",
      "details": "Ref image isn't passed for context windows processing",
      "from": "Kijai"
    },
    {
      "limitation": "Poor quality with reference + control video combination",
      "details": "May work better using either reference OR control, not both",
      "from": "seitanism"
    },
    {
      "limitation": "Start/end frame feature is flawed with I2V model",
      "details": "Hacky way to force it, never worked perfectly as model not trained for it",
      "from": "Kijai"
    },
    {
      "limitation": "VACE reference image flexibility",
      "details": "Extremely locked into reference image angle and position, not very flexible for pose changes",
      "from": "AJO"
    },
    {
      "limitation": "fp8_fast model compatibility",
      "details": "fp8_fast models not supported on cards with compute capability < 9.0 (RTX 3060 and lower)",
      "from": "Faux"
    },
    {
      "limitation": "Control reduces coherence",
      "details": "Anytime you give control input, you lose some coherence in the output",
      "from": "Piblarg"
    },
    {
      "limitation": "VACE mask input unclear",
      "details": "Purpose of mask input in VACE not well understood, works better without it",
      "from": "Kijai"
    },
    {
      "limitation": "VACE doesn't work with Fun models",
      "details": "Not compatible with Fun Control system",
      "from": "Kijai"
    },
    {
      "limitation": "VACE has issues with full reference images",
      "details": "Model doesn't work well with full reference like that, needs separated subjects or background removed",
      "from": "Kijai"
    },
    {
      "limitation": "Frame count limitation",
      "details": "More than 81 frames adds artifacts, works best at 81 frames",
      "from": "seitanism"
    },
    {
      "limitation": "Limited character consistency scenarios",
      "details": "Can't use reference image like PuLID for face override at any angle",
      "from": "AJO"
    },
    {
      "limitation": "Sapiens normals single resolution support",
      "details": "Only supports single input resolution, doesn't work well in landscape orientations",
      "from": "Kijai"
    },
    {
      "limitation": "SkyReels-A2 lacks control features",
      "details": "No control capabilities compared to VACE, making it less useful for creative workflows",
      "from": "Kijai"
    },
    {
      "limitation": "DG_Wan models poor LoRA compatibility",
      "details": "Don't respond to LoRAs as well as base models, affecting V2V quality",
      "from": "David Snow"
    },
    {
      "limitation": "VACE is memory intensive",
      "details": "Requires significant VRAM, 12GB cards struggle with 81 frames even with block swapping",
      "from": "Ashtar"
    },
    {
      "limitation": "Blurred masks don't work well with VACE",
      "details": "Blurred mask inputs appear to not be functioning properly",
      "from": "Zuko"
    },
    {
      "limitation": "No spline control for varying control strength over time",
      "details": "Cannot reduce control strength over time while keeping reference at full strength",
      "from": "Hashu"
    },
    {
      "limitation": "Mediapipe face detection inconsistent",
      "details": "Fails to detect obvious faces in about half of frames",
      "from": "David Snow"
    },
    {
      "limitation": "Model generates different animation initially",
      "details": "First few seconds show different animation in preview before matching prompt and introducing reference",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Bounding box control randomness in T2V",
      "details": "Often doesn't create anything in bounding box on first frame with T2V, more useful with I2V",
      "from": "Kijai"
    },
    {
      "limitation": "VACE blocks memory usage",
      "details": "Blocks themselves are tiny, but huge input size uses lots of memory",
      "from": "Kijai"
    },
    {
      "limitation": "TexturePacker canvas size limitation",
      "details": "Limited to 512 squares for canvas, meant for texture packing (e.g., 1024x512)",
      "from": "Kijai"
    },
    {
      "limitation": "No frame-range LoRA application",
      "details": "Cannot apply LoRA effects to specific frame ranges only",
      "from": "Kijai"
    },
    {
      "limitation": "ComfyUI animation system",
      "details": "Lacks proper animation system and keyframe settings options",
      "from": "David Snow"
    },
    {
      "limitation": "VACE inpainting barely preserves likeness in reference image",
      "details": "Using pose or no input_frames preserves likeness fine, but inpainting struggles",
      "from": "Zuko"
    },
    {
      "limitation": "5 steps is very poor quality in base Wan",
      "details": "Need higher step counts for decent results with base model",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Can't use reference on one VACE and not another when chaining",
      "details": "Using reference adds latent, so all chained VACE nodes need reference (can use blank grey)",
      "from": "Kijai"
    },
    {
      "limitation": "Text gets distorted in Wan",
      "details": "Never successfully preserved text shape",
      "from": "David Snow"
    },
    {
      "limitation": "High motion is problematic",
      "details": "Vid2vid works great for static shots but struggles with movement-heavy content",
      "from": "3Dmindscaper2000"
    },
    {
      "limitation": "VACE doesn't play well with overlayed controlnets",
      "details": "Causes artifacts like pose dots bleeding through in output",
      "from": "traxxas25"
    },
    {
      "limitation": "TeaCache incompatible with ancestral/SDE samplers",
      "details": "Samplers that do substeps and add noise don't work with TeaCache",
      "from": "Kijai"
    },
    {
      "limitation": "TeaCache pointless with low step counts",
      "details": "Causes mosaic/noisy outputs when used with few steps",
      "from": "Kijai"
    },
    {
      "limitation": "Start/end images don't work with context options",
      "details": "Some windows won't have start/end frames in context mode",
      "from": "Kijai"
    },
    {
      "limitation": "SageAttention auto mode causes black output with I2V",
      "details": "Suspected issue with cross attention in fp8 models",
      "from": "Kijai"
    },
    {
      "limitation": "LoRAs may not work perfectly with SkyreelsA2",
      "details": "Base model weights are changed so existing LoRAs won't be perfect",
      "from": "Kijai"
    },
    {
      "limitation": "14B VACE will be very heavy to run",
      "details": "Even on 4090 will likely be unusable due to double compute requirement",
      "from": "Draken"
    },
    {
      "limitation": "Fun models lack basic concepts",
      "details": "Missing things like 'red panda' from training, much worse than base 1.3B",
      "from": "Kijai"
    },
    {
      "limitation": "Control networks destroy character consistency",
      "details": "Adding any control network inevitably breaks consistency in faces and clothing",
      "from": "wange1002"
    },
    {
      "limitation": "VACE human subjects get generalized",
      "details": "Tend to look similar within categories (asian woman, blonde woman, etc.)",
      "from": "ArtOfficial"
    },
    {
      "limitation": "Can't search Discord by file attachment extension",
      "details": "User frustrated about search limitations",
      "from": "fredbliss"
    },
    {
      "limitation": "SkyReels v2 can't do video-to-video",
      "details": "No way to input video frames for v2v",
      "from": "JmySff"
    },
    {
      "limitation": "VACE doesn't understand normal maps well",
      "details": "Requires desaturating or other processing to work",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows show transitions in backgrounds",
      "details": "Uncontrolled background areas change between windows",
      "from": "JmySff"
    },
    {
      "limitation": "Native ComfyUI Wan can't use more than 1 clip embed for SkyReels",
      "details": "Fails to keep images besides the first one",
      "from": "Kijai"
    },
    {
      "limitation": "VACE only works with 1.3B models",
      "details": "Cannot use VACE module with 14B models currently",
      "from": "Kijai"
    },
    {
      "limitation": "VACE struggles with blurred masks",
      "details": "Faded/blurred masks leave artifacts on edges, solid masks work better",
      "from": "A.I.Warper"
    },
    {
      "limitation": "Combined controls require reduced strength",
      "details": "Using multiple control inputs at full strength doesn't work, need to reduce to 0.5 or less",
      "from": "Kijai"
    },
    {
      "limitation": "14B models not compatible with preprocessors",
      "details": "Cannot use tiled diffusion or similar preprocessing with 14B variants",
      "from": "David Snow"
    },
    {
      "limitation": "1.3B quality issues",
      "details": "1.3B can look awful without proper LoRAs, especially for stylized content",
      "from": "froyo"
    },
    {
      "limitation": "Complex pose handling",
      "details": "Model struggles with characters doing weird/complex poses like Roger Rabbit",
      "from": "David Snow"
    },
    {
      "limitation": "Reward LoRAs compatibility",
      "details": "Reward LoRAs likely don't work with regular models, only with Fun variants",
      "from": "Lumi"
    },
    {
      "limitation": "VACE 8GB VRAM limit",
      "details": "VACE runs out of memory on 8GB RTX 2080 for 640x360x10 frames without optimizations",
      "from": "xwsswww"
    },
    {
      "limitation": "Reference images don't work with DG models",
      "details": "Reference image functionality is not available when using distilled guidance models",
      "from": "JmySff"
    },
    {
      "limitation": "Fun Control lacks VACE flexibility",
      "details": "Fun Control can't do overlapping controls like VACE, and no way to set inpaint mask strength in native nodes",
      "from": "JmySff"
    },
    {
      "limitation": "14B model extremely slow on 8GB",
      "details": "14B model takes 3 hours for 17 frames on laptop 4070, making it impractical",
      "from": "froyo"
    },
    {
      "limitation": "Tiled VACE encoding very slow",
      "details": "Tiled encoding works but is incredibly slow compared to normal encoding",
      "from": "Kijai"
    },
    {
      "limitation": "Fun 1.3B quality is poor",
      "details": "Much worse than VACE 1.3B in general quality",
      "from": "Kijai"
    },
    {
      "limitation": "VACE has inference time overhead",
      "details": "Overhead on inference time compared to base models",
      "from": "Kijai"
    },
    {
      "limitation": "Multiple Fun Control models multiply inference time",
      "details": "Would mean running 2nd full model alongside, multiplying inference time",
      "from": "Kijai"
    },
    {
      "limitation": "Fun Control chaining doesn't work",
      "details": "Only last control node in chain applies, others are ignored",
      "from": "Kijai"
    },
    {
      "limitation": "Reward LoRAs reduce motion",
      "details": "LoRAs for quality improvement tend to kill movement/motion",
      "from": "Jas"
    },
    {
      "limitation": "VACE reference and inpainting are all part of same context",
      "details": "Makes it difficult to use multiple VACE encodes effectively as they interfere with each other",
      "from": "Kijai"
    },
    {
      "limitation": "Text prompt does nearly nothing at full VACE strength",
      "details": "When using control + reference frames, text prompts have minimal influence",
      "from": "Draken"
    },
    {
      "limitation": "Gemini API outputs images one at a time",
      "details": "Even when requesting multiple images, they come back individually without context to each other",
      "from": "Fill"
    },
    {
      "limitation": "Character likeness drift on longer videos",
      "details": "81 frame videos show likeness drift, especially noticeable between reference/first frames and later generated frames",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "limitation": "Slow motion tendency in outputs",
      "details": "90% of outputs have slow motion with very little character movement",
      "from": "Jas"
    },
    {
      "limitation": "Wan doesn't work well over 81 frames",
      "details": "Performance and quality issues beyond 81 frames",
      "from": "Draken"
    },
    {
      "limitation": "1.3B struggles with small details",
      "details": "Small things like hands and distant eyes turn into blobs, close-up faces have decent eyes but far shots they become blobs",
      "from": "Draken"
    },
    {
      "limitation": "VACE tends to drift from start image",
      "details": "Vace tends to drift from the start image after a few frames, which is why many use additional reference frames",
      "from": "David Snow"
    },
    {
      "limitation": "Native ComfyUI lacks VACE support",
      "details": "Native ComfyUI doesn't have VACE support yet, would take effort to implement properly",
      "from": "Kijai"
    },
    {
      "limitation": "Fun-Control model quality",
      "details": "Prone to heinous render artifacts, glitches and artifacts galore",
      "from": "David Snow"
    },
    {
      "limitation": "Control lora strength at high denoise",
      "details": "Control lora not strong enough to deal with 1.0 denoise setting",
      "from": "David Snow"
    },
    {
      "limitation": "Reward loras introduce fireflies",
      "details": "Consistently introduce fireflies, need to disable when using Fun-Control",
      "from": "David Snow"
    },
    {
      "limitation": "LoRA incompatibility with Fun models",
      "details": "LoRAs made with standard models don't work well on Fun models, VACE won't work with Fun models",
      "from": "Kijai"
    },
    {
      "limitation": "ReCamMaster struggles against video motion",
      "details": "Works better when camera motion aligns with existing video motion",
      "from": "Kijai"
    },
    {
      "limitation": "Camera paths only work for max 81 frames",
      "details": "Camera control paths have a limitation of 81 frames maximum",
      "from": "Kijai"
    },
    {
      "limitation": "VACE doesn't like masks with holes",
      "details": "Input masks with holes in them cause issues with VACE",
      "from": "IllumiReptilien"
    },
    {
      "limitation": "Style drift over frames",
      "details": "Tendency to move away from input image style and shift from 2D to 3D over 10+ frames",
      "from": "David Snow"
    },
    {
      "limitation": "Camera movement limitations",
      "details": "360 degree camera movements don't work, even 180 degrees may not work - turns back after 90 degrees",
      "from": "Kijai"
    },
    {
      "limitation": "Reference image burn-in",
      "details": "First couple frames can have reference image burned into them when using VACE",
      "from": "A.I.Warper"
    },
    {
      "limitation": "Base weights may not work with VACE",
      "details": "Camera embeds definitely don't work with other weights",
      "from": "Kijai"
    },
    {
      "limitation": "2-step generation quality too low",
      "details": "While LoRA effects work best at 2 steps, quality is insufficient for practical use",
      "from": "David Snow"
    },
    {
      "limitation": "Multiple VACE encodes cause issues",
      "details": "Using multiple VACE encodes for different controls can cause tensor mismatch errors and raging inferno results",
      "from": "David Snow"
    },
    {
      "limitation": "VACE not good with certain art styles",
      "details": "VACE struggles with stylized content like flat color/cartoon styles",
      "from": "Kijai"
    },
    {
      "limitation": "Recam requires existing video",
      "details": "Cannot use Recam with single image input - needs video generation first or duplicate single image",
      "from": "David Snow"
    },
    {
      "limitation": "TeaCache conflicts with context windows",
      "details": "TeaCache doesn't work well with context windows, causes issues",
      "from": "Kijai"
    },
    {
      "limitation": "OptimalSteps only works well with 14B models",
      "details": "For 1.3B models, OptimalSteps gives worse results each time",
      "from": "Kijai"
    },
    {
      "limitation": "OptimalStepsSchedule is situational and not useful",
      "details": "Way too situational to be useful, doesn't work with unipc sampler",
      "from": "Kijai"
    },
    {
      "limitation": "1.3B model struggles with complex prompts",
      "details": "1.3B kinda struggles with prompts anyway, not suitable for complex descriptions",
      "from": "Kijai"
    },
    {
      "limitation": "14B LoRAs not compatible with 1.3B models",
      "details": "LoRAs trained for 14B models won't work with 1.3B models",
      "from": "David Snow"
    },
    {
      "limitation": "WAN has heavy bias towards certain face types",
      "details": "Very heavy bias towards certain type of face, nearly identical to default 'flux face'",
      "from": "David Snow"
    },
    {
      "limitation": "WAN doesn't do much camera motion",
      "details": "Model limitation for camera movement effects",
      "from": "Johnjohn7855"
    },
    {
      "limitation": "VACE doesn't work with native ComfyUI nodes currently",
      "details": "Need Kijai's wrapper, not compatible with regular ComfyUI nodes at this time",
      "from": "Kijai"
    },
    {
      "limitation": "Car motion direction issues",
      "details": "Models frequently get car direction wrong, sending them backwards, sideways, or making them transform",
      "from": "David Snow"
    },
    {
      "limitation": "VACE doesn't work well with reference images for V2V",
      "details": "VACE model doesn't respect reference image input for video-to-video tasks",
      "from": "Pol"
    },
    {
      "limitation": "Lotus depth causes video flickering",
      "details": "While good for images, lotus depth creates flickering artifacts in video generation",
      "from": "David Snow"
    },
    {
      "limitation": "BF16 to FP16 conversion loses quality",
      "details": "Converting from BF16 to FP16 results in loss of both range and precision",
      "from": "Kijai"
    },
    {
      "limitation": "1.3B and 14B LoRAs not compatible",
      "details": "LoRAs trained for different model sizes are completely incompatible",
      "from": "Kijai"
    },
    {
      "limitation": "Fun Control doesn't support end images",
      "details": "Both Fun Control models break when using end image, only VACE supports start/end images",
      "from": "DawnII"
    },
    {
      "limitation": "Color matching isn't perfect for loops",
      "details": "Still has issues with color shifting despite color matching attempts",
      "from": "Cubey"
    },
    {
      "limitation": "Wan fixed at 16fps",
      "details": "No fps conditioning available, requires interpolation for other frame rates",
      "from": "Kijai"
    },
    {
      "limitation": "RTX 8000 very slow for inference",
      "details": "2.5-3 hours for generations that take 1 hour on 4090",
      "from": "Benjimon"
    },
    {
      "limitation": "FLF2V does 'middle step' transitions often",
      "details": "Often creates intermediate steps between start and end frames rather than direct transitions",
      "from": "Kijai"
    },
    {
      "limitation": "720p model may be inferior to 480p",
      "details": "Observations suggest 720p model not as good as 480p, possibly due to training on less data",
      "from": "MilesCorban"
    },
    {
      "limitation": "VACE doesn't understand plan changes in video unlike Fun",
      "details": "VACE has trouble with dynamic changes compared to Fun model",
      "from": "N0NSens"
    },
    {
      "limitation": "SynCamMaster subject positioning is broken",
      "details": "Places subjects at exact same position regardless of prompt, though background angles work correctly",
      "from": "Kijai"
    },
    {
      "limitation": "Orbit camera can't handle over 90 degrees",
      "details": "Model struggles with camera rotations greater than 90 degrees",
      "from": "Kijai"
    },
    {
      "limitation": "FramePack limited prompt complexity",
      "details": "Works well for simple prompts like dancing, but not complex scenarios like 1-minute Tom & Jerry episodes",
      "from": "JohnDopamine"
    },
    {
      "limitation": "Motion still context-windowy in FramePack",
      "details": "Despite generating longer videos, motion characteristics still resemble HYV's context window limitations",
      "from": "Cubey"
    },
    {
      "limitation": "Early gens more brave, later stuff stiffens",
      "details": "In FramePack generation, earlier chunks show more dynamic motion while later parts become more static",
      "from": "Cubey"
    },
    {
      "limitation": "Quality degradation in consecutive i2v generations",
      "details": "Running multiple different images through i2v in queue causes permanent quality degradation until cache cleared",
      "from": "lostintranslation"
    },
    {
      "limitation": "FLF2V requires precise prompting",
      "details": "Model seems to need very specific prompt formulation to work well",
      "from": "Kijai"
    },
    {
      "limitation": "FramePack has texture smoothing issues",
      "details": "Demo videos show weird smoothing on textures, gives pause about texture quality",
      "from": "Kytra"
    },
    {
      "limitation": "Eye movement is particularly hard across all models",
      "details": "Capturing eye movement is challenging for VACE, Fun, and vanilla 1.3B",
      "from": "David Snow"
    },
    {
      "limitation": "VACE memory requirements limit long sequences",
      "details": "Extra 15 blocks for VACE make longer sequences difficult due to VRAM constraints",
      "from": "A.I.Warper"
    },
    {
      "limitation": "Fun Control too rigid for illustrative content",
      "details": "Controlnet adherence too strong for artistic/illustrative use cases",
      "from": "Rishi Pandey"
    },
    {
      "limitation": "FramePack generates fixed backgrounds",
      "details": "Backgrounds don't move and appear static, making videos look unnatural",
      "from": "wange1002"
    },
    {
      "limitation": "Fast motion handling issues",
      "details": "VACE struggles with fast motion, step count doesn't help much",
      "from": "N0NSens"
    },
    {
      "limitation": "Quality degradation on longer generations",
      "details": "Longer video generations show increasing quality degradation",
      "from": "N0NSens"
    },
    {
      "limitation": "VACE adds unwanted warm colorization",
      "details": "Model consistently adds warm tones that may not match intended output",
      "from": "xwsswww"
    },
    {
      "limitation": "Models rarely trained on people smiling",
      "details": "AI video models seem to have limited training on smiling subjects",
      "from": "A.I.Warper"
    },
    {
      "limitation": "WAN can't simply generate more frames with weights alone",
      "details": "Needs new inference code and scheduler for longer generation",
      "from": "Kijai"
    },
    {
      "limitation": "VACE struggles with extreme eye movements",
      "details": "Cannot pick up rapid eye crossing movements that Fun-control can handle",
      "from": "David Snow"
    },
    {
      "limitation": "SD1.5 era pose preprocessors are really bad",
      "details": "Poor quality compared to newer methods like mediapipe",
      "from": "David Snow"
    },
    {
      "limitation": "Characters won't shut up with certain LoRAs",
      "details": "Mouths constantly in motion unless heavily constrained",
      "from": "David Snow"
    },
    {
      "limitation": "UniAnimate won't work with 8GB cards",
      "details": "Memory requirements too high for 8GB VRAM",
      "from": "xwsswww"
    },
    {
      "limitation": "1.3B model poor skin rendering",
      "details": "Human skin always seems to degrade, becomes airbrushed looking",
      "from": "David Snow"
    },
    {
      "limitation": "Fast motion handling",
      "details": "Model doesn't handle fast motion well in general, especially hands",
      "from": "David Snow"
    },
    {
      "limitation": "VACE style consistency with new surfaces",
      "details": "Restyling breaks when new unseen surfaces appear in rolling objects",
      "from": "sneako1234"
    },
    {
      "limitation": "Reference images cause artifacts in VACE",
      "details": "Causes flashes at start and glitches throughout video with no known workaround",
      "from": "David Snow"
    },
    {
      "limitation": "CFG distilled models don't work with low steps",
      "details": "Produces total garbage even at 8 steps, needs different sampler or approach",
      "from": "Kijai"
    },
    {
      "limitation": "UniAnimate doesn't do face pose",
      "details": "Doesn't support OpenPose face detection, only body pose",
      "from": "Kijai"
    },
    {
      "limitation": "Sapiens has single resolution limitation",
      "details": "Annoying limitation that restricts flexibility",
      "from": "Kijai"
    },
    {
      "limitation": "1.3B model has distortion issues",
      "details": "Temporal attention problems causing weird distortions, doesn't happen with 14B I2V",
      "from": "PirateWolf"
    },
    {
      "limitation": "Wan models limited to 720p and 480p resolutions",
      "details": "Native resolution limitations",
      "from": "N0NSens"
    },
    {
      "limitation": "UniAnimate doesn't accept face input",
      "details": "Cannot be used for lip sync applications",
      "from": "PolygenNoa"
    },
    {
      "limitation": "DWPose detection failures",
      "details": "Not perfect and sometimes can't detect pose, will fail on some videos",
      "from": "Kijai"
    },
    {
      "limitation": "VACE draws unwanted nipples on smooth surfaces",
      "details": "Cannot be negative prompted out, sees breast shapes and draws nipples",
      "from": "David Snow"
    },
    {
      "limitation": "UniAnimate won't work with VACE currently",
      "details": "May be possible when 14B VACE comes out",
      "from": "Kijai"
    },
    {
      "limitation": "SkyReels struggles with abstract content",
      "details": "Human-centric model has trouble with non-human subjects",
      "from": "Kijai"
    },
    {
      "limitation": "Color consistency issues",
      "details": "Struggles to keep colors on some input images",
      "from": "Kijai"
    },
    {
      "limitation": "Diffusion Forcing not fully implemented",
      "details": "Requires new sampling process, not yet supported in wrapper",
      "from": "Kijai"
    },
    {
      "limitation": "Control LoRAs don't work with VACE on same step",
      "details": "Control LoRA changes input channels, VACE only handles 16 channels",
      "from": "Kijai"
    },
    {
      "limitation": "Wan LoRAs limited compatibility",
      "details": "1.3B style LoRAs work somewhat but not well, 14B motion LoRAs don't work with 1.3B",
      "from": "Jas"
    },
    {
      "limitation": "Limited style LoRAs for 1.3B model",
      "details": "Less than five good style loras available, most are 'basically just tits and weird kink shit'",
      "from": "David Snow"
    },
    {
      "limitation": "1.3B DF quality degradation in long generations",
      "details": "1.3B model starts to lose quality in extended generations, works better with videos it generates itself",
      "from": "Kijai"
    },
    {
      "limitation": "Reference images cause artifacts in VACE",
      "details": "Using reference images tends to cause flash at start of animation and glitches",
      "from": "David Snow"
    },
    {
      "limitation": "VACE currently only works with 1.3B variants",
      "details": "VACE is only an addon module for 1.3B wan variants, not compatible with 14B models yet",
      "from": "DawnII"
    },
    {
      "limitation": "OptimalSteps scheduler limitations",
      "details": "Only works for 14B T2V and won't work with lots of other optimizations",
      "from": "Kijai"
    },
    {
      "limitation": "Native VACE can only use one embed",
      "details": "Unlike wrapper which can use multiple VACE embeds, native implementation limited to single embed",
      "from": "Kijai"
    },
    {
      "limitation": "DG models don't handle I2V well",
      "details": "Very fast at 6 steps but not good for image-to-video tasks",
      "from": "N0NSens"
    },
    {
      "limitation": "Pose matching requirement for UniAnimate with DF",
      "details": "Pose has to match the init frame, works somewhat with 14B DF model",
      "from": "Kijai"
    },
    {
      "limitation": "Prompt following issues when extending",
      "details": "Sometimes ignores prompt, sometimes 'jumps' during video extension",
      "from": "Kijai"
    },
    {
      "limitation": "Swimming noise on fine textures",
      "details": "Wan has issues with fine details like rocks, may be due to lack of training material",
      "from": "andrewrasmussen."
    },
    {
      "limitation": "VACE only works with 1.3B T2V model",
      "details": "Cannot use VACE with I2V models currently",
      "from": "Draken"
    },
    {
      "limitation": "Cannot merge VACE with I2V models",
      "details": "Shape mismatch errors occur when trying to merge VACE with I2V models",
      "from": "Kijai"
    },
    {
      "limitation": "14B model requires 54GB VRAM officially",
      "details": "Inference takes incredible VRAM due to DF sampling method",
      "from": "Kijai"
    },
    {
      "limitation": "Phantom incompatible with other reference methods",
      "details": "Needs reference in latents making it incompatible with other approaches",
      "from": "Kijai"
    },
    {
      "limitation": "Video quantization challenges",
      "details": "4-bit quantization may harm temporal attention too much for video models",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "limitation": "1.3B models insufficient for detailed facial fidelity",
      "details": "Phantom and 1.4b VACE are too weak to capture true facial fidelity from reference pics, especially for celebrity-like accuracy",
      "from": "Zuko"
    },
    {
      "limitation": "TeaCache degrades hand quality",
      "details": "Higher TeaCache threshold settings tend to make hands look messy/degraded",
      "from": "lostintranslation"
    },
    {
      "limitation": "Phantom has sampling time overhead",
      "details": "33% additional sampling time due to doing 3 passes during generation",
      "from": "Kijai"
    },
    {
      "limitation": "MultiGPU wrapper has device compatibility issues",
      "details": "Known bug causing 'Expected all tensors to be on the same device' errors when splitting models across GPUs",
      "from": "MilesCorban"
    },
    {
      "limitation": "Short video generation unreliable",
      "details": "Videos under 33 frames don't work well for quality testing, model tuned for 81 frames",
      "from": "Kijai"
    },
    {
      "limitation": "DF model incompatible with torch compile",
      "details": "Quality and performance suffer when using torch compile with DF model",
      "from": "Kijai"
    },
    {
      "limitation": "Blurry hands issue unsolved",
      "details": "No good solution found for blurry hands problem",
      "from": "Piblarg"
    },
    {
      "limitation": "SLG causes color burnout",
      "details": "SLG improves motion but creates snotty/burned appearance requiring CFG adjustment",
      "from": "lostintranslation"
    },
    {
      "limitation": "Phantom grid input doesn't work well",
      "details": "Feeding 2x3 grid of character images in one latent distorts results compared to separate latents",
      "from": "mamad8"
    },
    {
      "limitation": "Phantom and VACE combination works badly",
      "details": "Phantom image takes over everything, and ending Phantom early loses all reference",
      "from": "Kijai"
    },
    {
      "limitation": "fp8_fast always been really bad for Wan",
      "details": "Should not be used with Wan models",
      "from": "Kijai"
    },
    {
      "limitation": "Torch compile slow and eats extra memory",
      "details": "Uses about 4GB more VRAM",
      "from": "Kijai"
    },
    {
      "limitation": "Character LoRAs not working perfectly with DF models",
      "details": "Wan14b T2V LoRAs working only to some degree",
      "from": "seitanism"
    },
    {
      "limitation": "UniAnimate does not work on SkyReels V2 DF models",
      "details": "Compatibility issue with newer DF architecture",
      "from": "DawnII"
    },
    {
      "limitation": "DF quality degrades with complex scenes",
      "details": "Lots of detail in scenes causes more degradation over extensions",
      "from": "seitanism"
    },
    {
      "limitation": "Cannot resample middle clips in DF chain",
      "details": "Resampling breaks continuity with other clips, only works for single clips or last clip before continuing",
      "from": "seitanism"
    },
    {
      "limitation": "Video generators are 8-bit with no dynamic range",
      "details": "Cannot recover clipped highlights in post-processing",
      "from": "Fabricatedgirls"
    },
    {
      "limitation": "Skyreels may have cheap video look baked in",
      "details": "Clipped highlights and oversaturated colors that are harder to fix",
      "from": "Fabricatedgirls"
    },
    {
      "limitation": "UniAnimate only works with 14B models",
      "details": "No 1.3B version exists, limiting accessibility for users with less VRAM",
      "from": "Kijai"
    },
    {
      "limitation": "fp8_fast ruins I2V quality",
      "details": "While it works fine for T2V with speed benefits, it completely destroys quality in I2V mode",
      "from": "Kijai"
    },
    {
      "limitation": "DF input frame restrictions",
      "details": "Cannot input more frames than the generation frame count setting, requires preprocessing to extract correct number of frames",
      "from": "Kijai"
    },
    {
      "limitation": "Skyreels has baked-in oversaturation",
      "details": "Color palette appears oversaturated and difficult to control via prompting",
      "from": "seitanism"
    },
    {
      "limitation": "Wan fp8 quality degradation",
      "details": "Even with fix, doing FFN layers on fp8 still has huge quality hit especially with I2V. Excluding FFN layers fixes quality but then it's rarely any faster",
      "from": "Kijai"
    },
    {
      "limitation": "SkyReels I2V 720P memory requirements",
      "details": "500GB of memory still consistently runs out of memory, model loading takes tens of minutes",
      "from": "Soutlkf"
    },
    {
      "limitation": "Input frame override",
      "details": "Input frames are very strong and often override the prompt, making it difficult to control generation",
      "from": "seitanism"
    },
    {
      "limitation": "Camera control only supports panning movements",
      "details": "No zoom functionality, only pan-and-tilt movements (left, right, up, down)",
      "from": "Kijai"
    },
    {
      "limitation": "Sparge attention parameters very model specific",
      "details": "Limited to 1.3B T2V only and possibly limited to other generation parameters as well",
      "from": "Kijai"
    },
    {
      "limitation": "Multiple control models require extra passes",
      "details": "Each additional control model adds 33% more to inference time",
      "from": "Kijai"
    },
    {
      "limitation": "Fun models have quality issues",
      "details": "Quality is not as great as other models, less than ideal results",
      "from": "Hashu"
    },
    {
      "limitation": "Step1X-Edit requires 46GB VRAM",
      "details": "Very high VRAM requirement for 768x768 generation",
      "from": "mamad8"
    },
    {
      "limitation": "Loop_args don't work with 14B model",
      "details": "According to Kijai, loop_args parameter doesn't like the 14B model, works better with 1.3B",
      "from": "David Snow"
    },
    {
      "limitation": "VACE start image feature loses style after about a second",
      "details": "Start image feature doesn't properly capture reference image style and what it does capture phases out quickly",
      "from": "David Snow"
    },
    {
      "limitation": "Same start/end image produces non-moving video",
      "details": "Setting start and end image to the same thing results in expensive static image rather than loop",
      "from": "ameasure"
    },
    {
      "limitation": "Quality degradation with DF model extending different resolution clips",
      "details": "DF 540p model doesn't understand input from 720p model well, quality degrades fast when extending",
      "from": "seitanism"
    },
    {
      "limitation": "Block swap implementations may not work properly",
      "details": "Current block swap code either doesn't do anything or runs blocks on CPU, may interfere with ComfyUI's normal offloading",
      "from": "Kijai"
    },
    {
      "limitation": "FantasyTalking only 81 frames",
      "details": "Limited length output, though can be extended with loops",
      "from": "Kijai"
    },
    {
      "limitation": "FantasyTalking doesn't work with DF",
      "details": "DF isn't I2V model, doesn't use image cross attention",
      "from": "Kijai"
    },
    {
      "limitation": "FantasyTalking designed for 720p model but runs at 512p",
      "details": "Using 480p gives artifacts but works",
      "from": "Kijai"
    },
    {
      "limitation": "Video degradation with DF extension",
      "details": "Especially obvious with 1.3B version when scene content doesn't change much",
      "from": "Ablejones"
    },
    {
      "limitation": "Base Wan DG models don't work well with VACE",
      "details": "Contradictory reports, but generally poor compatibility",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "limitation": "Fantasy Talking English only",
      "details": "Model only supports English audio, Chinese and other languages don't work",
      "from": "Kijai"
    },
    {
      "limitation": "Fantasy Talking mono audio only",
      "details": "Only supports mono audio input, stereo gets converted by using left channel",
      "from": "Kijai"
    },
    {
      "limitation": "Motion degradation beyond 81 frames",
      "details": "While model can generate 121+ frames, motion quality decreases significantly",
      "from": "Kijai"
    },
    {
      "limitation": "Control LoRA VACE incompatibility",
      "details": "Cannot use control LoRAs with VACE due to input channel modifications",
      "from": "Kijai"
    },
    {
      "limitation": "LoRA model size compatibility",
      "details": "LoRAs trained for 1.3B cannot be used with 14B models and vice versa",
      "from": "Fill"
    },
    {
      "limitation": "Fantasy Talking resolution sensitivity",
      "details": "Effect becomes weaker at higher resolutions than training resolution",
      "from": "Kijai"
    },
    {
      "limitation": "Fantasy Talking frame limit",
      "details": "Limited to 81 frames maximum, more than that kills LoRA functionality",
      "from": "Stad"
    },
    {
      "limitation": "Fantasy Talking language support",
      "details": "Only works in English",
      "from": "Stad"
    },
    {
      "limitation": "VACE doesn't work with Fun models",
      "details": "Only works with normal 1.3B base models",
      "from": "Kijai"
    },
    {
      "limitation": "Camera control doesn't understand roll",
      "details": "Model may not comprehend roll movements in camera control",
      "from": "Kijai"
    },
    {
      "limitation": "LoRA flash artifacts",
      "details": "EX Video LoRA causes 5+ frame flash at video start",
      "from": "Gavmakes"
    },
    {
      "limitation": "DF brightening over time",
      "details": "Sliding models build up brightness over time from context passing",
      "from": "Draken"
    },
    {
      "limitation": "I2V cannot generate new objects not present in source image",
      "details": "Works for simple modifications like adding hats but fails to create entirely new elements",
      "from": "Draken"
    },
    {
      "limitation": "Text generation worse in I2V than T2V",
      "details": "Due to low resolution vision encoder (326x326) used under the hood",
      "from": "pom"
    },
    {
      "limitation": "Vision encoder resolution too small",
      "details": "Old siglip 326x326 resolution causes issues with text generation and fine details",
      "from": "pom"
    }
  ],
  "hardware": [
    {
      "requirement": "4090 performance",
      "details": "fp16_fast: 12.19s/it, bf16: 15.26s/it for 81 frames 40 steps at 480x832",
      "from": "ezMan"
    },
    {
      "requirement": "A6000 limitations",
      "details": "Using A6000 for fp16 model takes over an hour with jet engine noise levels",
      "from": "Doctor Shotgun"
    },
    {
      "requirement": "VRAM for VACE",
      "details": "VACE model is double the size of fp8 wan1.3b but still manageable for 1.3B, extra compute required",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM usage",
      "details": "832x480x45 frames: 19GB, can spike to 31GB for 480p generations",
      "from": "multiple users"
    },
    {
      "requirement": "16GB VRAM support",
      "details": "Can run on 16GB VRAM around 12GB usage",
      "from": "zelgo_"
    },
    {
      "requirement": "Model size",
      "details": "Only ~3.5GB in fp16, so offloading doesn't save much",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM for long generations",
      "details": "333 frames at 480x768 used 14GB VRAM, took 2hr10m",
      "from": "PookieNumnums"
    },
    {
      "requirement": "16GB VRAM performance",
      "details": "41 frames in one minute at 10 steps FP16, 3 minutes at 30 steps",
      "from": "VK (5080 128gb)"
    },
    {
      "requirement": "Context windows memory usage",
      "details": "Doesn't use more memory than 81 frames would, slower but more efficient",
      "from": "Kijai"
    },
    {
      "requirement": "12GB VRAM VACE settings",
      "details": "Need 27+ block swap, start with 480x480 resolution and 33 frames, then scale up gradually",
      "from": "Ashtar"
    },
    {
      "requirement": "VACE memory usage",
      "details": "With 81 frames at 480x832, uses ~2GB less memory after TeaCache fix (down to 17.031 GB max reserved)",
      "from": "Kijai"
    },
    {
      "requirement": "Torch 2.7+ required",
      "details": "torch.backends.cuda.matmul.allow_fp16_accumulation requires torch 2.7.0.dev2025 minimum",
      "from": "Ashtar"
    },
    {
      "requirement": "GPU compute capability",
      "details": "RTX 3060 has insufficient compute capability for fp8_fast models, need 9.0 or 8.9+",
      "from": "Faux"
    },
    {
      "requirement": "Memory savings with fp8_scaled",
      "details": "Using fp8_scaled over bf16 saves 11GB VRAM",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "Speed comparison fp16 vs fp32",
      "details": "Setting base precision fp16 with no quants is much faster than fp32 with fp8 quant",
      "from": "seitanism"
    },
    {
      "requirement": "VACE memory usage",
      "details": "960x960 at 60 steps works on 3090 with 128GB RAM using block swap, large resolutions need memory management",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "Performance benchmark",
      "details": "61 frames at 1280x544 V2V takes 139 seconds on undervolted 4090 at 50% power without optimizations",
      "from": "David Snow"
    },
    {
      "requirement": "Long video generation RAM usage",
      "details": "VAE works on few frames at a time, main limitation is RAM not VRAM for length. 1300+ frames possible",
      "from": "Kijai"
    },
    {
      "requirement": "VACE VRAM requirements",
      "details": "12GB VRAM struggles with 81 frames, needs aggressive block swapping. Works with 27 frames.",
      "from": "Ashtar"
    },
    {
      "requirement": "720p context generation",
      "details": "Very slow process, approximately 35.28s per iteration",
      "from": "A.I.Warper"
    },
    {
      "requirement": "4090 performance",
      "details": "A.I.Warper and Kytra both using 4090, prompt execution around 1500-2115 seconds for 41 frames 1280x720",
      "from": "A.I.Warper"
    },
    {
      "requirement": "3060 12GB capability",
      "details": "RTX3060 12GB can handle 81 frames at height=960 with block swap after git pull update",
      "from": "Ashtar"
    },
    {
      "requirement": "5090 setup",
      "details": "Users reporting successful setup with PyTorch nightly cu128",
      "from": "Piblarg"
    },
    {
      "requirement": "DG model speed",
      "details": "4 steps with DG model, cfg 1.0, generates in 7 seconds",
      "from": "Kijai"
    },
    {
      "requirement": "VACE with DG inference time",
      "details": "DG High V4 + VACE (5 steps) generates in 50 seconds",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "MMAudio VRAM",
      "details": "Takes about 8GB VRAM",
      "from": "Benjimon"
    },
    {
      "requirement": "DG model performance",
      "details": "Very fast, 38 seconds vs 10 minutes for normal Wan, 2s/it for 960x608x61",
      "from": "burgstall"
    },
    {
      "requirement": "SkyreelsA2 VRAM usage",
      "details": "Same as 14B model, no extra inputs required",
      "from": "Kijai"
    },
    {
      "requirement": "Wan latents memory usage",
      "details": "Takes much VRAM due to huge input with many frames, calculations not done in quantized precision even with 16GB quantized model",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM usage for VACE",
      "details": "Nearly as much VRAM as 14B Fun models, heavier than expected for 1.3B",
      "from": "DawnII"
    },
    {
      "requirement": "Triton compatibility",
      "details": "Works with Python 3.12/CUDA 12 on Windows 11, can install in ComfyUI embedded Python",
      "from": "Lumi"
    },
    {
      "requirement": "VACE VRAM usage optimized",
      "details": "832x480x81 frames: ~12GB VRAM without offloading, 1024x1024x81: ~13GB VRAM, significant reduction from previous versions",
      "from": "Kijai"
    },
    {
      "requirement": "Highest resolution tested",
      "details": "720x960 with 720p model using GGUF on limited GPU",
      "from": "traxxas25"
    },
    {
      "requirement": "VRAM for 14B inference",
      "details": "173 frames at 848x480 takes 15 mins on 4070 Ti (12GB VRAM) with sage attention and block swap",
      "from": "traxxas25"
    },
    {
      "requirement": "8GB VRAM capability",
      "details": "Possible to run 14B T2V on laptop 4070 8GB taking 3 hours for 17 frames",
      "from": "froyo"
    },
    {
      "requirement": "Training VRAM needs",
      "details": "7GB VRAM for training with fp8 model, 8GB training is possible",
      "from": "Piblarg"
    },
    {
      "requirement": "14B speed benchmarks",
      "details": "81 frames at 768x768, 8 steps: 631 seconds on undervolted 4090 (50% power)",
      "from": "David Snow"
    },
    {
      "requirement": "VACE VRAM usage",
      "details": "Takes 3-5 minutes on 96GB VRAM runpod instance",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "VACE memory optimized",
      "details": "832x480x81 frames now uses only 6GB VRAM during sampling with optimizations",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 2080 bf16 limitation",
      "details": "RTX 2080 doesn't fully support bf16 - must use fp16 for models and VAE",
      "from": "Kijai"
    },
    {
      "requirement": "14B VRAM with offloading",
      "details": "14B can work with 8GB through offloading but will be extremely slow",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM for tiled encoding",
      "details": "Tiled encoding recommended for 8GB VRAM or less, encoding heavier than decoding",
      "from": "Kijai"
    },
    {
      "requirement": "A6000 compatibility",
      "details": "Works on A6000 with proper quantization settings",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "requirement": "HD video processing time",
      "details": "1920x1080, 45 frames took 3+ hours with white padding reference image",
      "from": "xwsswww"
    },
    {
      "requirement": "14B model inference time",
      "details": "14B model took half hour for generation",
      "from": "Flipping Sigmas"
    },
    {
      "requirement": "RTX 4090 block swapping",
      "details": "Question about block swapping needed for Fun Control 14B on RTX4090",
      "from": "JmySff"
    },
    {
      "requirement": "VRAM for 1920x1080 81f generation",
      "details": "Possible on 4090 with tiled VAE bf16, took about 30 minutes",
      "from": "Pedro (@LatentSpacer)"
    },
    {
      "requirement": "Render time for 1280x720",
      "details": "20 minutes on RTX 3090",
      "from": "burgstall"
    },
    {
      "requirement": "Speed improvement with DG model",
      "details": "First test inference went from 7:12 to 3:42",
      "from": "DawnII"
    },
    {
      "requirement": "14B model needs 24GB VRAM",
      "details": "Can run 14B at 81 frames decent resolution without swap if you don't use monitor or use GGUF q6, but 16GB will lose speed benefits to offloading",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM for video depth",
      "details": "Had to reduce to 512 due to OOM issues, potential memory leak",
      "from": "traxxas25"
    },
    {
      "requirement": "Generation times",
      "details": "20 min on 480x832 with 81 frames on some setups, 90 seconds after flux inpaint on others, 5 mins on 3090",
      "from": "claygraffix"
    },
    {
      "requirement": "4090 performance",
      "details": "User with 4090 can run wan 1.3b in both bf16 and fp32 with quantized peripheral models",
      "from": "notid"
    },
    {
      "requirement": "WAN 14B on 24GB VRAM",
      "details": "14B model works on 24GB VRAM GPUs",
      "from": "Guillaume"
    },
    {
      "requirement": "Training time on 4090",
      "details": "Style LoRA training takes about 1 hour on RTX 4090",
      "from": "Jas"
    },
    {
      "requirement": "I2V inference on 3090",
      "details": "14B I2V takes long time even on RTX 3090",
      "from": "\u2727\u0e05\u0e42\u0e51\u2180\u11ba\u2180 \u0e51\u0e43\u0e05\u2727PookieNumnums"
    },
    {
      "requirement": "Generation time with high-end GPU",
      "details": "RTX 4060Ti 16GB: 165 frames at 848x480, 25 steps took 55.75s/it for 10 seconds of video",
      "from": "V\u00e9role"
    },
    {
      "requirement": "VACE generation performance",
      "details": "245 frames at 848x480 with 8 steps took 11:31 total (86.41s/it) on 4060Ti 16GB",
      "from": "V\u00e9role"
    },
    {
      "requirement": "Cloud system timing",
      "details": "GGUF 8 at 8 steps: 81 frames in 2 mins 45 seconds. 25 steps: 347 seconds per scene",
      "from": "AJO"
    },
    {
      "requirement": "81 frames vs 49 frames performance difference",
      "details": "4090 with 128GB RAM: 49 frames under 6 minutes, 81 frames hangs at 12% and stops progressing",
      "from": "boorayjenkins"
    },
    {
      "requirement": "VACE performance improvement",
      "details": "Got 720p for 81 frames in 4 mins with VACE workflow on 4090",
      "from": "boorayjenkins"
    },
    {
      "requirement": "720 model VRAM usage",
      "details": "VRAM goes to 100% usage, 720 model is hefty beast, recommend 20 blocks for block swap",
      "from": "David Snow"
    },
    {
      "requirement": "Multi-GPU support",
      "details": "Wan supports multi GPU, can use CUDA_VISIBLE_DEVICES=1 in Linux or custom nodes for multiple GPUs",
      "from": "Benjimon"
    },
    {
      "requirement": "4060 ti 16GB performance",
      "details": "220 frames at 368x640 in 2min 17sec using 6.5GB VRAM with Fun 1.3B + ExVideo LoRA",
      "from": "Pol"
    },
    {
      "requirement": "720p VRAM usage",
      "details": "81 frames at 720p uses only 9GB VRAM with Fun model, much less than VACE",
      "from": "A.I.Warper"
    },
    {
      "requirement": "SageAttention 2.0 speed improvement",
      "details": "More than 2x faster with larger input sizes, requires CUDA 12.8",
      "from": "Kijai"
    },
    {
      "requirement": "4070ti performance comparison",
      "details": "225 frames at 848x480 took 8 hours with VACE model, much slower than Fun",
      "from": "traxxas25"
    },
    {
      "requirement": "RTX 8000 compatibility",
      "details": "Works with FP16 and SDPA attention, no BF16 support, very slow inference",
      "from": "Benjimon"
    },
    {
      "requirement": "VRAM for TeaCache",
      "details": "Can use main_device for faster performance if spare VRAM available, otherwise offload_device",
      "from": "DawnII"
    },
    {
      "requirement": "RIFE interpolation",
      "details": "Low VRAM usage for frame interpolation",
      "from": "seitanism"
    },
    {
      "requirement": "Multi-GPU usage",
      "details": "Can run separate Wan sessions on each GPU, NVLink not beneficial",
      "from": "Benjimon"
    },
    {
      "requirement": "FLF2V VRAM usage",
      "details": "81 frames at 768x768 with 30 steps: Max allocated 15.175 GB, Max reserved 16.812 GB with TeaCache",
      "from": "Kijai"
    },
    {
      "requirement": "H100 80GB doesn't need offloading",
      "details": "Depending on resolution, H100 with 80GB may not need model offloading",
      "from": "Kijai"
    },
    {
      "requirement": "16GB VRAM with fp8 scaled",
      "details": "Workable with offloading for 81 frames of 720x480, but struggles with LoRAs on Q8 GGUF",
      "from": "lostintranslation"
    },
    {
      "requirement": "FramePack VRAM",
      "details": "Minimal 6GB for 1-minute video at 30fps with 13B model, laptop GPUs okay",
      "from": "zelgo_"
    },
    {
      "requirement": "FramePack generation speed",
      "details": "4090: 9.6 minutes for 5 second clip without optimizations, under 2 minutes per batch with teacache/sage/flash/xformers",
      "from": "TK_999"
    },
    {
      "requirement": "Long video generation time",
      "details": "About 1 hour estimated for 30 second video on consumer hardware",
      "from": "Cubey"
    },
    {
      "requirement": "FLF2V on 12GB VRAM",
      "details": "Can generate above 480p resolution with 81 frames using 37-40 blocks_to_swap and fp8 model",
      "from": "DawnII"
    },
    {
      "requirement": "FLF2V on 24GB VRAM",
      "details": "Can generate 640x640, 33 frames with 40 blocks_to_swap. OOM issues can be RAM related, not VRAM",
      "from": "mamad8"
    },
    {
      "requirement": "VACE memory usage",
      "details": "Uses extra 15 blocks compared to regular inference, limiting long sequence generation",
      "from": "A.I.Warper"
    },
    {
      "requirement": "VRAM for high resolution VACE",
      "details": "720x1280 resolution may be challenging on 16GB GPUs like 4060ti",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "requirement": "Cloud GPU pricing",
      "details": "H100s available for $0.50/hour, eight 24GB GPUs for under $0.20/hour on vast.ai",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "requirement": "3090 fp8 support",
      "details": "Can use fp8_e5m2 weights with torch compile, SageAttention 2.0+ works with direct cuda mode",
      "from": "Kijai"
    },
    {
      "requirement": "UniAnimate VRAM requirements",
      "details": "Will not work with 8GB cards",
      "from": "xwsswww"
    },
    {
      "requirement": "Long video generation hardware",
      "details": "720p 1000 frame generation possible, requires significant compute power",
      "from": "Flipping Sigmas"
    },
    {
      "requirement": "Memory for depth processing",
      "details": "VideoDepthAnything can cause OOM at 720x480x41 frames on 16GB VRAM",
      "from": "lostintranslation"
    },
    {
      "requirement": "Processing time for upscaling workflow",
      "details": "320 seconds for full upscaling workflow on test hardware",
      "from": "lostintranslation"
    },
    {
      "requirement": "UniAnimate speed on 4090",
      "details": "Around 3 minutes with every optimization for same performance as 14B I2V",
      "from": "Kijai"
    },
    {
      "requirement": "Storage needs for video models",
      "details": "Multiple users reporting 1TB+ of video diffusion models, requiring 4-8TB storage drives",
      "from": "CJ"
    },
    {
      "requirement": "720p generation on 24GB VRAM",
      "details": "Need to split into 41 frame runs on 24GB VRAM",
      "from": "A.I.Warper"
    },
    {
      "requirement": "4090 performance",
      "details": "345 seconds for style transfer generation",
      "from": "Gavmakes"
    },
    {
      "requirement": "VRAM for 1.3B at 544x960",
      "details": "Around 8GB at fp16",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM for full resolution 97 frames",
      "details": "Under 9GB for 960x544",
      "from": "zelgo_"
    },
    {
      "requirement": "14B requirements",
      "details": "Same as regular Wan (fits in 24GB)",
      "from": "Benjimon"
    },
    {
      "requirement": "Generation time on H100",
      "details": "128 seconds for 1.3B model",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "4070Ti performance",
      "details": "97 frames 544x960 30 steps in ~7.21s/it with optimizations",
      "from": "Corneilious Pickleberry"
    },
    {
      "requirement": "MAGI-1 VRAM needs",
      "details": "24B model needs 8x4090 recommended, 4.5B variant will run on 1x4090, FP8 27GB, Q4 should be runnable on 16GB VRAM",
      "from": "yi"
    },
    {
      "requirement": "SkyReels DF on 12GB card",
      "details": "Got 14B DF model working on 12GB card at 480x480 passing 33 frames between samplers",
      "from": "Davidodave"
    },
    {
      "requirement": "14B models performance",
      "details": "14B models take about 20 minutes per generation, too slow for some users",
      "from": "Colin"
    },
    {
      "requirement": "MAGI-1 VRAM",
      "details": "Requires 8x4090 GPUs",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "requirement": "Block swap optimization",
      "details": "Trial and error, reduce blocks until OOM then add 1-2 back",
      "from": "zelgo_"
    },
    {
      "requirement": "System fallback setting",
      "details": "Disable 'prefer no system fallback' in Nvidia control panel to prevent slowdown from VRAM spilling to system RAM",
      "from": "zelgo_"
    },
    {
      "requirement": "14B 480p VRAM usage",
      "details": "Around 83% VRAM usage with 16GB, fails without system fallback",
      "from": "lostintranslation"
    },
    {
      "requirement": "14B DF model VRAM",
      "details": "Officially needs 54GB, practically uses ~30GB during sampling on RTX 5090",
      "from": "Kijai"
    },
    {
      "requirement": "TeaCache memory usage",
      "details": "Standard mode: ~4GB, 'e' mode: ~600MB",
      "from": "Kijai"
    },
    {
      "requirement": "Time step embedding",
      "details": "Takes 7GB alone when using DF sampling",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 5090 limits",
      "details": "Max 69 frames on first gen with 14B DF model before OOM",
      "from": "jellybean5361"
    },
    {
      "requirement": "Phantom VRAM usage",
      "details": "Uses 10.6GB VRAM for 1280x768x81 generation, around 9GB base usage",
      "from": "TK_999"
    },
    {
      "requirement": "SkyCaptioner memory usage",
      "details": "Loads in 16.7GB RAM with no quantization",
      "from": "TK_999"
    },
    {
      "requirement": "MultiGPU setup recommendation",
      "details": "Put model on 4090, VAE+text on 3090 for optimal memory distribution",
      "from": "MilesCorban"
    },
    {
      "requirement": "Minimum VRAM trend",
      "details": "16GB is becoming minimum requirement as developers move away from 8GB GPU support",
      "from": "deleted_user_2ca1923442ba"
    },
    {
      "requirement": "ComfyUI-LatentSyncWrapper VRAM",
      "details": "Requires 20GB VRAM for lipsyncing, causes OOM on lower VRAM",
      "from": "xwsswww"
    },
    {
      "requirement": "4080 generation speed",
      "details": "~470s to generate 81 frames at 512x768 with teacache 0.25, improved to 300s with fp16_fast",
      "from": "lostintranslation"
    },
    {
      "requirement": "4090 vs 3090 torch compile compatibility",
      "details": "4090 can use e4m3fn with ~30% speed boost, 3090 needs e5m2 format",
      "from": "Kijai"
    },
    {
      "requirement": "Multi-GPU performance",
      "details": "35 second generations for Phantom on 8 GPUs at 832x480, 50 steps",
      "from": "Kytra"
    },
    {
      "requirement": "Generation time comparison",
      "details": "480p model: 6min, 720p model: 7min for same resolution output",
      "from": "N0NSens"
    },
    {
      "requirement": "RTX 3090 compatibility with 14B model",
      "details": "24GB VRAM works for DF 14B model",
      "from": "MilesCorban"
    },
    {
      "requirement": "Resolution recommendations for DF model",
      "details": "640x480 works fine for testing, can play with aspect ratios",
      "from": "MilesCorban"
    },
    {
      "requirement": "RAM requirements causing slowdowns",
      "details": "32GB RAM can cause 5+ minute delays when changing prompts due to swapping",
      "from": "mamad8"
    },
    {
      "requirement": "A6000 torch compile support",
      "details": "Torch compile now works on A6000, speed improvement from 14.22s/it to 12.22s/it",
      "from": "MilesCorban"
    },
    {
      "requirement": "RTX 4090 VRAM limits",
      "details": "At 640x480 max around 40-50 frames, overflow to RAM without block swap",
      "from": "MilesCorban"
    },
    {
      "requirement": "Long generation times on RTX 5090",
      "details": "832x528 with 20 steps taking hours, much longer than expected",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "A6000 for testing",
      "details": "Switched to A6000 for VRAM capacity, taking speed hit for testing",
      "from": "MilesCorban"
    },
    {
      "requirement": "DF 720p model VRAM",
      "details": "720x720x81 frames used 10.253GB max allocated, 12.406GB max reserved with 30 blocks swapped on 4090",
      "from": "Kijai"
    },
    {
      "requirement": "Phantom model VRAM",
      "details": "Takes about 10GB VRAM, can be lowered to 8GB",
      "from": "slmonker"
    },
    {
      "requirement": "DF generation time",
      "details": "~20 minutes for DF generation on 4090",
      "from": "TK_999"
    },
    {
      "requirement": "Skyreels speed improvement",
      "details": "fp8_fast gives 20-25% speed boost, reducing generation from 4.5min to 3.5min",
      "from": "seitanism"
    },
    {
      "requirement": "fp8_fast GPU compatibility",
      "details": "fp8_fast only works on 4000 series GPUs and up, provides 30% speedup. fp16_fast works on 3090 but fp8_fast does not",
      "from": "Kijai"
    },
    {
      "requirement": "3090 I2V performance",
      "details": "3090 24GB VRAM can run Wan2_1-SkyReels-V2-I2V-14B-720P at 81 frames, 480x480, 10 blockswap in 14 minutes with teacache",
      "from": "mamad8"
    },
    {
      "requirement": "4090 performance comparison",
      "details": "4090 generates 77 frames at 1280x720 faster than 5090 at 30 steps vs 20 steps",
      "from": "ezMan"
    },
    {
      "requirement": "VRAM usage for Step1X-Edit",
      "details": "46GB VRAM required for 768x768 generation",
      "from": "mamad8"
    },
    {
      "requirement": "H100 performance",
      "details": "77 frames at 1024x560 with 20 steps completed in 483 seconds, max memory 52.4GB",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "4090 limitations",
      "details": "Skyreels DF 720 at 640px for 121 frames works on 4090, but OOM at higher resolutions",
      "from": "boorayjenkins"
    },
    {
      "requirement": "850W PSU adequacy",
      "details": "850W PSU should be adequate for 3090 unless powering high-end CPU at full load",
      "from": "seitanism"
    },
    {
      "requirement": "RTX 5090 VRAM usage",
      "details": "Max allocated 16.100 GB, max reserved 16.969 GB for 89 frames at 1024x576 with Phantom",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "RTX 4090 with fp8 quantization",
      "details": "640x480 clips fit mostly in VRAM with minimal system RAM offload",
      "from": "MilesCorban"
    },
    {
      "requirement": "RTX 4090 with fp16 models",
      "details": "fp16 works but requires significant offloading to system RAM",
      "from": "MilesCorban"
    },
    {
      "requirement": "14B model VRAM usage",
      "details": "Pegs VRAM at 97% usage during generation",
      "from": "David Snow"
    },
    {
      "requirement": "RTX 5090 pricing",
      "details": "Card cost $2900 with protection plan, MSRP around $2000",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "VRAM for HiDream",
      "details": "Colin runs full on 16GB VRAM, can do fp16 but takes longer",
      "from": "Colin"
    },
    {
      "requirement": "FantasyTalking VRAM",
      "details": "Claims 5GB VRAM support despite using 720p model",
      "from": "Stad"
    },
    {
      "requirement": "Fantasy Talking VRAM",
      "details": "5GB VRAM possible with num_persistent_param_in_dit=0, but quality tradeoffs exist",
      "from": "Faux"
    },
    {
      "requirement": "Swap memory for 14B",
      "details": "User needed 60GB RAM but had 32GB, used 28GB SSD as swap memory to run 14B model on 4060",
      "from": "Stad"
    },
    {
      "requirement": "RTX 5090 performance",
      "details": "1024x576, 89 frames, 30 steps completed in 3:07 (198 seconds total), 16.1GB max VRAM",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "Fun_Inp model size",
      "details": "1.3B Fun model is 3GB weight, potentially usable on 8GB GPUs",
      "from": "Blink"
    },
    {
      "requirement": "Fantasy Talking on RTX 5090",
      "details": "2 minutes 44 seconds for 81 frames at 512x512 with 20 steps, max 19.3GB VRAM",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "Compute capability for torch._scaled_mm",
      "details": "Requires CUDA compute capability >= 9.0 or 8.9, or ROCm MI300+",
      "from": "J_Pyxal"
    },
    {
      "requirement": "DF 14B 720p on RTX 3090",
      "details": "No OOM but won't progress past first step, too heavy",
      "from": "PirateWolf"
    },
    {
      "requirement": "Wan I2V 14B VRAM usage",
      "details": "Max allocated 19.738 GB, Max reserved 21.781 GB for 720x544, 81 frames, 25 steps - took 249.73 seconds",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "requirement": "RTX 5090 setup issues",
      "details": "Fresh install with triton and sage causing SystemError with recent ComfyUI update",
      "from": "Baku"
    }
  ],
  "community_creations": [
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "ComfyUI wrapper for Wan models by Kijai, more stable than native",
      "from": "Kijai"
    },
    {
      "creation": "VACE integration",
      "type": "node",
      "description": "Kijai added VACE model support to ComfyUI wrapper for video control and inpainting",
      "from": "Kijai"
    },
    {
      "creation": "VACE workflow examples",
      "type": "workflow",
      "description": "Multiple users sharing working VACE workflows with different control types",
      "from": "Hashu"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "ComfyUI wrapper for Wan models by Kijai",
      "from": "mentioned in context"
    },
    {
      "creation": "VACE embed node",
      "type": "node",
      "description": "Node that connects to sampler for VACE functionality",
      "from": "Draken"
    },
    {
      "creation": "WanVideoWrapper VACE support",
      "type": "node",
      "description": "Added VACE Encode node to Kijai's wrapper for comprehensive video control",
      "from": "Kijai"
    },
    {
      "creation": "Control LoRA for upscaling",
      "type": "lora",
      "description": "spacepixel made control LoRA for 1.3B model tiling upscale",
      "from": "Piblarg"
    },
    {
      "creation": "Lofi girl style LoRA",
      "type": "lora",
      "description": "Custom LoRA for creating seamless lofi girl style videos with specific art style",
      "from": "Mint"
    },
    {
      "creation": "pad image batch interleaved",
      "type": "node",
      "description": "Node to make gray frame creation simpler, can also pad single image and add empty frames between images for interpolation",
      "from": "Kijai"
    },
    {
      "creation": "Auto segmenter",
      "type": "tool",
      "description": "Grabs the person or head or whatever automatically",
      "from": "AJO"
    },
    {
      "creation": "VACE helper node for start/end frames",
      "type": "node",
      "description": "New helper node for easier setup of start and end frame batching in VACE workflows",
      "from": "Kijai"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "wrapper",
      "description": "ComfyUI implementation wrapper for WAN models with VACE support",
      "from": "Kijai"
    },
    {
      "creation": "Start To End Frame node",
      "type": "node",
      "description": "Helper node for managing start and end frame workflows in VACE",
      "from": "Kijai"
    },
    {
      "creation": "Test Pattern node",
      "type": "node",
      "description": "Creates test patterns and adds empty frames between keyframes",
      "from": "Kijai"
    },
    {
      "creation": "ImagePad node",
      "type": "node",
      "description": "Simplifies padding images with white background for VACE reference requirements",
      "from": "Kijai"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "Kijai's ComfyUI wrapper for Wan video models",
      "from": "Kijai"
    },
    {
      "creation": "Standalone VACE loader",
      "type": "node",
      "description": "New node for loading VACE separately from main model",
      "from": "Kijai"
    },
    {
      "creation": "Lotus ComfyUI implementation",
      "type": "node",
      "description": "Kijai's implementation of Lotus depth/normal map generation",
      "from": "Kijai"
    },
    {
      "creation": "Spline editor for bounding boxes",
      "type": "workflow",
      "description": "Method for creating animated bounding boxes for VACE control",
      "from": "Hashu"
    },
    {
      "creation": "TAEW preview for Wan",
      "type": "tool",
      "description": "Enables low-pass preview during sampling for Wan models",
      "from": "Zuko"
    },
    {
      "creation": "ComfyUI-BS-Textchop",
      "type": "node",
      "description": "Custom node for extracting multiple text segments using start/end markers, supports up to 15 outputs",
      "from": "burgstall"
    },
    {
      "creation": "SkyreelsA2 ComfyUI implementation",
      "type": "node",
      "description": "ComfyUI wrapper node for SkyreelsA2 with example workflows",
      "from": "Kijai"
    },
    {
      "creation": "Custom bbox inpainting workflow",
      "type": "workflow",
      "description": "Custom workflow developed with yo9o for targeted inpainting",
      "from": "yo9o"
    },
    {
      "creation": "CivChan Wan LoRA",
      "type": "lora",
      "description": "LoRA for CivChan character",
      "from": "Piblarg"
    },
    {
      "creation": "Claydoh model",
      "type": "model",
      "description": "Claymation/Play-Doh style Wan 14B model using custom Blender sculpts dataset",
      "from": "Hot Hams, the God of Meats"
    },
    {
      "creation": "Frutiger Lora 14B",
      "type": "lora",
      "description": "Vaporwave/00s aesthetic style lora for WAN 14B model",
      "from": "Dream Making"
    },
    {
      "creation": "LLM Polymath node",
      "type": "node",
      "description": "Custom node for LLM integration with Gemini API support",
      "from": "yo9o"
    },
    {
      "creation": "VACE depth+pose blending workflow",
      "type": "workflow",
      "description": "Technique for combining depth and pose controls without showing rig artifacts",
      "from": "yo9o"
    },
    {
      "creation": "Ghibli LoRA 1.3B",
      "type": "lora",
      "description": "Studio Ghibli style LoRA trained for 1.3B model, undertrained but functional",
      "from": "Piblarg"
    },
    {
      "creation": "Ghibli LoRA 14B",
      "type": "lora",
      "description": "Studio Ghibli style LoRA trained overnight for 14B model",
      "from": "Piblarg"
    },
    {
      "creation": "Depth map upscale workflow",
      "type": "workflow",
      "description": "Standalone workflow using better depth maps with soft light blend mode",
      "from": "Flipping Sigmas"
    },
    {
      "creation": "Musubi trainer frontend",
      "type": "tool",
      "description": "H1111 - frontend built on top of musubi inference scripts",
      "from": "Benjimon"
    },
    {
      "creation": "WireframeDesigner",
      "type": "tool",
      "description": "Lightweight alternative to Blender for creating wireframe control videos for VACE",
      "from": "burgstall"
    },
    {
      "creation": "Mask expansion node",
      "type": "node",
      "description": "Node that can expand a mask in any direction, useful for bounding boxes with shadows or reflections",
      "from": "traxxas25"
    },
    {
      "creation": "Tiled WanVideo VACE Encode",
      "type": "node",
      "description": "2x2 tiled encoding option for handling higher resolution videos in VACE",
      "from": "Kijai"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "Comprehensive wrapper for Wan models with VACE support",
      "from": "Kijai"
    },
    {
      "creation": "VACE face swap workflow",
      "type": "workflow",
      "description": "Face swapping using VACE capabilities",
      "from": "xwsswww"
    },
    {
      "creation": "Upscale/refine workflow with reward LoRAs",
      "type": "workflow",
      "description": "High resolution workflow using multiple reward LoRAs",
      "from": "HeadOfOliver"
    },
    {
      "creation": "WanWeightedControlToVideo",
      "type": "node",
      "description": "Node for controlling weight and timeline application of weight for native WanFunControl node",
      "from": "CJ"
    },
    {
      "creation": "FL_gemini nodes",
      "type": "node",
      "description": "Nodes for integrating Gemini AI for prompt generation in ComfyUI",
      "from": "Fill"
    },
    {
      "creation": "VACE subject removal workflow",
      "type": "workflow",
      "description": "Uses BBOX from Florence2 to remove any subjects from videos, allows mask dilation",
      "from": "ArtOfficial"
    },
    {
      "creation": "Multi-pass refinement workflow",
      "type": "workflow",
      "description": "Combines VACE first pass with vanilla 1.3B refinement using enhancement LoRAs",
      "from": "David Snow"
    },
    {
      "creation": "ReCamMaster workflow",
      "type": "workflow",
      "description": "Basic workflow to turn single image into 81 frame video with camera controls",
      "from": "Kytra"
    },
    {
      "creation": "Face crop and stitch workflow",
      "type": "workflow",
      "description": "Padded face crop and stitch with video workflow",
      "from": "burgstall"
    },
    {
      "creation": "WanVideoWrapper nodes",
      "type": "node",
      "description": "Kijai's wrapper implementation with simple nodes merged to main",
      "from": "Kijai"
    },
    {
      "creation": "Sequence feature for API",
      "type": "tool",
      "description": "Added feature sequence with delay parameter to reduce API failure chances",
      "from": "Impactframes."
    },
    {
      "creation": "Camera path workflow",
      "type": "workflow",
      "description": "Workflow for camera movement control in WAN",
      "from": "Kijai"
    },
    {
      "creation": "WAN IPA implementation",
      "type": "tool",
      "description": "IPA (presumably IP-Adapter) implementation for WAN, works better as mix with i2v LoRA",
      "from": "pom"
    },
    {
      "creation": "Custom sigmas for OptimalSteps",
      "type": "tool",
      "description": "Custom implementation of OptimalSteps scheduling for WAN native",
      "from": "V\u00e9role"
    },
    {
      "creation": "ComfyUI-AIR-Nodes target crop",
      "type": "node",
      "description": "Node pack with target crop functionality for cropping and processing video regions",
      "from": "A.I.Warper"
    },
    {
      "creation": "Custom node for crop_data + mask",
      "type": "node",
      "description": "Written to accept crop_data plus new mask on generated output",
      "from": "A.I.Warper"
    },
    {
      "creation": "Mask input for context window",
      "type": "node",
      "description": "Added mask input to context window node",
      "from": "Kijai"
    },
    {
      "creation": "Wan looper workflow",
      "type": "workflow",
      "description": "Automatically grabs last frame and loops generation process multiple times",
      "from": "AJO"
    },
    {
      "creation": "Animated Logo LoRA",
      "type": "lora",
      "description": "First version of LoRA for creating animated logos",
      "from": "Alisson Pereira"
    },
    {
      "creation": "Fun 14B FP16 conversion",
      "type": "model",
      "description": "Converted 14B Fun model to FP16 for Turing GPU training compatibility",
      "from": "Benjimon"
    },
    {
      "creation": "Ghibli 1.3B LoRA",
      "type": "lora",
      "description": "High quality Ghibli style trained on synthetic 14B data",
      "from": "Piblarg"
    },
    {
      "creation": "Eyes In Camera LoRA",
      "type": "lora",
      "description": "Trained on Higgsfield dataset for camera-directed eye contact",
      "from": "NebSH"
    },
    {
      "creation": "Custom interpolation node",
      "type": "node",
      "description": "Internal logic for framerate interpolation using GIMM_VFI",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "creation": "TheDirector",
      "type": "node",
      "description": "Custom node for multi-scene wan generation with consistent characters",
      "from": "AJO"
    },
    {
      "creation": "Orbit camera generation node",
      "type": "node",
      "description": "Node to generate orbit camera movements, limited to under 90 degrees",
      "from": "Kijai"
    },
    {
      "creation": "ModelPatcher node",
      "type": "node",
      "description": "Workaround for using LoRAs with torchcompile",
      "from": "Kijai"
    },
    {
      "creation": "Kijai FramePack ComfyUI implementation",
      "type": "node",
      "description": "Very early mockup version that runs FramePack in ComfyUI with native models except transformer",
      "from": "Kijai"
    },
    {
      "creation": "Orbit creation node",
      "type": "node",
      "description": "Node to create camera orbits for RecamMaster",
      "from": "Kijai"
    },
    {
      "creation": "VACE temporal mask generation node",
      "type": "node",
      "description": "Generates temporal masks automatically based on inputs for VACE keyframe system",
      "from": "Kijai"
    },
    {
      "creation": "Probably Lora-Induced Degradation Fixer",
      "type": "workflow",
      "description": "Custom workflow to clear VRAM between batches to prevent LoRA-induced quality degradation",
      "from": "lostintranslation"
    },
    {
      "creation": "Mediapipe face control tool",
      "type": "tool",
      "description": "Custom mediapipe JS API implementation for face landmark detection with filtering options",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "creation": "ExVideo LoRA",
      "type": "lora",
      "description": "LoRA for video extension and style transfer, works well for long video generation",
      "from": "Flipping Sigmas"
    },
    {
      "creation": "DG models depth control LoRA",
      "type": "lora",
      "description": "Depth control LoRA used in combination with style transfer workflows",
      "from": "Flipping Sigmas"
    },
    {
      "creation": "Image batch manipulation nodes",
      "type": "node",
      "description": "Nodes for inserting keyframes and splitting image batches",
      "from": "Kijai"
    },
    {
      "creation": "UniAnimate DWPose alignment node",
      "type": "node",
      "description": "Node for aligning DWPose to reference images",
      "from": "Kijai"
    },
    {
      "creation": "SkyReels Wan version",
      "type": "model",
      "description": "Close to stock i2v model that supports Wan LoRAs",
      "from": "Benjimon"
    },
    {
      "creation": "FramePackWrapper with keyframe support",
      "type": "node",
      "description": "Fork with added keyframe support for better control",
      "from": "Ashtar"
    },
    {
      "creation": "Custom Sapiens pose node",
      "type": "node",
      "description": "Lite version for pose detection with part selection capability",
      "from": "Kijai"
    },
    {
      "creation": "Modified pose detector",
      "type": "tool",
      "description": "Built modified detector for testing UniAnimate pose inputs",
      "from": "Kijai"
    },
    {
      "creation": "DWPose failure workaround",
      "type": "tool",
      "description": "Added workaround to handle failed pose detections",
      "from": "Kijai"
    },
    {
      "creation": "One-time Runpod installation script",
      "type": "tool",
      "description": "Automatically downloads everything for FLF workflow",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "First working DF extension test",
      "type": "workflow",
      "description": "Initial implementation of Diffusion Forcing support",
      "from": "Kijai"
    },
    {
      "creation": "WanVideoWrapper DF support",
      "type": "node",
      "description": "Added Diffusion Forcing sampler support with VACE compatibility",
      "from": "Kijai"
    },
    {
      "creation": "VACE start/end frame automation node",
      "type": "node",
      "description": "Automatically generates grey frames and masks for VACE workflows",
      "from": "Kijai"
    },
    {
      "creation": "Style LoRA training success",
      "type": "lora",
      "description": "1.3B style LoRA trained using ostris ai-toolkit, working well on recent video generations",
      "from": "Jas"
    },
    {
      "creation": "Character LoRA in progress",
      "type": "lora",
      "description": "Training character LoRA on 14B model using 1000 epochs with 104 images per epoch",
      "from": "AJO"
    },
    {
      "creation": "Likeness LoRA improvement",
      "type": "lora",
      "description": "Likeness LoRA trained on Wan 14B works better on SkyReels",
      "from": "Zuko"
    },
    {
      "creation": "VACE upscale workflow",
      "type": "workflow",
      "description": "Multi-stage upscaling workflow using VACE with reference images",
      "from": "Cseti"
    },
    {
      "creation": "Structured prompting automation",
      "type": "tool",
      "description": "Automating SkyReels V2 structured prompting format",
      "from": "fredbliss"
    },
    {
      "creation": "Multi-stage sampling workflow",
      "type": "workflow",
      "description": "Sigma split workflow for better quality and speed",
      "from": "lostintranslation"
    },
    {
      "creation": "Phantom ComfyUI implementation",
      "type": "node",
      "description": "ComfyUI support for Phantom model with multi-reference image capability",
      "from": "Kijai"
    },
    {
      "creation": "SkyReels prompter based on paper methodology",
      "type": "tool",
      "description": "Tool that generates dynamics-only prompts for SkyReels V2 I2V following the paper's guidelines",
      "from": "fredbliss"
    },
    {
      "creation": "Character LoRAs for Wan",
      "type": "lora",
      "description": "Human likeness LoRAs working well with close expressions, trained on same pics across SDXL, HunYuan, and Wan",
      "from": "boorayjenkins"
    },
    {
      "creation": "Jennifer Connelly LoRA",
      "type": "lora",
      "description": "Trained on Wan for celebrity likeness",
      "from": "Fabricatedgirls"
    },
    {
      "creation": "ComfyUI-FramePacking",
      "type": "node",
      "description": "Node that rounds image sequences to nearest 4n+1 by padding last frame, plus trimming node",
      "from": "Rishi Pandey"
    },
    {
      "creation": "ComfyUI-FramePacking",
      "type": "tool",
      "description": "Node that pads image batch to 4n+1 frames for Wan by duplicating last frame",
      "from": "Rishi Pandey"
    },
    {
      "creation": "WAN VACE native workflow with auto masking",
      "type": "workflow",
      "description": "Combines VACE native implementation with segment anything for automatic masking",
      "from": "V\u00e9role"
    },
    {
      "creation": "ComfyUI-wanBlockswap",
      "type": "node",
      "description": "Node for block swap functionality, though effectiveness questioned",
      "from": "orssorbit"
    },
    {
      "creation": "Florence2 + Groq i2v workflow",
      "type": "workflow",
      "description": "Uses Florence2 for image analysis and Groq LLM for video prompt generation",
      "from": "David Snow"
    },
    {
      "creation": "Pepe meme LoRA",
      "type": "lora",
      "description": "LoRA for Pepe memes",
      "from": "Piblarg"
    },
    {
      "creation": "Throwaway 1.3B LoRAs",
      "type": "lora",
      "description": "vpunk filmstyle, smbnes gamestyle, mjspooky filmstyle",
      "from": "JohnDopamine"
    },
    {
      "creation": "Fantasy Talking ComfyUI nodes",
      "type": "node",
      "description": "ComfyUI implementation of Fantasy Talking lip sync model with audio CFG controls",
      "from": "Kijai"
    },
    {
      "creation": "Variable CFG scheduling",
      "type": "workflow",
      "description": "Method to vary audio CFG over generation steps for better quality/sync balance",
      "from": "Kijai"
    },
    {
      "creation": "Blender camera control script",
      "type": "tool",
      "description": "Converts Blender camera movements to Wan camera control format",
      "from": "Kijai"
    },
    {
      "creation": "Camera control Blender addon",
      "type": "tool",
      "description": "Blender addon for camera control integration",
      "from": "toyxyz"
    }
  ]
}