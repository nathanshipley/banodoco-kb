{
  "channel": "wan_chatter",
  "date_range": "2025-10-01 to 2025-11-01",
  "messages_processed": 18574,
  "chunks_processed": 47,
  "api_usage": {
    "input_tokens": 562875,
    "output_tokens": 117261,
    "estimated_cost": 3.44754
  },
  "extracted_at": "2026-02-03T16:28:52.922885Z",
  "discoveries": [
    {
      "finding": "VACE inpainting can be combined with normal latent inpaint and differential diffusion",
      "details": "VACE inpainting is a model feature that can be layered with other inpainting techniques for better results",
      "from": "Kijai"
    },
    {
      "finding": "High Noise model fails with large inpaint masks",
      "details": "The 2.2 High Noise model has issues with large inpaint areas, dependent on mask size",
      "from": "Ablejones"
    },
    {
      "finding": "Unmerged LoRAs provide better quality than merged",
      "details": "Unmerged LoRAs use more VRAM but maintain base precision and allow runtime strength adjustment with better quality",
      "from": "Kijai"
    },
    {
      "finding": "Some T2V LoRAs don't work well with fp8/fp16 but work fine with GGUF",
      "details": "Certain T2V-trained LoRAs have compatibility issues with fp8 and fp16 formats but function properly with GGUF models",
      "from": "Samy"
    },
    {
      "finding": "LoRA can fall off when hitting VRAM limits",
      "details": "If you hit VRAM limit, the LoRA stops being used entirely",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "finding": "Palingenesis model appears to be base 2.2 + lightning merge",
      "details": "It's a rebirth/regeneration concept by merging distill loras into the base model",
      "from": "hicho"
    },
    {
      "finding": "SA-ODE sampler is just Adams-Bashforth method from 1883",
      "details": "The sampler gets 'released' by someone every 3 months or so, it's an old mathematical method",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "finding": "DYNO model achieves 70% dynamic performance of Wan2.2 14b standard model",
      "details": "Original LightX2V LoRA only reached 50%, DYNO is a fully new high-noise model trained based on 2.2, not a LoRA",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "finding": "LightX2V LoRA extraction requires very high rank",
      "details": "Extraction took rank 512 before it came close to the full model, some methods end up to rank 1000",
      "from": "Kijai"
    },
    {
      "finding": "Pose and face detection should take 1-2 seconds for 81 frames",
      "details": "If taking longer, it's likely running on CPU instead of GPU due to missing onnxruntime-gpu",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 low noise style LoRAs work with WanAnimate",
      "details": "User tested and confirmed that LoRAs trained on Wan 2.2 low noise model are compatible with WanAnimate",
      "from": "TheSwoosh"
    },
    {
      "finding": "Improved pose processing for WanAnimate",
      "details": "New preprocessor uses vitpose (the one WanAnimate was trained with), better face crop and masking through pose keypoints or bbox instead of manual point editor",
      "from": "Kijai"
    },
    {
      "finding": "WanAnimate model architecture relationship",
      "details": "Wan 2.2 low is a sort of finetune of Wan 2.1 and WanAnimate despite being called for 2.2 is also based on 2.1, which explains LoRA compatibility",
      "from": "Lodis"
    },
    {
      "finding": "GPU optimization for ComfyUI mask operations",
      "details": "Kijai rewrote blockify code in pytorch with device option, achieving massive speedup from CPU to GPU processing",
      "from": "Kijai"
    },
    {
      "finding": "Most 2.1 LoRAs work on 2.2 low noise",
      "details": "2.2 low noise is essentially a modified 2.1 model",
      "from": "mdkb"
    },
    {
      "finding": "AI Toolkit supports blockswapping by default",
      "details": "Sends 40 blocks to CPU during training, allowing 256 res video training on 5090",
      "from": "Ryzen"
    },
    {
      "finding": "HuMO hugely benefits from higher resolution",
      "details": "Ideally 720p (1280x720) for best results",
      "from": "Ablejones"
    },
    {
      "finding": "Dyno model has built-in LightX2V LoRA",
      "details": "High noise model comes with LightX2V attached, only low noise model needs separate LoRA",
      "from": "Rainsmellsnice"
    },
    {
      "finding": "New Lightning LoRAs solve motion speed issues",
      "details": "250928 LoRAs extremely good for preserving motion on high noise",
      "from": "pom"
    },
    {
      "finding": "ByteDance VAE works with VibeVoice without finetuning",
      "details": "Can take output audio tensor and feed it into ByteDance VAE for reconstruction without needing to finetune VibeVoice, though finetuning might improve speed by avoiding extra VAE refiner stage",
      "from": "MysteryShack"
    },
    {
      "finding": "Wan 2.2 MOE LoRAs work with Wan Animate using only Low Noise sampler",
      "details": "Use only the Low Noise LoRA at weight 1.0 instead of both High and Low, since Wan Animate is based on 2.1 architecture",
      "from": "Screeb"
    },
    {
      "finding": "OVI uses symmetric twin backbone design",
      "details": "Parallel audio and video branches built on identical DiT architecture, video branch initialized from Wan 2.2 5B, audio branch trained from scratch, totaling 11B parameters",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "finding": "Training High and Low Noise LoRAs separately may reduce body deformations",
      "details": "Training both LoRAs together in AI Toolkit may cause body deformities, separate training of each model appears to resolve this issue",
      "from": "Kytra"
    },
    {
      "finding": "Smooth mix model retains 99% movement/prompt following of regular wan while being faster",
      "details": "Merged light loras in a way that retained movement and prompt adherence, works at 1cfg 8 steps",
      "from": "Ada"
    },
    {
      "finding": "Standard Wan2.2 can achieve similar results to smooth mix using 3 sampler setup",
      "details": "Though it takes more steps, can get similar motion quality",
      "from": "garbus"
    },
    {
      "finding": "2.2 loras work fine on 2.1, especially if loaded on low noise only",
      "details": "For character loras and details, can load on low model only. Big changes or motion need high noise too",
      "from": "Juampab12"
    },
    {
      "finding": "WanAnimate V2 preprocessing nodes workflow available",
      "details": "Kijai released example workflow V2 that uses preprocessing nodes, with fixed scaled fp8 model",
      "from": "Charlie"
    },
    {
      "finding": "Smooth Mix model trades motion characteristics for frame interpolation compatibility",
      "details": "Exaggerates motion at 16fps which looks better after frame interpolation to 32fps, designed for 30fps playback",
      "from": "Rainsmellsnice"
    },
    {
      "finding": "HuMo can generate 277 frames (~11s) in one shot with big GPU",
      "details": "More frames causes original image reference to degrade over time",
      "from": "Dever"
    },
    {
      "finding": "Dual model setups fail when exceeding frame limits",
      "details": "High Noise model limited to 81 frames, Low Noise can handle 121+ frames. Using Phantom/MagRef in LN slot with >81 frames causes camera blending compensation",
      "from": "mdkb"
    },
    {
      "finding": "Radial Attention causes black video output but reduces generation time significantly",
      "details": "Issue resolved by updating sage",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "finding": "T2V requires only 1+1 steps with certain configurations",
      "details": "For T2V generation, can use just 1+1 steps",
      "from": "hicho"
    },
    {
      "finding": "5B + 2.2 low model bypass technique",
      "details": "Can combine 5B content with Wan 2.2 graphics by using 5B + 2.2 low to bypass its VAE",
      "from": "hicho"
    },
    {
      "finding": "Black output caused by incorrect resolution",
      "details": "Resolution must follow specific rules - multiples of 64 work, with video token number divisible by 128",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "finding": "Video token calculation formula",
      "details": "For Wan 2.1 and 2.2 14B: width/16 * height/16 * (length+3)/4. For Wan 2.2 5B: width/32 * height/32 * (length+3)/4. Result must be divisible by 128",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "finding": "Only one dimension needs to be 128 multiple",
      "details": "Height X Width must be divisible by 128, but easiest way is having at least one dimension as multiple of 128",
      "from": "Instability01"
    },
    {
      "finding": "Speed LoRAs work best on low model only",
      "details": "Character LoRAs from Wan 2.1 work better when connected to low noise model only",
      "from": "Dream Making"
    },
    {
      "finding": "I2V generates slower motions",
      "details": "I2V models seem to overall generate slower motions compared to T2V",
      "from": "Lodis"
    },
    {
      "finding": "T2V generally superior to I2V",
      "details": "T2V is generally far better with video models because it resembles optimal transport far better",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "finding": "FP8 scaled model causing pixelated/blurry outputs",
      "details": "User experiencing pixelated outputs with FP8 scaled model, switching to FP16 resolved the issue completely",
      "from": "Ryzen"
    },
    {
      "finding": "Pytorch native RMS norm function provides significant speedup",
      "details": "New native rms_norm function is ~9x faster than custom Wan rms norm when not using torch compile, though whole generation speedup is smaller since it's only small part of inference",
      "from": "Kijai"
    },
    {
      "finding": "Lightning LoRA settings for high quality fast generation",
      "details": "Using Lightning LoRA v0.7 strength with CFG 1.4 on low-noise phase, achieving 720p in 160 seconds on RTX 5090",
      "from": "Lan8mark"
    },
    {
      "finding": "Dynamic shift parameter tuning required",
      "details": "Shift parameter needs to be tuned depending on other settings and is set incorrectly in default ComfyUI workflow",
      "from": "Lan8mark"
    },
    {
      "finding": "Converting latent to regular images, upscaling, then converting back to latent works better than direct latent upscaling",
      "details": "User found that direct latent upscaling is poorly implemented and doesn't achieve good quality, so they decode to images first, upscale x2, then encode back to latent",
      "from": "Lan8mark"
    },
    {
      "finding": "Manual denoise strength calibration between 0.2-0.4 produces better results than automatic calculation",
      "details": "Comparing automatic denoise strength calculated by KSampler Advanced vs manually calibrated strength shows significant improvement in facial expression and details",
      "from": "Lan8mark"
    },
    {
      "finding": "High-noise phase at native 720p isn't necessary - can halve resolution for high-noise then upscale for low-noise",
      "details": "User cuts resolution in half on both sides for high-noise steps, then upscales x2 and feeds into low-noise phase without quality loss. Achieves 81 frames at 720p in 160 seconds on 5090",
      "from": "Lan8mark"
    },
    {
      "finding": "CLIP vision model not necessary for Wan 2.2",
      "details": "Even there it's not necessary, not that it ever was fully necessary with 2.1 either",
      "from": "Kijai"
    },
    {
      "finding": "OVI is 11B parameters total",
      "details": "It's Wan 2.2 5B twice and then some extra param",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "finding": "AniSora 3.2 is already distilled",
      "details": "The 3.2 is already distilled, so no need for that, the older versions do work with the loras, and it's better than lightx2v or lightning too for high noise, 8 steps",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 5B is better base than 1.3B",
      "details": "The 1.3B was described as 'too squishy' while 5B offers something better that's still cheaper than 14B",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "finding": "OVI is 11B audio-video model",
      "details": "Based on Wan2.2 5B and MM Audio, works like Veo 3/Sora 2 for audio-video generation",
      "from": "yi"
    },
    {
      "finding": "rCM LoRA available as SOTA distillation for Wan",
      "details": "Nvidia's 'SOTA distillation' method, works like LightX2V but supposedly better",
      "from": "yi"
    },
    {
      "finding": "Consistency models must be distillations",
      "details": "Consistency models map points on trajectory of parent model directly to finished output, they find shortcuts but need existing trajectories from parent model",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "finding": "Wan 2.2 performs better on consumer but Step benchmarks higher in research",
      "details": "Step consistently benches higher on arxiv papers and is considered stronger in research community, but Wan is more popular with consumers",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "finding": "Wan animate temporal mask works for keeping/generating frames",
      "details": "The wan-fun inpaint model can do white/black mask for keeping/generating frames and has temporal mask functionality",
      "from": "Kijai"
    },
    {
      "finding": "e4m3fn vs e5m2 FP8 model performance comparison",
      "details": "e5m2 is about 2 seconds faster than e4m3fn (34s vs 36s), with similar VRAM usage. e4s work better on 4090+, e5s work better on cards lower than 4090, especially 30 series",
      "from": "happy.j"
    },
    {
      "finding": "Torch Compile node only works with e5 models on fresh install",
      "details": "Previously worked with e4 models but now only functions with e5 models after fresh ComfyUI install",
      "from": "\u25b2"
    },
    {
      "finding": "OVI lip-sync quality varies with steps",
      "details": "50 steps works better than lower step counts for lip-sync quality in OVI model",
      "from": "tarn59"
    },
    {
      "finding": "MAGREF working with WAN 2.2 for character consistency",
      "details": "Successfully integrated MAGREF with WAN 2.2 for good multi-character consistency in I2V generation",
      "from": "Elvaxorn"
    },
    {
      "finding": "SecNodes segmentation accuracy",
      "details": "SEC segmentation model works very well for object tracking and masking, with high accuracy even through cuts and when objects are out of frame",
      "from": "V\u00e9role"
    },
    {
      "finding": "SAM 3 waitlist is available",
      "details": "SAM 3 has been announced by Meta with a waitlist signup",
      "from": "Kijai"
    },
    {
      "finding": "SEC masking is solid for Wan animate",
      "details": "SEC works better than SAM2 alone and is still SAM2.1 with extra guidance for segmentation",
      "from": "ArtOfficial"
    },
    {
      "finding": "New RCM LoRA works well for character consistency",
      "details": "RCM distill LoRA by Nvidia devs works as alternative to lightx2v, provides good character consistency with prompt adherence",
      "from": "Elvaxorn"
    },
    {
      "finding": "MagRef provides excellent character consistency",
      "details": "MagRef works with both T2V and I2V, outperforms other consistency models in testing",
      "from": "Elvaxorn"
    },
    {
      "finding": "Linear quadratic scheduler works better than simple scheduler for new Lightx2v MoE",
      "details": "Simple scheduler causes ghosting issues with the new model, linear quadratic fixes this",
      "from": "FL13"
    },
    {
      "finding": "New Lightx2v 2.2 I2V full model works better than LoRA version",
      "details": "LoRA version has ghosting and glitchy output, full model produces cleaner results",
      "from": "JohnDopamine"
    },
    {
      "finding": "Kijai extracted LoRA works better than official repo LoRA",
      "details": "Official LoRA from lightx2v repo doesn't work properly, Kijai's extraction fixes the issues",
      "from": "FL13"
    },
    {
      "finding": "New Lightx2v I2V only needs high noise LoRA, can use 2.1 LoRA for low noise",
      "details": "Low noise component can reuse existing 2.1 lightx2v LoRA",
      "from": "Kijai"
    },
    {
      "finding": "New Light MoE High LoRA provides significantly more motion",
      "details": "Using Kijai's extracted 64 rank LoRA at strength 3.00 gives lots more motion compared to previous setups",
      "from": "phazei"
    },
    {
      "finding": "FlashVSR uses only 11GB VRAM",
      "details": "Peak VRAM usage is 11GB, much less than any other diffusion based upscaler",
      "from": "yi"
    },
    {
      "finding": "Linear quadratic scheduler is most reliable with new LoRA",
      "details": "After experimenting with few schedulers, linear quadratic is the most reliable when using new LoRA",
      "from": "FL13"
    },
    {
      "finding": "Triton 3.5.0 fixes fp8 compile issues",
      "details": "New triton release specifically mentions fp8 patch that's been merged to fix compilation issues with fp8 models",
      "from": "phazei"
    },
    {
      "finding": "FlashVSR can do 384->1024 upscale for 81 frames in ~6 seconds using only 1 step with 1.3B model",
      "details": "Whole process took approximately 6 seconds, with VAE decode being 3 seconds vs 20 seconds for normal VAE",
      "from": "Kijai"
    },
    {
      "finding": "Running only FlashVSR low q projection on image input and adding to first block result works with single step without using their VAE",
      "details": "Achieved upscaling results without the full FlashVSR pipeline",
      "from": "Kijai"
    },
    {
      "finding": "TaylorSeer Lite version provides significant speed improvement over TeaCache",
      "details": "Getting around 5s/it compared to 15s/it, works well but quality takes a hit with lots of movement",
      "from": "Zabo"
    },
    {
      "finding": "CFG scheduling can be done with list of values for each step in Kijai's wrapper",
      "details": "Can provide different CFG values for each sampling step",
      "from": "Kijai"
    },
    {
      "finding": "PyTorch 2.9.0 stable breaks Wan VAE, causing double VRAM usage",
      "details": "PyTorch 2.9.0 stable causes VAE to use twice the VRAM in both native and wrapper nodes, but specific nightly builds work fine",
      "from": "Kijai"
    },
    {
      "finding": "fp32 conv3d workaround reduces VRAM usage on PyTorch 2.9.0+",
      "details": "Running conv3d operations in fp32 bypasses the PyTorch bug while using less VRAM than full fp32 VAE",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.2 high noise model offers significantly more motion than 2.1",
      "details": "When properly used with both high and low noise models, 2.2 generates way more motion than 2.1 ever could",
      "from": "Kijai"
    },
    {
      "finding": "FlashVSR combined with Cinescale LoRA works well with 0.75 denoise",
      "details": "Provides 20% speed increase with no noticeable quality loss, helps reduce over-sharpness",
      "from": "Elvaxorn"
    },
    {
      "finding": "New Wan2.2 4-step distill models are significantly better",
      "details": "Characters now have expressions and movements, prompt adherence is much improved",
      "from": "Zabo"
    },
    {
      "finding": "New distill models work at higher resolutions",
      "details": "Can now generate at 960x960",
      "from": "Zabo"
    },
    {
      "finding": "CineScale uses specific RoPE scaling values",
      "details": "Original used 1, 20, 20 then 1, 25, 25 for spatial rope scaling",
      "from": "DawnII"
    },
    {
      "finding": "CineScale LoRA possibly trained with scale of 20",
      "details": "RoPE scaling attempts to make larger resolutions work without repeat effect",
      "from": "Kijai"
    },
    {
      "finding": "VACE can be used for full frame inpainting to stitch videos",
      "details": "Using 10 frames input and output with masks for smoother video connections",
      "from": "Koba"
    },
    {
      "finding": "Torch 2.9.0 and 2.10 have VAE VRAM bug",
      "details": "Makes conv3d operations use 3x more VRAM when using half precision, affects Wan VAE",
      "from": "Kijai"
    },
    {
      "finding": "Accidentally combining new distill 4 step base model with 256 rank lightx2v lora massively amplifies motion",
      "details": "When forgetting to turn off the lora while using the distill model, the combination achieves much more motion than either component alone",
      "from": "Ruairi Robinson"
    },
    {
      "finding": "FlashVSR needs at least 2x upscale for good results",
      "details": "Using less than 2x upscale produces poor quality, especially for faces. 4x upscale works well but uses 20GB VRAM",
      "from": "Kijai"
    },
    {
      "finding": "RoPE frequency scaling prevents repetition artifacts at high resolutions",
      "details": "Especially important for 1080p and up, particularly high vertical resolutions to avoid repeat patterns",
      "from": "Kijai"
    },
    {
      "finding": "WAN models can learn detailed object features from small training sets",
      "details": "Training a WAN LoRA on just 10 images of the same green car produced impressive detail learning, though color flexibility was limited",
      "from": "Dever"
    },
    {
      "finding": "Wan 2.2 first frame corruption on longer generations",
      "details": "Wan 2.2 corrupts first 5-16 frames on longer generations (121 frames). Only the 5B model is truly 24fps",
      "from": "garbus"
    },
    {
      "finding": "FP8 vs FP16 quality difference in video extension",
      "details": "FP/BF16 models retain much more quality than FP8 when extending videos using last frame, difference becomes huge at 25 seconds",
      "from": "Zabo"
    },
    {
      "finding": "CFG on high noise steps improves results",
      "details": "Adding CFG on first step or all high steps makes huge difference in test results",
      "from": "Dever"
    },
    {
      "finding": "FlashVSR effective for fixing bad renders",
      "details": "FlashVSR at 0.8 strength can fix up bad looking renders very nicely, only needs 1 step",
      "from": "mdkb"
    },
    {
      "finding": "InfiniteTalk can do vid2vid for adding lipsync",
      "details": "InfiniteTalk supports vid2vid workflow for just adding lipsync, but requires figuring out masking or it changes rest of video",
      "from": "Kijai"
    },
    {
      "finding": "Wan VAE excellent compression for long videos",
      "details": "Wan 2.1 VAE can encode and decode long high quality videos (16fps and 24fps tested) with minimal quality loss",
      "from": "Ablejones"
    },
    {
      "finding": "New sageattn version that allows torch.compile reduces VRAM usage",
      "details": "Max allocated memory reduced from 15.749 GB to 14.667 GB, with tiny speed difference",
      "from": "Kijai"
    },
    {
      "finding": "Ditto LoRAs work with simple style transfer prompts",
      "details": "Uses prompts like 'Make it a Japanese anime style', 'Make it Pixel Art video', etc. for video style transfer",
      "from": "Kijai"
    },
    {
      "finding": "LoRA rank can be significantly reduced while maintaining quality",
      "details": "Original rank 128 vs average rank 29 showed minimal quality difference",
      "from": "Kijai"
    },
    {
      "finding": "Wan 2.1 Lightx I2V 480p still performs better than newer variants for certain tasks",
      "details": "Better lighting and look than 2.2 lightning for low noise, though lacks motion unless at very high strength",
      "from": "FL13"
    },
    {
      "finding": "Ditto LoRAs can turn VACE into RGB video in, stylized video out",
      "details": "These loras are designed for style transfer from input video to stylized output",
      "from": "Draken"
    },
    {
      "finding": "WanAnimate can do face control in addition to pose",
      "details": "More interesting than basic pose stuff since it includes facial animation",
      "from": "Kijai"
    },
    {
      "finding": "Vitpose pose detector can detect animals as well as humans",
      "details": "There are also specific animal pose detectors available for use with WanAnimate",
      "from": "Kijai"
    },
    {
      "finding": "Ditto global lora recognizes some characters",
      "details": "Can prompt with 'make it Elsa from Frozen' and it understands the character",
      "from": "DawnII"
    },
    {
      "finding": "Ditto full module vs LoRA comparison shows full module has less artifacting",
      "details": "Full module seems to have less artifacting and ghosting and is more reliable than using the LoRA",
      "from": "Hashu"
    },
    {
      "finding": "Krea model is based on Wan 2.1 and works like CausVid",
      "details": "Supposed to be used 3 latents at a time, like causvid, but surprisingly works with 21 latents",
      "from": "Kijai"
    },
    {
      "finding": "Krea realtime LoRA extracted from Wan 2.1 can be used on Wan 2.2",
      "details": "Using 4 steps similar to lighting, strength 4 on HN and 1.1 on LN with HPS at 0.75 to reduce noise",
      "from": "aipmaster"
    },
    {
      "finding": "MoCha only segments first frame, not the video",
      "details": "It's the simplest thing ever - ref is inserted last like Phantom",
      "from": "Kijai"
    },
    {
      "finding": "Krea realtime achieves 11fps inference speed using 4 steps on NVIDIA B200 GPU",
      "details": "Distilled from Wan 2.1 14B using Self-Forcing technique",
      "from": "Dever"
    },
    {
      "finding": "WoW world model can generate extremely close to real life physics",
      "details": "Can take a photo from robot vision, say 'pick up the cup' and it makes the video for training",
      "from": "Draken"
    },
    {
      "finding": "MoCha (from Orange-3DV-Team) is a video generation model that allows character/subject replacement using reference images and masks",
      "details": "Model concatenates control video with mask and reference in frame dimension, requires prompting for best results",
      "from": "Kijai"
    },
    {
      "finding": "MoCha works with context windows for longer videos",
      "details": "Each window contains noise + mask + ref, but longer sequences can cause issues with multiple characters",
      "from": "Kijai"
    },
    {
      "finding": "MoCha uses double frame dimension which increases VRAM usage significantly",
      "details": "Uses about double VRAM because whole control video is concatenated in frame dimension",
      "from": "Kijai"
    },
    {
      "finding": "Krea LoRA produces more realistic results compared to other 4-step models",
      "details": "aipmaster reports it looks less artificial than other fast models, generates 140 seconds 720p in ~140 seconds on 4090",
      "from": "aipmaster"
    },
    {
      "finding": "New Wan 2.2 distill LoRAs were updated",
      "details": "lightx2v updated their Wan 2.2 distill LoRAs, marked as version 1022 (Oct 22)",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "finding": "Rolling Forcing model released using WAN 2.1-14B",
      "details": "New reference-to-video model, shows better temporal consistency in comparisons",
      "from": "Kytra"
    },
    {
      "finding": "New LightX2V LoRAs dated 1022 released",
      "details": "New LoRAs with date 1022 released but unclear if they're just renames or actual new versions",
      "from": "Draken"
    },
    {
      "finding": "New LightX2V LoRA shows improved performance",
      "details": "The new 1022 LoRA performs better than previous versions, especially for complex prompts that failed on older versions",
      "from": "Ada"
    },
    {
      "finding": "QwenImage VAE converted to ComfyUI format",
      "details": "Kijai successfully converted e2e-qwenimage-vae to ComfyUI format using Claude-generated reverse script",
      "from": "Kijai"
    },
    {
      "finding": "QwenImage VAE shows more detail",
      "details": "QwenImage VAE produces more detailed results compared to Wan VAE",
      "from": "Kijai"
    },
    {
      "finding": "Rolling Force model is 1.3B with 17GB due to multiple weight copies",
      "details": "Model contains generator, critic, and generator_ema weights, causing large file size despite being 1.3B parameters",
      "from": "Kijai"
    },
    {
      "finding": "Stable Video Infinity (SVI) works with base 2.1 I2V model using LoRAs",
      "details": "SVI uses base Wan 2.1 I2V 480p model with special LoRAs and code modifications for infinite video generation",
      "from": "Kijai"
    },
    {
      "finding": "New lightx2v LoRA version released",
      "details": "The wan2.2_i2v_A14b_high_noise_lora_rank64_lightx2v_4step_1022.safetensors is confirmed to be different from the old MoE version and performs better for 2D animation",
      "from": "Kijai"
    },
    {
      "finding": "SVI LoRA padding mechanism",
      "details": "In shot workflow, padding is set to -1 which pads all frames with the reference image instead of black pixels",
      "from": "Kijai"
    },
    {
      "finding": "SVI uses different frame configurations",
      "details": "Shot-lora uses single start image with random_ref padded for whole video length, while film lora uses 5 start images with no padding",
      "from": "Kijai"
    },
    {
      "finding": "Multiple frames can be fed to I2V node",
      "details": "The WanVideo ImageToVideo Encode node can accept multiple images in start_image input, same as Fun InP",
      "from": "Kijai"
    },
    {
      "finding": "SVI reference frame structure",
      "details": "The ref is the original init image, the first frame is the previous last frame from the generation",
      "from": "Kijai"
    },
    {
      "finding": "SVI film LoRA works with Wan 2.2",
      "details": "Users successfully tested SVI film LoRA with Wan 2.2 workflows, showing coherent results",
      "from": "voxJT"
    },
    {
      "finding": "SVI uses 5 frames from end of previous generation for continuity",
      "details": "The film LoRA uses 5 frames from the end of the previous generation to maintain motion continuity",
      "from": "Kijai"
    },
    {
      "finding": "LightX2V released new lightweight VAE",
      "details": "31MB VAE that's much faster but with significant quality drop compared to standard VAE",
      "from": "JohnDopamine"
    },
    {
      "finding": "Ditto can generate longer videos beyond 3 seconds",
      "details": "Despite documentation suggesting 3s limit, users successfully generated 20s+ videos (600 frames)",
      "from": "JohnDopamine"
    },
    {
      "finding": "SVI film LoRA prevents I2V model flash/saturation issues",
      "details": "The LoRA helps maintain consistency and reduces oversaturation in I2V chaining",
      "from": "voxJT"
    },
    {
      "finding": "SVI supports T2V by using black input image",
      "details": "You can bypass color match nodes and use SVI with T2V by providing a black input image",
      "from": "garbus"
    },
    {
      "finding": "SVI LoRAs work with native ComfyUI nodes",
      "details": "Film, shot, and other SVI LoRAs converted to fp16 and native compatible format work well with native ComfyUI Wan video nodes",
      "from": "Ablejones"
    },
    {
      "finding": "Native ComfyUI handles masking automatically for video extensions",
      "details": "The native ComfyUI nodes create default masks and pad starting images properly without manual mask input",
      "from": "Ablejones"
    },
    {
      "finding": "SVI-shot uses reference frame padding while SVI-film uses zero padding",
      "details": "SVI-shot repeats the original input image for padding frames, while SVI-film uses zero padding (black frames in diffusers, 0.5 in ComfyUI)",
      "from": "Kijai"
    },
    {
      "finding": "ComfyUI uses 0-1 scale while diffusers uses -1 to 1 scale",
      "details": "In ComfyUI middle value is 0.5, in diffusers/original codes middle is 0. Zero padding in their code equals 0.5 padding in ComfyUI",
      "from": "Kijai"
    },
    {
      "finding": "SVI-shot works with reference frames, SVI-film fails completely",
      "details": "When using reference frame repetition technique, SVI-shot maintains scene continuity but SVI-film produces static output",
      "from": "Ablejones"
    },
    {
      "finding": "MTV Crafter works better when input pose is close to target",
      "details": "Model performs better when continuing from last frame rather than starting from very different poses, spends fewer frames transitioning",
      "from": "Kijai"
    },
    {
      "finding": "HoloCine can generate 121 frames straight out with just weights only",
      "details": "Kijai tested HoloCine weights without full implementation and got 121 frame generations, though it still loops and needs proper attention code",
      "from": "Kijai"
    },
    {
      "finding": "HoloCine uses structured prompting with [character1], [character2] tags and shot descriptions",
      "details": "Uses format like '[global caption] description [character1] details [character2] details This scene contains X shots. [per shot caption] shot descriptions [shot cut]'",
      "from": "Tachyon"
    },
    {
      "finding": "Method to improve SVI consistency by using reference image as first frame",
      "details": "VK found that using original reference image as first frame followed by last four frames from prior render helps maintain character consistency better than using just 5 frames from prior render",
      "from": "VK (5080 128gb)"
    },
    {
      "finding": "InfiniteTalk provides best consistency with minimal degradation/colorshift",
      "details": "Reported as having crazy good consistency compared to other extension methods",
      "from": "seitanism"
    },
    {
      "finding": "Context windows are 2x+ slower per frame due to overlap mechanics",
      "details": "161 frames takes 4 minutes instead of 2 minutes for twice the length due to doing 3-4 runs to join just 2 segments",
      "from": "blake37"
    },
    {
      "finding": "SVI \"random ref\" naming refers to training, not inference",
      "details": "In inference code it's actually the input image that's used, called 'random ref' because for training it's random",
      "from": "Kijai"
    },
    {
      "finding": "Different SVI LoRAs use different padding strategies",
      "details": "Tom LoRA doesn't use padding (0), only single input for normal I2V. Dance, talk, shot use reference padding which makes sense as they mostly stay in frame",
      "from": "Kijai"
    },
    {
      "finding": "Shot LoRA can help with degradation",
      "details": "Shot LoRA always has the original input there as reference, unlike film which is more creative and doesn't use that",
      "from": "Kijai"
    },
    {
      "finding": "Wan I2V can already do reference generation without MagRef",
      "details": "Wan 2.2 I2V can take an image of a face with white around and do t2v with reference, but MagRef makes it insanely good at this task",
      "from": "Draken"
    },
    {
      "finding": "More frames forwarded with film keeps motion better",
      "details": "With 2.2 i2v, 16 frames forwarded with film was working pretty well for motion continuation",
      "from": "DawnII"
    },
    {
      "finding": "5 frames has less degradation but worse motion continuation",
      "details": "Trying with 5 frames shows less image degradation but motion continuation not as good compared to 16 frames",
      "from": "DawnII"
    },
    {
      "finding": "Different content shows flash artifacts more",
      "details": "Flash is most noticeable in dark scenes/areas, less noticeable in white backgrounds or sky",
      "from": "Draken"
    },
    {
      "finding": "i2v model supports multiple frame inputs natively",
      "details": "The i2v model actually does support multiple frame inputs natively, not just single frame",
      "from": "Ablejones"
    },
    {
      "finding": "svi-film lora enables multi-frame input for base i2v",
      "details": "Normal I2V doesn't do well with more than one input image, but the film loras make that work",
      "from": "Kijai"
    },
    {
      "finding": "Position of character in reference frame dictates ViT pose starting position",
      "details": "The position of the character in the reference frame (in screen space) dictates the starting position of the ViT pose (in screen space)",
      "from": "Neex"
    },
    {
      "finding": "Film lora may be better to skip on first sampler",
      "details": "When using film lora that it's better to not use it for the first sampler - it seems weird motion-wise if it starts from single image",
      "from": "Kijai"
    },
    {
      "finding": "Two different film lora versions available",
      "details": "There are two film loras - regular and film-opt version, with film-opt being a newer version",
      "from": "Ablejones"
    },
    {
      "finding": "HoloCine enables long 241-frame generation in one pass without context windows",
      "details": "User generated 720x720x241 frames (15 seconds) in one go using 3 high 3 low steps with lightx2v lora",
      "from": "seitanism"
    },
    {
      "finding": "HoloCine sparse model enables ~1 minute video generation with similar VRAM to 81 frames",
      "details": "Sparse model maintains almost same visual quality while enabling long coherent cinematic video generation",
      "from": "mamad8"
    },
    {
      "finding": "SVI Shot LoRA doesn't work with Wan 2.2",
      "details": "People have tried using SVI Shot LoRA with 2.2 but it doesn't work even when used normally",
      "from": "Kijai"
    },
    {
      "finding": "Film LoRA can somewhat work with 2.2",
      "details": "Unlike Shot LoRA, Film LoRA doesn't do anything special and can work with 2.2",
      "from": "Kijai"
    },
    {
      "finding": "VAE decode/encode causes degradation that can't be avoided in latent space",
      "details": "Multiple users confirmed that latent manipulation always causes artifacts like flashing, color shifting, or ghosting when first frame is modified",
      "from": "lemuet"
    },
    {
      "finding": "FusionX LoRA with WAN 2.1 produces better results than with WAN 2.2 and LightX2V LoRA",
      "details": "Based on tests comparing the combinations",
      "from": "Govind Singh"
    },
    {
      "finding": "LongCat-Video can be run in WanVideoWrapper but doesn't work properly",
      "details": "Result doesn't have much to do with the prompt",
      "from": "Kijai"
    },
    {
      "finding": "Holocine high model + WAN 2.2 low model produces better quality",
      "details": "Combines better detail from original low model with Holocine capabilities",
      "from": "avataraim"
    },
    {
      "finding": "LongCat generates 6 second pieces in T2V, I2V, or multi-frame I2V mode",
      "details": "Has same repeating issue as WAN when trying 12+ seconds in one go",
      "from": "aikitoria"
    },
    {
      "finding": "WAN 2.2 works better with Qwen image inputs",
      "details": "Qwen images are soft enough that they don't cross into detail level where VAE fails into noise grids",
      "from": "aikitoria"
    },
    {
      "finding": "SVI film with WAN 2.2 doesn't meaningfully differ from WAN 2.2 alone",
      "details": "WAN 2.2 already does motion continuation by itself using previous frames",
      "from": "lemuet"
    },
    {
      "finding": "Wan 2.2 has sparse attention and distill LoRA as separate features",
      "details": "Both are available separately for optimization",
      "from": "Ada"
    },
    {
      "finding": "Video extension with 13 start frames works with LongCat",
      "details": "Only works if character keeps looking at camera at end of segment",
      "from": "aikitoria"
    },
    {
      "finding": "HoloCine generates 241 frames (15-16 seconds) in one pass",
      "details": "Uses windowed attention where frames attend only to global caption and shot-specific captions",
      "from": "shaggss"
    },
    {
      "finding": "SVI-Film method works with Wan 2.2 using 5-frame batches",
      "details": "Put 5-frame batch in ImageToVideo encode node start_image input, wrapper handles automatically",
      "from": "lemuet"
    },
    {
      "finding": "Kijai successfully merged LongCat distill LoRA by renaming layers and splitting fused components",
      "details": "Renamed layers, split fused qkv for self attention and kv for cross attention",
      "from": "Kijai"
    },
    {
      "finding": "LongCat handles T2V and I2V in single model",
      "details": "Model combines t2v and i2v capabilities, released main model and 2 loras that run as separate modules",
      "from": "Kijai"
    },
    {
      "finding": "LongCat crossattention split improvement",
      "details": "Fixed small mistake in LongCat crossattention split so it doesn't fade",
      "from": "Kijai"
    },
    {
      "finding": "LongCat attention split for input images",
      "details": "Has to do attention separately for the input image(s) in LongCat, which is why it diverges from input so much",
      "from": "Kijai"
    },
    {
      "finding": "Holocine shot control with frame cuts",
      "details": "Can control every shot duration with frame cuts like '50,100,150,200,250' for 5 shots, giving 50 frames per shot linearly",
      "from": "avataraim"
    },
    {
      "finding": "Resolution bucket importance for LongCat",
      "details": "LongCat is picky about input resolution and should test with bucket resolutions. 640x608 vs 640x640 shows significant quality difference",
      "from": "Kijai"
    },
    {
      "finding": "LongCat is 15fps by default and interpolates well to 30fps",
      "details": "The model generates at 15fps natively, and their refine pipeline trilinear interpolates from 15fps 480p to 30fps 720p",
      "from": "Kijai"
    },
    {
      "finding": "LongCat uses step and cfg distill LoRA",
      "details": "With distill LoRA can use 16 steps, without it requires at least 30 steps with cfg 4.0",
      "from": "Kijai"
    },
    {
      "finding": "LongCat I2V works like 5B or Pusa I2V",
      "details": "Uses any number of start_frames as extra latents, code replaces that part of noise with image and sets corresponding timestep to 0",
      "from": "Kijai"
    },
    {
      "finding": "SageAttention 1.0.6 causes first frame flash issue with LongCat",
      "details": "Updating to SageAttention 2.2.0 or switching to SDPA fixes the I2V first frame flash problem",
      "from": "Hashu"
    },
    {
      "finding": "fp16 precision causes black/NaN outputs with Wan models",
      "details": "fp16 consistently produces NaN results leading to black output, bf16 works fine",
      "from": "Kijai"
    },
    {
      "finding": "Changing to SPDA fixed black frames issue",
      "details": "User had black frame issues that were resolved by switching attention mechanism to SPDA",
      "from": "scf"
    },
    {
      "finding": "LongCat requires 17 frames minimum for proper extension, not 16",
      "details": "Original code uses 13 frames, should follow the 4+1 rule. 16 frames wouldn't work proper for extension",
      "from": "Kijai"
    },
    {
      "finding": "LongCat refinement lora runs at low sigmas without CFG",
      "details": "Refinement lora is meant to run at low sigmas without cfg, can adjust effect with lora strength",
      "from": "Kijai"
    },
    {
      "finding": "LongCat generates at 720p natively, not just 480p",
      "details": "It can generate 720p on its own, though their pipeline uses 2 passes with refiner lora",
      "from": "scf"
    },
    {
      "finding": "Holocine 241 must be at end of list or it crashes",
      "details": "Always use 241 in end of list, otherwise crashes",
      "from": "avataraim"
    },
    {
      "finding": "FP32 norm operations with fp8_scaled affects Wan 2.2 quality",
      "details": "Doing all norm operations with fp32 (using fp32 weight even) with fp8_scaled has an effect in Wan 2.2",
      "from": "Kijai"
    },
    {
      "finding": "Wan VAE noise patterns can be reduced with spacepxl's 2x upscaler VAE",
      "details": "Custom trained VAE decoder that acts as free 2x upscaler and kills noise grid patterns for images",
      "from": "spacepxl"
    },
    {
      "finding": "NeatVideo can filter out Wan VAE noise patterns in video",
      "details": "Enable temporal and set spatial to low weight or disabled to preserve detail while removing noise",
      "from": "spacepxl"
    },
    {
      "finding": "Magref can be used as low noise model in Wan 2.2 workflows",
      "details": "Using Magref with right loras for Wan 2.2 low noise reduces context window artifacts and improves image quality",
      "from": "blake37"
    },
    {
      "finding": "Uni3C controlnet needs higher strength at larger resolutions",
      "details": "Works well at 512x512 but needs increased strength for higher resolutions, should not increase strength on low noise side",
      "from": "Kijai"
    },
    {
      "finding": "LongCat refinement lora scaling issue fixed",
      "details": "Modulation layers were merged with alpha 0.5 but forgot to scale properly, causing it to not work well at 1.0 strength",
      "from": "Kijai"
    },
    {
      "finding": "WAN 2x VAE provides significant quality improvements with sharper images and better detail",
      "details": "Works with existing WAN models using same encoder/latent space, decoders are cross compatible",
      "from": "spacepxl"
    },
    {
      "finding": "Holocine implementation now includes proper attention code rather than just weights",
      "details": "Shot attention requires structured prompt metadata and text_cut_positions for multi-shot generation",
      "from": "NebSH"
    },
    {
      "finding": "LongCat refinement LoRA works at strength 1.0 with specific sampling parameters",
      "details": "Use 50 steps with Euler or UniPC, start from step 45, CFG 1.0",
      "from": "Kijai"
    },
    {
      "finding": "Video extension workflows use vid2vid process starting at later steps like really low denoise",
      "details": "Original code decodes, upscales in pixel space, encodes and runs as vid2vid",
      "from": "Kijai"
    },
    {
      "finding": "Smooth window parameter in Holocine creates interesting transition effects between different scenes",
      "details": "Values tested include 0, 6, and 12 smooth window settings",
      "from": "NebSH"
    },
    {
      "finding": "VRAM usage difference between operating systems",
      "details": "4GB less VRAM used on Linux compared to Windows with torch compile",
      "from": "Kijai"
    },
    {
      "finding": "Memory cleanup node significantly reduces RAM usage",
      "details": "Using Comfyui-Memory_Cleanup node reduced RAM usage from 99% to 45%, and generation time from 400s to 100s on 3060",
      "from": "avataraim"
    },
    {
      "finding": "VACE WAN 2.2 dual model workflow can run on 3060",
      "details": "Can run VACE WAN 2.2 dual model workflow with controlnet to 720p in 16 minutes on a 3060",
      "from": "mdkb"
    },
    {
      "finding": "PyTorch RAM reservation behavior",
      "details": "PyTorch can't literally move something from RAM to VRAM, it reserves the same amount from RAM",
      "from": "Kijai"
    },
    {
      "finding": "Qwen VL by AI lab OOMs on high resolution images",
      "details": "OOMs on 2K resolution images, works fine when using resize node at 512x512",
      "from": "hicho"
    },
    {
      "finding": "LongCat uses Wan 2.1 VAE but has significant architectural differences",
      "details": "Uses Wan architecture but trained from scratch with adjustments to model size and structure",
      "from": "JohnDopamine"
    },
    {
      "finding": "Wan 2.2 I2V can do multi-frame interpolation without any LoRA",
      "details": "Base Wan 2.2 I2V model naturally supports interpolation between multiple frames",
      "from": "Kijai"
    },
    {
      "finding": "Morph LoRA appears to be mostly regular Wan 2.2 functionality",
      "details": "Testing shows 95% of functionality is just Wan 2.2 A14B I2V, LoRA provides minimal improvement",
      "from": "Kijai"
    },
    {
      "finding": "LightX2V 1030 model significantly improved performance",
      "details": "New wan2.2_i2v_A14b_high_noise_lightx2v_4step_1030.safetensors model shows substantial quality improvements over previous versions",
      "from": "Zabo"
    },
    {
      "finding": "Torch compile causes 50% VRAM increase but 2x speed drop in some cases",
      "details": "User found disabling torch compile reduced VRAM from 15.5+2 shared to 11.3GB plain, speed dropped from 27 it/s to 57 it/s overall",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "New LoRA storage method improves block swap efficiency",
      "details": "Unmerged LoRAs are now stored as buffers in layers themselves, moved WITH block swap instead of always CPU to GPU",
      "from": "Kijai"
    },
    {
      "finding": "ChronoEdit uses Wan for image editing",
      "details": "NVIDIA's ChronoEdit model is based on Wan 14B and uses temporal understanding for image edits, working surprisingly well",
      "from": "aikitoria"
    },
    {
      "finding": "LongCat provides motion continuity through multi-frame input",
      "details": "Can start from multiple frames to maintain motion continuity, which regular Wan with Lynx IP embeds cannot achieve",
      "from": "Kijai"
    },
    {
      "finding": "ChronoEdit uses temporal reasoning tokens during generation",
      "details": "Model can put whatever it wants in intermediate frames as long as it helps get to the right final result - intermediate frames work like register tokens",
      "from": "spacepxl"
    },
    {
      "finding": "ChronoEdit works at 81 frames beyond its 29 frame design",
      "details": "Kijai successfully tested it at 81 frames despite being designed for 29 frames",
      "from": "Kijai"
    },
    {
      "finding": "ChronoEdit temporal reasoning mode modifies input for specific steps",
      "details": "Code selects first and last frame when temporal reasoning is enabled",
      "from": "Kijai"
    },
    {
      "finding": "New LightX2V 1030 LoRA has better camera control",
      "details": "Specifically stated no camera movement and previous failed with crazy zoom, newest didn't move camera and did wanted character motion",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "VRAM requirements differ significantly between Chinese cards and NVIDIA",
      "details": "Huawei Atlas 300I DUO has 96GB for $1.5k but much lower performance (TOPS rating like 30 series) vs RTX 6000 Pro at $10k",
      "from": "Aaron_PhD"
    },
    {
      "finding": "ChronoEdit is a Wan finetune that takes an image and creates a video where the last frame is the edited version",
      "details": "It was trained on 5 frames (normal) and 29 frames ('temporal reasoning' mode)",
      "from": "Kiwv"
    },
    {
      "finding": "ChronoEdit uses Qwen VL for prompt enhancement",
      "details": "There are 3 ComfyUI implementations available for Qwen VL",
      "from": "aikitoria"
    },
    {
      "finding": "LongCat was trained on video extension",
      "details": "Could pass in 30 frames and get 60 out",
      "from": "Kiwv"
    },
    {
      "finding": "LongCat can do 6 second segments with multi start frames on I2V for extension with motion continuity",
      "details": "But has no reference images support, so if character turns around it becomes useless",
      "from": "aikitoria"
    },
    {
      "finding": "FlashVSR 1.1 has been released",
      "details": "New version available on HuggingFace",
      "from": "yi"
    },
    {
      "finding": "New wan2.2_lightx2v_1030 LoRA has been released",
      "details": "Available in Kijai's repo at /LoRAs/Wan22_Lightx2v/",
      "from": "avataraim"
    },
    {
      "finding": "LightX LoRA can generate 5-second segments with only 4 steps",
      "details": "Takes about 12 minutes to generate 1 minute long video this way, though motion is not totally continuous",
      "from": "aikitoria"
    },
    {
      "finding": "ChronoEdit has a distill LoRA included",
      "details": "Found Wan_2_1_I2V_14B_ChronoEdit_distill_lora_rank32.safetensors in the repo, mentioned in paper",
      "from": "mamad8"
    },
    {
      "finding": "Qwen Edit at 40 steps produces significantly sharper outputs",
      "details": "40 steps is night and day different for the sharpness of qwen edit outputs",
      "from": "aikitoria"
    },
    {
      "finding": "ChronoEdit removes image softening effect",
      "details": "It mostly removes that effect of the image being softened that happens with Qwen",
      "from": "aikitoria"
    }
  ],
  "troubleshooting": [
    {
      "problem": "PC crashes when generating images after working fine with videos",
      "solution": "Hardware-related issue, may need system diagnostics",
      "from": "Drommer-Kille"
    },
    {
      "problem": "Memory issues with 162 frames using 81 length context windows",
      "solution": "Increase disk swap size, use --cache-none option, or try --highvram/--gpu-only/--normalvram flags",
      "from": "Ablejones"
    },
    {
      "problem": "DWPose Estimator taking 17 minutes for 10sec clip",
      "solution": "Change bbox detector to .torchscript format, reduces time from 17min to 3min",
      "from": "xiver2114"
    },
    {
      "problem": "Preview nodes with multiple frames slow down ComfyUI",
      "solution": "Delete preview node completely, not just bypass it, to fix the performance issue",
      "from": "mdkb"
    },
    {
      "problem": "VRAM not clearing properly between workflow runs",
      "solution": "Need to restart ComfyUI when caught, ongoing issue since recent updates",
      "from": "mdkb"
    },
    {
      "problem": "Missing detection model file backbone.blocks.21.norm2.bias",
      "solution": "VitPose H model needs the .bin file in addition to other model files",
      "from": "Kijai"
    },
    {
      "problem": "Face detection error with wrong aspect ratio",
      "solution": "Make sure resolution matches video aspect ratio, swapping width/height causes detection failures",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "problem": "Black video output with palingenesis i2v model",
      "solution": "Model appears to be busted or incorrectly uploaded",
      "from": "FL13"
    },
    {
      "problem": "Pose detection taking too long",
      "solution": "Install onnxruntime-gpu to run on GPU instead of CPU",
      "from": "Kijai"
    },
    {
      "problem": "Can't add denoise to I2I workflow",
      "solution": "Denoise appears fixed at 1.0 in certain samplers, defeating I2I purpose",
      "from": "Kenk"
    },
    {
      "problem": "Face shaking with movement in new WanAnimate workflow",
      "solution": "Issue occurs with new preprocessor workflow, older workflow didn't have this problem",
      "from": "Kevin \"Literally Who?\" Abanto"
    },
    {
      "problem": "Losing face likeness in WanAnimate without LightX LoRA",
      "solution": "Reduce pose strength and increase face strength, or use LightX LoRA which preserves face better",
      "from": "\ud83e\udd99rishappi"
    },
    {
      "problem": "Pose lines appearing in VACE output",
      "solution": "Usually caused by improper composite blending, root cause is the composite method",
      "from": "A.I.Warper"
    },
    {
      "problem": "Overexposure degradation in long WanAnimate runs",
      "solution": "Use LCM if not already, make settings less strong, use fewer steps, or try context windows instead of frame windowing",
      "from": "Kijai"
    },
    {
      "problem": "Slow CPU performance on cloud setups",
      "solution": "Cloud setups often have subpar CPU performance, consider using GPU optimized nodes",
      "from": "Kijai"
    },
    {
      "problem": "LoRA seems sped up",
      "solution": "Make sure video is set to correct fps (16fps)",
      "from": "Ryzen"
    },
    {
      "problem": "HuMO changes ref image significantly",
      "solution": "Use I2V version of LightX2V LoRA instead of T2V version, avoid FastWan LoRA",
      "from": "mdkb"
    },
    {
      "problem": "Snow/dust artifacts in WAN Animate",
      "solution": "Add artifacts to negative prompt to prevent them appearing again",
      "from": "Charlie"
    },
    {
      "problem": "ComfyUI queue breaking",
      "solution": "SyntaxError from unexpected JSON character at position 4",
      "from": "Drommer-Kille"
    },
    {
      "problem": "Flickering black blocks in Wan Animate output",
      "solution": "Enable the fl2v switch on the node for 2.2 models",
      "from": "Kijai"
    },
    {
      "problem": "Character LoRAs not working in Wan 2.2 Animate",
      "solution": "Use only 2.1 LoRAs or use only the Low Noise LoRA from 2.2 MOE at weight 1.0, since Wan Animate is based on 2.1",
      "from": "Screeb"
    },
    {
      "problem": "OVI permission errors on Windows",
      "solution": "Need to change code in io_utils.py file, specific fix shared privately",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "problem": "Body deformations in Wan 2.2 LoRA training",
      "solution": "Train High Noise and Low Noise LoRAs separately instead of together to avoid deformations",
      "from": "Kytra"
    },
    {
      "problem": "Onnxruntime-gpu crashes in ComfyUI-WanAnimatePreprocess",
      "solution": "Issue reported but specific solution not provided in discussion",
      "from": "Kevin \"Literally Who?\" Abanto"
    },
    {
      "problem": "Negative prompt not working with CFG 1.0",
      "solution": "Use NAG node - CFG at 1.0 ignores negative prompt embed by design",
      "from": "Kytra"
    },
    {
      "problem": "OVI installation flash attention errors",
      "solution": "Try skipping flashattn or compile from scratch, may need CUDA 12.4 headers",
      "from": "seitanism"
    },
    {
      "problem": "Conda environment conflicts when creating new env while another is active",
      "solution": "Deactivate current environment before creating/activating new one to prevent cross-contamination",
      "from": "seitanism"
    },
    {
      "problem": "OOM issues during generation cancellation",
      "solution": "Don't cancel generation during 2nd (low noise) sampler stage as it doesn't properly offload the model",
      "from": "seitanism"
    },
    {
      "problem": "ONNX runtime errors in Animate v2",
      "solution": "Update/re-install onnxruntime-gpu and onnx, uninstall onnxruntime (without -gpu), or run on CPU",
      "from": "Kijai"
    },
    {
      "problem": "ONNX Runtime JIT compilation error on RTX 50-series GPUs",
      "solution": "Set ONNX device to CPU for WanAnimate preprocessing nodes",
      "from": "D-EFFECTS"
    },
    {
      "problem": "Black mask box around WanAnimate V2 output",
      "solution": "Add black images instead of disconnecting face input, or ensure face strength is 1.0+ when using masking",
      "from": "Kijai"
    },
    {
      "problem": "Weird noise pattern with Native Wan Animate",
      "solution": "Issue appears with WanAnimatePreprocess template but not with wrapper",
      "from": "Nekodificador"
    },
    {
      "problem": "Video gets more color contrast over time in I2V",
      "solution": "Model struggles to maintain style across 81 frames, especially with conflicting LoRA training data",
      "from": "Draken"
    },
    {
      "problem": "Static scene wobble/shake and dust/grain effects",
      "solution": "No specific solution provided, user seeking help",
      "from": "humangirltotally"
    },
    {
      "problem": "Black video output",
      "solution": "Check resolution - must be multiples of 64, with video token number divisible by 128",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "problem": "GGUF compatibility error with dtype mismatch",
      "solution": "Fixed by Kijai - Byte and Half dtype issue resolved",
      "from": "Gigi8"
    },
    {
      "problem": "Vitpose and YOLO models not found in custom paths",
      "solution": "Use symlinks - models are only picked from ComfyUI absolute path",
      "from": "Gigi8"
    },
    {
      "problem": "Wan Animate doesn't work with Uni3C",
      "solution": "Disconnect both bg_images and mask inputs",
      "from": "Kijai"
    },
    {
      "problem": "Blurry outputs in I2V",
      "solution": "Use multiples of 64 for resolution, try 1280x720 instead of 800x800",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "problem": "OVI resolution changing not working",
      "solution": "Bug in RH-ovi node where setting 720p still outputs 544 res video",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "problem": "Pixelated/blurry outputs on default workflow",
      "solution": "Switch from FP8 scaled model to FP16 model",
      "from": "Ryzen"
    },
    {
      "problem": "ImageResizeKJv2 node failing with latest ComfyUI updates",
      "solution": "Re-set the per_batch parameter to 0 or desired value due to bug where new widgets in old workflows don't get default values",
      "from": "Kijai"
    },
    {
      "problem": "OVI node showing as red/unavailable after installation",
      "solution": "Check console for missing dependencies and pip install them (e.g., pydub)",
      "from": "Thom293"
    },
    {
      "problem": "SageAttn3 not working on RTX 6000 Pro",
      "solution": "SageAttn3 is Blackwell-only (B200/B300 cards), won't work on older cards",
      "from": "Kijai"
    },
    {
      "problem": "Burned-in look from too many end-frame I2V continuations",
      "solution": "Use USDU upscale in latent space with VACE Wan 2.2 dual model workflow, masking seams and low denoise upscale/detail runs",
      "from": "mdkb"
    },
    {
      "problem": "Radial attention error with 'sm_120' processor and LLVM ERROR: Cannot select: intrinsic %llvm.nvvm.shfl.sync.bfly.i32",
      "solution": "Install SpargeAttn package from https://github.com/woct0rdho/SpargeAttn/releases and add dense attention settings node",
      "from": "Kijai"
    },
    {
      "problem": "Palingenisis model producing black boxes with i2v",
      "solution": "Use insight version or switch to smoothmix which many people prefer",
      "from": "FL13"
    },
    {
      "problem": "OVI installation errors and frustration",
      "solution": "Wait for native ComfyUI wrapper implementation that's being worked on",
      "from": "D-EFFECTS"
    },
    {
      "problem": "KeyError: 'vace_blocks.0._orig_mod.self_attn.q.weight' when using lightx loras with Fun-VACE",
      "solution": "Update wrapper, commit a few hours ago to address",
      "from": "DawnII"
    },
    {
      "problem": "OVI workflow not working properly",
      "solution": "Load ovi video on main loader and ovi audio on extra, use default rope for now",
      "from": "patientx/Kijai"
    },
    {
      "problem": "Slow generation on 5090 with offloading",
      "solution": "Keep model in VRAM, no offload. With offload it's slow as an old car",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "problem": "NaNs with fp16",
      "solution": "Use bf16 instead as I had some NaNs with fp16 before",
      "from": "Kijai"
    },
    {
      "problem": "Installing sageattention",
      "solution": "Don't use ChatGPT as a guide to install sageattention. Everything just broke",
      "from": "Zabo"
    },
    {
      "problem": "OVI model producing noise output",
      "solution": "Try using sageattn instead of sdpa",
      "from": "TransformerMan"
    },
    {
      "problem": "e4m3fn_scaled not working on 3090 after fresh install",
      "solution": "Use e5m2 instead - e4 doesn't actually work on 3000 series GPUs",
      "from": "Kijai"
    },
    {
      "problem": "Tiled VAE with VACE extend creates blurry output and position shift",
      "solution": "Issue with tile size calculation - original defaults are tile_size=(34, 34), tile_stride=(18, 16)",
      "from": "Kijai"
    },
    {
      "problem": "Scaled FP8 models not working without selecting scaled option",
      "solution": "Scaled FP8 weights don't work without the scaling - need 'scaled_fp8' key in model metadata",
      "from": "Kijai"
    },
    {
      "problem": "Sage attention causing blank outputs with Wan 2.2 animate",
      "solution": "Switch to different attention mechanism",
      "from": "berserk4501"
    },
    {
      "problem": "Models producing noisy output when not using scaled option",
      "solution": "If model shows as scaled when loaded, it must be handled as scaled",
      "from": "Dita"
    },
    {
      "problem": "ImageResizeKJv2 node per_batch conversion error",
      "solution": "Set per_batch to 1 instead of NaN value",
      "from": "NoHuman"
    },
    {
      "problem": "dtype mismatch error: Half and Float8_e4m3fn",
      "solution": "Switch from v2 to v1 of Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ model",
      "from": "neitherchef"
    },
    {
      "problem": "Wan animate generating unchanged video with black artifacts",
      "solution": "Check that mask is being processed correctly by the sampler and inputted correctly on the wanvideo animate embeds node",
      "from": "happy.j"
    },
    {
      "problem": "ComfyUI files not saving to output folder",
      "solution": "Files can be found in tmp folder when this happens. API nodes don't save to temp folder",
      "from": "Drommer-Kille"
    },
    {
      "problem": "OOM errors on 32GB VRAM after few runs",
      "solution": "Bypass or remove blockcache node entirely instead of setting blockcache to 0",
      "from": "lostintranslation"
    },
    {
      "problem": "Videos coming out very dark with OVI",
      "solution": "Certain scene prompts can cause this darkness issue",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "problem": "5090 crashing with VRAM temperature not accessible",
      "solution": "Check GPU power connector and system RAM fitting - power connection issues can cause crashes and memory errors",
      "from": "Zabo"
    },
    {
      "problem": "RTX 5090 crashes with black screen and freezing",
      "solution": "Remove second GPU from dual GPU setup, update to PyTorch 2.7 or 2.9 (avoid 2.8), check PCIe configuration",
      "from": "Charlie"
    },
    {
      "problem": "PyTorch 2.8.0 causing stability issues",
      "solution": "Use PyTorch 2.7.0 or 2.9.0 instead, 2.8.0 has known problems",
      "from": "Tony(5090)"
    },
    {
      "problem": "First frame flash in VACE 2.2",
      "solution": "Try lowering CFG scale start to 1.5 or lower, use Lora Block Edit node to switch off first block",
      "from": "Rainsmellsnice"
    },
    {
      "problem": "5090 FE crashes during inference",
      "solution": "Power limit GPU to 70% for stability, may be power phase related issue",
      "from": "seitanism"
    },
    {
      "problem": "Alpha transparency in Wan workflow conversion",
      "solution": "Use proper VAE - 2.2 VAE is needed for 5b model, not TAE",
      "from": "Juampab12"
    },
    {
      "problem": "New Lightx2v LoRA from repo produces glitchy output with lines of noise",
      "solution": "Use Kijai's extracted LoRA version instead, or use 12 steps minimum",
      "from": "Ashtar"
    },
    {
      "problem": "LoRA shows 'lora key not loaded' errors in console",
      "solution": "Use the full model or Kijai's properly extracted LoRA",
      "from": "Ashtar"
    },
    {
      "problem": "Ghosting issues with new Lightx2v MoE on simple scheduler",
      "solution": "Switch to linear quadratic scheduler",
      "from": "FL13"
    },
    {
      "problem": "Need higher steps to avoid ghosting with speed LoRAs",
      "solution": "Turn up number of steps, ghosting goes away with more steps",
      "from": "Zabo"
    },
    {
      "problem": "Super burned out results with scheduler",
      "solution": "At low steps you really need the schedule to be precise. Ideally for distil use the schedule it was trained with",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "problem": "Torch compile not worth using with CFG scheduler",
      "solution": "Because it calls each step with different CFG, it's not worth using torch compile as it will just slow things down due to recompiling",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "problem": "Model reloading slowing down CFG scheduler",
      "solution": "Look into model patch wrapper functions or cfg patches to patch without reloading. Use pre and post cfg functions for dynamic CFG changes",
      "from": "Kijai"
    },
    {
      "problem": "Vertical line artifacts only on widescreen aspect ratios",
      "solution": "Artifacts happen on both LoRA and finetune versions for landscape/16:9 images but not on portrait images",
      "from": "blake37"
    },
    {
      "problem": "OVI audio latents error",
      "solution": "Change from fp32 to bf16 on the OVI MMaudio VAE node",
      "from": "Colin"
    },
    {
      "problem": "fp8 compile errors on 3000 series",
      "solution": "Use triton 3.4+ build with fp8 patches, or switch to e5m2 models instead of e4m3",
      "from": "phazei"
    },
    {
      "problem": "SageAttention DLL import error with '_fused' module",
      "solution": "Check torch version matches sageattention version, ensure CUDA torch is installed (not CPU-only), add torch DLL folder to PATH",
      "from": "woctordho"
    },
    {
      "problem": "FP8 models not working on older GPUs with 'fp8e4nv not supported' error",
      "solution": "Leave quantization disabled when using fp8 models on unsupported architectures like RTX 3060",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "problem": "OOM issues with FlashVSR on limited VRAM",
      "solution": "Use text encode cache version to offload cache to file and remove from memory after use",
      "from": "mdkb"
    },
    {
      "problem": "WebM video save error in VACE workflows",
      "solution": "Change codec/format to h264 instead of webm",
      "from": "Kijai"
    },
    {
      "problem": "Recent ComfyUI update causing OOM on decode",
      "solution": "Remove fp16_accumulation from startup script, or downgrade resolution to 832x480",
      "from": "Ashtar"
    },
    {
      "problem": "VAE decode crashes after ComfyUI 0.3.65 update with PyTorch 2.9.0",
      "solution": "Switch VAE to fp32 precision or use Kijai's workaround in WanVideoWrapper",
      "from": "Martin_H"
    },
    {
      "problem": "1280x720 resolution breaks generation",
      "solution": "Issue seems related to Triton connection being broken with new update",
      "from": "Martin_H"
    },
    {
      "problem": "FlashVSR produces ghosting with rapid movements when using FlashVSR VAE",
      "solution": "Use full Wan VAE instead of FlashVSR VAE for better quality with less artifacts",
      "from": "Elvaxorn"
    },
    {
      "problem": "FlashVSR over-sharpness issue",
      "solution": "Use denoise settings to reduce excessive sharpness, though it may cause color shifts",
      "from": "Elvaxorn"
    },
    {
      "problem": "OOM errors on imagetovideoencode node after ComfyUI update",
      "solution": "Change encoder to fp32",
      "from": "Charlie"
    },
    {
      "problem": "Long video OOM during resizing for upscaling",
      "solution": "Process in batches using load video node and meta batch manager",
      "from": "Lodis"
    },
    {
      "problem": "Ghosting/double images with new distill models",
      "solution": "Switch sampler to uni_pc instead of euler",
      "from": "aikitoria"
    },
    {
      "problem": "Temporal consistency issues when chunking videos",
      "solution": "Use overlaps (10-20% of frames) when stitching, avoid latent space operations",
      "from": "happy.j"
    },
    {
      "problem": "VRAM issues causing runs to lock up and 'Exception in thread Thread-21' errors",
      "solution": "Roll back to October 10-11 commit where everything was working fine",
      "from": "Miku"
    },
    {
      "problem": "MOE lora produces wobbly outputs in I2V workflows",
      "solution": "Use simple unipc on both samplers with 2/2 steps, or try 2.1 LoRA at 3.0 strength with cfg instead",
      "from": "Draken"
    },
    {
      "problem": "FlashVSR produces grid noise/texture that doesn't match video motion",
      "solution": "This is typical diffusion 'vines' and 'salt' artifacts - at least it doesn't try to resolve motion blur like other upscalers",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "problem": "WanAnimate cutting off hair in reference images",
      "solution": "Check resize settings - currently set to 'stretch' by default in KJ workflow",
      "from": "BobbyD4AI"
    },
    {
      "problem": "Wan 2.2 first frame corruption on longer gens",
      "solution": "Trim first 16 frames or so, or use 5B model for true 24fps",
      "from": "garbus"
    },
    {
      "problem": "Torch compile VRAM issues after ComfyUI update",
      "solution": "Update ComfyUI - recent fix for pytorch 2.9 issues available",
      "from": "JohnDopamine"
    },
    {
      "problem": "VRAM bloat and weird workflow fails",
      "solution": "Remove 'ComfyUI Essential' custom node preview nodes - unmaintained for 10 months",
      "from": "mdkb"
    },
    {
      "problem": "OOM on VAE decode",
      "solution": "Switch to tiled VAE decode when hitting VRAM limits",
      "from": "mdkb"
    },
    {
      "problem": "WanVideoScheduler node broken",
      "solution": "Scheduler doesn't have lower method - fixed in latest update",
      "from": "pagan"
    },
    {
      "problem": "Torch compile produces oversaturated rainbow output",
      "solution": "Don't use 'fullgraph' option on torch compile",
      "from": "phazei"
    },
    {
      "problem": "'WanModel' object has no attribute 'multitalk_audio_proj'",
      "solution": "Error occurs when using T2V model instead of appropriate model for MultiTalk",
      "from": "Draken"
    },
    {
      "problem": "Getting noise when using 10+10 steps setup",
      "solution": "Issue was resolved automatically in recent ComfyCloud update, now works out of the box",
      "from": "Drommer-Kille"
    },
    {
      "problem": "WanModel object has no attribute 'multitalk_audio_proj' error",
      "solution": "Occurs when trying to use multitalk without having multitalk model loaded",
      "from": "Kijai"
    },
    {
      "problem": "HuggingFace download failures",
      "solution": "Use HF_HUB_DISABLE_XET=1 and HF_HUB_ENABLE_HF_TRANSFER=0 environment variables",
      "from": "Kijai"
    },
    {
      "problem": "Save Latents node crashing ComfyUI",
      "solution": "New issue reported, no solution provided yet",
      "from": "mdkb"
    },
    {
      "problem": "Woman getting stuck in generation",
      "solution": "Decrease LoRA strength, or remove ffn layers from lightx2v (like causvid v2 lora approach)",
      "from": "Gateway/Kijai"
    },
    {
      "problem": "Ditto LoRAs break other VACE functionalities",
      "solution": "This is expected behavior when using style transfer LoRAs",
      "from": "Draken"
    },
    {
      "problem": "Memory error: Unable to allocate 29.3 GiB for array",
      "solution": "Try --cache-none command line flag to prevent cached stuff from building up, reduce frame count",
      "from": "Gateway"
    },
    {
      "problem": "Video breaks at more than 10 steps on High model",
      "solution": "Update ComfyUI - this was a known issue that got fixed",
      "from": "Zabo/JohnDopamine"
    },
    {
      "problem": "Context windows with uni3c camera control not working",
      "solution": "Update Kijai's nodes - bug was just fixed",
      "from": "Kijai"
    },
    {
      "problem": "Audio out of sync when combining video",
      "solution": "Align FPS between input video and output - input had 24fps while output was 16fps",
      "from": "BobbyD4AI"
    },
    {
      "problem": "High sampler adding too much noise at the end, making video unrecognizable",
      "solution": "Increase shift value - double it to at least 16. High noise model meant to operate on high sigmas",
      "from": "Kijai"
    },
    {
      "problem": "Mocha shape error '[127776, 1, -1]' is invalid for input",
      "solution": "Update and clear triton cache - it's triton cache bullshit",
      "from": "Kijai"
    },
    {
      "problem": "VACE just stopping after making masks with no errors",
      "solution": "No specific solution provided in discussion",
      "from": "amli"
    },
    {
      "problem": "Context window tensor mismatch with MoCha",
      "solution": "Update to latest version - context windows should work again",
      "from": "Kijai"
    },
    {
      "problem": "Shape mismatch error with two reference images in MoCha",
      "solution": "Kijai provided fix for weights being masked",
      "from": "scf"
    },
    {
      "problem": "dtype error 'Half and Float8_e4m3fn' with Wan 2.2 animate workflow",
      "solution": "Issue likely with scheduler or model loading setup",
      "from": "topmass"
    },
    {
      "problem": "Sage attention error on RTX 6000 PRO",
      "solution": "GPU compute capability issue, not node related, user reverted",
      "from": "HeadOfOliver"
    },
    {
      "problem": "VRAM error 'Expected size 48 but got size 16' with MoCha",
      "solution": "Was using wrong VAE, switching to correct VAE fixed it",
      "from": "VK (5080 128gb)"
    },
    {
      "problem": "Wan 2.2 inpainting with VACE adding strange artifacts in final result",
      "solution": "Disable HPS (Human Preference Score) node, use simpler NAG negative prompts",
      "from": "Kijai"
    },
    {
      "problem": "Wan 2.1/2.2 videos play once then show as corrupted on Linux",
      "solution": "Likely codec/driver related issue, need to update OpenGL drivers and use VLC player",
      "from": "Provydets"
    },
    {
      "problem": "LoRA key not loaded errors with new LightX2V LoRA",
      "solution": "The diff_m modulation keys don't seem to do anything when loaded and can be ignored",
      "from": "Kijai"
    },
    {
      "problem": "Running out of VRAM with I2V + 3-4 LoRAs",
      "solution": "Use --reserve-vram 2 on Windows, ensure ComfyUI is updated to fix torch 2.9 3x VRAM bug",
      "from": "Kijai"
    },
    {
      "problem": "Triton installation not working",
      "solution": "Don't use torch.compile node or download wheel from ComfyUI wheels repo",
      "from": "Kijai"
    },
    {
      "problem": "VHS node webm encoding errors",
      "solution": "Check the VHS node video format settings, use VHS video combine with h264 format for better system compatibility",
      "from": "Kijai"
    },
    {
      "problem": "CUDA runtime error mid-render",
      "solution": "Rebuild CUDA paths to point to conda environment libraries instead of system CUDA",
      "from": "HeadOfOliver"
    },
    {
      "problem": "Video playback issues in Linux",
      "solution": "Use VHS Video Combine node with h264 format instead of default save video node",
      "from": "Kijai"
    },
    {
      "problem": "Ditto giving 'WanModel' object has no attribute 'vace_patch_embedding' error",
      "solution": "Need to download global model from Google Drive, not HuggingFace, or use VACE model normally with Ditto LoRA",
      "from": "Gentleman bunny"
    },
    {
      "problem": "WSL getting OOM on 480p generation",
      "solution": "Create .wslconfig file with memory=60GB and swap=60GB settings, restart WSL",
      "from": "seitanism"
    },
    {
      "problem": "Points Editor canvas not rendering in ComfyCloud",
      "solution": "Browser compatibility issue, check browser console for errors",
      "from": "Nekodificador"
    },
    {
      "problem": "EasyCache causing quality degradation and warping",
      "solution": "Lower threshold values (0.06 caused issues), may not work well with Wan 2.2",
      "from": "seitanism"
    },
    {
      "problem": "SVI quality degrading over time in chains",
      "solution": "Use 'shot' LoRA which fills empty frames with initial input to help against degradation",
      "from": "Kijai"
    },
    {
      "problem": "Light/Tiny VAEs don't load in native",
      "solution": "Requires code changes, not currently supported",
      "from": "Kijai"
    },
    {
      "problem": "WSL performance issues and disk space filling rapidly",
      "solution": "Set dynamic to false in torch compile, block swap debug to false, non-blocking true. Consider native Linux or Windows instead",
      "from": "seitanism"
    },
    {
      "problem": "SVI LoRAs not working in native",
      "solution": "Original models are fp32 LoRAs in diffsynth format, need conversion to fp16 safetensors for ComfyUI native",
      "from": "Kijai"
    },
    {
      "problem": "FlashVSR OOM errors",
      "solution": "Change attention from SDPA to sage attention",
      "from": "patientx"
    },
    {
      "problem": "MTV Crafter pose detection issues",
      "solution": "Different predictor versions need different inputs, ensure input pose is close to target pose",
      "from": "Kijai"
    },
    {
      "problem": "SVI causing character degradation and color shifts during extension",
      "solution": "Use VK's method of including original reference image as first frame in each new render sequence",
      "from": "VK (5080 128gb)"
    },
    {
      "problem": "Context window artifacts and ghosting at transitions",
      "solution": "Need to batch 4 generations to hopefully get 1 good one without artifacts",
      "from": "blake37"
    },
    {
      "problem": "Random reference frame appearing in SVI extended videos",
      "solution": "Delete the stray frame in post-processing using After Effects or use 'image from batch' node to exclude specific frames",
      "from": "VK (5080 128gb)"
    },
    {
      "problem": "Can't get Sage Attention 2 in WanWrapper",
      "solution": "Can't have sage1 and sage2 installed simultaneously - it uses whichever version you have installed",
      "from": "Kijai"
    },
    {
      "problem": "SVI film example script issues with padding",
      "solution": "When ref_pad_num is set to 0, there's no special padding applied - contrary to what developers claimed",
      "from": "Kijai"
    },
    {
      "problem": "Flash artifacts in SVI continuations",
      "solution": "Use different seeds for each generation, same seed makes burning worse. Also less noticeable in bright areas like sky",
      "from": "Kijai"
    },
    {
      "problem": "Grey frame appearing with 4 frames instead of 5",
      "solution": "4 frames = full latent, freaks out not having a starting image for the next. Stick to 5 frames",
      "from": "Draken"
    },
    {
      "problem": "SVI LoRAs not working well on 2.2",
      "solution": "No 2.1 LoRA works proper on 2.2 high noise, it's too different. Try increasing LoRA strength for high noise or use 2.1",
      "from": "Kijai"
    },
    {
      "problem": "VAE encode/decode degradation in extensions",
      "solution": "Taking last frames, decoding and re-encoding has big limit. VAE isn't perfect and degrades over time, unlike framepack which worked with latents",
      "from": "Draken"
    },
    {
      "problem": "Code bug preventing proper reference padding",
      "solution": "Fun inpaint code was zeroing out frames, needed to be fixed to allow proper reference padding in shot method",
      "from": "Kijai"
    },
    {
      "problem": "Reference padding bug was fixed recently",
      "solution": "Kijai fixed a bug with ref frame padding - previous versions would zero out reference frames when using temporal masks",
      "from": "Kijai"
    },
    {
      "problem": "Film lora freezes with reference frames",
      "solution": "svi-film lora WILL NOT work with reference frames and WILL freeze - this is correct behavior. Only works with shot/talk/dance loras",
      "from": "Ablejones"
    },
    {
      "problem": "Flashing between extensions",
      "solution": "Update to latest version for ref frame padding bug fix, use merged lora instead of unmerged, avoid reference padding which can limit movement",
      "from": "Kijai"
    },
    {
      "problem": "ViT pose cropping issue",
      "solution": "Turn off padding in the draw vit pose node to fix cropping, though positioning becomes an issue",
      "from": "Neex"
    },
    {
      "problem": "Masking confusion between wrapper and native",
      "solution": "In wrapper: black masks for overlap frames. In native ComfyUI: white masks for overlap frames. But for svi-film with 5 frames, no masks needed",
      "from": "Ablejones"
    },
    {
      "problem": "ComfyUI freezing at Image Batch Multi node",
      "solution": "Check temp folder before doing anything else - it usually keeps generated videos there",
      "from": "JohnDopamine"
    },
    {
      "problem": "UnicodeDecodeError when using experimental args",
      "solution": "Error relates to torch inductor compilation, may be system-specific encoding issue",
      "from": "NebSH"
    },
    {
      "problem": "Slow model loading times on RunPod",
      "solution": "Copy models from ComfyUI network volume to temp folder at root of machine",
      "from": "mamad8"
    },
    {
      "problem": "Video flickering every 5s when using SVI-film lora or opt model",
      "solution": "SVI-film model works fine, issue appears specific to lora/opt variants",
      "from": "army"
    },
    {
      "problem": "WAN 2.2 not working after moving regions, suggesting insufficient VRAM",
      "solution": "Check if torch was updated to 2.9 which may cause issues",
      "from": "conjark"
    },
    {
      "problem": "Holocine generates pixelated and noisy results",
      "solution": "Use same workflow as WAN 2.2, just replace the models",
      "from": "avataraim"
    },
    {
      "problem": "ComfyUI crashes when using video attention split",
      "solution": "Happens when compiling torch, cause unknown",
      "from": "avataraim"
    },
    {
      "problem": "WAN sometimes turns into anime mode for T2I",
      "solution": "Try adding terms like lens flare that wouldn't appear in anime, or put 'anime' in negative prompt",
      "from": "TimHannan"
    },
    {
      "problem": "HoloCine doesn't work in fp16, gets NaNs instantly",
      "solution": "Must use fp32",
      "from": "Kijai"
    },
    {
      "problem": "Wan T2I giving bad results without distill LoRA",
      "solution": "Try upping the weight or use distill LoRA",
      "from": "Ada"
    },
    {
      "problem": "SVI Film workflow not working on second run",
      "solution": "Text embeds were missing on second generation",
      "from": "VK (5080 128gb)"
    },
    {
      "problem": "Official HoloCine code is vastly slower than claimed benchmarks",
      "solution": "Code needs optimization, claimed speeds don't reflect actual implementation",
      "from": "aikitoria"
    },
    {
      "problem": "Video upscaling preserves WAN VAE noise grids",
      "solution": "No known fix - SeedVR2 and Topaz Starlight both preserve the artifacts",
      "from": "aikitoria"
    },
    {
      "problem": "LongCat sensitive to sampler",
      "solution": "Didn't like dpm++_sde, use longcat_distill_euler instead of lcm",
      "from": "Kijai"
    },
    {
      "problem": "LongCat generating black frames",
      "solution": "Specific images can cause black frame generation",
      "from": "scf"
    },
    {
      "problem": "Holocine crashing on ksampler",
      "solution": "Use without compile, disable sparse attention and try without passing holocine_args first",
      "from": "avataraim"
    },
    {
      "problem": "Text encode node caching issues",
      "solution": "Add a space somewhere in the prompt and try again or use the shot builder workflow",
      "from": "shaggss"
    },
    {
      "problem": "LongCat I2V showing first frame then turning to T2V",
      "solution": "Update SageAttention from 1.0.6 to 2.2.0 or switch to SDPA attention",
      "from": "Hashu"
    },
    {
      "problem": "Black frame outputs with some images",
      "solution": "Use bf16 instead of fp16, or switch to SPDA attention mechanism",
      "from": "scf"
    },
    {
      "problem": "Frame mismatch causing judder in transitions",
      "solution": "Ensure consistent frame counts between generations (e.g., both 9 or both 13 frames)",
      "from": "Kijai"
    },
    {
      "problem": "Shot attention error with missing text_cut_positions",
      "solution": "Use shot builder workflow or previous version of holocine_args node",
      "from": "shaggss"
    },
    {
      "problem": "Connection lost issues with modified HoloCine wrapper",
      "solution": "Reinstall normal WanVideoWrapper node to resolve dependency conflicts",
      "from": "Gill Bastar"
    },
    {
      "problem": "text_cut_position becoming nonetype error",
      "solution": "Adjusting the prompt to run the textencode solved it, also related to disk_cache usage",
      "from": "Cubey"
    },
    {
      "problem": "WanVideo wrapper crashes after restart",
      "solution": "Make restart of ComfyUI to fix crashes",
      "from": "avataraim"
    },
    {
      "problem": "KeyError 'blocks.0.ffn.0.bias' with LongCat",
      "solution": "Need to switch to longcat branch in WanVideoWrapper repo",
      "from": "Zabo"
    },
    {
      "problem": "Torch compile issues with LongCat on RTX3090",
      "solution": "Install triton-windows or just don't use compile, it's not necessary",
      "from": "Kijai"
    },
    {
      "problem": "VACE 2.2 + pose controlnet produces noisy output at strength 1.0",
      "solution": "Lower strength reduces noise but less pose following, consider using WanAnimate for pose instead",
      "from": "sneako1234"
    },
    {
      "problem": "Wan 2.2 LightVAE quality loss",
      "solution": "Stick with Wan2.1 VAE as LightVAE shows considerable quality loss despite claims",
      "from": "Fawks"
    },
    {
      "problem": "ComfyUI-VAE-Utils installation error 'No module named comfy.ldm.wan.vae2_2'",
      "solution": "Update ComfyUI to version that includes Wan 2.2 5B support",
      "from": "spacepxl"
    },
    {
      "problem": "WanFunCameraControl tensor mismatch error",
      "solution": "Check that camera motion follows 4+1 rule on frame count",
      "from": "Kijai"
    },
    {
      "problem": "Shot attention error in Holocine",
      "solution": "Only works with CFG 1.0, higher CFG values cause structured prompt errors",
      "from": "NebSH"
    },
    {
      "problem": "LightX2V distill lora causing overexposed lighting",
      "solution": "Issue identified but no specific solution provided yet",
      "from": "topmass"
    },
    {
      "problem": "SVI-film flashing between segments",
      "solution": "Try disabling first block (0) for SVI-Film, or it may be from HuMo itself in i2v mode",
      "from": "JohnDopamine"
    },
    {
      "problem": "Shot attention error: 'text_cut_positions is missing'",
      "solution": "Issue occurs when using smooth window value of 12, problem with structured prompt metadata",
      "from": "NebSH"
    },
    {
      "problem": "Warning appears when increasing resolution that shot embeddings not injecting properly",
      "solution": "This is the same problem that existed before the code fix",
      "from": "shaggss"
    },
    {
      "problem": "OVI has 10 second maximum limit, 15 second generations fail",
      "solution": "Keep generations to 8 seconds or less for better results",
      "from": "avataraim"
    },
    {
      "problem": "Video upscaling should be in pixel space, not latent upscale",
      "solution": "Use pixel space upscaling for proper results",
      "from": "Kijai"
    },
    {
      "problem": "Need to use add_noise for video extension workflows",
      "solution": "Enable add_noise setting in the workflow",
      "from": "Kijai"
    },
    {
      "problem": "WanVideoImageToVideoMultiTalk node not working with looping methods",
      "solution": "Fixed issue with timesteps assignment, now works with WanAnimate loop",
      "from": "Kijai"
    },
    {
      "problem": "Torch compile breaking after ComfyUI update",
      "solution": "Mixed precision update broke compile in general",
      "from": "Kijai"
    },
    {
      "problem": "Mocha tensor size error with wrapper",
      "solution": "Mocha only uses one mask for first frame, not whole video - it propagates the mask",
      "from": "Kijai"
    },
    {
      "problem": "Memory issues with WAN 2.2 taking 95-98% RAM on Windows",
      "solution": "Use --cache-none flag to prevent RAM reservation, or use memory cleanup nodes",
      "from": "Kijai"
    },
    {
      "problem": "Port already in use error after Python terminal crash",
      "solution": "Use netstat -ano | findstr :8188 to find PID, then taskkill /PID {ID} /F",
      "from": "avataraim"
    },
    {
      "problem": "use_disc_cache on text encode keeping prior prompts",
      "solution": "Disable use_disc_cache on text encode node to use new prompts properly",
      "from": "JohnDopamine"
    },
    {
      "problem": "Floating menus in ComfyUI causing navigation confusion",
      "solution": "No current solution found, users adapting to new UI",
      "from": "JohnDopamine"
    },
    {
      "problem": "Morph LoRA showing no effect at normal strength",
      "solution": "Try higher strengths (3.0-4.0) or lower (0.5 as recommended)",
      "from": "Kijai"
    },
    {
      "problem": "longcat_distill_euler sampler not available",
      "solution": "Remove longcat branch and download again, ensure ComfyUI restart after branch switch",
      "from": "Zabo"
    },
    {
      "problem": "EXR files causing pose system errors",
      "solution": "EXR files are not compatible with pose system, use different image formats",
      "from": "Guus"
    },
    {
      "problem": "Torch compile causing silent crashes",
      "solution": "Use startup args instead: --reserve-vram 2 --max-upload-size 500 --fast pinned_memory --async-offload --use-sage-attention --fast fp16_accumulation",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "FlashVSR meta batch causing jumps/seams",
      "solution": "Try using SeedVR2 first before FlashVSR, or use SDXL upscale refiner then downscale back to Wan resolution",
      "from": "Koba"
    },
    {
      "problem": "Sage-attention doesn't work with LongCat on RX 6800",
      "solution": "Triton issue with _get_path_to_hip_runtime_dylib can't find file - model has different architecture than other Wan models",
      "from": "patientx"
    },
    {
      "problem": "Lightning LoRA has exposure issues",
      "details": "Changes output exposure to be brighter",
      "from": "Kijai"
    },
    {
      "problem": "ChronoEdit doesn't work well with short prompts",
      "solution": "Use LLM node or Qwen for prompt enhancement as suggested in official scripts",
      "from": "JohnDopamine"
    },
    {
      "problem": "LongCat only showing first frame then black",
      "solution": "Doesn't work on fp16, needs bf16 base precision",
      "from": "Kijai"
    },
    {
      "problem": "Context windows creating artifacts and transitions",
      "solution": "Use Magref as the low noise model - better with context windows and fine details",
      "from": "blake37"
    },
    {
      "problem": "ChronoEdit workflow scale_t parameter",
      "solution": "Should be 8.0 not 7.0, but in latest version correct value is 7.0 again",
      "from": "comfy"
    },
    {
      "problem": "Frame number mismatch in Flash node",
      "solution": "Frame number in Flash node needs to match the input frames",
      "from": "ingi // SYSTMS"
    },
    {
      "problem": "LongCat lack of reference input for face consistency",
      "solution": "Use simple face inpaint pass with Phantom to fix face consistency issues",
      "from": "Ablejones"
    },
    {
      "problem": "Kijai nodes broke with new ComfyUI update",
      "solution": "Had to bump up blocks to swap from 20 to 30 to prevent OOMs",
      "from": "aipmaster"
    },
    {
      "problem": "Unmerged LoRAs causing memory issues",
      "solution": "Unmerged loras now work differently and are part of the block swap, making it obey the block swap rather than being fully swapped out",
      "from": "Kijai"
    },
    {
      "problem": "Color degradation into yellow during long video generation",
      "solution": "Throwing a color match node between each segment seems to be enough to stop the slow degradation of colors",
      "from": "aikitoria"
    },
    {
      "problem": "Videos getting more blurry during long generation",
      "solution": "Needs to keep exact level of softness from qwen image edit before being passed to wan to avoid vae noise grids",
      "from": "aikitoria"
    }
  ],
  "comparisons": [
    {
      "comparison": "High Noise vs Low Noise for inpainting",
      "verdict": "Low Noise model performs better for large inpainting tasks, High Noise good for smaller masks",
      "from": "Ablejones"
    },
    {
      "comparison": "Kijai's scaled fp8 vs regular fp8",
      "verdict": "Kijai's scaled models have better quality results in testing",
      "from": "Kijai"
    },
    {
      "comparison": "Merged vs Unmerged LoRAs",
      "verdict": "Unmerged LoRAs use more VRAM but provide better quality and runtime flexibility",
      "from": "Kijai"
    },
    {
      "comparison": "Palingenesis vs normal model",
      "verdict": "Palingenesis shows some improvement but original looks much better in some cases, gets CFG burn type effects",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "comparison": "DYNO vs standard Wan 2.2 14b",
      "verdict": "DYNO reaches 70% of standard model's dynamic performance vs 50% for original LightX2V",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "Photoshop Beta upscaler vs Wan 2.2 upscaler",
      "verdict": "Photoshop takes 5s vs 10 minutes for Wan, same quality",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "DC-VideoGen speed vs regular Wan",
      "verdict": "DC-VideoGen: 3.67 mins/video vs regular Wan: 27.88 mins/video",
      "from": "Ada"
    },
    {
      "comparison": "WanAnimate with vs without LightX LoRA",
      "verdict": "LightX LoRA preserves face likeness better, especially for non-Western subjects. Without LightX produces artifacts and poor face similarity",
      "from": "Christian Sandor"
    },
    {
      "comparison": "InfiniteTalk vs WanAnimate vs HuMo ranking",
      "verdict": "Infinitetalk > WanAnimate > Humo",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "CPU vs GPU performance for mask operations",
      "verdict": "GPU processing is 33x faster than CPU for DrawMaskOnImage operations (11409.67 vs 341.85 it/s)",
      "from": "Kijai"
    },
    {
      "comparison": "Context windows vs frame windowing",
      "verdict": "Context windows don't have degradation issues but have worse temporal coherency between windows and are slower",
      "from": "Kijai"
    },
    {
      "comparison": "832x480 vs 1024x576 for HuMO",
      "verdict": "1024x576 has better background consistency",
      "from": "mdkb"
    },
    {
      "comparison": "USDU vs ClownShark tiling",
      "verdict": "USDU faster (25 min vs 35 min) but similar quality",
      "from": "FL13"
    },
    {
      "comparison": "Sage 2 vs Sage 3",
      "verdict": "Sage 3 has significant quality degradation, radial attention better quality",
      "from": "Kijai"
    },
    {
      "comparison": "MagRef vs HuMO likeness",
      "verdict": "MagRef was more solid at sticking to reference image",
      "from": "mdkb"
    },
    {
      "comparison": "Wan 2.2 5B vs 14B performance",
      "verdict": "5B felt like failed experiment, can do 3 runs of 14B in time of single 5B run due to VAE overhead",
      "from": "Draken"
    },
    {
      "comparison": "OVI video quality vs Wan 5B",
      "verdict": "OVI video prompt adherence looks like 5B quality, not impressive compared to larger models",
      "from": "MysteryShack"
    },
    {
      "comparison": "T2V with LoRAs vs I2V for character likeness",
      "verdict": "T2V with LoRAs is often better than I2V for character consistency",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "comparison": "Smooth mix vs regular Wan 2.2",
      "verdict": "Better prompt adherence and more motion than previous lightx, closer to what full inference 2.2 should be",
      "from": "Juampab12"
    },
    {
      "comparison": "OVI vs standard models",
      "verdict": "Takes 24GB VRAM in fp8, 5B VAE takes 4x as long to decode, seemed worse than 1.3B",
      "from": "Draken"
    },
    {
      "comparison": "Lightning LoRA T2V vs I2V performance",
      "verdict": "T2V version always over-exposes results, I2V performance varies",
      "from": "Kijai"
    },
    {
      "comparison": "Dyno as LoRA vs full model",
      "verdict": "Works poorly as LoRA, that's why full model was released after LoRA version",
      "from": "Kijai"
    },
    {
      "comparison": "InfiniteTalk with FantasyPortrait vs other lip-sync methods",
      "verdict": "Best lip-sync tracking, especially for multi-person scenes with masking capabilities",
      "from": "mdkb"
    },
    {
      "comparison": "Wan vs Sora 2 pricing",
      "verdict": "Wan is 3x cheaper - $0.05-0.15 per second vs Sora 2's higher pricing, but quality gap may make price difference irrelevant",
      "from": "Juampab12"
    },
    {
      "comparison": "OVI vs standard models",
      "verdict": "OVI achieves 70% of Sora 2 quality, significantly better than standard 5B model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "Wrapper vs Native performance",
      "verdict": "Wrapper completes 121 frames in under 10min, while Native with block swap takes 29:54",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "FusionX quality assessment",
      "verdict": "FusionX quality is still unmatched for Wan 2.1, considered the perfect finale to Wan 2.1",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "comparison": "FP16 vs FP8 quality and prompt adherence",
      "verdict": "FP16 superior for quality and better for complex prompts, FP8 is faster but struggles with advanced prompts",
      "from": "Dream Making"
    },
    {
      "comparison": "BF16 vs FP16 detail rendering",
      "verdict": "FP16 far better in rendering fine details",
      "from": "Dream Making"
    },
    {
      "comparison": "FP32 vs FP16 quality",
      "verdict": "Quality boost barely notable between FP32 and FP16",
      "from": "Dream Making"
    },
    {
      "comparison": "Wan 2.2 high model vs 2.1",
      "verdict": "Quality is better than 2.1 but motion coherence is worse, still looking for right LoRAs",
      "from": "Dream Making"
    },
    {
      "comparison": "fp8_e5m2 vs k8 gguf on RTX 3090 with Wan 2.2",
      "verdict": "fp8_e5m2 = 170s gen time, k8 gguf = 360s gen time - something feels wrong with the large difference",
      "from": "Mattis"
    },
    {
      "comparison": "Different Lightning LoRA versions on low-noise",
      "verdict": "Lightning LoRA 250928 + 0.25 denoise strength produces better facial expressions and details than 4-steps + auto denoise",
      "from": "Lan8mark"
    },
    {
      "comparison": "Wan 5B vs other models",
      "verdict": "The wan 5b is starting to look okay to my eye on good seeds",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "comparison": "AniSora vs lightx2v/lightning",
      "verdict": "Better than lightx2v or lightning too for high noise",
      "from": "Kijai"
    },
    {
      "comparison": "Native vs wrapper results",
      "verdict": "Not exact same results, there are some differences especially with schedulers",
      "from": "Kijai"
    },
    {
      "comparison": "Wan 2.2 vs Step Video",
      "verdict": "yi claims Wan 2.2 is 2-3x better and more dynamic, mallardgazellegoosewildcat says Step still benches higher on internal benchmarks",
      "from": "yi"
    },
    {
      "comparison": "Wan 2.2 vs Step Video motion",
      "verdict": "Step video fell into too much DPO and SFT trap, much less dynamic so things don't move much",
      "from": "yi"
    },
    {
      "comparison": "OVI vs Wan 2.2",
      "verdict": "If you want audio use OVI, but if you don't need audio, 2.2 is still better",
      "from": "Juampab12"
    },
    {
      "comparison": "Sage 3 vs earlier versions",
      "verdict": "Considerable quality drop but a bit faster, probably not worth it",
      "from": "HeadOfOliver"
    },
    {
      "comparison": "5B vs 1.3B models",
      "verdict": "Best thing about 5B is that it's not 1.3B - many 1.3B works are proof of concept only and not practically useful",
      "from": "Kijai"
    },
    {
      "comparison": "OVI fp8 vs bf16",
      "verdict": "fp8 is softer and has mouth problems, bf16 is better quality",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "Lightx2v vs new rCM LoRA",
      "verdict": "Lightx2v looks better and has more prompt adherence",
      "from": "HeadOfOliver"
    },
    {
      "comparison": "Sec vs SAM2",
      "verdict": "Sec-4b looks better than SAM2 for segmentation",
      "from": "slmonker"
    },
    {
      "comparison": "SEC vs SAM2",
      "verdict": "SEC is much better than SAM2 alone, uses more VRAM but worth it for quality",
      "from": "Juampab12"
    },
    {
      "comparison": "Sage attention versions",
      "verdict": "Sage 1 still good, Sage 3 is fast but bad quality, SDPA best for quality, Flash deprecated",
      "from": "Dream Making"
    },
    {
      "comparison": "LightX 2.2 vs 2.1 LoRAs",
      "verdict": "Most LightX 2.2 LoRAs are bad quality, 2.1 LoRAs good quality but mess up movements",
      "from": "Dream Making"
    },
    {
      "comparison": "TAE vs full 2.2 VAE",
      "verdict": "TAE is preview quality only, full 2.2 VAE needed for proper outputs despite being slower",
      "from": "Draken"
    },
    {
      "comparison": "MagRef vs other consistency models",
      "verdict": "MagRef outperforms other consistency models, works with both T2V and I2V",
      "from": "Elvaxorn"
    },
    {
      "comparison": "New Lightx2v 2.2 vs Lightning vs Anisora for HN cfg skimming",
      "verdict": "New lightx wins with cfg, may be step distilled rather than cfg distilled",
      "from": "DawnII"
    },
    {
      "comparison": "2.1 Lightx2v vs 2.2 Lightx2v LoRA vs full model",
      "verdict": "Old 2.1 lightx2v still undefeated as baseline, but new full model shows promise",
      "from": "Kijai"
    },
    {
      "comparison": "Local vs cloud GPU costs",
      "verdict": "Local costs more than cloud after electricity - 540\u20ac/year for local vs cloud pricing",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "Wan 2.5 vs Sora 2 vs VEO 3",
      "verdict": "Wan 2.5 is most expensive and worst quality (plastic without details). VEO 3 good for talking videos but overpriced. Sora 2 better than Wan for some tasks like skateboarding",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "StepVideo vs Magi Video vs Wan 2.2",
      "verdict": "StepVideo and Magi Video are a lot better but depend on GPU. StepVideo gets VAE errors sometimes but does 8-10 seconds duration",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "comparison": "GIMM vs RIFE vs Topaz",
      "verdict": "GIMM beats Topaz easily and is probably the best interpolator out there",
      "from": "HeadOfOliver"
    },
    {
      "comparison": "New vs old LightX2V LoRA extraction",
      "verdict": "New full model extracts fine at lower ranks, unlike previous dyno model which needed higher than 512 rank to match full model",
      "from": "Kijai"
    },
    {
      "comparison": "FlashVSR TCDecoder vs normal VAE",
      "verdict": "FlashVSR is much faster (3 vs 20 seconds) with similar quality",
      "from": "Kijai"
    },
    {
      "comparison": "TaylorSeer vs TeaCache",
      "verdict": "TaylorSeer is better and much faster, but quality degrades with high movement",
      "from": "Zabo"
    },
    {
      "comparison": "Multiple samplers vs standard 2 samplers",
      "verdict": "Multiple samplers mainly useful for speed optimization with LoRAs, standard 2 samplers sufficient otherwise",
      "from": "DawnII"
    },
    {
      "comparison": "FlashVSR vs Topaz Starlight",
      "verdict": "FlashVSR is much faster (150s vs 10-15min) but Topaz produces better quality with less artifacts",
      "from": "Elvaxorn"
    },
    {
      "comparison": "Wan 2.2 low-only vs using both high and low models",
      "verdict": "Low noise model is basically a slightly finetuned 2.1 model, need high model for improved motion",
      "from": "Kijai"
    },
    {
      "comparison": "FlashVSR VAE vs full Wan VAE",
      "verdict": "Full Wan VAE produces better quality with less artifacts but is slower",
      "from": "Elvaxorn"
    },
    {
      "comparison": "Wan2.2 4-step distill vs original",
      "verdict": "Much better than Lightning distill, but 2.1 with cfg still better overall",
      "from": "Kijai"
    },
    {
      "comparison": "Kandinsky 5 2B vs Wan 14B",
      "verdict": "Kandinsky has analog looking style, different motion characteristics",
      "from": "yi"
    },
    {
      "comparison": "GGUF Q6 vs fp8_scaled_v2",
      "verdict": "Visual differences shown through difference blend mode comparison",
      "from": "Valle"
    },
    {
      "comparison": "2.2 Lightning vs 2.1 LightX2V LoRAs",
      "verdict": "Lightning versions have annoying side effects like making everything brighter, recommend forgetting Lightning versions",
      "from": "Kijai"
    },
    {
      "comparison": "rCM vs LightX LoRAs for I2V",
      "verdict": "rCM changes identity/likeness too much when used in Low noise, and creates artifacts when strength is high enough to have effect in High",
      "from": "blake37"
    },
    {
      "comparison": "FlashVSR vs other upscalers for motion blur",
      "verdict": "FlashVSR doesn't try to resolve motion blur like most upscalers that incorrectly interpret blurry things as sharp items",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "comparison": "rCM vs LightX2V",
      "verdict": "rCM wins for prompt adherence, LightX2V can turn things halloween themed unexpectedly",
      "from": "Kijai"
    },
    {
      "comparison": "FP8 vs FP/BF16 models",
      "verdict": "FP/BF16 dramatically better for video extension quality, especially noticeable at longer durations",
      "from": "Zabo"
    },
    {
      "comparison": "Wan 2.2 5B vs other variants",
      "verdict": "Only 5B model is true 24fps, other variants have frame rate limitations",
      "from": "garbus"
    },
    {
      "comparison": "rCM scheduler vs dpm++_sde",
      "verdict": "No compute difference, dpm++_sde probably better but rCM gives different results",
      "from": "Kijai"
    },
    {
      "comparison": "DGX Spark vs RTX 6000 Pro Blackwell for Wan 2.2",
      "verdict": "DGX Spark is about 1/5 the speed of RTX 6000 Pro",
      "from": "tarn59"
    },
    {
      "comparison": "2.2 Lightning vs 2.1 Lightx2v for I2V",
      "verdict": "Lightning preferred for I2V generally, but 2.1 lightx2v still undefeated for low noise due to better lighting",
      "from": "FL13"
    },
    {
      "comparison": "Native vs Wrapper workflows",
      "verdict": "3 sampler native gives nice results and is faster than wrapper",
      "from": "FL13"
    },
    {
      "comparison": "Ditto full module vs LoRA",
      "verdict": "Full module has less artifacting, ghosting and is more reliable",
      "from": "Hashu"
    },
    {
      "comparison": "Using cfg vs speed LoRAs",
      "verdict": "cfg on first step works fine, variations work well for getting motion",
      "from": "Kijai"
    },
    {
      "comparison": "torch.compile + sdpa vs sage-attention",
      "verdict": "torch.compile + sdpa faster than sage-attention, but sage has lowest memory usage",
      "from": "patientx"
    },
    {
      "comparison": "WanAnimate + first frame vs other approaches",
      "verdict": "Better than basic pose control, though pose still has limitations",
      "from": "Juampab12"
    },
    {
      "comparison": "MoCha vs Wan-animate and Kling",
      "verdict": "MoCha demonstrates superior performance and doesn't need pose like Wan animate",
      "from": "Juampab12"
    },
    {
      "comparison": "Left Fast+Krea vs without fast lora on high sampler",
      "verdict": "Visual comparison shown but no specific verdict given",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "Krea vs other distillation LoRAs",
      "verdict": "Quality is disappointing even if it's 14B, similar to causvid issues",
      "from": "Kijai"
    },
    {
      "comparison": "MoCha vs other reference models",
      "verdict": "MoCha looks like CG, sometimes bad CG, fine for non-realistic but inconsistent quality",
      "from": "Screeb"
    },
    {
      "comparison": "Krea LoRA vs LightX LoRAs",
      "verdict": "No need for lightX loras with Krea, works better for realistic results",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "RCM vs Krea LoRA",
      "verdict": "Both are alternatives to lightX, but Krea is causal like CausVid",
      "from": "Juampab12"
    },
    {
      "comparison": "Rolling Forcing vs other models",
      "verdict": "Goes longer than other models before degrading, shows better temporal consistency",
      "from": "Draken"
    },
    {
      "comparison": "Fun vs Wanimate for character replacement",
      "verdict": "Fun is much better - Wanimate has sharper characters but doesn't fit the scene well",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "New 1022 LightX2V LoRA vs old versions",
      "verdict": "New version is the best LightX2V LoRA so far, works better on problematic prompts",
      "from": "Ada"
    },
    {
      "comparison": "MoCha similarity quality",
      "verdict": "Similarity is pretty bad, inputs real person but gets CGI output",
      "from": "berserk4501"
    },
    {
      "comparison": "Q5_K_S vs Q8 GGUF quality",
      "verdict": "Never use anything below Q8 unless absolutely necessary, especially not with RTX 5090",
      "from": "Kijai"
    },
    {
      "comparison": "New lightx2v LoRA vs old MoE version",
      "verdict": "New 1022 version performs much better, closest to 2.1 quality for 2D animation of any 2.2 distill so far",
      "from": "Kijai"
    },
    {
      "comparison": "Mocha vs WanAnimate",
      "verdict": "Mocha is clearly inferior to WanAnimate and doesn't keep character likeness as well",
      "from": "A.I.Warper"
    },
    {
      "comparison": "2.1 vs 2.2 quality",
      "verdict": "2.2 has higher quality and better prompt adherence than 2.1, but 2.1 still performs better for certain specific prompts",
      "from": "Juan Gea"
    },
    {
      "comparison": "Mocha vs other character models",
      "verdict": "Superior fidelity for body/facial movements but struggles with two-character scenarios and context windows",
      "from": "Visionmaster2"
    },
    {
      "comparison": "SVI vs VACE extensions",
      "verdict": "SVI for I2V continuations, VACE for T2V extensions with more control/flexibility",
      "from": "42hub"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 movement quality",
      "verdict": "Much better movement in 2.2 than 2.1",
      "from": "Zabo"
    },
    {
      "comparison": "Light VAE vs standard VAE",
      "verdict": "Big drop in details but really fast, uses less memory, unsure if better than tiny VAE",
      "from": "Kijai"
    },
    {
      "comparison": "Ditto vs Runway Gen3",
      "verdict": "Ditto quality degrades at longer durations, Runway maintains consistency better",
      "from": "Drommer-Kille"
    },
    {
      "comparison": "WSL vs native Linux vs native Windows",
      "verdict": "Native Linux fastest at loading, native Windows and Linux similar inference speed when everything works. WSL adds complexity for memory management",
      "from": "Kijai"
    },
    {
      "comparison": "SVI-film vs SVI-shot reference frame handling",
      "verdict": "SVI-shot works well with reference frame repetition for scene continuity, SVI-film fails completely with this technique",
      "from": "Ablejones"
    },
    {
      "comparison": "22.04 Ubuntu vs newer versions",
      "verdict": "22.04 more stable, used by Runpod and vast.ai. Newer versions may have compatibility issues",
      "from": "Colin"
    },
    {
      "comparison": "HoloCine vs SVI for extension",
      "verdict": "HoloCine examples look much better without the washed out bright issues that SVI has when videos get too long",
      "from": "Owlie"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 movement quality",
      "verdict": "After using 2.2, 2.1 looks slow and strange in comparison - SVI suffers from old 2.1 movements",
      "from": "Zabo"
    },
    {
      "comparison": "HoloCine vs base Wan 2.2 scene switching",
      "verdict": "Base 2.2 does harder cuts while HoloCine provides smoother transitions between scenes",
      "from": "VK (5080 128gb)"
    },
    {
      "comparison": "Context windows vs normal extension methods",
      "verdict": "Context windows always go back to I2V input for identity but are much slower and prone to artifacts",
      "from": "blake37"
    },
    {
      "comparison": "InfiniteTalk vs other extension methods",
      "verdict": "InfiniteTalk is still the king in terms of consistency with hardly any degradation/colorshift",
      "from": "seitanism"
    },
    {
      "comparison": "Film vs Shot LoRA methods",
      "verdict": "Film continues motion using 5 frames from last video but can degrade. Shot doesn't continue motion but stabilizes long generations by keeping original reference",
      "from": "Kijai"
    },
    {
      "comparison": "First-Frame-Last-Frame vs simple extension",
      "verdict": "FLF can work better because you never have to give a decoded frame, you have all keyframes. Simple extension degrades on second extension",
      "from": "Draken"
    },
    {
      "comparison": "5 vs 16 frames for motion continuation",
      "verdict": "16 frames better for motion continuation, 5 frames better for reducing degradation",
      "from": "DawnII"
    },
    {
      "comparison": "svi-film vs svi-shot",
      "verdict": "Film uses multiple frames (5) and degrades over time but works for longer videos. Shot uses single frame with reference image capability and can theoretically do infinite length",
      "from": "Kijai"
    },
    {
      "comparison": "merged vs unmerged LoRA",
      "verdict": "Merged seems to work better, unmerged can sometimes break loras completely or make them work better",
      "from": "Kijai"
    },
    {
      "comparison": "film vs film-opt lora",
      "verdict": "film-opt is a newer version of the film lora",
      "from": "Ablejones"
    },
    {
      "comparison": "infinitetalk vs humo for portrait",
      "verdict": "infinitetalk best for long portrait animation, humo good for short portrait animation",
      "from": "DawnII"
    },
    {
      "comparison": "HoloCine vs normal 2.2",
      "verdict": "HoloCine prevents corrupt first frame that occurs with normal weights",
      "from": "Kijai"
    },
    {
      "comparison": "Film vs Shot LoRA functionality",
      "verdict": "Film takes 5 input images and continues motion; Shot takes 1 input + reference for infinite generation without degradation but doesn't continue motion",
      "from": "Kijai"
    },
    {
      "comparison": "3 high 3 low with lightx vs 6 high no lora + 3 low with lightx",
      "verdict": "3+3 with both lightx gives better results",
      "from": "seitanism"
    },
    {
      "comparison": "LongCat vs WAN 2.2",
      "verdict": "LongCat has worse T2V scores than WAN 2.2 but half the size, unified model approach, and better long generation stability",
      "from": "Kijai"
    },
    {
      "comparison": "FusionX LoRA on WAN 2.1 vs WAN 2.2 with LightX2V",
      "verdict": "FusionX on WAN 2.1 produces better results",
      "from": "Govind Singh"
    },
    {
      "comparison": "LongCat distill vs original",
      "verdict": "16 step distill version is pretty bad compared to 50 step original",
      "from": "aikitoria"
    },
    {
      "comparison": "Holocine quality vs WAN 2.2",
      "verdict": "Slightly lower quality than normal WAN 2.2",
      "from": "seitanism"
    },
    {
      "comparison": "Wan 2.2 vs 2.1 motion understanding",
      "verdict": "Wan 2.2 not as smooth motion-wise and doesn't have same level of understanding for things like driving",
      "from": "Ada"
    },
    {
      "comparison": "HoloCine vs base Wan quality",
      "verdict": "HoloCine has worse prompt adherence than base but much less degradation",
      "from": "DawnII"
    },
    {
      "comparison": "SVI-Film vs VACE degradation",
      "verdict": "SVI-Film degradation much better than VACE when used as designed (Wan 2.1 I2V without distill LoRAs)",
      "from": "Ablejones"
    },
    {
      "comparison": "LongCat 30 steps cfg 4.0 vs 15 steps cfg 1.0 with distill LoRA",
      "verdict": "Distill LoRA allows faster generation with similar quality",
      "from": "Kijai"
    },
    {
      "comparison": "LongCat I2V vs Pusa",
      "verdict": "Looking at I2V now, seems similar/same to Pusa. LongCat cares about init image even less than Pusa",
      "from": "Kijai"
    },
    {
      "comparison": "LongCat model size vs others",
      "verdict": "14B is 5120, 5B is 3072, and LongCat is 4096 - 8 more blocks but slightly smaller dim",
      "from": "Kijai"
    },
    {
      "comparison": "Holocine vs manual prompting",
      "verdict": "Shot builder workflow vs normal text encode - shot builder missed fewer shots",
      "from": "shaggss"
    },
    {
      "comparison": "LongCat vs Wan 2.2",
      "verdict": "LongCat is on same level as Wan 2.2, essentially an improved Wan 2.2 5B version but trained from scratch",
      "from": "Lodis"
    },
    {
      "comparison": "LongCat vs Wan speed",
      "verdict": "Almost identical speed to Wan despite different architecture",
      "from": "Kijai"
    },
    {
      "comparison": "MoCha vs Wan Animate",
      "verdict": "Sometimes much better, other times worse. Only does replacement and worse at long gens without innate long generation ability",
      "from": "Kijai"
    },
    {
      "comparison": "Euler vs UniPC vs DPM++ samplers for WanAnimate",
      "verdict": "Euler turned out much better than LCM, UniPC seems slightly sharper in eyes, minimal difference overall",
      "from": "A.I.Warper"
    },
    {
      "comparison": "Distilled vs non-distilled lora for ID preservation",
      "verdict": "ID feels better with no lora, non-distilled doesn't exhibit eye makeup artifacts as much",
      "from": "A.I.Warper"
    },
    {
      "comparison": "LongCat vs other video models",
      "verdict": "LongCat is very promising, everything else was a bit 'meh' in tests",
      "from": "Gill Bastar"
    },
    {
      "comparison": "Wan2.2 LightVAE vs Wan2.1 VAE",
      "verdict": "Wan2.1 VAE is better, LightVAE shows considerable quality loss",
      "from": "Fawks"
    },
    {
      "comparison": "Wan VAE vs video codecs",
      "verdict": "Wan VAE has compression artifacts like choosing different jpg blocks each frame instead of using motion vectors like real video codecs",
      "from": "aikitoria"
    },
    {
      "comparison": "16x vs 32x VAE compression",
      "verdict": "Should use 16x spatial compression in VAE with no patchify on transformer for best quality, but nobody does this",
      "from": "spacepxl"
    },
    {
      "comparison": "LongCat vs Wan quality and length",
      "verdict": "LongCat feels like mix between Wan 2.1 and 2.2 quality, extension only way to get longer than 6 seconds",
      "from": "Zabo"
    },
    {
      "comparison": "WAN 2x VAE vs original VAE",
      "verdict": "2x VAE provides significantly better detail and sharpness, comparable to what SDXL refiner wished it could have been",
      "from": "spacepxl"
    },
    {
      "comparison": "GGUF vs FP8 scaled for models",
      "verdict": "Q8 vs FP8 scaled is a tossup, technically Q8 might be slightly better but FP8 scaled generally used",
      "from": "spacepxl"
    },
    {
      "comparison": "Original LTX vs other models",
      "verdict": "Original LTX generates uncontrollable garbage, terrible at anything not movie scenes/realism",
      "from": "aikitoria"
    },
    {
      "comparison": "WAN 2.5 vs LTX-2 via API",
      "verdict": "WAN 2.5 has potential and is a step up from 2.2, LTX-2 seems mediocre at best",
      "from": "blake37"
    },
    {
      "comparison": "FlashVSR vs WAN 2.5",
      "verdict": "FlashVSR results are better than WAN 2.5 but below Veo/Sora",
      "from": "Draken"
    },
    {
      "comparison": "HoloCine vs WAN 2.5",
      "verdict": "HoloCine is already better than WAN 2.5 preview minus the sound aspect",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "comparison": "LongCat vs Wan models",
      "verdict": "Better Wan 2.1 with native video continuation training, prevents color drift and quality degradation",
      "from": "JohnDopamine"
    },
    {
      "comparison": "10 steps vs 16 steps on LongCat",
      "verdict": "Less steps burns less but more mistakes in motion",
      "from": "Kijai"
    },
    {
      "comparison": "dpm++_sde vs other samplers",
      "verdict": "dmp++_sde with 10 steps ruins generation, not recommended",
      "from": "Kijai"
    },
    {
      "comparison": "Morph LoRA vs base Wan 2.2 I2V",
      "verdict": "Wan 2.2 I2V transitions seemed better in testing examples",
      "from": "shaggss"
    },
    {
      "comparison": "LightX2V 1030 vs 1022",
      "verdict": "1030 is 'a looooot better' according to early testing",
      "from": "Zabo"
    },
    {
      "comparison": "LTX 2 vs Wan quality",
      "verdict": "Wan has better video quality and coherence than LTX 2, despite LTX 2's technical specs",
      "from": "Kijai"
    },
    {
      "comparison": "LongCat vs Wan foundation",
      "verdict": "LongCat is clearly an improvement over Wan but may not be enough for people to start training ecosystems on it",
      "from": "Kijai"
    },
    {
      "comparison": "Lightning vs LightX2V LoRAs",
      "verdict": "LightX2V better - Lightning has exposure issues and not best motion, LightX2V has better prompt adherence",
      "from": "Kijai"
    },
    {
      "comparison": "New 1030 vs previous LightX2V",
      "verdict": "1030 has better prompt adherence and camera control",
      "from": "Aaron_PhD"
    },
    {
      "comparison": "Qwen Image Edit vs ChronoEdit",
      "verdict": "Qwen image edit still wins overall",
      "from": "Kiwv"
    },
    {
      "comparison": "Wan 2.1 vs 2.2 for low noise",
      "verdict": "2.1 LightX2V LoRA works fine with 2.2 low noise, no need for new 2.2 low noise LoRA",
      "from": "Kijai"
    },
    {
      "comparison": "ChronoEdit vs Qwen Image 2509",
      "verdict": "Qwen is definitely better, ChronoEdit fails on same prompts where Qwen fails",
      "from": "aikitoria"
    },
    {
      "comparison": "LongCat vs Wan for T2V",
      "verdict": "LongCat base model seems to understand concepts better and you don't need 57 models for similar quality as WAN 2.2",
      "from": "Pandaabear"
    },
    {
      "comparison": "LongCat vs Wan memory usage",
      "verdict": "WAN brings close to OOM often, LongCat barely uses 60% with higher resolution and more frames",
      "from": "Pandaabear"
    },
    {
      "comparison": "MOE vs standard models",
      "verdict": "MOE model gives better motion without having to increase strength above 1.0 or 1.2",
      "from": "blake37"
    },
    {
      "comparison": "VACE vs LongCat frame extension",
      "verdict": "VACE gets autoregressive burn in rather quickly and has color drift issues, while LongCat is really good for video extension",
      "from": "Benjaminimal"
    },
    {
      "comparison": "LongCat 2D animation vs LTX 2",
      "verdict": "LongCat has far better 2D animation quality than LTX 2 which does terrible there",
      "from": "Ada"
    },
    {
      "comparison": "Grok video model vs Wan",
      "verdict": "Grok looks like Wan trained on porn and audio doesn't line up properly, sounds like mmaudio",
      "from": "Kiwv"
    },
    {
      "comparison": "ChronoEdit vs Qwen",
      "verdict": "Can't do anything qwen can't so kinda useless, prompts that qwen fails at this also fails at",
      "from": "aikitoria"
    },
    {
      "comparison": "ChronoEdit vs Qwen quality",
      "verdict": "Quality of qwen edit 2509 at 1664x928 40 steps is vastly better than what ChronoEdit can do before it breaks down",
      "from": "aikitoria"
    },
    {
      "comparison": "Sora vs Veo quality",
      "verdict": "Sora blows veo out of the water on almost every shot generated, though veo 3.1 is sharper",
      "from": "Ruairi Robinson"
    },
    {
      "comparison": "LTX2 vs Wan potential",
      "verdict": "If LTX2 is slightly worse quality but cheaper and faster, would still make loras for LTX because of cost/speed advantages",
      "from": "Kiwv"
    }
  ],
  "tips": [
    {
      "tip": "Stick with Low Noise model for inpainting, try High Noise for smaller tasks",
      "context": "When using VACE inpainting with Wan 2.2",
      "from": "Ablejones"
    },
    {
      "tip": "Use white masks instead of gray for better results",
      "context": "When working with VACE",
      "from": "scf"
    },
    {
      "tip": "Turn on face detection settings for better lip sync results",
      "context": "Using Wan Animate for facial animation",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "tip": "Try stronger pose settings like 1.3 for lip sync delay issues",
      "context": "When experiencing timing issues in facial animation",
      "from": "Charlie"
    },
    {
      "tip": "Check riflex_freq_index setting in sampler - should be zero",
      "context": "Using VACE, non-zero values add strange effects",
      "from": "scf"
    },
    {
      "tip": "Increase LoRA strength when using fp8 models",
      "context": "Motion LoRAs may need higher strength values with fp8 compared to GGUF",
      "from": "Kijai"
    },
    {
      "tip": "Use .torchscript bbox detector for faster pose detection",
      "context": "DWPose processing optimization",
      "from": "xiver2114"
    },
    {
      "tip": "Use reference image with character in same pose and position as first frame of guiding video",
      "context": "For better character consistency in WanAnimate",
      "from": "seitanism"
    },
    {
      "tip": "Use torch compile to speed up generation",
      "context": "For faster inference when doing I2V with multiple LoRAs",
      "from": "seitanism"
    },
    {
      "tip": "Use Photoshop Beta for free upscaling during beta period",
      "context": "Has built-in Nano Banana and FLUX Kontext, much faster than Wan upscaling",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Avoid suspicious code with encrypted/binary files",
      "context": "Red flag when code is shared as .pyd files without source",
      "from": "Kijai"
    },
    {
      "tip": "Better sampling and more steps reduce noise artifacts",
      "context": "When using LightX 4-step models",
      "from": "Ablejones"
    },
    {
      "tip": "Try 6 steps first when troubleshooting noise",
      "context": "Instead of sticking to 4 steps with LightX models",
      "from": "Ablejones"
    },
    {
      "tip": "Good sampler can't fix a bad prompt",
      "context": "The sampler needs to be given something to find, prompt sets up the distribution",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Start with plain and simple characters",
      "context": "When having trouble getting good results, avoid difficult characters initially",
      "from": "Tony(5090)"
    },
    {
      "tip": "WanAnimate relight LoRA is hit or miss",
      "context": "Leave it in and turn to 0 or 1 depending on need, works better when sources have similar brightness",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "tip": "Use 3 samplers with Lightning LoRA",
      "context": "0-25-0.5 HN LoRA on first, 1.0 on HN on second, 1.0 LN on third",
      "from": "pom"
    },
    {
      "tip": "Audio scale settings for lipsync",
      "context": "Audio scale at 1.4, audio cfg at 1 for InfiniteTalk",
      "from": "Charlie"
    },
    {
      "tip": "Don't use fp8 if avoidable",
      "context": "For best results with model extraction",
      "from": "Kijai"
    },
    {
      "tip": "Use svd_lowrank algorithm for speed",
      "context": "When extracting model differences",
      "from": "Kijai"
    },
    {
      "tip": "Use higher resolution reference images for better face results",
      "context": "When using Wan Animate for character likeness, more pixels dedicated to face = higher quality face texture",
      "from": "oskarkeo"
    },
    {
      "tip": "Match reference frame to video control for better results",
      "context": "Use control net to generate ref image at similar pose to video control",
      "from": "hudson223"
    },
    {
      "tip": "Use clownshark samplers with res_5s for fewer steps",
      "context": "They solve the model more accurately allowing less steps and have different SDE math adding more noise",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Don't do sample images during LoRA training to save time",
      "context": "GPU to CPU switching for sample generation is costly during training",
      "from": "Kytra"
    },
    {
      "tip": "Use detailed text prompt on high noise, then low noise does remaining 20%",
      "context": "For character generation with 2.2",
      "from": "Juampab12"
    },
    {
      "tip": "Clear ComfyUI cache manually from temp folder",
      "context": "ComfyUI doesn't always clear cache on restart, leaves images/videos in temp folder",
      "from": "JohnDopamine"
    },
    {
      "tip": "Disconnect face_images noodle if you don't want face performance transfer",
      "context": "In Wan Animate, works fine without blocky black mask",
      "from": "CaptHook"
    },
    {
      "tip": "Iterate prompts multiple times for complex motions",
      "context": "Models may seem incapable but careful prompt engineering can achieve desired results",
      "from": "Ablejones"
    },
    {
      "tip": "Use CPU fallback for ONNX preprocessing when GPU fails",
      "context": "Still fast on good CPU due to tiny internal resolution",
      "from": "D-EFFECTS"
    },
    {
      "tip": "Run low-settings generations first, then increase quality",
      "context": "Save time by testing at lower resolution/steps before full quality generation",
      "from": "Rainsmellsnice"
    },
    {
      "tip": "Trees are problematic in FFLF with camera moves",
      "context": "Always one tree wanders through shot, difficult to fix in post",
      "from": "mdkb"
    },
    {
      "tip": "Use math nodes for automatic resizing to valid resolutions",
      "context": "Set to multiples of 64 or 128 to avoid black outputs",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "tip": "For character animation, disconnect bg_images and mask",
      "context": "When using WanAnimate for character animation vs character replacement",
      "from": "Kijai"
    },
    {
      "tip": "Compose character into desired background first",
      "context": "When wanting to generate background from another reference image",
      "from": "42hub"
    },
    {
      "tip": "Use speed LoRAs with reduced steps",
      "context": "With LoRAs can keep steps at 4-8 instead of 20 for faster generation",
      "from": "Dream Making"
    },
    {
      "tip": "Better I2V without LoRAs for base model",
      "context": "Base model should not be blurry for I2V without additional LoRAs",
      "from": "Charlie"
    },
    {
      "tip": "Use 2 RAM sticks instead of 4 for Intel 13th/14th gen",
      "context": "4 sticks strain the memory controller and prevent XMP from working properly",
      "from": "Ryzen"
    },
    {
      "tip": "Use wide-tooth comb for curly hair training data",
      "context": "When training LoRAs for realistic hair combing videos",
      "from": "phazei"
    },
    {
      "tip": "Close and restart ComfyUI if models get corrupted",
      "context": "When experiencing issues after ComfyUI has been open too long, reselect models after restart",
      "from": "Thom293"
    },
    {
      "tip": "FP8 is mandatory in user's experience",
      "context": "For practical usage despite quality differences",
      "from": "Dream Making"
    },
    {
      "tip": "For distill LoRAs, sampler choice matters less - stick with simple ones",
      "context": "When doing low steps with distill loras, use dpm++ sde or lcm, or even just euler",
      "from": "Kijai"
    },
    {
      "tip": "Use full distill model instead of LoRAs to prevent 'falling off'",
      "context": "Save model + lora combination as some software can have loras fall off",
      "from": "hicho"
    },
    {
      "tip": "Search through high number of combinations when training LoRAs",
      "context": "Biggest cause of poor LoRA results is not fully exploring settings combinations - can be automated with scripts",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Don't apply sage3 to first and last steps",
      "context": "When using distill lora with 4 steps, sage3 would only apply to 2 steps and still lose quality while gaining almost nothing",
      "from": "Kijai"
    },
    {
      "tip": "Don't use Lightning LoRA on high-noise model for WAN",
      "context": "I actually haven't thought about not using lightning loras for high. That's actually a pretty good idea! Works wonders too!",
      "from": "Zabo"
    },
    {
      "tip": "Use different negative prompts for video and audio in OVI",
      "context": "Video and audio uses same positive, but they can use different negative...at least original code does it like that",
      "from": "Kijai"
    },
    {
      "tip": "Parameter optimization needs to be minimised",
      "context": "Parameter combinations go to infinity",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Save really good seeds for months of reuse",
      "context": "You can save really good seeds and use them for months",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Don't jump models constantly",
      "context": "Better to focus on one model and master it",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Always merge at highest precision then quantize",
      "context": "When working with LoRAs and model merging",
      "from": "Kijai"
    },
    {
      "tip": "SDE samplers need at least 60 steps and no distil loras",
      "context": "When using SDE sampling methods",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "UniPC is a good safe general sampler",
      "context": "When unsure about sampler compatibility",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Be careful with SDE samplers at low steps",
      "context": "SDE samplers add noise and can do harm at low step counts",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Use dialogue tags for OVI audio generation",
      "context": "Format: 'The man says: <S>Hello, how do you do?<E>'",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "tip": "Install Triton on Windows directly",
      "context": "For speed improvements",
      "from": "Shadows"
    },
    {
      "tip": "Don't use fp8 quantization for WAN",
      "context": "fp8 makes everything blurry and motion is far worse, use Q8 GGUF if you need quantization",
      "from": "Ada"
    },
    {
      "tip": "Use Kijai's LoRA instead of PT files",
      "context": "For better compatibility and performance",
      "from": "Zabo"
    },
    {
      "tip": "Install nightly version of nodes for latest updates",
      "context": "To get the absolute latest version and avoid compatibility issues",
      "from": "Kijai"
    },
    {
      "tip": "Never clone HF repos, use HF CLI instead",
      "context": "Cloning can download outdated model files and create cache issues",
      "from": "aikitoria"
    },
    {
      "tip": "Prompt length affects quality",
      "context": "Too short and prompt tends to bleed in, too long and it tends to short circuit",
      "from": "TK_999"
    },
    {
      "tip": "50 steps and higher resolution improve OVI quality",
      "context": "Better than 20 steps at lower resolution for final quality",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Use manual HF downloads instead of auto-download",
      "context": "Auto-download can pull unnecessary files and uses single stream",
      "from": "mdkb"
    },
    {
      "tip": "Use LayerForge node with Matting button for character replacement",
      "context": "For character consistency workflows with MagRef",
      "from": "Elvaxorn"
    },
    {
      "tip": "Avoid going fully cold with serverless deployments",
      "context": "Keep different levels of warm starts for better performance",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Lower CFG for faces to get softer results",
      "context": "Face generation in Wan models",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Use 2.5 CFG on low noise instead of 3.5 for better faces",
      "context": "CFG degrades faces on low noise model",
      "from": "Lumifel"
    },
    {
      "tip": "Remove all other distill LoRAs when using speed LoRAs unless strength is low",
      "context": "They don't play well together",
      "from": "Elvaxorn"
    },
    {
      "tip": "Use 3.0 strength for high noise and 1.0 for low noise with 2.1 Lightx2v",
      "context": "Kijai's recommended settings",
      "from": "Kijai"
    },
    {
      "tip": "Use cfg skimming with higher CFG values to prevent burn while maintaining prompt adherence",
      "context": "For better prompt following with distilled models",
      "from": "GalaxyTimeMachine"
    },
    {
      "tip": "Use longer prompts for better motion",
      "context": "For Wan 2.2 to get more realistic movement and emotions, especially for T2V rather than I2V",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Use precise schedules for low step distillation",
      "context": "At low steps you really need the schedule to be precise, at high steps it doesn't matter as much",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "tip": "Disable torch compile when using dynamic CFG",
      "context": "When CFG changes each step, torch compile will recompile and slow things down",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "tip": "Use 3 sampler workflow for better results",
      "context": "Still worth using 3 sampler setup: 1 step HN no lora cfg 3.5 + 2 step HN with new lightx2v lora + 3 step LN with old 2.1 lightx2v lora",
      "from": "FL13"
    },
    {
      "tip": "For pose detection issues with distant characters, crop in on person, upscale to at least 480p, generate, then outpaint with VACE",
      "context": "When character is far away from viewer",
      "from": "ArtOfficial"
    },
    {
      "tip": "Use LoRAs to add movement to T2V when LightX LoRA would be slow motion",
      "context": "Training custom LoRAs for movement",
      "from": "Drommer-Kille"
    },
    {
      "tip": "For CFG scheduling without lighting LoRAs, use 20 steps (2x10), CFG 3.5 on high and 2.5 on low",
      "context": "Standard CFG settings for quality",
      "from": "Lumifel"
    },
    {
      "tip": "Clear triton caches when updating torch and using torch compile",
      "context": "After torch updates",
      "from": "Kijai"
    },
    {
      "tip": "Don't use speed optimization techniques like teacache with only 4 steps",
      "context": "Speed optimizations are useless when running minimal steps",
      "from": "Kijai"
    },
    {
      "tip": "Use hybrid approaches with CFG for first step as best compromise for Wan 2.2",
      "context": "When distillation LoRAs fail to produce dynamic motion like CFG does",
      "from": "Kijai"
    },
    {
      "tip": "Use Kijai's color match node over other versions",
      "context": "For video extension workflows, provides better accuracy",
      "from": "Tachyon"
    },
    {
      "tip": "Use difference blend mode to compare model outputs",
      "context": "When testing different model versions to spot subtle differences",
      "from": "Valle"
    },
    {
      "tip": "Generate image first then do I2V instead of T2V",
      "context": "More control over final output",
      "from": "aikitoria"
    },
    {
      "tip": "Use overlaps when stitching video segments",
      "context": "10-20% frame overlap maintains consistency",
      "from": "Dever"
    },
    {
      "tip": "For LoRA extractions, get the ones Kijai did on his HF as they have all non-block changes",
      "context": "Some official Lightx2v LoRAs didn't have all changes so performed oddly",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use VACE with no image input and character loras for face swaps",
      "context": "Works really well, or use with loras to stabilize likeness in addition to ref image",
      "from": "Ruairi Robinson"
    },
    {
      "tip": "For long video upscaling, split to latent files and process in sections",
      "context": "Upscale in 120 frame sections with 100 frame steps, leaves 17 frames overlap, then use USDU node at low denoise 0.1-0.3",
      "from": "mdkb"
    },
    {
      "tip": "Include color information in captions when training LoRAs",
      "context": "When training object LoRAs, mentioning specific colors helps with proper generation",
      "from": "Dever"
    },
    {
      "tip": "Run longer generation and trim if getting pause at start",
      "context": "When dealing with first frame issues, sometimes better to generate extra and chop off frames using Video Helper Suite",
      "from": "garbus"
    },
    {
      "tip": "Use 2.2 high with 2.1 low for best results",
      "context": "Recommended combination for high/low noise expert models",
      "from": "Tachyon"
    },
    {
      "tip": "Lower LoRA strengths and use block editing for mixing LoRAs",
      "context": "Helps avoid grainy results when mixing multiple LoRAs",
      "from": "flo1331"
    },
    {
      "tip": "Use scaled models over fp8 for better quality",
      "context": "General recommendation for maintaining model quality",
      "from": "JohnDopamine"
    },
    {
      "tip": "Restart ComfyUI after every run to avoid GPU overload",
      "context": "Workaround for current VRAM issues after updates",
      "from": "mdkb"
    },
    {
      "tip": "Use highest resolution possible for I2V to maintain character likeness",
      "context": "Bigger input provides better reference for models to keep character consistency",
      "from": "blake37"
    },
    {
      "tip": "Be careful with multiple LoRAs overwhelming character identity",
      "context": "Too many strong LoRAs can override character training data",
      "from": "blake37"
    },
    {
      "tip": "Use CFG 2 or higher for better style transfer results",
      "context": "CFG 1 may not achieve desired stylization effect",
      "from": "Draken"
    },
    {
      "tip": "Use cfg only on step 1 for good results with minimal complexity",
      "context": "Works well with or without LoRA, easier than complex multi-step configurations",
      "from": "Kijai"
    },
    {
      "tip": "Split high/low steps at right point when using speed LoRAs",
      "context": "If you increase high steps without increasing low steps it gets unbalanced",
      "from": "Kijai"
    },
    {
      "tip": "Use reduced VACE strength with cfg",
      "context": "Examples show using cfg 2.0 on first step with reduced VACE strength",
      "from": "Kijai"
    },
    {
      "tip": "Match dtypes when extracting LoRAs",
      "context": "Try to always match the dtypes, use model loaders to force bf16 to avoid mismatches",
      "from": "Kijai"
    },
    {
      "tip": "Lock critical dependencies to prevent environment breakage",
      "context": "Lock numpy and other critical packages so they don't auto-update and break ComfyUI",
      "from": "L\u00e9on"
    },
    {
      "tip": "Muted colors are a GOOD thing - it's always better to have a flat image because it's easier to grade",
      "context": "Contrast is the enemy, oversaturated overcontrasted look baked in AI models can't be undone in post",
      "from": "Quality_Control"
    },
    {
      "tip": "Use total pixel resize setting",
      "context": "When working with Mocha - change crop to total pixel then h and w as 640 640",
      "from": "hicho"
    },
    {
      "tip": "Disable wan preset in VHS video loader",
      "context": "When using Mocha - if you refresh, VHS changes the resolution",
      "from": "hicho"
    },
    {
      "tip": "Use denoised output for high sampler preview",
      "context": "End of high sampler supposed to have leftover noise before passing to low noise - that's how MoE arch works",
      "from": "DawnII"
    },
    {
      "tip": "Always use prompts with MoCha for best results",
      "context": "Without prompts, model can produce body horror or weird artifacts",
      "from": "Kijai"
    },
    {
      "tip": "Use upper body shots as reference for better results",
      "context": "When doing character replacement with MoCha",
      "from": "Kijai"
    },
    {
      "tip": "For NAG prompts, use simple negatives only",
      "context": "NAG negative should only contain unwanted elements, not generic negative prompt word salad",
      "from": "Kijai"
    },
    {
      "tip": "Use manual masking for better control",
      "context": "Better than auto-masking, only need to do first frame",
      "from": "Draken"
    },
    {
      "tip": "Try using two references (face + body) for full character transfer",
      "context": "May help avoid face detection issues in body areas",
      "from": "Draken"
    },
    {
      "tip": "Slow down control video if camera movement is too fast",
      "context": "For Uni3C camera control, misinterpretation of scale causes speed issues",
      "from": "Juampab12"
    },
    {
      "tip": "You can use Qwen VAE for Wan single image generations",
      "context": "Just not for video generation",
      "from": "Kijai"
    },
    {
      "tip": "Use fp8_fast mode for ~30% speed improvement",
      "context": "When using fp8 model on RTX 5090",
      "from": "Kijai"
    },
    {
      "tip": "Avoid normal fp8 (non-scaled) and GGUF below Q8",
      "context": "For optimal quality",
      "from": "Kijai"
    },
    {
      "tip": "Don't run ComfyUI with --high-vram",
      "context": "Should use automatic model offloading instead",
      "from": "Kijai"
    },
    {
      "tip": "Use nvtop to monitor VRAM",
      "context": "Best tool for VRAM monitoring on Linux",
      "from": "Kijai"
    },
    {
      "tip": "Use 2+2 step split for new light LoRA",
      "context": "Most reasonable split for 4 total steps, can use 3+3 for better image quality",
      "from": "Ablejones"
    },
    {
      "tip": "Avoid 3+1 step configuration",
      "context": "When splitting steps between samplers for distill LoRAs",
      "from": "Ablejones"
    },
    {
      "tip": "Test without distill LoRAs for accurate comparisons",
      "context": "Distill LoRAs can contaminate tests of new techniques and tools",
      "from": "Ablejones"
    },
    {
      "tip": "Use CFG settings appropriately",
      "context": "CFG is important for ID consistency, higher CFG with distill LoRAs can overcook results",
      "from": "DawnII"
    },
    {
      "tip": "Use strength 3 on high and 1 on low for SVI with Wan 2.2",
      "context": "Initial testing parameters for SVI compatibility",
      "from": "Zabo"
    },
    {
      "tip": "closeups work much better than wide shots",
      "context": "For character consistency in video generation",
      "from": "Drommer-Kille"
    },
    {
      "tip": "Use fixed seed and same prompt node plus color match",
      "context": "To reduce color shift between generations in multi-batch workflows",
      "from": "hicho"
    },
    {
      "tip": "Set WSL config with nonblocking and 1 prefetch blocks",
      "context": "For optimal WSL performance with video generation",
      "from": "seitanism"
    },
    {
      "tip": "Don't use block swap if you don't have VRAM issues",
      "context": "Block swapping can use additional RAM and may cause OOM issues",
      "from": "seitanism"
    },
    {
      "tip": "Use grey pixels (0.5) for best video extension padding",
      "context": "When manually setting padding frame pixel values for video extensions",
      "from": "Ablejones"
    },
    {
      "tip": "Close background apps and restart browser for better performance",
      "context": "When experiencing memory issues during generation",
      "from": "seitanism"
    },
    {
      "tip": "Use LLMs for Linux configuration but not video AI techniques",
      "context": "LLMs good for basic Linux setup but unreliable for video model specifics",
      "from": "42hub"
    },
    {
      "tip": "Use LightX2V LoRA with 4 steps and CFG 1 for InfiniteTalk to avoid extremely long generation times",
      "context": "When doing long audio-driven video generation",
      "from": "seitanism"
    },
    {
      "tip": "Don't always use init image as first frame - can use random frame from earlier video for consistency",
      "context": "When extending videos beyond 2-3 segments",
      "from": "VK (5080 128gb)"
    },
    {
      "tip": "Increase RAM capacity and use fp8 quantized models to speed up loading",
      "context": "When models fit in RAM, loading is very fast after first load",
      "from": "seitanism"
    },
    {
      "tip": "Store models within WSL file system rather than accessing from outside",
      "context": "When using WSL on Windows for better performance",
      "from": "seitanism"
    },
    {
      "tip": "Use less random samplers for better SVI results",
      "context": "No _sde or lcm samplers work better for SVI",
      "from": "Kijai"
    },
    {
      "tip": "Use different seeds for each SVI generation",
      "context": "Same seed makes burning/flash artifacts worse in continuations",
      "from": "Kijai"
    },
    {
      "tip": "Manually set mask for only first frame in shot method",
      "context": "Important for shot LoRA to work properly, default masks all images",
      "from": "Kijai"
    },
    {
      "tip": "Consider content type for flash visibility",
      "context": "Flash artifacts most noticeable in dark scenes, barely visible in bright areas like sky",
      "from": "Draken"
    },
    {
      "tip": "For 2.2, increase LoRA strength for high noise when using 2.1 LoRAs",
      "context": "2.1 LoRAs don't work properly on 2.2 high noise due to differences",
      "from": "Kijai"
    },
    {
      "tip": "Use merged fp16 lora instead of unmerged",
      "context": "When using special loras like shot, sometimes unmerged applying can affect negatively",
      "from": "Kijai"
    },
    {
      "tip": "Leave off padding frames with film lora",
      "context": "Film lora can't handle padded reference frames",
      "from": "Ablejones"
    },
    {
      "tip": "For svi-film just send 5 overlap frames as start image",
      "context": "No need to worry about masks when using svi-film, just send the 5 overlap frames",
      "from": "Ablejones"
    },
    {
      "tip": "Can adjust audio scaling or lock pose with unianimate",
      "context": "To reduce hand movement in portrait animations",
      "from": "DawnII"
    },
    {
      "tip": "Reference padding only works with shot, talk and dance loras",
      "context": "Film lora will freeze if you try to use reference padding",
      "from": "Kijai"
    },
    {
      "tip": "Use middle grey (127) not black for padded frames in SVI workflows",
      "context": "When replicating SVI workflows manually outside wrapper, because diffusers converts 0 to middle grey",
      "from": "42hub"
    },
    {
      "tip": "Lower lightx2v lora strength works better: 1.5 strength on high noise, 1.0 on low noise",
      "context": "When using HoloCine with lightx2v lora",
      "from": "VK (5080 128gb)"
    },
    {
      "tip": "Use structured prompts: general description + specific scene prompts separated by |",
      "context": "For HoloCine multi-shot generation - general prompt describes overall setting, specific prompts describe camera motion/cuts for each scene",
      "from": "seitanism"
    },
    {
      "tip": "Try no SVI LoRA for first sampler to help with motion in first 5 seconds",
      "context": "When first part of video appears too static",
      "from": "garbus"
    },
    {
      "tip": "Use LightX quantile 0.15 LoRA with high 3.0, low 1.0 strength",
      "context": "For T2V generation",
      "from": "Gill Bastar"
    },
    {
      "tip": "Generate at lower resolution with max frames, then upscale using context options",
      "context": "For better consistency with long generations",
      "from": "mamad8"
    },
    {
      "tip": "Keep exact aspect ratio when generating at low resolution",
      "context": "To ensure proper scene switching",
      "from": "VK"
    },
    {
      "tip": "Feed video with pure background colors that change at each shot end with denoise 0.9-0.95",
      "context": "To force high noise model to differentiate scenes",
      "from": "mamad8"
    },
    {
      "tip": "Use a 2nd pass with low model to fix quality when using sparse attention",
      "context": "For improving sparse attention results",
      "from": "Ada"
    },
    {
      "tip": "SVI works much better with WAN22 fun inp model and waninpainttovideo node",
      "context": "For video extension workflows",
      "from": "42hub"
    },
    {
      "tip": "Delete the added 5 frames and use ones from previous generation when stitching with SVI",
      "context": "The added frames are glitchy but motion is kept after deletion",
      "from": "lemuet"
    },
    {
      "tip": "Installing Flash Attention 2.8.3 reduces generation time by 25%",
      "context": "For HoloCine optimization (Blackwell cannot install Flash Attention 3)",
      "from": "BNP4535353"
    },
    {
      "tip": "Use bucket resolutions for LongCat",
      "context": "Model is picky about input resolution",
      "from": "Kijai"
    },
    {
      "tip": "Use different seeds per clip extension in SVI",
      "context": "Important to avoid burning and looping if prompt is same",
      "from": "JohnDopamine"
    },
    {
      "tip": "Make sure all cuts are 4t+1 for Holocine",
      "context": "When using holocine shot args",
      "from": "shaggss"
    },
    {
      "tip": "Holocine limit is 15 seconds for quality",
      "context": "Beyond 15sec quality degrades significantly",
      "from": "avataraim"
    },
    {
      "tip": "Use detailed prompts with MoCha for better character likeness",
      "context": "Generic prompts work but more detail improves results, especially for character consistency",
      "from": "DawnII"
    },
    {
      "tip": "Use 10 steps for good balance of quality and speed with LongCat",
      "context": "5 steps too fast, 15 steps slower, 10 steps provides good quality/speed balance",
      "from": "avataraim"
    },
    {
      "tip": "Clear triton caches when torch compile runs old code",
      "context": "When using torch compile and getting unexpected behavior",
      "from": "Kijai"
    },
    {
      "tip": "For extending videos, take x frames from end of previous gen to start next",
      "context": "Standard extend workflow technique, join result to previous gen minus x frames",
      "from": "Kijai"
    },
    {
      "tip": "Use shot builder with Holocine text encode instead of WanVideo text encode",
      "context": "When facing weird errors with WanVideo text encode node",
      "from": "shaggss"
    },
    {
      "tip": "Use different seeds for each shot when extending",
      "context": "For video extension with LongCat using last frame method",
      "from": "avataraim"
    },
    {
      "tip": "LongCat likes specific prompt structure",
      "context": "More misses without proper prompt structure for T2V",
      "from": "Pandaabear"
    },
    {
      "tip": "Always connect image embed node or workflow won't work",
      "context": "Workflow needs image embed connected to something to function",
      "from": "Cubey"
    },
    {
      "tip": "Use NeatVideo for denoising Wan VAE artifacts",
      "context": "Enable temporal and set spatial to low weight to preserve detail",
      "from": "spacepxl"
    },
    {
      "tip": "Treat VAE training as conditional GAN instead of just decoder",
      "context": "For better detail generation and noise reduction",
      "from": "spacepxl"
    },
    {
      "tip": "Use context windows for first/middle/last frame reference",
      "context": "Can give each window a latent for frame conditioning",
      "from": "blake37"
    },
    {
      "tip": "Reduce contrast in driver video for Uni3C controlnet",
      "context": "Helps with stronger overlay effects at higher resolutions",
      "from": "paulHAX"
    },
    {
      "tip": "For WAN low noise refinement use shift 1, denoise 0.2-0.3, with distill LoRA use 2 steps, without distill use 8-10 steps and CFG 3-5",
      "context": "When refining images after generation",
      "from": "spacepxl"
    },
    {
      "tip": "Use different seed when doing img2img to avoid same noise causing problems",
      "context": "When getting repeated artifacts in img2img",
      "from": "spacepxl"
    },
    {
      "tip": "Set shift using SD3 node, default is 5 which makes img2img annoying",
      "context": "For better img2img control",
      "from": "spacepxl"
    },
    {
      "tip": "Try DPM++ SDE/beta sampler with 6/3 steps for OVI",
      "context": "For faster OVI inference",
      "from": "avataraim"
    },
    {
      "tip": "Use interpolation on WAN 2.2 for better frame rates",
      "context": "Can get decent 24fps gens with interpolation",
      "from": "b\u0336\u0308\u0301\u0360o\u0336\u0317\u0305n\u0336\u033d\u0312k\u0335\u033d\u033f"
    },
    {
      "tip": "Disconnect T5 if prompt is cached",
      "context": "To reduce RAM usage, better to use Cached node",
      "from": "Kijai"
    },
    {
      "tip": "Use layer offloading with AI toolkit for efficient training on 4090",
      "context": "For LoRA training when you need some offloading",
      "from": "Kiwv"
    },
    {
      "tip": "For video continuation without degradation, use LongCat which is natively pretrained on video continuation tasks",
      "context": "When needing long video generation",
      "from": "JohnDopamine"
    },
    {
      "tip": "Use 2.1 lightx2v as safe bet for distill LoRA",
      "context": "When choosing distill LoRA for Wan 2.2",
      "from": "Kijai"
    },
    {
      "tip": "Avoid Lightning LoRAs if you don't want style/exposure changes",
      "context": "When wanting neutral LoRA effects",
      "from": "Kijai"
    },
    {
      "tip": "For anime style, use AniSora 3.2 for high noise model",
      "context": "When generating anime content",
      "from": "Kijai"
    },
    {
      "tip": "Don't bother much with low noise side, basic 2.1 lightx2v is good and neutral",
      "context": "When setting up LoRA configuration",
      "from": "Kijai"
    },
    {
      "tip": "Generate at 720p for better pose following in WanAnimate",
      "context": "Higher resolution gives better results and pose adherence than lower resolutions",
      "from": "Charlie"
    },
    {
      "tip": "Focus on developing integration tools rather than waiting for perfect models",
      "context": "Open source models succeed by using them together in sequence/simultaneously to fill each other's gaps",
      "from": "Ablejones"
    },
    {
      "tip": "Use controlled methods for production work",
      "context": "Pure prompt-to-video or image-to-video currently looks like 'AI slop' - controlled methods are the only usable option for production",
      "from": "Kijai"
    },
    {
      "tip": "Learning current tools gives advantage over waiting",
      "context": "People taking time to learn now will have more capability than those only starting with future tools",
      "from": "DawnII"
    },
    {
      "tip": "For object removal in video, use Qwen edit or inpaint model on one frame then fill rest with VACE if minimax/VACE alone isn't working",
      "context": "Video object removal workflows",
      "from": "spacepxl"
    },
    {
      "tip": "Run video through upscale on low denoise second time to reduce degradation",
      "context": "Dealing with video quality degradation",
      "from": "VK (5080 128gb)"
    },
    {
      "tip": "Use Anisora at 0.5 strength for cartoon content",
      "context": "Cartoon/animated content generation",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "Don't use CFG with newest LightX2V - adding 2 CFG steps resulted in trash results",
      "context": "Using new v1030 LoRA",
      "from": "Gleb Tretyak"
    },
    {
      "tip": "Use 'shot' instead of 'camera' in prompts",
      "context": "When trying to control camera movement",
      "from": "DawnII"
    },
    {
      "tip": "Use cached node to avoid speed issues",
      "context": "After initial processing with slower models",
      "from": "DawnII"
    },
    {
      "tip": "Can use last frame and continue for extensions",
      "context": "Simple method for extending LongCat videos",
      "from": "avataraim"
    },
    {
      "tip": "Clean HD regularly to free up space",
      "context": "Spend 30m clearing unused models - often find 300gb of unused stuff",
      "from": "Kiwv"
    },
    {
      "tip": "Use T5 and CLIP text encoders because they are naturally tuned to relate to images",
      "context": "Compared to other encoders like Qwen which require the video model to do much more work with embeds",
      "from": "Kiwv"
    },
    {
      "tip": "Scaling up quality data is as effective as scaling up params or compute for improving model clarity",
      "context": "When training video models",
      "from": "Kiwv"
    },
    {
      "tip": "Use 40 steps for Qwen Edit for much better sharpness",
      "context": "When generating at higher resolutions like 1664x928",
      "from": "aikitoria"
    },
    {
      "tip": "ChronoEdit works best for animating existing image rather than redrawing from different POV",
      "context": "If you prompt it to redraw the image from a different pov, it's ass",
      "from": "Ness"
    },
    {
      "tip": "Don't update ComfyUI frequently to avoid breaking nodes",
      "context": "For stability with custom nodes",
      "from": "Kiwv"
    }
  ],
  "news": [
    {
      "update": "Hunyuan working on new video model",
      "details": "200GB model size mentioned, unknown if it will be open source",
      "from": "yi"
    },
    {
      "update": "Kijai updated wrapper example to use new preprocessing nodes",
      "details": "Updated example workflow available",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI PR for optimizations submitted",
      "details": "GitHub PR #10141 for VRAM optimizations",
      "from": "JohnDopamine"
    },
    {
      "update": "LanPaint now supports Wan 2.2",
      "details": "Supports text to image generation with Wan2.2 T2V model",
      "from": "s2k"
    },
    {
      "update": "New LightX2V updates coming with date-based versioning",
      "details": "Team adopted new algorithm, subsequent updates will be relatively fast, using dates instead of version numbers",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "Ovi model released",
      "details": "11B video model with audio output based on wan2.2 5B and Mmaudio, weights available",
      "from": "yi"
    },
    {
      "update": "Face bbox output added to preprocessing workflow",
      "details": "Same bbox that crops the face, now has output for easier face swapping",
      "from": "Kijai"
    },
    {
      "update": "New optimization models released",
      "details": "Palingenesis released a week ago, and Wan 2.2-Lightning 4-step LoRA released 5 days ago",
      "from": "Drommer-Kille"
    },
    {
      "update": "Sora 2 code released by OpenAI",
      "details": "Invite codes available for Sora 2 access",
      "from": "Draken"
    },
    {
      "update": "Ovi model release",
      "details": "Character.ai released Ovi model based on 5B parameters with audio generation capabilities",
      "from": "BecauseReasons"
    },
    {
      "update": "No official VACE for A14B planned",
      "details": "There won't be any official VACE for A14B model, but Fun version works okay",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI mask operations optimized",
      "details": "Blockify and DrawMaskOnImage nodes now run on GPU for massive speed improvements",
      "from": "Kijai"
    },
    {
      "update": "New Lightning LoRAs released",
      "details": "250928 LoRAs released 5 days ago, very good for motion preservation",
      "from": "pom"
    },
    {
      "update": "Ovi model available with fp8 quantization",
      "details": "Reduces VRAM requirement to 16GB",
      "from": "Stad"
    },
    {
      "update": "DC-VideoGen source code released",
      "details": "Promises up to 3.75x speedup for Wan models",
      "from": "slmonker"
    },
    {
      "update": "OVI model released",
      "details": "11B parameter audio-video model based on Wan 2.2 5B with added audio backbone, supports speech generation and sound effects",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "update": "OVI ComfyUI integration request opened",
      "details": "GitHub issue opened for ComfyUI support but no timeline confirmed",
      "from": "Lodis"
    },
    {
      "update": "OVI developer working on ComfyUI implementation",
      "details": "Character.ai team developing native ComfyUI support",
      "from": "Stad"
    },
    {
      "update": "Self Forcing++ from ByteDance announced",
      "details": "Promises minute-long videos, code will be released soon",
      "from": "Shubhooooo"
    },
    {
      "update": "Civitai added Sora 2.5 listing",
      "details": "Indicates potential upcoming release",
      "from": "Ryzen"
    },
    {
      "update": "OVI ComfyUI implementation being worked on",
      "details": "Developers mentioned on GitHub they're working on it, but no official confirmation yet",
      "from": "Stad"
    },
    {
      "update": "OVI model now available in ComfyUI",
      "details": "RunningHub created ComfyUI_RH_Ovi implementation, Kijai looking at adding to WanVideoWrapper",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "Sora 2 API pricing announced",
      "details": "Per second pricing: $0.05 for 480p, $0.10 for 720p, $0.15 for 1080p",
      "from": "Juampab12"
    },
    {
      "update": "Wan 2.5 speculation",
      "details": "Community expects Wan 2.5 might be 40B MoE model with activated parameters within consumer hardware range",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "Kijai merged Lynx branch into main WanVideoWrapper",
      "details": "Includes sample workflow, now available in main branch",
      "from": "phazei"
    },
    {
      "update": "OVI nodes now available for ComfyUI",
      "details": "ComfyUI_RH_Ovi repository available on GitHub",
      "from": "Lodis"
    },
    {
      "update": "Ovi people pushing Ovi nodes to WanVideoWrapper",
      "details": "8k lines PR being submitted to integrate Ovi into WanVideoWrapper",
      "from": "Stad"
    },
    {
      "update": "OVI native ComfyUI implementation in progress",
      "details": "Kytra working on native implementation with native comfy model loading/patching/encoding/decoding, T2V already working",
      "from": "Kytra"
    },
    {
      "update": "AI-Toolkit now supports training LoRAs with mostly RAM",
      "details": "About 50% speed difference, 17s/it for 3000 steps (~14hrs on 6GB laptop), works for various models",
      "from": "Ada"
    },
    {
      "update": "Hunyuan 3D 2.1 released",
      "details": "Fast mesh generation from images, works well in Blender",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "update": "OVI (Open source Veo3-like) released by Kijai",
      "details": "Video with audio generator based on Wan2.2 5B and MMAudio, supports T2V and I2V",
      "from": "Stad/Charlie"
    },
    {
      "update": "AniSora extracted into LoRA format",
      "details": "I also extracted it into a lora that works ok",
      "from": "Kijai"
    },
    {
      "update": "OVI model available in test branch",
      "details": "11B audio-video model in the ovi branch of WanVideoWrapper",
      "from": "Kijai"
    },
    {
      "update": "rCM LoRA extracted and available",
      "details": "SOTA distillation method from Nvidia, available as LoRA",
      "from": "Kijai"
    },
    {
      "update": "Warning about fake 'Eddy' releases",
      "details": "Kijai warns against fake releases like 'sageattn4', 'wan animate v3' etc. - described as bullshit and potentially malicious",
      "from": "Kijai"
    },
    {
      "update": "GAGA model coming according to bdsqlsz",
      "details": "Announcement on Twitter about upcoming GAGA release",
      "from": "Tony(5090)"
    },
    {
      "update": "Fake Wan 2.5 on HuggingFace",
      "details": "Someone uploaded a fake wan-2.5 model to HuggingFace",
      "from": "Lodis"
    },
    {
      "update": "Wan upscaler workflow completed",
      "details": "Handles 720p to 2K and 360p to 2K upscaling with improved stability and sharpness",
      "from": "Lan8mark"
    },
    {
      "update": "Neex bumped testing to next week Thursday morning",
      "details": "Will be doing evaluation testing on Thursday morning",
      "from": "Neex"
    },
    {
      "update": "ComfyUI memory leak fix implemented",
      "details": "Recent commit addressed memory leak issue causing OOM problems",
      "from": "JohnDopamine"
    },
    {
      "update": "SAM 3 announced by Meta",
      "details": "Available through waitlist signup, successor to SAM 2.1",
      "from": "Kijai"
    },
    {
      "update": "New RCM LoRA released",
      "details": "Alternative to lightx2v by Nvidia developers for speed/distillation",
      "from": "JohnDopamine"
    },
    {
      "update": "SEC nodes available",
      "details": "ComfyUI SEC nodes released for enhanced SAM2 guidance",
      "from": "ArtOfficial"
    },
    {
      "update": "PyTorch 2.9 and 2.10 available",
      "details": "Newer PyTorch versions available, 2.8 has known issues",
      "from": "Tony(5090)"
    },
    {
      "update": "New Wan 2.2 I2V Lightx2v full distilled models released",
      "details": "24GB models available on HuggingFace, both high and low noise versions",
      "from": "yi"
    },
    {
      "update": "Lightx2v team released LoRA versions alongside full models",
      "details": "LoRAs available in the same repo as the full models",
      "from": "JohnDopamine"
    },
    {
      "update": "New LightX2V High model released",
      "details": "Full 28GB model released with better extraction compared to original LoRA that was missing patches",
      "from": "JohnDopamine"
    },
    {
      "update": "FlashVSR super resolution model released",
      "details": "New super resolution model based on Wan with only 11GB VRAM usage, much less than other diffusion upscalers",
      "from": "yi"
    },
    {
      "update": "Triton 3.5.0 released with fp8 fixes",
      "details": "New release specifically mentions fp8 patch merged to fix compilation issues",
      "from": "phazei"
    },
    {
      "update": "Windows wheels available for SageAttention and Triton",
      "details": "Prebuilt packages available at woct0rdho repos",
      "from": "Kijai"
    },
    {
      "update": "AI-toolkit now supports video training and has queue feature",
      "details": "Works with Wan 2.2 but has bugs with 2.1",
      "from": "Drommer-Kille"
    },
    {
      "update": "Fake news about Higgsfield acquiring Wan 2.5 for $100M",
      "details": "Multiple sources confirm it's AI-generated spam articles on Medium knockoff sites",
      "from": "JohnDopamine"
    },
    {
      "update": "Kijai released workaround for PyTorch 2.9.0 VAE issue",
      "details": "Added fix that does conv3d in fp32 to bypass bug while using less VRAM than full fp32",
      "from": "Kijai"
    },
    {
      "update": "HuMo best practice guide coming soon",
      "details": "Team announced a Best-Practice Guide for HuMo will be released soon",
      "from": "aiacsp"
    },
    {
      "update": "New Wan2.2 4-step distill models released",
      "details": "High noise model is completely new, low noise reuses old one. Much improved over Lightning distill",
      "from": "aikitoria"
    },
    {
      "update": "22B video model by cloneofsimo",
      "details": "Trained by small team at Fal, Hunyuan Video level quality, not yet released",
      "from": "yi"
    },
    {
      "update": "Kandinsky 5 Pro confirmed for end of November",
      "details": "Expected to compete with Waver, uses Hunyuan VAE",
      "from": "yi"
    },
    {
      "update": "DimensionX finally released after 10 month wait",
      "details": "ComfyUI support available",
      "from": "Kijai"
    },
    {
      "update": "Lightx2v released reorganized WAN 2.2 distilled models",
      "details": "New release has same SHA256 as previous models, just better organized. High uses MOE lora, Low uses existing models",
      "from": "FL13"
    },
    {
      "update": "FlashVSR developers working on consumer GPU version",
      "details": "Working on version without Block-Sparse Attention that maintains quality but will run slower on consumer GPUs",
      "from": "JohnDopamine"
    },
    {
      "update": "New UniMMVSR video upscaler from Kling announced",
      "details": "Video upscaler that can use reference image for guidance, similar to what WAN 2.2 I2V does. Code coming soon",
      "from": "Shubhooooo"
    },
    {
      "update": "rCM LoRAs released for Wan 2.1",
      "details": "720p and 480p rCM LoRAs extracted and available, 4-step distillation models",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.5 T2I available at FAL",
      "details": "Text-to-image model with 1500 token limit vs 512 for previous versions",
      "from": "Drommer-Kille"
    },
    {
      "update": "ComfyUI memory leak patches released",
      "details": "Recent ComfyUI updates patched memory leaks in past week or two",
      "from": "JohnDopamine"
    },
    {
      "update": "rCM scheduler added to WanVideoWrapper",
      "details": "New scheduler option available, though not necessarily better than existing options",
      "from": "Kijai"
    },
    {
      "update": "New VACE model Ditto released with 3 specialized modules",
      "details": "Ditto uses WanVideoWrapper as ComfyUI example, has global style, global, and sim2real variants",
      "from": "Kijai"
    },
    {
      "update": "Ditto added CC 4.0 non-commercial license to their code",
      "details": "License applied retroactively as there was no license before",
      "from": "Kijai"
    },
    {
      "update": "Ditto dataset released",
      "details": "1TB+ dataset still uploading to HuggingFace, contains synthetic-to-real training data",
      "from": "Dever"
    },
    {
      "update": "Krea realtime video model released",
      "details": "Based on Wan 2.1, available on HuggingFace",
      "from": "Desto Geima"
    },
    {
      "update": "Context window bug with uni3c fixed",
      "details": "Bug that prevented uni3c camera control from working with context windows has been resolved",
      "from": "Kijai"
    },
    {
      "update": "Krea realtime went open source",
      "details": "Didn't sell well according to speculation",
      "from": "hicho"
    },
    {
      "update": "Wavespeed got new Wan 2.5 modality (video-extend)",
      "details": "New extension capability",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "update": "MUG-V 10B video model released",
      "details": "Paper available, no samples yet",
      "from": "yi"
    },
    {
      "update": "MoCha merged into main branch of WanVideoWrapper",
      "details": "No longer in PR, available in main with new workflow including block swap",
      "from": "Kijai"
    },
    {
      "update": "MoCha model released by Orange-3DV-Team",
      "details": "Character replacement model working in ComfyUI via Kijai's implementation",
      "from": "Dever"
    },
    {
      "update": "Krea realtime video model and LoRA released",
      "details": "HuggingFace release with distill LoRA for faster generation",
      "from": "s2k"
    },
    {
      "update": "Rolling Forcing model initial commit on HuggingFace",
      "details": "Reference-to-video model using WAN 2.1-14B",
      "from": "JohnDopamine"
    },
    {
      "update": "Updated Wan 2.2 distill LoRAs from lightx2v",
      "details": "New version marked as 1022, replaces previous versions",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "update": "Mixture of Groups Attention paper released",
      "details": "Technique for long video generation, could improve temporal consistency",
      "from": "DawnII"
    },
    {
      "update": "New LightX2V distill LoRAs released",
      "details": "Dated 1022, appear to be improved versions with better prompt following",
      "from": "Ada"
    },
    {
      "update": "QwenImage VAE converted for ComfyUI",
      "details": "Available at Kijai's HuggingFace repo in fp32 format",
      "from": "Kijai"
    },
    {
      "update": "Rolling Force model released",
      "details": "1.3B T2V causal model from TencentARC, 17GB due to multiple weight copies",
      "from": "yi"
    },
    {
      "update": "Stable Video Infinity released",
      "details": "Claims infinite length video generation with high temporal consistency",
      "from": "Kijai"
    },
    {
      "update": "SVI (Stable Video Infinity) model released",
      "details": "Claims unlimited context length, supports up to 4 minute videos with no quality decrease, includes shot and film LoRAs",
      "from": "Ada"
    },
    {
      "update": "New lightx2v distill LoRA available",
      "details": "Version 1022 released with improved performance over previous MoE version",
      "from": "Kijai"
    },
    {
      "update": "ByteDance open sourced Video-As-Prompt (VAP)",
      "details": "49-frame video prompting system for Wan 2.1 14B with in-context learning capabilities",
      "from": "JohnDopamine"
    },
    {
      "update": "LTX 2 announced by Lightricks",
      "details": "Audio+video generation, 10s+ videos, open source weights coming later this year",
      "from": "yi"
    },
    {
      "update": "SVI team planning to train for Wan 2.2",
      "details": "Mentioned in FAQ, will support the 5B model",
      "from": "DawnII"
    },
    {
      "update": "LTX 2 coming late November",
      "details": "LTX 2 announced for late November release, weights not available yet",
      "from": "Lodis"
    },
    {
      "update": "SVI LoRAs converted to native ComfyUI format",
      "details": "All SVI LoRAs now available in fp16 format compatible with native ComfyUI nodes",
      "from": "Kijai"
    },
    {
      "update": "Wan 2.5 expected to respond to LTX 2",
      "details": "Speculation that Wan team will release 2.5 open source version around LTX 2 launch",
      "from": "hicho"
    },
    {
      "update": "HoloCine released by Alibaba - T2V model with minute-level generation capability",
      "details": "14B and 5B variants, supports structured prompting with character consistency, trained on 400k multi-shot samples",
      "from": "42hub"
    },
    {
      "update": "New LightX2V variant called '1022' released",
      "details": "Another speed optimization LoRA variant",
      "from": "42hub"
    },
    {
      "update": "Multiple HoloCine model variants planned",
      "details": "HoloCine-14B-full, HoloCine-14B-sparse, versions for >1 minute, 5B variants for limited memory, HoloCine-audio in planning",
      "from": "NebSH"
    },
    {
      "update": "SVI developers request removal of fp8 versions from Kijai's repo",
      "details": "Claim fp8 versions have issues, request use of their codebase instead",
      "from": "DawnII"
    },
    {
      "update": "Suplex LoRA uploaded to Civitai",
      "details": "Wrestling move LoRA now available",
      "from": "Lodis"
    },
    {
      "update": "HoloCine license changed from Apache 2.0 to CC 4.0 share alike non-commercial",
      "details": "Changed 4 hours after initial commit, but once Apache always Apache - can't change retroactively",
      "from": "Kijai"
    },
    {
      "update": "HoloCine released but no ComfyUI implementation yet",
      "details": "Model released 7 hours ago, weights can be tried but multiple shots for long video not implemented yet",
      "from": "Kijai"
    },
    {
      "update": "HoloCine sparse model uploaded",
      "details": "Sparse version now available on HuggingFace",
      "from": "NebSH"
    },
    {
      "update": "New UniLumos model coming",
      "details": "Another lumos model coming from Alibaba-DAMO-Academy",
      "from": "DawnII"
    },
    {
      "update": "Reference padding bug fixed",
      "details": "Kijai fixed bug where reference frames were being zeroed out when using temporal masks",
      "from": "Kijai"
    },
    {
      "update": "HoloCine fp8 versions available",
      "details": "Kijai released fp8 quantized versions of HoloCine models",
      "from": "Lodis"
    },
    {
      "update": "HoloCine sparse models uploaded",
      "details": "Sparse model versions now available, enabling longer generation with less VRAM",
      "from": "Lodis"
    },
    {
      "update": "SVI for I2V confirmed not working with HoloCine setup",
      "details": "SVI won't work in the current HoloCine implementation",
      "from": "VK (5080 128gb)"
    },
    {
      "update": "ComfyUI Cloud beta doesn't support custom nodes or models",
      "details": "Only pre-made templates and API nodes available",
      "from": "Govind Singh"
    },
    {
      "update": "LongCat-Video released by Meituan (Chinese food delivery platform)",
      "details": "14B open video model, MIT license, single model for T2V and I2V",
      "from": "yi"
    },
    {
      "update": "Holocine models available as FP8 versions",
      "details": "Available in Kijai's HuggingFace folder",
      "from": "avataraim"
    },
    {
      "update": "HoloCine one minute version already trained",
      "details": "Will be released after further dataset scaling",
      "from": "NebSH"
    },
    {
      "update": "SVI Film 2.2 (both 5B and 14B) on Wan team's todo list",
      "details": "Readme updated a few days ago with this information",
      "from": "DawnII"
    },
    {
      "update": "UltraGen released supporting 4K resolution",
      "details": "Uses Wan 1.3B model",
      "from": "Shubhooooo"
    },
    {
      "update": "LongCat ComfyUI implementation available",
      "details": "Testing branch available at ComfyUI-WanVideoWrapper/tree/longcat with models on HuggingFace",
      "from": "Kijai"
    },
    {
      "update": "Holocine nodes added to ComfyUI",
      "details": "WanVideoHolocineShotBuilder and related nodes now available",
      "from": "shaggss"
    },
    {
      "update": "SVI repo updated with seed guidance",
      "details": "SVI devs updated readme to emphasize using different seed per clip extension",
      "from": "JohnDopamine"
    },
    {
      "update": "HoloCine support added to ComfyUI via pull request",
      "details": "Working implementation available though described as messy, allows 15 second generation",
      "from": "Kijai"
    },
    {
      "update": "LongCat fp8 version uploaded",
      "details": "Kijai provided fp8 quantized version of LongCat model",
      "from": "Lodis"
    },
    {
      "update": "SageAttention 2.2.0 available with better torch compile support",
      "details": "Gets rid of graph breaks so torch compile works better, fixes first run VRAM issues",
      "from": "Kijai"
    },
    {
      "update": "LongCat implementation in WanVideoWrapper available",
      "details": "Kijai implementing LongCat in separate branch, workflow available in LongCat folder",
      "from": "Kijai"
    },
    {
      "update": "Holocine support being added via PR to wrapper",
      "details": "Much work in progress, PR up for wrapper adding HoloCine support",
      "from": "Kijai"
    },
    {
      "update": "Latest SageAttention 2.2.0 supports torch compile",
      "details": "Can run Wan without any graph breaks, inductor full graph works, avoids first run VRAM issue",
      "from": "Kijai"
    },
    {
      "update": "Triton wheel now supports e4m3fn compilation on 3090",
      "details": "With latest triton wheel compiling e4m3fn works on 3090, finally solved",
      "from": "Kijai"
    },
    {
      "update": "Kaleido 14B-S2V model released",
      "details": "Reference to video model based on Wan 2.1 T2V, uses R-RoPE mechanism",
      "from": "yi"
    },
    {
      "update": "Ditto developers added Wan denoise/enhance code",
      "details": "Open sourced style/edit tool with Wan enhancement capabilities",
      "from": "JohnDopamine"
    },
    {
      "update": "WAN 2.5 is currently commercialized for live streaming and may not be open source",
      "details": "Chinese community feedback suggests performance not ideal yet, prefer refinement before release. Sora 2 and Veo 3.1 launches gave momentum",
      "from": "\u9752\u9f8d\u8056\u8005@bdsqlsz"
    },
    {
      "update": "Holocine proper implementation available with attention code",
      "details": "Repository moved to https://github.com/Dango233/ComfyUI-WanVideoWrapper-Multishot",
      "from": "NebSH"
    },
    {
      "update": "New helper node being developed for video extension workflows",
      "details": "Kijai building it for LongCat example, will include in example workflow",
      "from": "Kijai"
    },
    {
      "update": "LTX-2 expected open source release",
      "details": "Apparently LTX-2 will be open source end of November",
      "from": "Kiwv"
    },
    {
      "update": "WAN 2.5 open source status unclear",
      "details": "WAN team said today's live stream only discusses commercial applications, no notification yet about open source release",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "update": "DITTO released this week with Denoise Enhancing",
      "details": "New release includes code for Denoise Enhancing related to their work",
      "from": "JohnDopamine"
    },
    {
      "update": "Sage 2.2.0 available with pip",
      "details": "Finally available with pip but still seems to compile",
      "from": "Kijai"
    },
    {
      "update": "LongCat model released - Wan architecture trained from scratch",
      "details": "Features native video continuation, refinement LoRA for 720p/30fps output",
      "from": "JohnDopamine"
    },
    {
      "update": "New ComfyUI UI changes with floating menus and subgraph support",
      "details": "UI updates causing navigation issues for some users",
      "from": "JohnDopamine"
    },
    {
      "update": "Morph LoRA released for multi-frame video generation",
      "details": "Claims to support up to 5 frame inputs for better interpolation",
      "from": "VK (5080 128gb)"
    },
    {
      "update": "ChronoEdit released by NVIDIA",
      "details": "Wan-based image editing model with code and weights available",
      "from": "JohnDopamine"
    },
    {
      "update": "New ComfyUI offloading flag for faster generation when offloading",
      "details": "Performance improvement for memory-constrained setups",
      "from": "Lumifel"
    },
    {
      "update": "Qwen team has open source music model coming soon",
      "details": "Announced by Qwen CEO, both Qwen and Wan are developed by different Alibaba teams",
      "from": "JohnDopamine"
    },
    {
      "update": "UMG and Udio reached licensing agreement",
      "details": "New Licensed AI Music Creation Platform - indicates copyright holders are accepting AI licensing rather than fighting it",
      "from": "Draken"
    },
    {
      "update": "Major ComfyUI WanVideo update released",
      "details": "Includes LongCat support, norm layers in fp32, improved LoRA storage method, and various optimizations",
      "from": "Kijai"
    },
    {
      "update": "Qwen and Wan confirmed as both Alibaba projects",
      "details": "Qwen from Alibaba Cloud team (LLMs), Wan from Ali-Vilab Team (video generation)",
      "from": "JohnDopamine"
    },
    {
      "update": "ChronoEdit models released by Kijai",
      "details": "Converted NVIDIA ChronoEdit models available - full model and distill LoRA rank32, defaults: 8 steps, cfg 1.0, shift 2.0",
      "from": "Kijai"
    },
    {
      "update": "New LightX2V 1030 LoRA released",
      "details": "Third iteration of 2.2 high noise LoRA with improvements in prompt following and camera control",
      "from": "Kijai"
    },
    {
      "update": "LongCat merged into main branch",
      "details": "LongCat architecture now available in main ComfyUI implementation",
      "from": "Lodis"
    },
    {
      "update": "FlashVSR v1.1 released",
      "details": "Available on HuggingFace with improvements",
      "from": "yi"
    },
    {
      "update": "New wan2.2_lightx2v_1030 LoRA available",
      "details": "High noise LoRA for 4 steps, rank 64, bf16",
      "from": "avataraim"
    },
    {
      "update": "ChronoEdit GGUF version released",
      "details": "QuantStack has released ChronoEdit-14B-GGUF",
      "from": "YarvixPA"
    },
    {
      "update": "Alibaba officially announced partnership with Higgsfield",
      "details": "Posted on Alibaba Cloud X account",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "update": "Flash VSR v1.1 has been updated",
      "details": "New version available",
      "from": "Lodis"
    },
    {
      "update": "LTX 2 weights expected end of November according to devs",
      "details": "Unless they change their minds, confirmed by LTX team member",
      "from": "ZeusZeus"
    },
    {
      "update": "Wan 2.5 may not be open sourced",
      "details": "Someone with insider info recently said they will not release the weights",
      "from": "seitanism"
    },
    {
      "update": "LongCat video extension will be possible on fal later today",
      "details": "Ability to upload your own video to be extended",
      "from": "Benjaminimal"
    },
    {
      "update": "LTX2 will release end of November",
      "details": "Expected to be competitive with Wan, with realtime 4k video capabilities mentioned",
      "from": "Stad"
    },
    {
      "update": "Sora Pro plan limits severely reduced",
      "details": "Went from 99 gens a day to 30 down to 5 for the pro plan",
      "from": "Ruairi Robinson"
    },
    {
      "update": "Control flow and loops coming to ComfyUI",
      "details": "Already possible but no purely native core node to do so right now",
      "from": "Kijai"
    }
  ],
  "workflows": [
    {
      "workflow": "Using Wan Alpha with transparent backgrounds for compositing",
      "use_case": "Creating B-movie horror effects by combining transparent video with backgrounds in GIMP and Shotcut",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "workflow": "VACE dual model workflow for multiple inpainting passes",
      "use_case": "Running workflow multiple times with no loss on unmasked areas",
      "from": "mdkb"
    },
    {
      "workflow": "Batch processing with wildcards for character and style variation",
      "use_case": "Loop 300 generations at 720x1280 with random characters and styles",
      "from": "Drommer-Kille"
    },
    {
      "workflow": "Using QWen VL for face bbox detection",
      "use_case": "Creating face masks for VACE workflows, can describe any part via natural language",
      "from": "piscesbody"
    },
    {
      "workflow": "InfiniteTalk for I2V with FantasyPortrait for V2V",
      "use_case": "Audio-driven video generation, stronger with FantasyPortrait, supersedes MultiTalk",
      "from": "mdkb"
    },
    {
      "workflow": "Dual model workflow with InfiniteTalk",
      "use_case": "Memory-efficient alternative to Magref fp8 setup, though InfiniteTalk doesn't do much on high noise model",
      "from": "mdkb"
    },
    {
      "workflow": "WanAnimate with built-in embed windows",
      "use_case": "Achieving 25-second consistent video generation without traditional context windows",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Using context windows to avoid degradation",
      "use_case": "Disable other windowing by setting frame_window_size equal to total frames, then use context as normal",
      "from": "Kijai"
    },
    {
      "workflow": "HuMO + InfiniteTalk combination",
      "use_case": "Enhanced lipsync with more expression, using 6 second audio blocks at 65 frames",
      "from": "mdkb"
    },
    {
      "workflow": "3-sampler Lightning LoRA setup",
      "use_case": "Best motion preservation with high/low noise split",
      "from": "pom"
    },
    {
      "workflow": "ClownShark tiling for upscaling",
      "use_case": "Video upscaling with preview of whole video instead of individual tiles",
      "from": "FL13"
    },
    {
      "workflow": "ByteDance VAE + VibeVoice integration",
      "use_case": "Audio reconstruction without finetuning, though adding extra VAE refiner stage takes more time",
      "from": "MysteryShack"
    },
    {
      "workflow": "Wan 2.2 MOE step splitting",
      "use_case": "Use High Noise for steps 0-3, Low Noise for steps 3-8 in 8-step generation, or 0-10 and 10-20 for 20 steps",
      "from": "Kytra"
    },
    {
      "workflow": "OVI audio-video generation",
      "use_case": "Speech wrapped in <S>...<E> tags, optional audio description in <AUDCAP>...<ENDAUDCAP> tags",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "workflow": "VACE Start End Frame with Replace Image in Batch",
      "use_case": "Insert middle keyframes for first-middle-last frame morphing",
      "from": "HeadOfOliver"
    },
    {
      "workflow": "Music video generator using batch latents",
      "use_case": "Automated music video creation with frame drift compensation",
      "from": "Fill"
    },
    {
      "workflow": "Multiple renders with different frame lengths in sequence",
      "use_case": "Running list of latents into single ksampler for varied content",
      "from": "Fill"
    },
    {
      "workflow": "FFLF VACE driven wrapper with dual model setup",
      "use_case": "Creating controlnet driving videos for higher quality clips later",
      "from": "mdkb"
    },
    {
      "workflow": "Face/head swap workflow posted on Civitai",
      "use_case": "Character replacement in videos",
      "from": "Alisson Pereira"
    },
    {
      "workflow": "Hybrid model setup: 2.1 + Low 2.2 H + 2.1 distill",
      "use_case": "Achieving amazing and different quality results",
      "from": "hicho"
    },
    {
      "workflow": "5B + 2.2 low model combination",
      "use_case": "Getting 5B content quality with Wan 2.2 graphics by bypassing VAE",
      "from": "hicho"
    },
    {
      "workflow": "Character animation setup",
      "use_case": "Using reference image for background and character with video for motion by disconnecting bg_images and mask",
      "from": "Kijai"
    },
    {
      "workflow": "Speed LoRA configuration",
      "use_case": "Using LightX2V with manual sigma lists - speed LoRA on one model with low steps, other model with high steps",
      "from": "42hub"
    },
    {
      "workflow": "USDU upscale with VACE dual model for fixing extended videos",
      "use_case": "Combat burned-in look from multiple I2V continuations, fixing seams in extended videos",
      "from": "mdkb"
    },
    {
      "workflow": "Lightning LoRA with low-noise phase processing",
      "use_case": "High quality fast T2V generation - 720p in 160 seconds",
      "from": "Lan8mark"
    },
    {
      "workflow": "High-noise resolution reduction then upscale",
      "use_case": "Speed optimization for video generation - use half resolution for high-noise steps, then upscale for low-noise phase",
      "from": "Lan8mark"
    },
    {
      "workflow": "Triple chained samplers with different sigma handoff percentages",
      "use_case": "I2V workflows for WAN 2.2, though user seeking definitive best practices",
      "from": "32Bit_Badman"
    },
    {
      "workflow": "Latent upscaling method for 3-4x speed boost",
      "use_case": "5 steps run at 384p, and just 3 at full 720p. From 1100 seconds down to 160 with no visible quality loss",
      "from": "Lan8mark"
    },
    {
      "workflow": "VACE for keyframing",
      "use_case": "You can use vace as inpaint, as in you can use it to any generate/fill any frames in a sequence, start, middle, end, anywhere. Can even be used as interpolation",
      "from": "lemuet"
    },
    {
      "workflow": "Hybrid setup: WAN Fun Control High Noise + WAN I2V Low Noise",
      "use_case": "The details carried over perfectly for preserving image details",
      "from": "Lan8mark"
    },
    {
      "workflow": "WAN 2.2-based 2K upscaler",
      "use_case": "High-quality video upscaling, claimed as fastest and highest-quality available",
      "from": "Lan8mark"
    },
    {
      "workflow": "Dual model approach with shift control",
      "use_case": "Using shift parameter to control step point and work on structure",
      "from": "mdkb"
    },
    {
      "workflow": "WAN animate with temporal masking",
      "use_case": "Scene replacement and character replacement with masking support",
      "from": "Draken"
    },
    {
      "workflow": "OVI full scene change",
      "use_case": "Complete scene transformation rather than just character replacement",
      "from": "Juampab12"
    },
    {
      "workflow": "MAGREF + WAN 2.2 character consistency",
      "use_case": "Multi-character consistency in I2V generation",
      "from": "Elvaxorn"
    },
    {
      "workflow": "Upscaler workflow for WAN videos",
      "use_case": "720p to 2K upscaling in ~150s, 480p to 2K in ~350s on 5090",
      "from": "Lan8mark"
    },
    {
      "workflow": "VACE 2.2 character replacement",
      "use_case": "Replace characters in video while maintaining consistency using MagRef and RCM LoRA",
      "from": "Elvaxorn"
    },
    {
      "workflow": "Dual model Wan 2.2 with VACE",
      "use_case": "High and low noise model combination with VACE for character control",
      "from": "mdkb"
    },
    {
      "workflow": "Video to video upscaling",
      "use_case": "Use Wan 2.2 low noise model for detail enhancement and upscaling",
      "from": "ingi // SYSTMS"
    },
    {
      "workflow": "3 KSampler method with new Lightx2v",
      "use_case": "Using 2-2-2 step split works fine with new LoRA",
      "from": "asd"
    },
    {
      "workflow": "Using skimmed CFG with MoE sampler for better prompt adherence",
      "use_case": "Higher CFG on first steps, dropping to 1.0 to prevent burn",
      "from": "GalaxyTimeMachine"
    },
    {
      "workflow": "3-sampler Lightning setup",
      "use_case": "Better results with new LoRA: 1 step HN no lora cfg 3.5 + 2 step HN with new lightx2v + 3 step LN with old 2.1 lightx2v",
      "from": "FL13"
    },
    {
      "workflow": "2-part OVI + InfiniteTalk",
      "use_case": "Combining OVI video generation with InfiniteTalk for talking videos",
      "from": "Kijai"
    },
    {
      "workflow": "VACE for AI interpolation",
      "use_case": "Using Wan VACE for true AI interpolation between frames",
      "from": "FL13"
    },
    {
      "workflow": "Using TaylorSeer with Wan models for faster inference",
      "use_case": "Speeding up generation by skipping steps with minimal quality loss",
      "from": "yi"
    },
    {
      "workflow": "FlashVSR upscaling integrated with Wan video generation",
      "use_case": "Fast video upscaling from 384 to 1024 resolution",
      "from": "Kijai"
    },
    {
      "workflow": "Multi-sampler setup with different CFG values per step",
      "use_case": "Fine-tuning generation quality and speed",
      "from": "Mattis"
    },
    {
      "workflow": "3-sampler system for Wan 2.2 without speed LoRAs on high model",
      "use_case": "8 steps total for high model, 4 steps for low with speed LoRA - achieves better motion and quality",
      "from": "Zabo"
    },
    {
      "workflow": "FlashVSR with Cinescale LoRA and context windows",
      "use_case": "Video upscaling with 0.75 denoise, processed in chunks like 81 frames at a time",
      "from": "Elvaxorn"
    },
    {
      "workflow": "VACE video stitching using frame inpainting",
      "use_case": "Creating long videos by connecting shorter clips with 10 frame overlap",
      "from": "Koba"
    },
    {
      "workflow": "CineScale two-sampler setup",
      "use_case": "Generate 1080p then latent upscale to second sampler with RoPE scaling",
      "from": "DawnII"
    },
    {
      "workflow": "Video chunking for long video upscaling",
      "use_case": "Use load video node and meta batch manager for processing long videos",
      "from": "happy.j"
    },
    {
      "workflow": "Two-stage pose concatenation system",
      "use_case": "Takes two videos and concatenates 3D pose estimations projected into 2D space, with controllable strength mixing",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "workflow": "rcm + lightning combination for speed",
      "use_case": "Using rank148 rcm at high 4.0 + lightning at 1.0 on low, 4 steps dpm++sde, generates 1024x576 upscaled to FHD in under 3 minutes",
      "from": "aipmaster"
    },
    {
      "workflow": "Sectioned video upscaling for long clips",
      "use_case": "Split long videos to latent files, upscale in 120 frame sections with 100 frame overlap, combine in DaVinci",
      "from": "mdkb"
    },
    {
      "workflow": "OVI + InfiniteTalk pipeline",
      "use_case": "Generate video with OVI then add lipsync with InfiniteTalk at low denoise",
      "from": "Charlie"
    },
    {
      "workflow": "FlashVSR for render cleanup",
      "use_case": "Fix bad looking renders at 0.8 strength in single step",
      "from": "mdkb"
    },
    {
      "workflow": "InfiniteTalk vid2vid for lipsync",
      "use_case": "Add lipsync to existing video, requires masking setup",
      "from": "Kijai"
    },
    {
      "workflow": "Clownshark sampler with temporal masks",
      "use_case": "Advanced sampling with regional conditioning over time",
      "from": "TK_999"
    },
    {
      "workflow": "3 sampler I2V setup",
      "use_case": "1st step no LoRA 3.5 CFG, 2nd step 2-step lightning on high, 3rd step 3-step with 2.1 lightx2v on low",
      "from": "FL13"
    },
    {
      "workflow": "Style transfer with Ditto LoRAs",
      "use_case": "Apply style transfer to videos using text prompts like 'Make it Pixel Art'",
      "from": "Kijai"
    },
    {
      "workflow": "RGB control for style transfer",
      "use_case": "Easier than pose estimation for basic video stylization",
      "from": "Draken"
    },
    {
      "workflow": "VACE extension for longer videos",
      "use_case": "Use VACE extend with last few frames for longer Ditto generations",
      "from": "DawnII"
    },
    {
      "workflow": "Multi-stage I2V looping",
      "use_case": "Taking last frame of generated video as starting frame for next generation in a loop",
      "from": "Wembleycandy"
    },
    {
      "workflow": "Krea LoRA with Wan 2.2",
      "use_case": "Using extracted Krea realtime LoRA on Wan 2.2 with specific strength settings",
      "from": "aipmaster"
    },
    {
      "workflow": "Colab tunneling for ComfyUI",
      "use_case": "Using built-in colab tunneling instead of external services",
      "from": "Draken"
    },
    {
      "workflow": "Sora watermark removal with FlashVSR upscaling",
      "use_case": "Combined workflow for watermark removal and upscaling",
      "from": "Govind Singh"
    },
    {
      "workflow": "MoCha character replacement with masking",
      "use_case": "Replace characters in videos using reference images and masks",
      "from": "Kijai"
    },
    {
      "workflow": "Krea LoRA with high/low dual sampler",
      "use_case": "Fast realistic video generation using Wan 2.2 setup",
      "from": "aipmaster"
    },
    {
      "workflow": "Wan 2.2 with Uni3C camera control",
      "use_case": "Camera movement control in video generation",
      "from": "DawnII"
    },
    {
      "workflow": "VACE inpainting with NAG for object removal",
      "use_case": "Remove objects/people from videos using inpainting",
      "from": "art13.beck"
    },
    {
      "workflow": "Chaining multiple I2V generations with 5 start frames",
      "use_case": "Creating longer videos using SVI technique",
      "from": "Kijai"
    },
    {
      "workflow": "Using multiple start frames for I2V",
      "use_case": "Improving temporal coherency and motion continuation",
      "from": "Kijai"
    },
    {
      "workflow": "I2V continuation with multiple frames",
      "use_case": "Smooth video continuation by feeding last 5 frames from previous generation into I2V node",
      "from": "Kijai"
    },
    {
      "workflow": "SVI shot workflow with padding",
      "use_case": "Uses single start image with reference image padding for extended video generation",
      "from": "Kijai"
    },
    {
      "workflow": "SVI film workflow",
      "use_case": "Uses 5 start images with zero padding for scene transitions and different prompts per segment",
      "from": "Kijai"
    },
    {
      "workflow": "SVI film LoRA for video extension",
      "use_case": "Extending 5-second videos to 10+ seconds with motion continuity",
      "from": "Tony(5090)"
    },
    {
      "workflow": "Ditto for style transfer and character replacement",
      "use_case": "Converting video styles (Ghibli, realistic, psychedelic) and replacing characters",
      "from": "Drommer-Kille"
    },
    {
      "workflow": "2-sampler batch setup for longer Ditto videos",
      "use_case": "72+72 frame batches merged together for extended video length",
      "from": "hicho"
    },
    {
      "workflow": "Video extension using SVI LoRAs",
      "use_case": "Extending videos by taking last 5 frames and generating continuation with film LoRA",
      "from": "Ablejones"
    },
    {
      "workflow": "Scene continuity with SVI-shot",
      "use_case": "Maintaining same scene for longer videos by repeating reference frame through entire sequence",
      "from": "Ablejones"
    },
    {
      "workflow": "Deforum output conversion through Wan",
      "use_case": "Processing Deforum outputs at 0.7 denoise to maintain motion while reducing flickering",
      "from": "VK (5080 128gb)"
    },
    {
      "workflow": "VK's improved SVI extension method",
      "use_case": "Better character consistency in video extension by including reference image as first frame in each segment",
      "from": "VK (5080 128gb)"
    },
    {
      "workflow": "InfiniteTalk audio-driven video generation",
      "use_case": "Creating long lip-sync videos with minimal degradation using audio file to drive length",
      "from": "seitanism"
    },
    {
      "workflow": "HoloCine structured prompting for multi-shot scenes",
      "use_case": "Creating cinematic sequences with character consistency using [character1] tags and [shot cut] markers",
      "from": "Tachyon"
    },
    {
      "workflow": "SVI Film method for motion continuation",
      "use_case": "Continue last generation with last 5 frames of previous generation, padded with black",
      "from": "Kijai"
    },
    {
      "workflow": "SVI Shot method for endless I2V",
      "use_case": "Last frame as input with original image as reference in all other frames, only first frame masked",
      "from": "Kijai"
    },
    {
      "workflow": "Using API calls with disk caching",
      "use_case": "More practical for iterative work than ComfyUI loops, allows trying each segment multiple times",
      "from": "Ablejones"
    },
    {
      "workflow": "SVI film with 5-frame input",
      "use_case": "Better consistency for video extension with multiple frame context",
      "from": "Kijai"
    },
    {
      "workflow": "infinitetalk + unianimate combination",
      "use_case": "Long portrait animation with pose control and lip sync",
      "from": "DawnII"
    },
    {
      "workflow": "Film SVI with ref image insertion",
      "use_case": "Insert ref image into 1 frame of five to help with consistency",
      "from": "VK (5080 128gb)"
    },
    {
      "workflow": "ViT pose retargeting for miniatures",
      "use_case": "Animating miniatures and toys with proportion retargeting",
      "from": "Neex"
    },
    {
      "workflow": "HoloCine T2V with multi-shot prompts",
      "use_case": "Generate 15+ second videos with scene transitions using | separated prompts",
      "from": "seitanism"
    },
    {
      "workflow": "Seamless video generation with automatic stray frame removal",
      "use_case": "Removes stray frames automatically for seamless output without post-editing",
      "from": "VK (5080 128gb)"
    },
    {
      "workflow": "2.2 + SVI film + infinitetalk combination",
      "use_case": "Using film LoRA with infinitetalk for enhanced continuity",
      "from": "DawnII"
    },
    {
      "workflow": "Holocine experimental workflow using models and video attention split",
      "use_case": "Long video generation with shot transitions",
      "from": "avataraim"
    },
    {
      "workflow": "SVI with WAN 2.2 using Fun InP model and waninpainttovideo node",
      "use_case": "Better SVI results",
      "from": "V\u00e9role"
    },
    {
      "workflow": "Context overlap method for long generation",
      "use_case": "350+ frame generation",
      "from": "VK"
    },
    {
      "workflow": "HoloCine structured prompting",
      "use_case": "Generate 15-second videos with scene cuts and character consistency using global caption, character descriptions, and shot-specific prompts with cut frames",
      "from": "BNP4535353"
    },
    {
      "workflow": "5-frame SVI method with Wan 2.2",
      "use_case": "Video extension while maintaining character appearance consistency",
      "from": "VK (5080 128gb)"
    },
    {
      "workflow": "LongCat I2V extension",
      "use_case": "Video extension with 13 start frames, requires character looking at camera at segment end",
      "from": "aikitoria"
    },
    {
      "workflow": "Holocine structured workflow",
      "use_case": "Chain WanVideoHolocineShotBuilder for each shot, feed into WanVideoHolocinePromptEncode",
      "from": "NebSH"
    },
    {
      "workflow": "Holocine manual prompt",
      "use_case": "Write prompt with [global caption] and [shot cut] markers, use with WanVideoTextEncode",
      "from": "NebSH"
    },
    {
      "workflow": "LongCat basic workflow",
      "use_case": "TI2V generation with example workflow in longcat folder",
      "from": "Kijai"
    },
    {
      "workflow": "LongCat video extension using last frame continuation",
      "use_case": "Creating longer videos by taking last frames and continuing generation",
      "from": "avataraim"
    },
    {
      "workflow": "HoloCine with shot cuts for 15 second structured video",
      "use_case": "Generating cinematic videos with multiple shots and cuts",
      "from": "42hub"
    },
    {
      "workflow": "LongCat video extension using last frame",
      "use_case": "Creating longer videos by continuing from previous last frame with different seeds",
      "from": "avataraim"
    },
    {
      "workflow": "LongCat refinement pipeline",
      "use_case": "Two-pass generation: low res first, then refiner lora for 720p/30fps",
      "from": "Draken"
    },
    {
      "workflow": "SVI with 5 frame batches",
      "use_case": "Using SVI film loras with 5 start frames instead of 1 for better continuity",
      "from": "Kijai"
    },
    {
      "workflow": "SVI-film + HuMo for extended generations",
      "use_case": "Long video generation with character consistency",
      "from": "Ablejones"
    },
    {
      "workflow": "Wan VAE 2x upscaler as hires fix",
      "use_case": "Image upscaling without separate upscale model, works with qwen",
      "from": "spacepxl"
    },
    {
      "workflow": "VACE for first/middle/last frame conditioning",
      "use_case": "Multi-keyframe video generation with frame conditions",
      "from": "spacepxl"
    },
    {
      "workflow": "OVI inpainting for lip sync and audio generation",
      "use_case": "Generate base videos in WAN 2.2 then v2v inpaint with OVI to get lipsync/audio",
      "from": "Hashu"
    },
    {
      "workflow": "Video extension using LongCat LoRA",
      "use_case": "Refining video generation with specific sampling parameters",
      "from": "Kijai"
    },
    {
      "workflow": "Multi-character LoRA per shot in Holocine",
      "use_case": "Using different character LoRAs for each shot in multi-shot generation",
      "from": "NebSH"
    },
    {
      "workflow": "Latent space upscaling with WAN 2.2",
      "use_case": "Upscaling between HN and LN processing, can run VACE WAN 2.2 to 720p on 3060 in 16 minutes",
      "from": "mdkb"
    },
    {
      "workflow": "Using LongCat refinement LoRA in V2V workflow",
      "use_case": "Not using as intended but still does decent job for sharpening",
      "from": "JohnDopamine"
    },
    {
      "workflow": "LongCat extended video generation with segment prompting",
      "use_case": "Creating long videos with different prompts for each segment",
      "from": "JohnDopamine"
    },
    {
      "workflow": "LLM-assisted prompting for video continuation",
      "use_case": "Automatically generating prompts for next segments using last frame",
      "from": "JohnDopamine"
    },
    {
      "workflow": "T2V using I2V workflow without image inputs",
      "use_case": "Text-to-video generation by removing image nodes from I2V workflow",
      "from": "Pandaabear"
    },
    {
      "workflow": "Sequential model usage for comprehensive video generation",
      "use_case": "Using multiple models in sequence/simultaneously to have each fill gaps of the previous",
      "from": "Ablejones"
    },
    {
      "workflow": "ChronoEdit image editing using Wan temporal understanding",
      "use_case": "Using Wan's video generation capabilities for sophisticated image editing tasks",
      "from": "slmonker"
    },
    {
      "workflow": "Using 2 or 3 KSamplers with Holocine + VACE 2.2 + 1030 lightx high + 1020 low",
      "use_case": "Getting results on par with Wan 2.5",
      "from": "Elvaxorn"
    },
    {
      "workflow": "First frame + last frame with base Wan 2.2 for 201 frames",
      "use_case": "Long video generation works surprisingly well",
      "from": "mamad8"
    },
    {
      "workflow": "ChronoEdit as I2V model at 5 or 29 frames, take last frame",
      "use_case": "Image editing using video model as diffusion process",
      "from": "Kiwv"
    },
    {
      "workflow": "ChronoEdit image editing",
      "use_case": "Use Wan I2V workflow with ChronoEdit model, use 5 or 29 frames",
      "from": "Kiwv"
    },
    {
      "workflow": "LongCat face consistency fix",
      "use_case": "Use face inpaint pass with Phantom to maintain character identity in extensions",
      "from": "Ablejones"
    },
    {
      "workflow": "Context windows for longer generations",
      "use_case": "Multiple chunks of 81 frames sampled at once, good for I2V identity preservation",
      "from": "blake37"
    },
    {
      "workflow": "LongCat generates long videos in chunks as video extensions",
      "use_case": "All long videos are by their nature video extensions",
      "from": "Benjaminimal"
    },
    {
      "workflow": "VACE autoregressive video extension",
      "use_case": "Filling left/right sides of LTX videos, but suffers from color drift and coherence issues",
      "from": "Benjaminimal"
    },
    {
      "workflow": "Long video generation using LightX LoRA with color matching",
      "use_case": "Generating 1-minute videos by chaining 5-second segments with color match nodes between each segment",
      "from": "aikitoria"
    }
  ],
  "settings": [
    {
      "setting": "riflex_freq_index in sampler",
      "value": "0",
      "reason": "Non-zero values add strange effects with VACE",
      "from": "scf"
    },
    {
      "setting": "Pose strength",
      "value": "1.3",
      "reason": "Helps with timing delays in lip sync",
      "from": "Charlie"
    },
    {
      "setting": "bbox_detector",
      "value": ".torchscript",
      "reason": "Dramatically faster than default, reduces processing from 17min to 3min",
      "from": "xiver2114"
    },
    {
      "setting": "Frame count preference",
      "value": "97 frames",
      "reason": "Avoids looping issues",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "setting": "Resolution matching",
      "value": "Match video aspect ratio exactly",
      "reason": "Prevents face detection failures",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "setting": "Adams-Bashforth sampler",
      "value": "Use under certain conditions",
      "reason": "Gives CFG burn effect due to overly strong momentum from multistep",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "setting": "WanAnimate without LightX parameters",
      "value": "steps=20, CFG=1, shift=5, scheduler=dmp++",
      "reason": "Based on official diffusers code reference",
      "from": "Christian Sandor"
    },
    {
      "setting": "Euler/beta sampler for 12GB VRAM",
      "value": "Euler/beta scheduler",
      "reason": "Speed - only slightly worse than dpm++ but much faster",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "setting": "CFG without Lightning LoRAs",
      "value": "CFG 3.5",
      "reason": "Need higher CFG when not using lightning or lightx loras",
      "from": "DennisM"
    },
    {
      "setting": "Frame window size for long videos",
      "value": "frame_window_size set to 77 for 600 frame video",
      "reason": "Enables longer video generation with built-in windowing",
      "from": "A.I.Warper"
    },
    {
      "setting": "Frame count for 3060 12GB",
      "value": "65 frames max at 1024x576",
      "reason": "81 frames causes OOM on second block",
      "from": "mdkb"
    },
    {
      "setting": "Audio scale for InfiniteTalk",
      "value": "1.4 audio scale, 1.0 audio cfg",
      "reason": "Good balance for lipsync without being crazy",
      "from": "Charlie"
    },
    {
      "setting": "HuMO audio settings",
      "value": "1.0 audio scale, 2.5 audio cfg scale",
      "reason": "Provides more lip action/expression",
      "from": "mdkb"
    },
    {
      "setting": "Wan Animate steps and CFG",
      "value": "20 steps total (2x10), CFG 3.5, shift 8",
      "reason": "Standard settings mentioned for 2.2 I2V FLF",
      "from": "Lumifel"
    },
    {
      "setting": "LoRA training parameters",
      "value": "5e-5 learning rate, rank 32-64, 1200-1500 steps",
      "reason": "Rank 64 for complex prompting, lower steps prevent overfitting",
      "from": "Kytra"
    },
    {
      "setting": "Wan 2.2 MOE split ratio",
      "value": "0.875 or around 0.5",
      "reason": "Official code reportedly uses around 0.5 split between high and low noise",
      "from": "Draken"
    },
    {
      "setting": "CFG",
      "value": "1.0 with 8 steps",
      "reason": "Works well with smooth mix model",
      "from": "Juampab12"
    },
    {
      "setting": "Resolution",
      "value": "864x864",
      "reason": "Good balance for testing",
      "from": "Juampab12"
    },
    {
      "setting": "OVI VRAM usage",
      "value": "16.4GB steady, climbs to 19GB",
      "reason": "With model_fp8_e4m3fn on 3090",
      "from": "yukass"
    },
    {
      "setting": "Lightning LoRA settings",
      "value": "High 2.5 strength, Low 2 strength, High shift 7.75, Low shift 9",
      "reason": "Recommended configuration for quality results",
      "from": "Juampab12"
    },
    {
      "setting": "Lightning LoRA steps",
      "value": "DDIM beta 3 steps + Euler beta 3 steps (6 total)",
      "reason": "Better than default 4 steps",
      "from": "Juampab12"
    },
    {
      "setting": "New lightning sampler settings",
      "value": "3 sampler with 0.25 to 0.5 and 3CFG",
      "reason": "Very nice results",
      "from": "pom"
    },
    {
      "setting": "HuMo generation",
      "value": "50 iterations, 9.74s/it with model_fp8_e4m3fn.safetensors",
      "reason": "Performance benchmark on RTX 3090",
      "from": "yukass"
    },
    {
      "setting": "T2V steps",
      "value": "1+1 steps",
      "reason": "Minimal steps needed for T2V generation",
      "from": "hicho"
    },
    {
      "setting": "I2V steps",
      "value": "2+2 steps",
      "reason": "Recommended for I2V generation",
      "from": "hicho"
    },
    {
      "setting": "T2V with speed LoRAs",
      "value": "10 steps, 5 split",
      "reason": "Testing configuration with speed LoRAs",
      "from": "Dream Making"
    },
    {
      "setting": "I2V standard",
      "value": "20 steps, CFG 2.5",
      "reason": "Standard I2V configuration",
      "from": "Ryzen"
    },
    {
      "setting": "Resolution multiples",
      "value": "64 or 128",
      "reason": "Prevents black output, ensures video token number divisible by 128",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "setting": "Shift parameter",
      "value": "5 for WanAnimate, 5-8 for Wan 2.2",
      "reason": "Controls noise distribution per timestep",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "0.7 with CFG 1.4",
      "reason": "For optimal quality in low-noise phase",
      "from": "Lan8mark"
    },
    {
      "setting": "Steps for Lightning LoRA",
      "value": "3 steps low noise, 6 total with 3 split",
      "reason": "Optimal balance for quality and speed",
      "from": "Lan8mark"
    },
    {
      "setting": "VAE selection",
      "value": "Wan 2.2 VAE only for 5B model, 14B uses 2.1 VAE",
      "reason": "Model-specific VAE compatibility",
      "from": "Charlie"
    },
    {
      "setting": "Shift and split point coordination",
      "value": "Must adjust shift while accounting for split point - original suggested values are shift 8 for high noise, shift 1 for low noise",
      "reason": "Foundation for all other parameter tweaks",
      "from": "Lan8mark"
    },
    {
      "setting": "High-noise sampler for quality",
      "value": "Euler, shift 12, 30 steps total (19 steps on high)",
      "reason": "Produces very good results but takes 34 minutes",
      "from": "flo1331"
    },
    {
      "setting": "Denoise strength for low-noise",
      "value": "0.2 to 0.4",
      "reason": "Manual calibration in this range finds ideal detail level",
      "from": "Lan8mark"
    },
    {
      "setting": "Steps for WAN generation",
      "value": "8 steps total: 5 at 384p, 3 at full 720p",
      "reason": "Optimal balance of speed and quality",
      "from": "Lan8mark"
    },
    {
      "setting": "Frame count for controlled cases",
      "value": "77 frames",
      "reason": "Works well with most looping method, used to be 81",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Maximum stable frames on 1 GPU",
      "value": "301 frames",
      "reason": "Pretty stable on 1 GPU, you can go higher in 4 H100",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "setting": "Triton Windows installation",
      "value": "pip install -U 'triton-windows<3.5'",
      "reason": "For Windows speed improvements",
      "from": "Shadows"
    },
    {
      "setting": "rCM LoRA steps",
      "value": "4 steps with dpm++_sde",
      "reason": "Original rCM has specific scheduler requirements",
      "from": "Kijai"
    },
    {
      "setting": "rCM LoRA strength",
      "value": "Bump up the strength a bit",
      "reason": "Need higher strength to match full model performance",
      "from": "Kijai"
    },
    {
      "setting": "Shift parameter for dual models",
      "value": "Raise shift to 11",
      "reason": "Makes step point change so it stays on HN and works more on structure",
      "from": "mdkb"
    },
    {
      "setting": "Steps for OVI",
      "value": "50 steps",
      "reason": "Better lip-sync quality and overall results",
      "from": "tarn59"
    },
    {
      "setting": "Resolution for WAN t2v testing",
      "value": "544x544",
      "reason": "Higher resolutions cause static artifacts",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "setting": "Steps for OVI testing",
      "value": "25 steps",
      "reason": "Good for testing and experimentation",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "setting": "Reserve VRAM with 5090",
      "value": "6GB",
      "reason": "Needed even with high-end GPU",
      "from": "Juampab12"
    },
    {
      "setting": "OVI generation settings",
      "value": "960x512, 20 steps",
      "reason": "Base settings, 50 steps and 1280x720 for better quality",
      "from": "Drommer-Kille"
    },
    {
      "setting": "Wan 2.2 stock settings",
      "value": "5.5 shift, 1 CFG, 8 steps 4 split",
      "reason": "Default configuration for Wan 2.2",
      "from": "Dream Making"
    },
    {
      "setting": "CFG for face closeups",
      "value": "5+5 CFG 3",
      "reason": "Good for closeup face generation",
      "from": "hicho"
    },
    {
      "setting": "Low noise CFG",
      "value": "2.5 instead of 3.5",
      "reason": "Prevents face degradation on low noise model",
      "from": "Lumifel"
    },
    {
      "setting": "Power limit for 5090 stability",
      "value": "70%",
      "reason": "Increases stability for problematic 5090 cards",
      "from": "seitanism"
    },
    {
      "setting": "Lightx2v LoRA strength",
      "value": "1.0 on both high and low",
      "reason": "Recommended starting point",
      "from": "Elvaxorn"
    },
    {
      "setting": "Steps for new Lightx2v to avoid ghosting",
      "value": "6x6 steps minimum, 12 steps for LoRA",
      "reason": "Lower steps cause severe ghosting",
      "from": "Zabo"
    },
    {
      "setting": "Scheduler for new Lightx2v",
      "value": "Linear quadratic instead of simple",
      "reason": "Simple scheduler causes ghosting issues",
      "from": "FL13"
    },
    {
      "setting": "CFG fall ratio",
      "value": "50% of steps",
      "reason": "Default hardcoded value in ComfyUI for CFG dropping",
      "from": "Kijai"
    },
    {
      "setting": "New Light MoE High LoRA strength",
      "value": "3.00",
      "reason": "Provides lots more motion compared to previous setups",
      "from": "phazei"
    },
    {
      "setting": "Linear quadratic scheduler",
      "value": "Most reliable",
      "reason": "Most reliable scheduler when using new LoRA",
      "from": "FL13"
    },
    {
      "setting": "Low step training resolution",
      "value": "256x256",
      "reason": "Trick to avoid OOM on 5090 when using 81 frames for video training",
      "from": "Drommer-Kille"
    },
    {
      "setting": "Native workflow steps",
      "value": "6 steps (3+3)",
      "reason": "Less slow motion effect than 4 steps with CFG 1",
      "from": "V\u00e9role"
    },
    {
      "setting": "FlashVSR sharpness",
      "value": "Change to 11 for less sharp results",
      "reason": "Default setting is very sharp",
      "from": "yi"
    },
    {
      "setting": "CFG for multi-sampler without lighting LoRAs",
      "value": "CFG 3.5 on high, 2.5 on low, 20 steps total",
      "reason": "Balanced quality and motion",
      "from": "Lumifel"
    },
    {
      "setting": "TaylorSeer usage",
      "value": "Use Lite version",
      "reason": "Standard version will cause OOM",
      "from": "yi"
    },
    {
      "setting": "Model sampling shift",
      "value": "5 or 8 for Wan (not 7)",
      "reason": "Standard values used by community, affects sigma computation",
      "from": "Lodis"
    },
    {
      "setting": "FlashVSR denoise",
      "value": "0.75",
      "reason": "Reduces over-sharpness while providing 20% speed increase",
      "from": "Elvaxorn"
    },
    {
      "setting": "Lightning LoRA strength",
      "value": "Not -4.75 (user error)",
      "reason": "Incorrect negative values cause generation issues",
      "from": "topmass"
    },
    {
      "setting": "CineScale RoPE scaling",
      "value": "1, 20, 20 then 1, 25, 25",
      "reason": "Prevents repeat effect at larger resolutions",
      "from": "DawnII"
    },
    {
      "setting": "New distill models",
      "value": "4 steps total, 2 high noise + 2 low noise, cfg 1, shift 5",
      "reason": "Recommended configuration for 4-step distill",
      "from": "aikitoria"
    },
    {
      "setting": "VAE encoding",
      "value": "fp32",
      "reason": "Workaround for torch 2.9.0/2.10 VRAM bug",
      "from": "Charlie"
    },
    {
      "setting": "VACE frame count",
      "value": "25-81 frames optimal",
      "reason": "Best quality and temporal consistency, follows 4n+1 formula",
      "from": "Dever"
    },
    {
      "setting": "rcm LoRA strength",
      "value": "4.0 on high noise with WAN 2.2",
      "reason": "Works well for speed with rank148 version, though lower strengths like 1.0 work better for some users",
      "from": "aipmaster"
    },
    {
      "setting": "FlashVSR upscale ratio",
      "value": "At least 2x, preferably 4x",
      "reason": "Less than 2x produces poor quality especially for faces",
      "from": "Kijai"
    },
    {
      "setting": "Lightning 2.2 I2V strength",
      "value": "0.5 on high noise",
      "reason": "Good I2V results when paired with lightx2v i2v 480p rank 256 at 1.0 on low",
      "from": "Persoon"
    },
    {
      "setting": "Shift parameter for rcm workflow",
      "value": "7",
      "reason": "Used in successful rcm + lightning combination workflow",
      "from": "aipmaster"
    },
    {
      "setting": "Blockswap",
      "value": "35 (increased from 25)",
      "reason": "Required more after ComfyUI update for 1120x704 97 frames",
      "from": "phazei"
    },
    {
      "setting": "FlashVSR strength",
      "value": "0.8",
      "reason": "Effective for fixing bad renders without over-processing",
      "from": "mdkb"
    },
    {
      "setting": "rCM steps",
      "value": "4 steps",
      "reason": "Used in official examples, 1 step not actually demonstrated",
      "from": "Kijai"
    },
    {
      "setting": "Clownshark sigma shift",
      "value": "4-10 range",
      "reason": "4 for static scenes, up to 10 for more dynamic content",
      "from": "Kinasato"
    },
    {
      "setting": "Sliding window frames",
      "value": "8",
      "reason": "Used with temporal mask setup",
      "from": "TK_999"
    },
    {
      "setting": "Region bleed",
      "value": "0.2",
      "reason": "Used with temporal mask configuration",
      "from": "TK_999"
    },
    {
      "setting": "High noise LoRA strength",
      "value": "1.7",
      "reason": "For 2Steps Lightx MoE",
      "from": "Canin17"
    },
    {
      "setting": "Low noise LoRA strength",
      "value": "1.5",
      "reason": "For 2Steps rCM",
      "from": "Canin17"
    },
    {
      "setting": "CFG for first sampler",
      "value": "3.5",
      "reason": "Used with no LoRA in 3-sampler native setup",
      "from": "FL13"
    },
    {
      "setting": "Scheduler recommendation",
      "value": "Euler with linear quadratic",
      "reason": "Used across all samplers in FL13's workflow",
      "from": "FL13"
    },
    {
      "setting": "cfg",
      "value": "2.0 on first step only",
      "reason": "Provides good motion with minimal complexity",
      "from": "Kijai"
    },
    {
      "setting": "Ditto Global LoRA strength",
      "value": "1.2 with lightx t2v adaptive at 1",
      "reason": "Good balance for style transfer",
      "from": "DawnII"
    },
    {
      "setting": "High/Low model split",
      "value": "6 steps split in middle, first step cfg 2.0",
      "reason": "lightx2v lora at 3.0 strength high, 1.0 strength low",
      "from": "Kijai"
    },
    {
      "setting": "Krea LoRA strength",
      "value": "3.6+ on high model",
      "reason": "Going lower than 3.6 on high doesn't work well",
      "from": "aipmaster"
    },
    {
      "setting": "Krea LoRA strength",
      "value": "4 on HN and 1.1 on LN with HPS at 0.75",
      "reason": "To reduce noise when using on Wan 2.2",
      "from": "aipmaster"
    },
    {
      "setting": "Shift value for I2V",
      "value": "Target 0.9 sigma at split",
      "reason": "High noise model meant to operate on high sigmas, default 0.9 for I2V",
      "from": "seitanism"
    },
    {
      "setting": "Shift value for T2V",
      "value": "Target 0.875 sigma at split",
      "reason": "Default sigma value for T2V workflows",
      "from": "Kijai"
    },
    {
      "setting": "Shift value minimum",
      "value": "At least 16 (double from 8)",
      "reason": "To prevent high sampler from adding too much noise",
      "from": "Kijai"
    },
    {
      "setting": "MoCha VRAM usage with fp8",
      "value": "11GB at default resolution",
      "reason": "Makes it feasible on mid-range GPUs",
      "from": "Kijai"
    },
    {
      "setting": "Krea generation time",
      "value": "140 seconds for 140 seconds 720p",
      "reason": "Real-time generation capability on 4090",
      "from": "aipmaster"
    },
    {
      "setting": "HPS strength adjustment",
      "value": "Bump up if output too noisy",
      "reason": "Controls image quality vs noise trade-off",
      "from": "aipmaster"
    },
    {
      "setting": "--reserve-vram",
      "value": "2",
      "reason": "To offload more VRAM when using monitor on same GPU",
      "from": "Kijai"
    },
    {
      "setting": "New LightX2V LoRA strength",
      "value": "1.0",
      "reason": "Works well for complex prompts that failed on older versions",
      "from": "Ada"
    },
    {
      "setting": "Step split for lightx2v LoRA",
      "value": "2+2 or 3+3",
      "reason": "Most reasonable split for 4 total steps, 3+3 gives better image quality",
      "from": "Ablejones"
    },
    {
      "setting": "SVI padding configuration",
      "value": "Padding -1 for shot, 0 for film",
      "reason": "Shot uses reference image padding, film uses zero padding",
      "from": "Kijai"
    },
    {
      "setting": "Shift values for low step count",
      "value": "Higher shift (like 11) for very low steps (1 step)",
      "reason": "The less steps the more shift needed",
      "from": "Ada"
    },
    {
      "setting": "SVI film frames",
      "value": "5 frames from end",
      "reason": "Optimal for motion continuity between generations",
      "from": "Kijai"
    },
    {
      "setting": "Ditto character LoRA strength",
      "value": "2.0",
      "reason": "Better character likeness and recognition",
      "from": "Drommer-Kille"
    },
    {
      "setting": "WSL memory allocation",
      "value": "memory=60GB, swap=60GB",
      "reason": "Prevent OOM issues in WSL environment",
      "from": "seitanism"
    },
    {
      "setting": "LightX2V usage",
      "value": "3 low noise steps with LoRA vs 25 without",
      "reason": "Better quality results with fewer steps when using speed LoRA",
      "from": "seitanism"
    },
    {
      "setting": "SVI LoRA strength",
      "value": "High: 3, Low: 1 for Wan 2.2",
      "reason": "Working configuration for SVI with Wan 2.2",
      "from": "V\u00e9role"
    },
    {
      "setting": "Anisora 3.2 distill CFG",
      "value": "CFG 1",
      "reason": "Distilled model designed to work with CFG 1, higher values don't impact much",
      "from": "DawnII"
    },
    {
      "setting": "FlashVSR attention",
      "value": "Sage attention instead of SDPA",
      "reason": "Prevents OOM errors",
      "from": "patientx"
    },
    {
      "setting": "Block swap configuration",
      "value": "8 normal blocks, 3 VACE blocks",
      "reason": "Good balance for VRAM management",
      "from": "Kijai"
    },
    {
      "setting": "InfiniteTalk steps with LightX2V",
      "value": "4 steps",
      "reason": "Avoid extremely long generation times (6+ hours)",
      "from": "seitanism"
    },
    {
      "setting": "InfiniteTalk CFG with LightX2V",
      "value": "1",
      "reason": "Works with the 4-step acceleration",
      "from": "seitanism"
    },
    {
      "setting": "SVI ref_pad_num in film mode",
      "value": "0",
      "reason": "No special padding applied when set to 0",
      "from": "Kijai"
    },
    {
      "setting": "Steps for SVI with distill",
      "value": "8 steps with lightx2v",
      "reason": "Works for quick testing",
      "from": "Kijai"
    },
    {
      "setting": "Steps for non-distill SVI",
      "value": "40 steps",
      "reason": "Better quality but painfully slow",
      "from": "Kijai"
    },
    {
      "setting": "CFG for SVI",
      "value": "5.0",
      "reason": "Used in testing",
      "from": "Kijai"
    },
    {
      "setting": "Resolution for long generations",
      "value": "960x540",
      "reason": "Needed for VRAM handling of 245 frames with full blockswap",
      "from": "Hashu"
    },
    {
      "setting": "Steps with distill lora",
      "value": "8 steps, euler",
      "reason": "For 1 minute generation efficiency",
      "from": "Kijai"
    },
    {
      "setting": "Steps without distill",
      "value": "30 steps, 3.5 cfg",
      "reason": "For quality testing",
      "from": "Ablejones"
    },
    {
      "setting": "svi-shot mask setup",
      "value": "1 black mask frame and 80 white mask frames in wrapper",
      "reason": "Proper masking for shot lora",
      "from": "Ablejones"
    },
    {
      "setting": "HoloCine generation",
      "value": "720x720x241 frames, 3 high 3 low steps with lightx2v lora",
      "reason": "Achieves 15-second generation in one pass",
      "from": "seitanism"
    },
    {
      "setting": "Split attention experimental",
      "value": "Avoid using on first step (0) as it's too strong",
      "reason": "Works with 2.2 because low noise fixes it, but first step makes it too strong",
      "from": "Kijai"
    },
    {
      "setting": "HoloCine sparse model",
      "value": "30.5GB VRAM with 20 blockswap for 281 frames at 832x480",
      "reason": "Enables minute-long video generation",
      "from": "mamad8"
    },
    {
      "setting": "LightX LoRA strength",
      "value": "High 3.0, Low 1.0",
      "reason": "Optimal balance for quality",
      "from": "avataraim"
    },
    {
      "setting": "Swap block",
      "value": "40",
      "reason": "For 30 second generation on RTX 4090",
      "from": "avataraim"
    },
    {
      "setting": "Video attention split",
      "value": "0",
      "reason": "For experimental Holocine workflow",
      "from": "avataraim"
    },
    {
      "setting": "Denoise for scene differentiation",
      "value": "0.9 or 0.95",
      "reason": "Forces high noise model to differentiate scenes",
      "from": "mamad8"
    },
    {
      "setting": "HoloCine generation parameters",
      "value": "num_frames=241, shot_cut_frames=[37, 77, 117, 157, 197]",
      "reason": "For 15-second videos with proper scene transitions",
      "from": "BNP4535353"
    },
    {
      "setting": "LongCat CFG",
      "value": "cfg 4.0 for 30 steps or cfg 1.0 for 15 steps with distill LoRA",
      "reason": "Balanced quality and speed",
      "from": "Kijai"
    },
    {
      "setting": "Ovi resolution hack",
      "value": "512x512 at 25 steps",
      "reason": "Faster generation, changed area calculations in ovi_fusion_engine.py",
      "from": "reallybigname"
    },
    {
      "setting": "LongCat distill schedule",
      "value": "1000.0000, 994.7090, 988.7640, 982.0360, 974.3589, 965.5172, 955.2239, 943.0895, 923.0769, 904.1096, 880.3089, 849.5575, 808.2901, 750.0000, 661.4174, 510.6383",
      "reason": "Custom schedule for longcat_distill_euler sampler",
      "from": "Kijai"
    },
    {
      "setting": "Holocine shot cuts",
      "value": "41,81,121,161,201 or 50,100,150,200,250",
      "reason": "Frame cut points for multi-shot generation",
      "from": "shaggss"
    },
    {
      "setting": "LightX2V LoRA strength",
      "value": "1",
      "reason": "For both high and low noise models with Holocine",
      "from": "NebSH"
    },
    {
      "setting": "LongCat steps with distill LoRA",
      "value": "16 steps",
      "reason": "Default with distill LoRA, 30+ steps needed without it",
      "from": "Kijai"
    },
    {
      "setting": "LongCat quality/speed balance",
      "value": "10 steps",
      "reason": "Good balance between quality and generation time",
      "from": "avataraim"
    },
    {
      "setting": "LongCat fps setting",
      "value": "15 fps",
      "reason": "Native generation rate, interpolates well to 30fps",
      "from": "Kijai"
    },
    {
      "setting": "Video combine node for LongCat",
      "value": "15",
      "reason": "To reflect original code implementation",
      "from": "Kijai"
    },
    {
      "setting": "LongCat extension frames",
      "value": "17 frames minimum",
      "reason": "Follows 4+1 rule, 16 wouldn't work properly",
      "from": "Kijai"
    },
    {
      "setting": "LongCat refinement steps",
      "value": "50 steps with timestep threshold 500",
      "reason": "Only runs steps under threshold, results in about 4 actual steps",
      "from": "Kijai"
    },
    {
      "setting": "WanAnimate original settings",
      "value": "shift=5.0, steps=20, guide_scale=1.0, solver=dpm++",
      "reason": "Default settings from original implementation",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "setting": "LongCat refinement lora strength",
      "value": "0.6 recommended",
      "reason": "Full strength (1.0) creates noisy texture, 0.6 provides better balance",
      "from": "Kijai"
    },
    {
      "setting": "NeatVideo denoising",
      "value": "Enable temporal, set spatial to low weight or disabled",
      "reason": "Preserves detail while removing VAE noise",
      "from": "spacepxl"
    },
    {
      "setting": "Holocine CFG",
      "value": "1.0",
      "reason": "Higher CFG values cause structured prompt errors",
      "from": "NebSH"
    },
    {
      "setting": "Smooth windows token sharing",
      "value": "1-12 range",
      "reason": "Controls information passed between shots for consistency",
      "from": "NebSH"
    },
    {
      "setting": "LongCat LoRA strength",
      "value": "1.0",
      "reason": "Works better at full strength with specific sampling setup",
      "from": "Kijai"
    },
    {
      "setting": "LongCat sampling",
      "value": "50 steps, start step 45, Euler or UniPC, CFG 1.0",
      "reason": "Optimal parameters for LongCat refinement",
      "from": "Kijai"
    },
    {
      "setting": "WAN low refine",
      "value": "Shift 1, denoise 0.2-0.3, 8-10 steps, CFG 3-5",
      "reason": "For refining generated images without distill LoRA",
      "from": "spacepxl"
    },
    {
      "setting": "OVI generation length",
      "value": "8 seconds maximum",
      "reason": "10 second limit exists, 15 seconds fails",
      "from": "avataraim"
    },
    {
      "setting": "--cache-none flag",
      "value": "enabled",
      "reason": "Prevents RAM reservation, never reserves that RAM",
      "from": "Kijai"
    },
    {
      "setting": "Batch size for LoRA training",
      "value": "16 or more",
      "reason": "To remedy cases where WAN picks one category and maxes that out",
      "from": "Kiwv"
    },
    {
      "setting": "LongCat steps",
      "value": "10-16 steps minimum with distill LoRA",
      "reason": "Balance between quality and generation time",
      "from": "Kijai"
    },
    {
      "setting": "LongCat T2V distill LoRA",
      "value": "1.0 strength with shift 11",
      "reason": "Provides good quality without major degradation",
      "from": "Pandaabear"
    },
    {
      "setting": "Morph LoRA strength",
      "value": "0.5 (official) or 3.0-4.0 for visible effects",
      "reason": "Official recommendation vs observed effectiveness",
      "from": "Kijai"
    },
    {
      "setting": "Steps for acceptable T2V quality",
      "value": "6-7 steps minimum",
      "reason": "Below 5 causes massive lighting smoothness issues and blurriness",
      "from": "Pandaabear"
    },
    {
      "setting": "New LightX2V 1030 LoRA",
      "value": "LoRA strength 1.0, CFG 1.0, 6 steps without CFG",
      "reason": "Optimized settings for the improved 1030 model",
      "from": "Kijai"
    },
    {
      "setting": "Torch compile alternative startup args",
      "value": "--reserve-vram 2 --max-upload-size 500 --fast pinned_memory --async-offload --use-sage-attention --fast fp16_accumulation",
      "reason": "Avoids VRAM issues while maintaining decent performance",
      "from": "Gleb Tretyak"
    },
    {
      "setting": "VACE generation settings",
      "value": "20 steps, CFG 2.5, SDPA attention",
      "reason": "Balanced quality and performance settings",
      "from": "Lumifel"
    },
    {
      "setting": "ChronoEdit defaults",
      "value": "8 steps, cfg 1.0 with distill lora, shift 2.0",
      "reason": "Official recommended settings",
      "from": "Kijai"
    },
    {
      "setting": "LightX2V 1030 usage",
      "value": "Plain 4+4 euler simple, no CFG",
      "reason": "Adding CFG steps results in trash results",
      "from": "Gleb Tretyak"
    },
    {
      "setting": "Anisora strength for cartoons",
      "value": "0.5",
      "reason": "Works well for cartoon content",
      "from": "Gleb Tretyak"
    },
    {
      "setting": "ChronoEdit scale_t",
      "value": "7.0 in latest version",
      "reason": "Corrected from earlier 8.0 value",
      "from": "comfy"
    },
    {
      "setting": "ChronoEdit frames",
      "value": "5 frames (normal) or 29 frames (temporal reasoning)",
      "reason": "What it was trained on",
      "from": "Kiwv"
    },
    {
      "setting": "wan2.2_lightx2v_1030 timing",
      "value": "170 secs for 1024x576 on 4090, 7 steps split at 4",
      "reason": "Performance benchmark",
      "from": "Atlas"
    },
    {
      "setting": "Qwen Edit steps",
      "value": "40 steps",
      "reason": "Night and day different for sharpness compared to default",
      "from": "aikitoria"
    },
    {
      "setting": "LightX LoRA steps",
      "value": "4 steps",
      "reason": "Can generate 5-second segments efficiently",
      "from": "aikitoria"
    },
    {
      "setting": "Blocks to swap",
      "value": "30 instead of 20",
      "reason": "To prevent OOMs with new ComfyUI update",
      "from": "aipmaster"
    }
  ],
  "concepts": [
    {
      "term": "Differential diffusion",
      "explanation": "Thresholds non-binary masks per step so it blends better than normal latent inpaint",
      "from": "Kijai"
    },
    {
      "term": "High Noise vs Low Noise models",
      "explanation": "High Noise better for prompt following, composition, and motion. Low Noise better for large inpainting tasks",
      "from": "Ablejones"
    },
    {
      "term": "Merged vs Unmerged LoRAs",
      "explanation": "Merged LoRAs integrate weights into model (no VRAM hit, precision loss). Unmerged LoRAs use more VRAM but maintain base precision and allow runtime adjustment",
      "from": "Kijai"
    },
    {
      "term": "VACE inpainting",
      "explanation": "Model feature for inpainting that can be combined with other inpainting techniques",
      "from": "Kijai"
    },
    {
      "term": "Palingenesis",
      "explanation": "Term meaning 'rebirth' or 'regeneration', refers to merging distill loras into base model",
      "from": "Tony(5090)"
    },
    {
      "term": "SA-ODE sampler",
      "explanation": "Adams-Bashforth method from 1883, provides decent performance but with CFG burn effects",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "Second order lang dynamics",
      "explanation": "Advanced sampling technique in LanPaint, most powerful sampler feature",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "Generative upscaler",
      "explanation": "Essentially img2img processing, can be server-based or local",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "Prompt distribution sampling",
      "explanation": "Your prompt sets up the distribution that the sampler is sampling from - better to have a better distribution",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "ComfyUI as shared inference engine",
      "explanation": "ComfyUI seen as a common language for projects to interact, with lots of different projects being compatible",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "Blockswapping in AI Toolkit",
      "explanation": "Sends 40 blocks to CPU during training to reduce VRAM usage",
      "from": "Ryzen"
    },
    {
      "term": "High/Low noise expert split",
      "explanation": "MoE architecture in Wan 2.2 with separate experts for different noise levels",
      "from": "mdkb"
    },
    {
      "term": "PUSA purpose",
      "explanation": "Allows T2V model to do I2V with minimal training cost, use with flowmatch_pusa scheduler and Pusa Noise node",
      "from": "JohnDopamine"
    },
    {
      "term": "Wan 2.2 MOE architecture",
      "explanation": "High/Low noise expert split with 5B hybrid model, uses different VAE than 14B version",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "OVI symmetric twin backbone",
      "explanation": "Parallel audio and video branches with identical DiT architecture, video initialized from Wan 2.2 5B",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "FLF (First-Last-Frame)",
      "explanation": "Video morphing technique using start and end frames, official FLF model available for 2.2",
      "from": "Kijai"
    },
    {
      "term": "VACE mask values",
      "explanation": "Can use partial denoise with 50% white mask, or spatially varying non-uniform masks for selective diffusion",
      "from": "spacemathapocalypse"
    },
    {
      "term": "NAG node",
      "explanation": "Makes negative prompt work with 1.0 CFG, though may not work with base ComfyUI nodes for WAN",
      "from": "Kytra"
    },
    {
      "term": "Context windows with | split",
      "explanation": "Single | in text encoder allows different prompts for different windows of context",
      "from": "Kijai"
    },
    {
      "term": "Self-Forcing/Rolling Forcing methods",
      "explanation": "New method by ByteDance for real-time generation, trades quality for speed",
      "from": "ericxtang"
    },
    {
      "term": "Smooth Mix model behavior",
      "explanation": "Model that exaggerates motion at lower fps for better frame interpolation results",
      "from": "Rainsmellsnice"
    },
    {
      "term": "Video token number",
      "explanation": "Must be divisible by 128. Calculated as frame_size * num_frame. Different formulas for different model sizes",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "term": "Character Animation vs Character Replacement",
      "explanation": "Character Animation uses reference image for background and character with video for motion. Character Replacement inserts character from reference into video background",
      "from": "km"
    },
    {
      "term": "Model distillation benefit",
      "explanation": "Model distillation often improves quality, as seen with OVI improving visual quality of 5B model",
      "from": "Juampab12"
    },
    {
      "term": "Shift parameter",
      "explanation": "Controls the amount of noise given per timestep - low shift does much at start and falls off, high shift does more for longer. Beginning timesteps give structure, later timesteps give detail",
      "from": "Juampab12"
    },
    {
      "term": "RMS norm function",
      "explanation": "Normalization function where new native PyTorch version is much faster than custom Wan implementation, but changes output slightly",
      "from": "Kijai"
    },
    {
      "term": "Shift parameter",
      "explanation": "Simply adjusts the sigmas in the sampling process - effect is complex but the mechanism is simple",
      "from": "Kijai"
    },
    {
      "term": "Sigma sweet spot",
      "explanation": "Optimal noise levels for different phases of generation, important to keep steady when switching between high and low noise models",
      "from": "Lan8mark"
    },
    {
      "term": "OVI",
      "explanation": "Video with audio generator based on Wan2.2 5B and MMAudio, open source kinda veo3",
      "from": "Stad/Charlie"
    },
    {
      "term": "Teacache",
      "explanation": "Should not be doing much at 10 steps, threshold has to be pretty high to be culling 5 steps",
      "from": "DawnII"
    },
    {
      "term": "Latent upscaling vs pixel space upscaling",
      "explanation": "What you're doing is running steps at lower res, then decoding into pixel space, upscaling in pixel space and encoding back to latents. Latent upscaling is when you upscale H and W in latent space",
      "from": "shaggss"
    },
    {
      "term": "Consistency Model",
      "explanation": "A type of distillation that maps points on the trajectory of a parent model directly to finished output - finds shortcuts but needs existing trajectories from parent model",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "SDE Samplers",
      "explanation": "Samplers that add noise during generation, can cause harm at low step counts",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "Scaled FP8",
      "explanation": "Quantized model format that requires scaling to work properly, needs 'scaled_fp8' key in metadata",
      "from": "Kijai"
    },
    {
      "term": "V2 model versions",
      "explanation": "V2 versions are for native node compatibility, not necessarily upgrades",
      "from": "DawnII"
    },
    {
      "term": "Temporal mask",
      "explanation": "Feature in WAN that allows masking across time for video generation control",
      "from": "Kijai"
    },
    {
      "term": "Block cache vs blockcache",
      "explanation": "Setting blockcache to 0 can cause memory issues, bypassing entirely is better",
      "from": "lostintranslation"
    },
    {
      "term": "SEC masking",
      "explanation": "Enhancement for SAM2.1 that provides extra guidance for segmentation, still uses SAM but with better tracking",
      "from": "Kijai"
    },
    {
      "term": "RCM LoRA",
      "explanation": "Distillation LoRA by Nvidia developers, alternative to lightx2v for speed without killing motion",
      "from": "Elvaxorn"
    },
    {
      "term": "TAE VAE",
      "explanation": "Tiny VAE for preview purposes only, not suitable for final outputs due to quality loss",
      "from": "Kijai"
    },
    {
      "term": "fp8_scaled_e4m3fn vs e5m2",
      "explanation": "e4m3fn uses 4 bits exponent, 3 bits mantissa - good balance. e5m2 uses 5 bits exponent, 2 bits mantissa - prioritizes dynamic range over precision",
      "from": "Dever"
    },
    {
      "term": "HN and LN",
      "explanation": "High Noise and Low Noise models in the MoE architecture",
      "from": "FL13"
    },
    {
      "term": "CFG skimming",
      "explanation": "Anti-CFG-burn tool that allows higher CFG on early steps then drops to prevent artifacts",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "CFG Zero Star",
      "explanation": "Was for rectified flow DiTs in general using heuristic approximation, but might not be good for Wan 2.2",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "VACE",
      "explanation": "Video control system that doesn't modify main model weights at all, only adds extra layers",
      "from": "Kijai"
    },
    {
      "term": "Simple consistency model distillation",
      "explanation": "Distillation method that can be trained in a few hours, makes mappings from points on parent model path to finished output",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "term": "Block sparse attention",
      "explanation": "Sage version made for radial attention use, different from standard version",
      "from": "Kijai"
    },
    {
      "term": "Text encode cache",
      "explanation": "Offloads prompt encoding cache to file, removes from memory after use, helps with OOM issues",
      "from": "mdkb"
    },
    {
      "term": "CFG scheduling",
      "explanation": "Using different CFG values for each sampling step by providing a list of values",
      "from": "Kijai"
    },
    {
      "term": "Model sampling shift",
      "explanation": "Parameter used to compute sigmas, affects model behavior - think of it as model creativity level",
      "from": "42hub"
    },
    {
      "term": "Wan 2.2 MoE architecture",
      "explanation": "Uses High/Low noise expert split with separate models for different noise levels",
      "from": "Kijai"
    },
    {
      "term": "RoPE scaling",
      "explanation": "Scaling spatial rope to make larger resolutions work without repeat effect",
      "from": "Kijai"
    },
    {
      "term": "VACE frame inpainting",
      "explanation": "Using entire frames as input/output masks to generate missing frames in the middle",
      "from": "Koba"
    },
    {
      "term": "Meta batch processing",
      "explanation": "Using meta batch manager to handle long video processing in chunks",
      "from": "happy.j"
    },
    {
      "term": "MoE (Mixture of Experts)",
      "explanation": "Architecture with High/Low noise expert split used in WAN 2.2, available as 5B hybrid model",
      "from": "community context"
    },
    {
      "term": "Block-Sparse Attention",
      "explanation": "Optimization technique in FlashVSR that only works well on A100/A800 GPUs, causes quality degradation on consumer GPUs",
      "from": "JohnDopamine"
    },
    {
      "term": "RoPE frequency scaling",
      "explanation": "Technique to prevent repetition artifacts at high resolutions, especially important for 1080p+ and high vertical resolutions",
      "from": "Kijai"
    },
    {
      "term": "High/Low noise expert split",
      "explanation": "Wan 2.2 MoE architecture splits processing between high noise and low noise experts",
      "from": "garbus"
    },
    {
      "term": "Substep samplers",
      "explanation": "Samplers that use multiple sub-steps within each main step for better convergence, underutilized but powerful",
      "from": "Ablejones"
    },
    {
      "term": "vid2vid with noise addition",
      "explanation": "Process where you provide encoded latents, skip steps, add noise based on skipped sigmas, model continues from that point",
      "from": "Kijai"
    },
    {
      "term": "Temporal masks",
      "explanation": "Masks that change over time for regional conditioning in video generation",
      "from": "TK_999"
    },
    {
      "term": "VACE",
      "explanation": "Video control system that includes style transfer, inpainting, subject-driven, and outpainting capabilities",
      "from": "context"
    },
    {
      "term": "Ditto",
      "explanation": "New VACE model with 3 modules (global_style, global, sim2real) for video style transfer tasks",
      "from": "Kijai"
    },
    {
      "term": "First frame VACE",
      "explanation": "Putting image as first frame and freezing it, but Ditto isn't expecting this approach",
      "from": "Draken"
    },
    {
      "term": "sim2real capability",
      "explanation": "Training model to map stylized videos back to their original real-world source videos",
      "from": "ucren"
    },
    {
      "term": "RGB control",
      "explanation": "Direct video-to-video style transfer without needing pose estimation",
      "from": "Draken"
    },
    {
      "term": "Shift in sigma curve",
      "explanation": "Shifts the sigma curve - high noise model meant to operate on high sigmas, modifies sigmas so high will cut off at proper point",
      "from": "Kijai"
    },
    {
      "term": "MoE architecture noise handling",
      "explanation": "End of high sampler supposed to have leftover noise before passing to low noise - that's how MoE arch works",
      "from": "DawnII"
    },
    {
      "term": "Self-Forcing technique",
      "explanation": "Technique for converting regular video diffusion models into autoregressive models, used in Krea realtime",
      "from": "Dever"
    },
    {
      "term": "Frame dimension concatenation",
      "explanation": "MoCha concatenates control video with mask and reference in frame dimension, doubling VRAM usage",
      "from": "Kijai"
    },
    {
      "term": "Context windows for video",
      "explanation": "Processing longer videos in chunks, each window contains noise + mask + ref",
      "from": "Kijai"
    },
    {
      "term": "Causal LoRA",
      "explanation": "Krea LoRA is causal like CausVid, affecting temporal consistency",
      "from": "Juampab12"
    },
    {
      "term": "NAG (Negative Augmented Generation)",
      "explanation": "Technique for better prompt adherence, negative should be simple and specific",
      "from": "Kijai"
    },
    {
      "term": "Model parameter sizing",
      "explanation": "1536 = 1.3B, 3072 = 5B, 5120 = 14B parameters based on patch_embedding weight dimensions",
      "from": "Kijai"
    },
    {
      "term": "SVI (Stable Video Infinity)",
      "explanation": "Multiple I2V generations chained together rather than single generation, uses reference frames and special masking",
      "from": "Kijai"
    },
    {
      "term": "REPA training method",
      "explanation": "End-to-end VAE training method that speeds up convergence and achieved SOTA ImageNet scores",
      "from": "yi"
    },
    {
      "term": "SVI padding mechanism",
      "explanation": "Shot workflow sets padding to -1 to pad all frames with reference image instead of black pixels, film workflow uses 0 padding",
      "from": "Kijai"
    },
    {
      "term": "I2V conditioning channels",
      "explanation": "16 channels for noise, 16 for image conditioning, 4 for image conditioning mask. Image cond normally is start image + black pixels for rest, mask marks image as 1 and black pixels as 0",
      "from": "Kijai"
    },
    {
      "term": "SVI film LoRA",
      "explanation": "LoRA that enables I2V model to work with multiple frames for video continuation",
      "from": "Kijai"
    },
    {
      "term": "Video-As-Prompt (VAP)",
      "explanation": "In-context learning system that uses reference videos to guide generation style and motion",
      "from": "JohnDopamine"
    },
    {
      "term": "VACE patch embedding",
      "explanation": "Specific model architecture component required for VACE functionality, missing in some model variants",
      "from": "DawnII"
    },
    {
      "term": "I2V channel mask",
      "explanation": "36 channels total: 16 for noise, 16 for I2V images, 4 for mask. Binary mask (1 for keep, 0 for generate) concatenated along channels",
      "from": "Kijai"
    },
    {
      "term": "SVI padding methods",
      "explanation": "SVI-film uses zero padding (black frames), SVI-shot uses original input image padding for reference frames",
      "from": "Kijai"
    },
    {
      "term": "empty_frame_level in VACE",
      "explanation": "Bias value interpreted by model, >0.5 makes frames lighter, <0.5 darker, but feeding values model wasn't trained on",
      "from": "Ablejones"
    },
    {
      "term": "Varlen attention",
      "explanation": "Variable length attention mechanism that HoloCine requires, only supported by flash and sage attention methods",
      "from": "Kijai"
    },
    {
      "term": "Shot cut frames",
      "explanation": "Frame numbers where you want cuts to happen in video generation, supported by some models",
      "from": "BobbyD4AI"
    },
    {
      "term": "Context window transitions",
      "explanation": "Points where extended video segments join together, often prone to artifacts and ghosting",
      "from": "blake37"
    },
    {
      "term": "SVI Shot method",
      "explanation": "Uses last frame as input with original image as reference in all other frames, only first frame masked. Doesn't continue motion but prevents degradation",
      "from": "Kijai"
    },
    {
      "term": "SVI Film method",
      "explanation": "Uses 5 frames from last video, padded with black. Continues motion but can degrade over time",
      "from": "Kijai"
    },
    {
      "term": "Reference padding",
      "explanation": "Filling non-masked frames with reference image to maintain consistency and reduce degradation",
      "from": "Kijai"
    },
    {
      "term": "SVI (Shot/Film) loras",
      "explanation": "Special loras that enable multi-frame input and improve consistency. Film works with multiple frames, Shot works with single frame plus reference",
      "from": "Kijai"
    },
    {
      "term": "Reference padding",
      "explanation": "Using previous frames as reference input to maintain consistency, only works with shot/talk/dance loras, not film",
      "from": "Kijai"
    },
    {
      "term": "Control embeds",
      "explanation": "Padding with reference frames in the wrapper system",
      "from": "scf"
    },
    {
      "term": "HoloCine split attention",
      "explanation": "Full method splits whole self attention (not just cross attention like | in prompts), includes RoPE stuff needed for longer generations",
      "from": "Kijai"
    },
    {
      "term": "Video latent first frame encoding",
      "explanation": "VAE encodes single image to first latent, I2V only works with single image. Can't extract single image from latent containing 4 images without VAE",
      "from": "Kijai"
    },
    {
      "term": "Temporal and spatial compression",
      "explanation": "Video latents use both temporal and spatial compression simultaneously, making latent manipulation complex",
      "from": "Kijai"
    },
    {
      "term": "Holocine prompt structure",
      "explanation": "Uses global caption followed by per-shot captions separated by | and [shot cut] markers, should specify number of shots in global caption",
      "from": "mamad8"
    },
    {
      "term": "Video attention split",
      "explanation": "Method to handle long video generation by splitting attention across video segments",
      "from": "avataraim"
    },
    {
      "term": "Multi-frame I2V extension",
      "explanation": "LongCat feature allowing extension with motion continuity using 11 frame context",
      "from": "aikitoria"
    },
    {
      "term": "Sparse inter-shot self attention",
      "explanation": "Holocine technique for handling transitions between shots",
      "from": "mamad8"
    },
    {
      "term": "HoloCine windowed attention",
      "explanation": "Frames in shot(i) are masked to attend only to global caption and caption(i) so character description in global is visible to all shots",
      "from": "shaggss"
    },
    {
      "term": "SVI noise estimation",
      "explanation": "Trying to estimate how much 'noise' or bad stuff is added in each pass and reverse it to prevent degradation",
      "from": "Ablejones"
    },
    {
      "term": "WAN VAE compression artifacts",
      "explanation": "Too highly compressed so if there is sufficient detail in an area it dies, creating noise grids",
      "from": "aikitoria"
    },
    {
      "term": "LongCat TI2V",
      "explanation": "Text-to-Image-to-Video model that combines T2V and I2V capabilities in single model",
      "from": "slmonker"
    },
    {
      "term": "Holocine shot attention",
      "explanation": "System for multi-shot video generation with sparse_flash_attn, sparse_fallback, or full attention backends",
      "from": "NebSH"
    },
    {
      "term": "Bucket resolutions",
      "explanation": "Predefined aspect ratios from 0.26 to 4.00 that LongCat expects for optimal performance",
      "from": "Kijai"
    },
    {
      "term": "LongCat architecture",
      "explanation": "New foundational model trained from scratch, different from Wan but uses same CLIP/VAE, 14B model that can do both T2V and I2V",
      "from": "Draken"
    },
    {
      "term": "Distill LoRA",
      "explanation": "Step and cfg distillation LoRA that allows using fewer steps (16 instead of 30+) and lower cfg",
      "from": "Kijai"
    },
    {
      "term": "Shot attention",
      "explanation": "Feature requiring structured prompt metadata and text_cut_positions for HoloCine multi-shot generation",
      "from": "Cubey"
    },
    {
      "term": "Block sparse attention",
      "explanation": "Efficiency mechanism in LongCat that's forced enabled when running refinement, requires flash attention implementation",
      "from": "Kijai"
    },
    {
      "term": "SVI film loras",
      "explanation": "LoRAs with 'film' in name that use 5 start frames instead of 1 for better video continuity",
      "from": "Kijai"
    },
    {
      "term": "4+1 rule",
      "explanation": "Frame count rule that LongCat follows for proper extension functionality",
      "from": "Kijai"
    },
    {
      "term": "R-RoPE (Reference Rotary Positional Encoding)",
      "explanation": "Mechanism that shifts spatial dimensions of image conditions to distinct positions from video tokens",
      "from": "Draken"
    },
    {
      "term": "Causal convolutions in VAE",
      "explanation": "Information carried forward between latent frames, need previous context to decode current frame but not future frames",
      "from": "spacepxl"
    },
    {
      "term": "LPIPS loss artifacts",
      "explanation": "VGG network in LPIPS has strided convolutions with uneven overlap and max pool causing grid artifacts",
      "from": "spacepxl"
    },
    {
      "term": "Shot attention",
      "explanation": "Requires structured prompt metadata and text_cut_positions for multi-shot video generation in Holocine",
      "from": "NebSH"
    },
    {
      "term": "Smooth window",
      "explanation": "Parameter in Holocine that creates transition effects between different scenes, needs careful planning for scene transitions",
      "from": "NebSH"
    },
    {
      "term": "Vid2vid process",
      "explanation": "Video extension method that starts at later steps, similar to really low denoise strength",
      "from": "Kijai"
    },
    {
      "term": "WAN training data limitations",
      "explanation": "WAN is trained on like 30m videos, while training on 500m videos at 24fps to 121 frames could be close to SOTA",
      "from": "Kiwv"
    },
    {
      "term": "24fps-ifier LoRA concept",
      "explanation": "Training a LoRA to make WAN work at 24fps for faster paced content that WAN currently can't handle",
      "from": "Kiwv"
    },
    {
      "term": "LongCat refinement LoRA",
      "explanation": "Special LoRA trained on model's own outputs to upscale from 480p/15fps to 720p/30fps during inference",
      "from": "JohnDopamine"
    },
    {
      "term": "Wan-based architecture",
      "explanation": "Uses Wan architecture but can be trained from scratch with modifications to model dimensions",
      "from": "Draken"
    },
    {
      "term": "Video continuation vs extension",
      "explanation": "Continuation uses latents from previous frames, extension typically re-encodes through VAE",
      "from": "aikitoria"
    },
    {
      "term": "Trilinear interpolation in LongCat refinement",
      "explanation": "3-dimensional interpolation (spatial and temporal) used in the refinement LoRA, interpolating before sampling the 2nd stage",
      "from": "Kijai"
    },
    {
      "term": "Block swap prefetch optimization",
      "explanation": "Method that gives zero speed loss by prefetching blocks, now includes LoRA data moving with block swap",
      "from": "Kijai"
    },
    {
      "term": "RGB space processing in refinement LoRA",
      "explanation": "Sample 1st stage, decode to pixel/RGB space, upscale and interpolate, encode, then sample 2nd stage with low denoise",
      "from": "Kijai"
    },
    {
      "term": "Temporal reasoning tokens",
      "explanation": "Intermediate frames in video generation that work like register tokens - model can put whatever it wants there as long as it helps reach the final result",
      "from": "spacepxl"
    },
    {
      "term": "ChronoEdit temporal reasoning mode",
      "explanation": "Uses 29 frames for generation vs 5 frames normal, modifies input for specific steps, selects first and last frame",
      "from": "Kijai"
    },
    {
      "term": "LightX2V vs Lightning naming confusion",
      "explanation": "LightX2V is team name, Lightning is different LoRA set. 2.1 LightX2V works with 2.2 low noise, 2.2 Lightning has exposure issues",
      "from": "Kijai"
    },
    {
      "term": "Context windows",
      "explanation": "Feature that lets you do longer generations with multiple chunks of 81 frames sampled at once, preserves identity better than other extension techniques but can create transition artifacts",
      "from": "blake37"
    },
    {
      "term": "Temporal reasoning mode",
      "explanation": "ChronoEdit's 29-frame mode vs normal 5-frame mode",
      "from": "Kiwv"
    },
    {
      "term": "Preview vs Full model releases",
      "explanation": "Preview means beta/still being developed, once development is done they drop the preview word but it's the same model",
      "from": "yi"
    },
    {
      "term": "Autoregressive burn in",
      "explanation": "Quality degradation that occurs when VACE extends videos autoregressively, causing color drift and coherence loss",
      "from": "Benjaminimal"
    },
    {
      "term": "Block swap for unmerged LoRAs",
      "explanation": "Unmerged loras are now part of the block swap system, making it faster and avoiding issues, rather than being fully swapped out",
      "from": "Kijai"
    }
  ],
  "resources": [
    {
      "resource": "WanAnimate Preprocessor",
      "url": "https://github.com/kijai/ComfyUI-WanAnimatePreprocess",
      "type": "repo",
      "from": "hudson223"
    },
    {
      "resource": "Wan Animate points editor tutorial",
      "url": "https://www.youtube.com/watch?v=xlsfp4Y_jEo",
      "type": "tutorial",
      "from": "CaptHook"
    },
    {
      "resource": "KJNodes resize PR",
      "url": "https://github.com/kijai/ComfyUI-KJNodes/pull/402",
      "type": "repo",
      "from": "stenandrimpy"
    },
    {
      "resource": "ComfyUI-Org Wan 2.2 Repackaged",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/tree/main/split_files/diffusion_models",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Kijai WanVideo fp8 scaled models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/I2V",
      "type": "model",
      "from": "Samy"
    },
    {
      "resource": "IntelligentVRAMNode",
      "url": "https://github.com/eddyhhlure1Eddy/IntelligentVRAMNode",
      "type": "tool",
      "from": "ulvord"
    },
    {
      "resource": "Palingenesis model",
      "url": "https://huggingface.co/eddy1111111/WAN22.XX_Palingenesis/tree/main",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "LanPaint with Wan 2.2 support",
      "url": "https://github.com/scraed/LanPaint",
      "type": "tool",
      "from": "s2k"
    },
    {
      "resource": "QWen VL Object Detection (modified)",
      "url": "https://github.com/piscesbody/Comfyui_Object_Detect_QWen_VL",
      "type": "tool",
      "from": "piscesbody"
    },
    {
      "resource": "Ovi model weights",
      "url": "https://huggingface.co/chetwinlow1/Ovi/tree/main",
      "type": "model",
      "from": "Rainsmellsnice"
    },
    {
      "resource": "DC-VideoGen paper",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1nw9x83/dcvideogen_efficient_video_generation_with_deep",
      "type": "paper",
      "from": "Izaan"
    },
    {
      "resource": "WAN22.XX_Palingenesis",
      "url": "https://huggingface.co/eddy1111111/WAN22.XX_Palingenesis/tree/main",
      "type": "model",
      "from": "Drommer-Kille"
    },
    {
      "resource": "Wan2.2-Lightning 4-step LoRA",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-250928",
      "type": "lora",
      "from": "Drommer-Kille"
    },
    {
      "resource": "Ovi model",
      "url": "https://github.com/character-ai/Ovi",
      "type": "model",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "resource": "VitPose ComfyUI",
      "url": "https://huggingface.co/Kijai/vitpose_comfy/tree/main/onnx",
      "type": "tool",
      "from": "hicho"
    },
    {
      "resource": "YOLOv10 ONNX",
      "url": "https://huggingface.co/onnx-community/YOLOv10/tree/main",
      "type": "tool",
      "from": "hicho"
    },
    {
      "resource": "Ovi HuggingFace",
      "url": "https://huggingface.co/chetwinlow1/Ovi/tree/main",
      "type": "model",
      "from": "BecauseReasons"
    },
    {
      "resource": "Wanx Troopers documentation",
      "url": "https://wanx-troopers.github.io/",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "Lightning LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-250928",
      "type": "model",
      "from": "pom"
    },
    {
      "resource": "ClownOptions Tile node",
      "url": "https://discord.com/channels/1076117621407223829/1386453178240733235/1423168728933204031",
      "type": "node",
      "from": "xwsswww"
    },
    {
      "resource": "DC-VideoGen",
      "url": "https://github.com/dc-ai-projects/DC-VideoGen",
      "type": "repo",
      "from": "Cubey"
    },
    {
      "resource": "ComfyUI Queue Manager",
      "url": "https://github.com/QuietNoise/ComfyUI-Queue-Manager",
      "type": "tool",
      "from": "hudson223"
    },
    {
      "resource": "Ovi fp8 quantized",
      "url": "https://huggingface.co/rkfg/Ovi-fp8_quantized/tree/main",
      "type": "model",
      "from": "Stad"
    },
    {
      "resource": "New 2.2 checkpoint",
      "url": "https://civitai.com/models/1995784?modelVersionId=2260110",
      "type": "model",
      "from": "Ada"
    },
    {
      "resource": "OVI GitHub repository",
      "url": "https://github.com/character-ai/Ovi/pull/11",
      "type": "repo",
      "from": "patientx"
    },
    {
      "resource": "OVI demo page",
      "url": "https://aaxwaz.github.io/Ovi/",
      "type": "demo",
      "from": "Screeb"
    },
    {
      "resource": "ComfyUI-Upscale-CUDAspeed plugin",
      "url": "https://github.com/piscesbody/ComfyUI-Upscale-CUDAspeed",
      "type": "tool",
      "from": "piscesbody"
    },
    {
      "resource": "Wan training Discord channel",
      "url": "https://discord.com/channels/1076117621407223829/1344309523187368046",
      "type": "community",
      "from": "seitanism"
    },
    {
      "resource": "Smooth mix merge model",
      "url": "https://civitai.com/models/1995784?modelVersionId=2259006",
      "type": "model",
      "from": "Juampab12"
    },
    {
      "resource": "WAN 2.2 Prompt Generator",
      "url": "https://chatgpt.com/g/g-6887849e21b8819183e20c1dc6bcf353-wan-2-2-prompt-generator?model=gpt-4o",
      "type": "tool",
      "from": "ArtificialMachine"
    },
    {
      "resource": "Matrix bullet time recreation video",
      "url": "https://www.youtube.com/watch?v=iq5JaG53dho",
      "type": "tutorial",
      "from": "Neex"
    },
    {
      "resource": "Smooth Mix Wan 2.2 I2V 14B",
      "url": "https://civitai.com/models/1995784/smooth-mix-wan-22-i2v-14b",
      "type": "model",
      "from": "Juampab12"
    },
    {
      "resource": "Face/head swap workflow",
      "url": "https://civitai.com/articles/20190",
      "type": "workflow",
      "from": "Alisson Pereira"
    },
    {
      "resource": "Wav2vec2 Chinese model for InfiniteTalk",
      "url": "",
      "type": "model",
      "from": "Dever"
    },
    {
      "resource": "Self-Forcing method discussion",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1nyxwe2/selfforcing_new_method_by_bytedance_built_upon/",
      "type": "article",
      "from": "Dream Making"
    },
    {
      "resource": "Radial Length Helper",
      "url": "https://github.com/Hud224/Radial_Length_Helper/tree/main",
      "type": "tool",
      "from": "hudson223"
    },
    {
      "resource": "WanAnimatePreprocess nodes",
      "url": "https://github.com/kijai/ComfyUI-WanAnimatePreprocess",
      "type": "repo",
      "from": "Dream Making"
    },
    {
      "resource": "ComfyUI_RH_Ovi",
      "url": "https://github.com/HM-RunningHub/ComfyUI_RH_Ovi",
      "type": "repo",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "WanVideoWrapper",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper",
      "type": "repo",
      "from": "Dream Making"
    },
    {
      "resource": "SmoothMix LoRA",
      "url": "https://civitai.com/models/1995784?modelVersionId=2260110",
      "type": "lora",
      "from": "Dream Making"
    },
    {
      "resource": "OVI ComfyUI nodes",
      "url": "https://github.com/HM-RunningHub/ComfyUI_RH_Ovi",
      "type": "repo",
      "from": "Lodis"
    },
    {
      "resource": "Lightning LoRA for Wan 2.2",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main",
      "type": "model",
      "from": "Dream Making"
    },
    {
      "resource": "RadialAttn for ComfyUI",
      "url": "https://github.com/woct0rdho/ComfyUI-RadialAttn",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Palingenesis model",
      "url": "https://huggingface.co/eddy1111111/WAN22.XX_Palingenesis/tree/main",
      "type": "model",
      "from": "phazei"
    },
    {
      "resource": "SpargeAttn package",
      "url": "https://github.com/woct0rdho/SpargeAttn/releases",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.2 5B Turbo models and LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Wan22-Turbo",
      "type": "model",
      "from": "crinklypaper"
    },
    {
      "resource": "MiniVeo3-Reasoner-Maze-5B",
      "url": "https://huggingface.co/thuml/MiniVeo3-Reasoner-Maze-5B",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Pruned WAN LoRAs collection",
      "url": "https://huggingface.co/woctordho/wan-lora-pruned/tree/main",
      "type": "model",
      "from": "woctordho"
    },
    {
      "resource": "OVI models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Ovi",
      "type": "model",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "AniSora LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/AniSora/Wan2_2_I2V_AniSora_3_2_HIGH_rank_64_fp16.safetensors",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "CineScale",
      "url": "https://github.com/Eyeline-Labs/CineScale",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "FreeScale",
      "url": "https://github.com/ali-vilab/FreeScale",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "NN Latent Upscale",
      "url": "https://github.com/Ttl/ComfyUi_NNLatentUpscale",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Latent upscale article",
      "url": "https://discord.com/channels/1076117621407223829/1425077677571838056",
      "type": "workflow",
      "from": "Lan8mark"
    },
    {
      "resource": "Flux bokeh LoRA",
      "url": "https://civitai.com/models/694867/fluxbokeh",
      "type": "lora",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "Long video generation methods",
      "url": "https://discord.com/channels/1076117621407223829/1386453178240733235",
      "type": "workflow",
      "from": "42hub"
    },
    {
      "resource": "rCM LoRA for Wan",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/rCM",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "OVI branch of WanVideoWrapper",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/ovi",
      "type": "repo",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "SageAttention Windows release",
      "url": "https://github.com/woct0rdho/SageAttention/releases/tag/v2.2.0-windows.post3",
      "type": "tool",
      "from": "Shadows"
    },
    {
      "resource": "ComfyUI acceleration helper",
      "url": "https://github.com/loscrossos/helper_comfyUI_accel",
      "type": "tool",
      "from": "\u25b2"
    },
    {
      "resource": "WanMoeKSampler fork with fixes",
      "url": "https://github.com/GalaxyTimeMachine/ComfyUI-WanMoeKSampler",
      "type": "node",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "resource": "Animal pose detection node",
      "url": "",
      "type": "node",
      "from": "happy.j"
    },
    {
      "resource": "Better segmentation for Wan Animate",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1o2sves/contextaware_video_segmentation_for_comfyui_sec4b/",
      "type": "tool",
      "from": "Shubhooooo"
    },
    {
      "resource": "ComfyUI-Ovi repository",
      "url": "https://github.com/snicolast/ComfyUI-Ovi",
      "type": "repo",
      "from": "Drommer-Kille"
    },
    {
      "resource": "MAGREF WAN 2.2 integration",
      "url": "https://github.com/MAGREF-Video/MAGREF/issues/17#issuecomment-3387138859",
      "type": "repo",
      "from": "Elvaxorn"
    },
    {
      "resource": "ComfyUI-SecNodes",
      "url": "https://github.com/9nate-drake/Comfyui-SecNodes",
      "type": "repo",
      "from": "V\u00e9role"
    },
    {
      "resource": "HF CLI blog",
      "url": "https://huggingface.co/blog/hf-cli",
      "type": "tool",
      "from": "happy.j"
    },
    {
      "resource": "HF transfer Rust tool",
      "url": "https://github.com/huggingface/hf_transfer",
      "type": "tool",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "resource": "ComfyUI Easy Install",
      "url": "https://github.com/Tavris1/ComfyUI-Easy-Install",
      "type": "tool",
      "from": "Ada"
    },
    {
      "resource": "SageAttention Windows installer",
      "url": "https://github.com/Justify87/Install-SageAttention-Windows-Comfyui",
      "type": "tool",
      "from": "Ada"
    },
    {
      "resource": "SAM 3 waitlist",
      "url": "https://ai.meta.com/sam3/",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SEC ComfyUI nodes",
      "url": "https://github.com/9nate-drake/Comfyui-SecNodes",
      "type": "node",
      "from": "ArtOfficial"
    },
    {
      "resource": "SageAttention releases",
      "url": "https://github.com/woct0rdho/SageAttention/releases",
      "type": "tool",
      "from": "ArtOfficial"
    },
    {
      "resource": "RCM LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/rCM",
      "type": "lora",
      "from": "JohnDopamine"
    },
    {
      "resource": "WanVideoWrapper workflows",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/main/example_workflows",
      "type": "workflow",
      "from": "FL13"
    },
    {
      "resource": "TAE VAE model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/taew2_2.safetensors",
      "type": "model",
      "from": "cocktailprawn1212"
    },
    {
      "resource": "Wan 2.2 I2V Lightx2v full models",
      "url": "https://huggingface.co/lightx2v/Wan2.2-I2V-A14B-Moe-Distill-Lightx2v",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "Kijai's extracted Lightx2v LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/Wan22_Lightx2v/Wan_2_2_I2V_A14B_HIGH_lightx2v_MoE_distill_lora_rank_64_bf16.safetensors",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "WanMoE KSampler with CFG adjusting",
      "url": "https://github.com/GalaxyTimeMachine/ComfyUI-WanMoeKSampler",
      "type": "node",
      "from": "Samy"
    },
    {
      "resource": "Skimmed CFG node pack",
      "url": "https://github.com/Extraltodeus/Skimmed_CFG",
      "type": "node",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "resource": "Kijai's extracted LightX2V High LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/86ac564b7ca986760f04cb6e0b4e44b31059c9db/LoRAs/Wan22_Lightx2v",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Pre-CFG ComfyUI nodes",
      "url": "https://github.com/Extraltodeus/pre_cfg_comfy_nodes_for_ComfyUI",
      "type": "tool",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "resource": "GIMM-VFI ComfyUI implementation",
      "url": "https://github.com/kijai/ComfyUI-GIMM-VFI",
      "type": "tool",
      "from": "Colin"
    },
    {
      "resource": "FastVideo sliding window attention",
      "url": "https://github.com/hao-ai-lab/FastVideo",
      "type": "repo",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "resource": "StepVideo T2V",
      "url": "https://github.com/stepfun-ai/Step-Video-T2V",
      "type": "repo",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "resource": "FlashVSR paper",
      "url": "https://zhuang2002.github.io/FlashVSR/",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "Triton 3.5.0 with fp8 fixes",
      "url": "https://github.com/woct0rdho/triton-windows/releases/tag/v3.5.0-windows.post21",
      "type": "tool",
      "from": "phazei"
    },
    {
      "resource": "Prebuilt Triton Windows wheels",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "Prebuilt SageAttention Windows wheels",
      "url": "https://github.com/woct0rdho/SageAttention",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "TaylorSeer ComfyUI node",
      "url": "https://github.com/philipy1219/ComfyUI-TaylorSeer",
      "type": "node",
      "from": "yi"
    },
    {
      "resource": "FlashVSR repository",
      "url": "https://github.com/OpenImagingLab/FlashVSR",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "Pusa Wan2.2 training dataset",
      "url": "https://huggingface.co/datasets/RaphaelLiu/PusaV1_training",
      "type": "dataset",
      "from": "JohnDopamine"
    },
    {
      "resource": "Working PyTorch nightly build",
      "url": "pip3 install --pre -U torch==2.9.0.dev20250909+cu128 torchvision==0.24.0.dev20250905+cu128 torchaudio==2.8.0.dev20250912+cu128 --index-url https://download.pytorch.org/whl/nightly/cu128",
      "type": "installation command",
      "from": "Kijai"
    },
    {
      "resource": "LoRA community tracking list",
      "url": "https://wanx-troopers.github.io/LoRA-alchemy.html",
      "type": "resource list",
      "from": "42hub"
    },
    {
      "resource": "Wan 2.2 VAE bf16",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_2_VAE_bf16.safetensors",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "SeedVR2 block swapping guide",
      "url": "https://discord.com/channels/1076117621407223829/1393656203241979924/1393656203241979924",
      "type": "workflow guide",
      "from": "JohnDopamine"
    },
    {
      "resource": "Wan2.2 4-step distill models",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Models/tree/main",
      "type": "model",
      "from": "ZeusZeus"
    },
    {
      "resource": "Wan2.2 distill LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Loras/tree/main",
      "type": "lora",
      "from": "Dever"
    },
    {
      "resource": "Wan22-animate-v2 for e5m2",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/blob/main/Wan22Animate/Wan2_2-Animate-14B_fp8_scaled_e5m2_KJ_v2.safetensors",
      "type": "model",
      "from": "Koba"
    },
    {
      "resource": "VACE GGUF versions",
      "url": "https://huggingface.co/QuantStack/Wan2.2-VACE-Fun-A14B-GGUF/tree/main",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "DimensionX",
      "url": "https://github.com/wenqsun/DimensionX",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI VideoUpscale",
      "url": "https://github.com/ShmuelRonen/ComfyUI-VideoUpscale_WithModel",
      "type": "tool",
      "from": "dipstik"
    },
    {
      "resource": "FlashWorld project",
      "url": "https://github.com/imlixinyang/FlashWorld",
      "type": "repo",
      "from": "happy.j"
    },
    {
      "resource": "rcm LoRA for WAN",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/rCM",
      "type": "lora",
      "from": "aipmaster"
    },
    {
      "resource": "CineScale LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/CineScale",
      "type": "lora",
      "from": "phazei"
    },
    {
      "resource": "WanAnimatePreprocess nodes",
      "url": "https://github.com/kijai/ComfyUI-WanAnimatePreprocess",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "UniMMVSR website",
      "url": "https://shiandu.github.io/UniMMVSR-website/",
      "type": "model",
      "from": "Shubhooooo"
    },
    {
      "resource": "FlashVSR upscale workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_1_3B_FlashVSR_upscale_example.json",
      "type": "workflow",
      "from": "Govind Singh"
    },
    {
      "resource": "rCM 720p LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/rCM/Wan_2_1_T2V_14B_720p_rCM_lora_average_rank_94_bf16.safetensors",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "rCM 480p LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/rCM/Wan_2_1_T2V_1_3B_480p_rCM_lora_average_rank_64_bf16.safetensors",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "Wan 2.5 T2I at FAL",
      "url": "https://fal.ai/models/fal-ai/wan-25-preview/text-to-image",
      "type": "model",
      "from": "Drommer-Kille"
    },
    {
      "resource": "ComfyUI HotReload fix",
      "url": "https://github.com/kijai/ComfyUI-HotReloadHack",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "InfiniteTalk example workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_I2V_InfiniteTalk_example_03.json",
      "type": "workflow",
      "from": "JohnDopamine"
    },
    {
      "resource": "Rapid AIO GGUF",
      "url": "https://huggingface.co/patientxtr/WAN2.2-14B-Rapid-AllInOne-GGUF/tree/main",
      "type": "model",
      "from": "patientx"
    },
    {
      "resource": "Ditto LoRAs converted for ComfyUI",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Ditto",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Ditto GitHub repository",
      "url": "https://github.com/EzioBy/Ditto",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Ditto models on HuggingFace",
      "url": "https://huggingface.co/QingyanBai/Ditto/tree/main/models_comfy",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Ditto dataset",
      "url": "https://huggingface.co/datasets/QingyanBai/Ditto-1M",
      "type": "dataset",
      "from": "Dever"
    },
    {
      "resource": "Krea realtime video model",
      "url": "https://huggingface.co/krea/krea-realtime-video",
      "type": "model",
      "from": "Desto Geima"
    },
    {
      "resource": "Ditto project page",
      "url": "https://editto.net/",
      "type": "webpage",
      "from": "ucren"
    },
    {
      "resource": "Ditto GitHub repo",
      "url": "https://github.com/EzioBy/Ditto",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Kijai's Ditto LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Ditto",
      "type": "lora",
      "from": "Govind Singh"
    },
    {
      "resource": "Sora watermark removal workflow",
      "url": "https://pastebin.com/LUq8Pjat",
      "type": "workflow",
      "from": "rob"
    },
    {
      "resource": "Krea realtime LoRA extracted for Wan",
      "url": "https://huggingface.co/aipmaster/krea-lora/blob/main/Wan_2_1_T2V_14B_krea_realtime_rank256_bf16.safetensors",
      "type": "model",
      "from": "aipmaster"
    },
    {
      "resource": "HPS LoRA for noise reduction",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/Wan22_FunReward/Wan2.2-Fun-A14B-InP-LOW-HPS2.1_resized_dynamic_avg_rank_15_bf16.safetensors",
      "type": "model",
      "from": "aipmaster"
    },
    {
      "resource": "MUG-V 10B video model",
      "url": "https://github.com/Shopee-MUG/MUG-V",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "WoW world model",
      "url": "https://huggingface.co/WoW-world-model/WoW-1-Wan-14B-600k/tree/main",
      "type": "model",
      "from": "asd"
    },
    {
      "resource": "MoCha model files",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/MoCha",
      "type": "model",
      "from": "CJ"
    },
    {
      "resource": "Ditto dataset mini test",
      "url": "https://huggingface.co/datasets/QingyanBai/Ditto-1M/tree/main/mini_test_videos",
      "type": "dataset",
      "from": "hicho"
    },
    {
      "resource": "MoCha model",
      "url": "https://orange-3dv-team.github.io/MoCha/",
      "type": "model",
      "from": "Dever"
    },
    {
      "resource": "Krea realtime video model",
      "url": "https://huggingface.co/krea/krea-realtime-video",
      "type": "model",
      "from": "s2k"
    },
    {
      "resource": "Krea LoRA",
      "url": "https://huggingface.co/aipmaster/krea-lora/tree/main",
      "type": "lora",
      "from": "Dever"
    },
    {
      "resource": "Rolling Forcing",
      "url": "https://huggingface.co/TencentARC/RollingForcing",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Updated Wan 2.2 Distill LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Loras/tree/main",
      "type": "lora",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "resource": "ReferEverything",
      "url": "https://github.com/miccooper9/ReferEverything",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Kaleido reference to video",
      "url": "https://github.com/CriliasMiller/Kaleido-OpenSourced",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "QwenImage VAE ComfyUI format",
      "url": "https://huggingface.co/Kijai/QwenImage_experimental/blob/main/e2e-qwenimage-vae_comfy_fp32.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Rolling Force model",
      "url": "https://huggingface.co/TencentARC/RollingForcing/tree/main",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "New LightX2V LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Loras/tree/main",
      "type": "lora",
      "from": "Ada"
    },
    {
      "resource": "Stable Video Infinity",
      "url": "https://github.com/vita-epfl/Stable-Video-Infinity",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI wheels for Triton",
      "url": "https://github.com/Comfy-Org/wheels/actions/runs/17170280663",
      "type": "tool",
      "from": "justinj"
    },
    {
      "resource": "TREAD training method",
      "url": "https://arxiv.org/abs/2501.04765",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "End-to-end VAE paper",
      "url": "https://arxiv.org/pdf/2504.10483",
      "type": "repo",
      "from": "mamad8"
    },
    {
      "resource": "New lightx2v LoRA",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Loras/blob/main/wan2.2_i2v_A14b_high_noise_lora_rank64_lightx2v_4step_1022.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SVI model repository",
      "url": "https://huggingface.co/vita-video-gen/svi-model/tree/main/version-1.0",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "VHS Video Helper Suite",
      "url": "https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "SVI examples and homepage",
      "url": "https://stable-video-infinity.github.io/homepage/",
      "type": "repo",
      "from": "Dever"
    },
    {
      "resource": "Qwen next-scene LoRA v2",
      "url": "https://huggingface.co/lovis93/next-scene-qwen-image-lora-2509",
      "type": "model",
      "from": "Dever"
    },
    {
      "resource": "SVI LoRA models",
      "url": "https://huggingface.co/vita-video-gen/svi-model",
      "type": "model",
      "from": "garbus"
    },
    {
      "resource": "Video-As-Prompt repository",
      "url": "https://github.com/bytedance/Video-As-Prompt",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "VAP HuggingFace models",
      "url": "https://huggingface.co/ByteDance/Video-As-Prompt-Wan2.1-14B/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "LightX2V Autoencoders",
      "url": "https://huggingface.co/lightx2v/Autoencoders/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Ditto models",
      "url": "https://huggingface.co/QingyanBai/Ditto_models/tree/main",
      "type": "model",
      "from": "Gentleman bunny"
    },
    {
      "resource": "Wan community knowledge base",
      "url": "https://nathanshipley.notion.site/Wan-2-1-Knowledge-Base-1d691e115364814fa9d4e27694e9468f",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "Wanx Troopers updates",
      "url": "https://wanx-troopers.github.io/",
      "type": "tool",
      "from": "JohnDopamine"
    },
    {
      "resource": "SVI LoRAs converted for ComfyUI native",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Stable-Video-Infinity",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "SVI film LoRA fp16",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/Stable-Video-Infinity/svi-film_lora_rank_128_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Video-as-Prompt paper",
      "url": "openreview.net/pdf/8171590dbc3bf6b0150a5d8db4e4e66286b2d4c9.pdf",
      "type": "research",
      "from": "Prelifik"
    },
    {
      "resource": "LTX 2 playground",
      "url": "https://app.ltx.studio/ltx-2-playground/t2v",
      "type": "tool",
      "from": "yi"
    },
    {
      "resource": "Custom mask input node for native ComfyUI",
      "url": "",
      "type": "node",
      "from": "Ablejones"
    },
    {
      "resource": "HoloCine fp8 weights",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/T2V/HoloCine",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "HoloCine official demo site",
      "url": "https://holo-cine.github.io/",
      "type": "demo",
      "from": "VK (5080 128gb)"
    },
    {
      "resource": "WanX Troopers timeline and LoRA tracking",
      "url": "https://wanx-troopers.github.io/timeline.html",
      "type": "documentation",
      "from": "42hub"
    },
    {
      "resource": "SVI models via Kijai",
      "url": "https://huggingface.co/vita-video-gen/svi-model",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "Aquif-Dream-6B-Exp alternative to Ovi",
      "url": "https://huggingface.co/aquif-ai/aquif-Dream-6B-Exp",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "HoloCine",
      "url": "https://github.com/yihao-meng/HoloCine",
      "type": "repo",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "resource": "HoloCine sparse model",
      "url": "https://huggingface.co/hlwang06/HoloCine/tree/main/HoloCine_dit/sparse",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "UniLumos model",
      "url": "https://huggingface.co/Alibaba-DAMO-Academy/UniLumos/tree/main",
      "type": "model",
      "from": "DawnII"
    },
    {
      "resource": "Multitalk + unianimate workflow example",
      "url": "https://www.reddit.com/r/comfyui/comments/1lsb5a1/testing_wan_21_multitalk_unianimate_lora_kijai/",
      "type": "workflow",
      "from": "Shubhooooo"
    },
    {
      "resource": "Film SVI workflow",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1431178558205726842",
      "type": "workflow",
      "from": "VK (5080 128gb)"
    },
    {
      "resource": "OpenModelDB for upscaling",
      "url": "https://openmodeldb.info/",
      "type": "resource",
      "from": "Dever"
    },
    {
      "resource": "HoloCine fp8 models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/T2V/HoloCine",
      "type": "model",
      "from": "seitanism"
    },
    {
      "resource": "SVI training script with dataset",
      "url": "https://github.com/vita-epfl/Stable-Video-Infinity/blob/main/scripts/train/svi_film.sh",
      "type": "repo",
      "from": "Ablejones"
    },
    {
      "resource": "SD-Latent-Interposer",
      "url": "https://github.com/city96/SD-Latent-Interposer",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "SVI film workflow for 2.2",
      "url": "https://discord.com/channels/1076117621407223829/1396263390296674324/1431508434431246336",
      "type": "workflow",
      "from": "DawnII"
    },
    {
      "resource": "LongCat-Video",
      "url": "https://huggingface.co/meituan-longcat/LongCat-Video",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "LongCat technical report",
      "url": "https://github.com/meituan-longcat/LongCat-Video/blob/main/longcatvideo_tech_report.pdf",
      "type": "documentation",
      "from": "yi"
    },
    {
      "resource": "Holocine FP8 models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/T2V/HoloCine",
      "type": "model",
      "from": "avataraim"
    },
    {
      "resource": "NotebookLLM with scraped messages",
      "url": "https://notebooklm.google.com/notebook/a08901b9-0511-4926-bbf8-3c86a12dc306",
      "type": "tool",
      "from": "Karo"
    },
    {
      "resource": "HoloCine fp8 models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/T2V/HoloCine",
      "type": "model",
      "from": "avataraim"
    },
    {
      "resource": "UltraGen 4K video generation",
      "url": "https://github.com/sjtuplayer/UltraGen/",
      "type": "repo",
      "from": "Shubhooooo"
    },
    {
      "resource": "Inpaint4Drag",
      "url": "https://github.com/Visual-AI/Inpaint4Drag",
      "type": "tool",
      "from": "ulvord"
    },
    {
      "resource": "LongCat discussion thread",
      "url": "https://github.com/meituan-longcat/LongCat-Video/issues/4#issuecomment-3448130131",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "LongCat ComfyUI models",
      "url": "https://huggingface.co/Kijai/LongCat-Video_comfy/tree/main",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "LongCat testing branch",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/longcat",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "LongCat test image",
      "url": "https://github.com/meituan-longcat/LongCat-Video/blob/main/assets/girl.png",
      "type": "resource",
      "from": "Kijai"
    },
    {
      "resource": "Holocine nodes implementation",
      "url": "https://github.com/Dango233/ComfyUI-WanVideoWrapper/tree/main",
      "type": "repo",
      "from": "shaggss"
    },
    {
      "resource": "Holocine FP8 models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/T2V/HoloCine",
      "type": "model",
      "from": "Shubhooooo"
    },
    {
      "resource": "LongCat testing workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/longcat/LongCat/longcat_i2v_testing.json",
      "type": "workflow",
      "from": "JohnDopamine"
    },
    {
      "resource": "SageAttention 2.2.0",
      "url": "https://github.com/woct0rdho/SageAttention",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "HoloCine examples and documentation",
      "url": "https://holo-cine.github.io/",
      "type": "documentation",
      "from": "avataraim"
    },
    {
      "resource": "LongCat examples",
      "url": "https://meituan-longcat.github.io/LongCat-Video",
      "type": "documentation",
      "from": "Pandaabear"
    },
    {
      "resource": "Video-As-Prompt (VAP)",
      "url": "https://github.com/bytedance/Video-As-Prompt",
      "type": "repo",
      "from": "DawnII"
    },
    {
      "resource": "LongCat models and distill lora",
      "url": "https://huggingface.co/Kijai/LongCat-Video_comfy/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "SVI LoRAs",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Stable-Video-Infinity",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "LongCat branch of WanVideoWrapper",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/tree/longcat/LongCat",
      "type": "repo",
      "from": "avataraim"
    },
    {
      "resource": "Triton Windows wheel",
      "url": "https://github.com/woct0rdho/triton-windows/releases",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "MediaSyncer video comparison tool",
      "url": "https://phazei.github.io/MediaSyncer/",
      "type": "tool",
      "from": "phazei"
    },
    {
      "resource": "PyTorch and dependencies setup commands",
      "url": "pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128",
      "type": "tool",
      "from": "avataraim"
    },
    {
      "resource": "Wan 2.1 VAE 2x upscaler",
      "url": "https://huggingface.co/spacepxl/Wan2.1-VAE-upscale2x",
      "type": "model",
      "from": "spacepxl"
    },
    {
      "resource": "ComfyUI-VAE-Utils",
      "url": "https://github.com/spacepxl/ComfyUI-VAE-Utils",
      "type": "repo",
      "from": "spacepxl"
    },
    {
      "resource": "Ditto with Wan enhancement",
      "url": "https://github.com/EzioBy/Ditto/commit/9912e339c2add7f01f1095b3c06cf228177c79e9",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Kaleido model",
      "url": "https://huggingface.co/Crilias/Kaleido-14B-S2V/tree/main",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "Kaleido experimental fp16",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Kaleido/Wan_2_1_kaleido_14B-S2V_experimental_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Deconv checkerboard explanation",
      "url": "https://distill.pub/2016/deconv-checkerboard/",
      "type": "resource",
      "from": "spacepxl"
    },
    {
      "resource": "ComfyUI-WanAnimate-Enhancer",
      "url": "https://github.com/wallen0322/ComfyUI-WanAnimate-Enhancer",
      "type": "tool",
      "from": "A.I.Warper"
    },
    {
      "resource": "ComfyUI-WanVideoWrapper-Multishot",
      "url": "https://github.com/Dango233/ComfyUI-WanVideoWrapper-Multishot",
      "type": "repo",
      "from": "NebSH"
    },
    {
      "resource": "ComfyUI-VAE-Utils",
      "url": "https://github.com/spacepxl/ComfyUI-VAE-Utils",
      "type": "repo",
      "from": "Quality_Control"
    },
    {
      "resource": "The Transformation Engine dev branch",
      "url": "https://github.com/fblissjr/the-transformation-engine/tree/dev",
      "type": "tool",
      "from": "fredbliss"
    },
    {
      "resource": "ComfyUI-QwenImageWanBridge",
      "url": "https://github.com/fblissjr/ComfyUI-QwenImageWanBridge",
      "type": "repo",
      "from": "fredbliss"
    },
    {
      "resource": "LongCat refinement LoRA",
      "url": "https://huggingface.co/Kijai/LongCat-Video_comfy/tree/main",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "DITTO with Denoise Enhancing",
      "url": "https://github.com/EzioBy/Ditto?tab=readme-ov-file#denoising-enhancing",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Comfyui-Memory_Cleanup",
      "url": "https://github.com/LAOGOU-666/Comfyui-Memory_Cleanup",
      "type": "tool",
      "from": "avataraim"
    },
    {
      "resource": "LongCat workflow by Kijai",
      "url": "https://discord.com/channels/1076117621407223829/1342763350815277067/1433114940385923192",
      "type": "workflow",
      "from": "JohnDopamine"
    },
    {
      "resource": "Morph frames-to-video LoRA",
      "url": "https://huggingface.co/morphic/Wan2.2-frames-to-video",
      "type": "model",
      "from": "VK (5080 128gb)"
    },
    {
      "resource": "Next Scene QWen Edit LoRA",
      "url": "https://huggingface.co/lovis93/next-scene-qwen-image-lora-2509",
      "type": "model",
      "from": "JohnDopamine"
    },
    {
      "resource": "VACE module 1.3B",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/Wan2_1-VACE_module_1_3B_bf16.safetensors",
      "type": "model",
      "from": "Lodis"
    },
    {
      "resource": "ChronoEdit by NVIDIA",
      "url": "https://github.com/nv-tlabs/ChronoEdit",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Wan2.2 SDNQ optimization",
      "url": "https://huggingface.co/Disty0/Wan2.2-I2V-A14B-SDNQ-uint4-svd-r32",
      "type": "model",
      "from": "Ada"
    },
    {
      "resource": "SDNQ optimization tool",
      "url": "https://github.com/Disty0/sdnq",
      "type": "repo",
      "from": "Ada"
    },
    {
      "resource": "LightX2V 1030 LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/LoRAs/Wan22_Lightx2v/Wan_2_2_I2V_A14B_HIGH_lightx2v_4step_lora_v1030_rank_64_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "ChronoEdit demo",
      "url": "https://huggingface.co/spaces/nvidia/ChronoEdit",
      "type": "demo",
      "from": "JohnDopamine"
    },
    {
      "resource": "Qwen music model announcement",
      "url": "https://x.com/JustinLin610/status/1982052327180918888",
      "type": "announcement",
      "from": "JohnDopamine"
    },
    {
      "resource": "Radial Attention Wan2.2 commit",
      "url": "https://github.com/mit-han-lab/radial-attention/commit/af371cad086ad0092a3320c1f8dc8d091b18e2a6",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "SDNQ Quantization method",
      "url": "https://github.com/vladmandic/sdnext/wiki/SDNQ-Quantization",
      "type": "repo",
      "from": "JohnDopamine"
    },
    {
      "resource": "ComfyUI WanAnimate Enhancer",
      "url": "https://github.com/wallen0322/ComfyUI-WanAnimate-Enhancer",
      "type": "tool",
      "from": "\u02d7\u02cf\u02cb\u26a1\u02ce\u02ca-"
    },
    {
      "resource": "SkyReels V2 I2V 1.3B model",
      "url": "https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "BRIA FIBO model",
      "url": "https://huggingface.co/briaai/FIBO",
      "type": "model",
      "from": "Tony(5090)"
    },
    {
      "resource": "CamCloneMaster project",
      "url": "https://camclonemaster.github.io/",
      "type": "project",
      "from": "Yan"
    },
    {
      "resource": "Video-As-Prompt model",
      "url": "https://huggingface.co/ByteDance/Video-As-Prompt-Wan2.1-14B",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "Morphic Wan2.2 frames-to-video",
      "url": "https://huggingface.co/morphic/Wan2.2-frames-to-video",
      "type": "model",
      "from": "hicho"
    },
    {
      "resource": "ChronoEdit full model",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/ChronoEdit/Wan2_1-I2V-14B_ChronoEdit_fp16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "ChronoEdit distill LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/ChronoEdit/Wan_2_1_I2V_14B_ChronoEdit_distill_lora_rank32.safetensors",
      "type": "lora",
      "from": "Kijai"
    },
    {
      "resource": "ChronoEdit FP8 versions",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/ChronoEdit",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "LightX2V 1030 LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/LoRAs/Wan22_Lightx2v",
      "type": "lora",
      "from": "Gleb Tretyak"
    },
    {
      "resource": "ChronoEdit prompting guide",
      "url": "https://github.com/nv-tlabs/ChronoEdit/blob/main/docs/PROMPT_GUIDANCE.md",
      "type": "documentation",
      "from": "Lodis"
    },
    {
      "resource": "KJNodes LoRA extraction node",
      "url": "",
      "type": "node",
      "from": "Hashu"
    },
    {
      "resource": "LightX2V team LoRAs",
      "url": "https://huggingface.co/lightx2v/Wan2.2-Distill-Loras/tree/main",
      "type": "lora",
      "from": "Elvaxorn"
    },
    {
      "resource": "ChronoEdit models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/ChronoEdit",
      "type": "model",
      "from": "Kiwv"
    },
    {
      "resource": "ChronoEdit 14B fp16",
      "url": "https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/blob/main/split_files/diffusion_models/chrono_edit_14B_fp16.safetensors",
      "type": "model",
      "from": "comfy"
    },
    {
      "resource": "ChronoEdit fp8 version",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/tree/main/ChronoEdit",
      "type": "model",
      "from": "Lodis"
    },
    {
      "resource": "FlashVSR v1.1",
      "url": "https://huggingface.co/JunhaoZhuang/FlashVSR-v1.1/tree/main",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "ChronoEdit GGUF",
      "url": "https://huggingface.co/QuantStack/ChronoEdit-14B-GGUF",
      "type": "model",
      "from": "YarvixPA"
    },
    {
      "resource": "Qwen VL ComfyUI implementations",
      "url": "https://github.com/SXQBW/ComfyUI-Qwen-VL, https://github.com/alexcong/ComfyUI_QwenVL, https://github.com/1038lab/ComfyUI-QwenVL",
      "type": "repo",
      "from": "aikitoria"
    },
    {
      "resource": "FlashVSR workflow",
      "url": "https://github.com/kijai/ComfyUI-WanVideoWrapper/blob/main/example_workflows/wanvideo_1_3B_FlashVSR_upscale_example.json",
      "type": "workflow",
      "from": "DawnII"
    },
    {
      "resource": "LongCat face fix workflow",
      "url": "provided as image",
      "type": "workflow",
      "from": "Ablejones"
    },
    {
      "resource": "Raylight - GPU pooling solution",
      "url": "https://github.com/komikndr/raylight",
      "type": "tool",
      "from": "Kinasato"
    },
    {
      "resource": "ChronoEdit",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/tree/main/ChronoEdit",
      "type": "tool",
      "from": "Ness"
    },
    {
      "resource": "Sora 2 prompting guide",
      "url": "https://cookbook.openai.com/examples/sora/sora2_prompting_guide",
      "type": "resource",
      "from": "Draken"
    },
    {
      "resource": "ChronoEdit distill LoRA",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy/blob/main/ChronoEdit/Wan_2_1_I2V_14B_ChronoEdit_distill_lora_rank32.safetensors",
      "type": "model",
      "from": "mamad8"
    },
    {
      "resource": "Kijai GGUF models",
      "url": "https://huggingface.co/Kijai/WanVideo_comfy_GGUF/tree/main",
      "type": "model",
      "from": "xwsswww"
    }
  ],
  "limitations": [
    {
      "limitation": "Wan 2.2 High Noise model fails with large inpaint masks",
      "details": "Performance depends on mask size, smaller masks work better",
      "from": "Ablejones"
    },
    {
      "limitation": "None of the i2v WAN models understand masking",
      "details": "Makes it difficult to work with different backgrounds in keyframes",
      "from": "km"
    },
    {
      "limitation": "Memory issues persist even with 64GB RAM",
      "details": "Video models have enormous memory requirements, may need 80GB+ swap",
      "from": "Ablejones"
    },
    {
      "limitation": "Current Wan 2.5 lipsync quality is poor",
      "details": "Lipsync performance described as 'terrible'",
      "from": "dj47"
    },
    {
      "limitation": "Palingenesis model lacks documentation",
      "details": "No model card description, difficult to understand what it actually improves",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "limitation": "Face detection fails on certain aspect ratios",
      "details": "Swapping width/height in resolution causes detection to fail completely",
      "from": "Gateway {Dreaming Computers}"
    },
    {
      "limitation": "SageAttention3 incompatible with Wan",
      "details": "Produces artifacts for Wan, workaround exists but not worth it",
      "from": "yi"
    },
    {
      "limitation": "WanAnimate character consistency issues",
      "details": "Poor character consistency (2/10) with certain settings combinations",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "Palingenesis shows no noticeable improvement",
      "details": "Multiple users report no visible difference from standard model, looks like different seeds",
      "from": "Draken"
    },
    {
      "limitation": "WanAnimate face similarity issues",
      "details": "Can't get face similarity without LightX LoRA, subjects change ethnicity (Japanese appears Indian/African)",
      "from": "Christian Sandor"
    },
    {
      "limitation": "Topaz still has plastic skin issues",
      "details": "Despite generative upscaler improvements, plastic skin artifacts remain",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Official WanAnimate pipeline unclear",
      "details": "Kijai admits not understanding how official WanAnimate is supposed to work with CFG 1.0",
      "from": "Kijai"
    },
    {
      "limitation": "Context windows have coherency issues",
      "details": "Temporal coherency between windows is probably worse and processing is much slower",
      "from": "Kijai"
    },
    {
      "limitation": "HuMO short generation length",
      "details": "Becomes unusable over certain time duration",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "limitation": "720p resolution on 3060",
      "details": "720p generations rare on 3060 due to VRAM constraints",
      "from": "mdkb"
    },
    {
      "limitation": "Sage 3 quality degradation",
      "details": "Significant quality loss, speed benefit minimal when limiting to certain steps/blocks",
      "from": "Kijai"
    },
    {
      "limitation": "One Trainer doesn't support Wan models",
      "details": "Lacks support for Wan model training",
      "from": "xwsswww"
    },
    {
      "limitation": "OVI English only",
      "details": "Currently only supports English language prompts and speech generation",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "OVI lacks scene changes and POV cuts",
      "details": "Missing important cinematic features like scene transitions that Sora 2 has",
      "from": "Rainsmellsnice"
    },
    {
      "limitation": "Wan 2.2 5B performance issues",
      "details": "Significantly slower than 14B version due to VAE overhead, questionable performance gains",
      "from": "Draken"
    },
    {
      "limitation": "OVI sound effects quality",
      "details": "Can do sound effects but not as good as speech generation",
      "from": "Screeb"
    },
    {
      "limitation": "Smooth mix can't be used with CFG",
      "details": "Merged lightx or lightning lora prevents CFG usage",
      "from": "Lumifel"
    },
    {
      "limitation": "OVI requires 24GB VRAM in fp8",
      "details": "5B distill likely won't work due to increased parameters and 5B VAE computational requirements",
      "from": "Draken"
    },
    {
      "limitation": "Wan 2.1 refuses helmet prompts",
      "details": "Simple prompts like putting on helmet don't work well",
      "from": "Dream Making"
    },
    {
      "limitation": "OVI has no ComfyUI support yet",
      "details": "Only gradio demo available, implementation may be half-baked",
      "from": "TK_999"
    },
    {
      "limitation": "Lip-sync quality degrades after 5 seconds",
      "details": "Most models drift in dialogue scenes, viewers notice small discrepancies",
      "from": "mdkb"
    },
    {
      "limitation": "Lightning LoRA causes child generation bias",
      "details": "With certain LoRAs, model has insane bias towards generating kids only",
      "from": "Dream Making"
    },
    {
      "limitation": "Merging High and Low models causes zero-g effect",
      "details": "Even 4% merge of High into Low causes floating objects",
      "from": "Thom293"
    },
    {
      "limitation": "Character LoRAs from 2.1 don't work well on 2.2",
      "details": "LoRAs trained on Wan 2.1 T2V perform badly on Wan 2.2 T2V",
      "from": "Dream Making"
    },
    {
      "limitation": "I2V face degradation",
      "details": "I2V shows noticeable face degradation and overly intense muscle movement",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "OVI NSFW capability limited",
      "details": "OVI has some NSFW capability but just barely",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "14B model architecture restriction",
      "details": "14B model has separate I2V and T2V, only 5B model supports both unified",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "limitation": "SageAttn3 only works on Blackwell cards",
      "details": "Limited to B200/B300 cards with sm120a, won't work on older GPUs like RTX 6000 Pro",
      "from": "Kijai"
    },
    {
      "limitation": "Burned-in look from multiple I2V continuations",
      "details": "Can get about 4 continuations before things start getting blown out due to VAE decoding and extending issues",
      "from": "voxJT"
    },
    {
      "limitation": "Wan 2.2 motion coherence worse than 2.1",
      "details": "While quality is better, motion coherence is not as good as 2.1",
      "from": "Dream Making"
    },
    {
      "limitation": "Most LoRAs not upgraded for 2.2",
      "details": "Only LightX got 2.2 version, CausVid and Movigen still on 2.1",
      "from": "Dream Making"
    },
    {
      "limitation": "SAGE3 attention speedup has limited benefit with distill LoRAs",
      "details": "With 4-step distill LoRA, SAGE3 only applies to 2 steps, loses quality and gains almost nothing",
      "from": "Kijai"
    },
    {
      "limitation": "Attention speedups can randomly harm video quality",
      "details": "Hard to predict when important attention connections are lost, causing random quality degradation",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "limitation": "Direct latent upscaling poorly implemented",
      "details": "Cannot upscale latents with good quality without first decoding to images",
      "from": "Lan8mark"
    },
    {
      "limitation": "VACE 2.1 14B only supports T2V LoRAs",
      "details": "vace is t2v, some keys will load i think, but you most likely want to use the light2x t2v distill lora. It will load 'some stuff' from the i2v lora, but not well at all",
      "from": "Draken"
    },
    {
      "limitation": "Lynx flashing issue with 121 frames",
      "details": "it just doesn't like the 121 frames I guess",
      "from": "Kijai"
    },
    {
      "limitation": "OVI lacks voice consistency",
      "details": "the lack of voice consistency is an issue for anything other than messing around",
      "from": "AiAuteur"
    },
    {
      "limitation": "Latent upscaling destroys fine details",
      "details": "Latent upscaling is when you upscale H and W in latent space it usually leads to artifacts or blurry frames its useless for i2v even t2v coz it destroys the face/character likeness eyes any sort of fine details",
      "from": "shaggss"
    },
    {
      "limitation": "VAE latents don't follow normal 2D geometry",
      "details": "The issue is VAE latents don't follow normal 2D geometry. They do a little bit though so it kinda works with errors. Same reason slicing VAE for decode is lossy",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "limitation": "OVI model currently 5 seconds maximum length",
      "details": "Audio-video generation limited to 5 second outputs",
      "from": "Juampab12"
    },
    {
      "limitation": "e4m3fn_scaled doesn't work on RTX 3000 series",
      "details": "Must use e5m2 instead on 3090 and similar GPUs",
      "from": "Kijai"
    },
    {
      "limitation": "Tiled VAE issues with VACE extend",
      "details": "Creates blurry output and position shifts at certain resolutions like 1280x720 and 1920x1072",
      "from": "Lumifel"
    },
    {
      "limitation": "Many 1.3B models are proof of concept only",
      "details": "Not practically useful compared to 5B alternatives",
      "from": "Kijai"
    },
    {
      "limitation": "Sage 3 has considerable quality drop",
      "details": "While faster, the quality hit makes it not worth upgrading to",
      "from": "HeadOfOliver"
    },
    {
      "limitation": "FP8 quantization quality loss",
      "details": "FP8 makes everything blurry and motion is far worse, especially bad for WAN",
      "from": "Ada"
    },
    {
      "limitation": "OVI camera angle prompting",
      "details": "OVI interprets cinema angles as part of dialogue even when placed elsewhere in prompt",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "OVI style consistency",
      "details": "Unless style is specified, OVI t2v jumps between realism and anime styles",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Face degradation in longer videos",
      "details": "Using last frame as input for next generation causes face quality to progressively worsen",
      "from": "Tachyon"
    },
    {
      "limitation": "WAN upscaler transitions",
      "details": "Transitions between 5-second clips are still slightly noticeable, needs soft padding",
      "from": "Lan8mark"
    },
    {
      "limitation": "SEC small object detection",
      "details": "Segmentation models like SEC likely have minimum bbox dimensions, may miss very small objects",
      "from": "psylent_gamer"
    },
    {
      "limitation": "TAE VAE quality loss",
      "details": "TAE VAE is only for preview, causes significant quality degradation for final outputs",
      "from": "Draken"
    },
    {
      "limitation": "CLIP embeds not used in 2.2 I2V",
      "details": "CLIP embeds don't make difference in Wan 2.2 I2V workflows",
      "from": "Draken"
    },
    {
      "limitation": "Radial attention quality hit",
      "details": "Radial attention reduces expressiveness, particularly noticeable in lip sync applications",
      "from": "blake37"
    },
    {
      "limitation": "LightX 2.2 LoRA quality issues",
      "details": "Most LightX 2.2 LoRAs have poor quality compared to 2.1 versions",
      "from": "Dream Making"
    },
    {
      "limitation": "First frame flash in VACE 2.2",
      "details": "VACE 2.2 has persistent first frame flashing issue that's difficult to resolve completely",
      "from": "mdkb"
    },
    {
      "limitation": "30xx cards cannot use e4m3fn format",
      "details": "Need to use e5m2 format for 30xx series GPUs",
      "from": "mdkb"
    },
    {
      "limitation": "Official Lightx2v LoRA produces glitchy output at low steps",
      "details": "Needs 12+ steps to work properly, defeats purpose of speed LoRA",
      "from": "Ashtar"
    },
    {
      "limitation": "T2V distill LoRAs greatly diminish movement when used on I2V",
      "details": "Cross-compatibility between T2V and I2V LoRAs is poor",
      "from": "Ada"
    },
    {
      "limitation": "LoRA cannot patch time_embedding and text_embedding layers",
      "details": "ComfyUI limitation prevents full LoRA application to all trained layers",
      "from": "JohnDopamine"
    },
    {
      "limitation": "Wan 2.2 slow motion issues",
      "details": "Still has slow motion effect with new distill models in 4 or 8 steps CFG1. Some call it 'cinematic' but it's actually slow motion",
      "from": "V\u00e9role"
    },
    {
      "limitation": "StepVideo VAE artifacts",
      "details": "Can have VAE artifacts due to more compressive VAE, pros and cons",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "limitation": "OVI 5B model quality",
      "details": "Has a '5B look' in video quality that's noticeable, probably could be refined with another pass",
      "from": "Kijai"
    },
    {
      "limitation": "New LightX2V vertical line artifacts",
      "details": "Artifacts happen on widescreen/16:9 aspect ratios for both LoRA and finetune versions, but not on portrait images",
      "from": "blake37"
    },
    {
      "limitation": "FlashVSR speed only on H100",
      "details": "The 11GB usage and speed benefits only really work at those speeds on H100/etc, big caveat for consumer GPUs",
      "from": "JohnDopamine"
    },
    {
      "limitation": "TaylorSeer quality degrades with high movement",
      "details": "Works well for static scenes but struggles with dynamic content",
      "from": "Zabo"
    },
    {
      "limitation": "FP8 models don't work with quantization on older GPUs",
      "details": "RTX 3060 and similar cards need quantization disabled",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "limitation": "No good solution for pose detection with distant characters",
      "details": "Current pose detection struggles when character is far from viewer",
      "from": "Kijai"
    },
    {
      "limitation": "Wan 2.2 generates noise with 20 steps without fast LoRA",
      "details": "Specific issue with step count and model combination",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "Distillation LoRAs fail to produce dynamic motion like CFG for Wan 2.2 high noise",
      "details": "Many attempts made but none match the motion quality of using CFG",
      "from": "Kijai"
    },
    {
      "limitation": "FlashVSR produces more artifacts at higher resolutions",
      "details": "Quality degradation increases with resolution, also has excessive sharpness issues",
      "from": "Elvaxorn"
    },
    {
      "limitation": "TaylorSeer has poor prompt adherence",
      "details": "Results are very wild and not recommended despite speed",
      "from": "Tachyon"
    },
    {
      "limitation": "Wan2.2 VAE noise grids",
      "details": "Generates artifacts especially with sharp images of people with hair, enhanced by SeedVR2",
      "from": "aikitoria"
    },
    {
      "limitation": "VACE strength parameter cannot be separated",
      "details": "Overall strength applies to both reference image and control video, cannot control independently",
      "from": "Kijai"
    },
    {
      "limitation": "No VACE for 5B model",
      "details": "VACE currently only exists for 14B model",
      "from": "Kijai"
    },
    {
      "limitation": "New distill models are I2V only",
      "details": "No T2V version available",
      "from": "Lodis"
    },
    {
      "limitation": "Wan artifacts at 17 frames",
      "details": "1 second videos create weird artifacts at the start",
      "from": "yi"
    },
    {
      "limitation": "Lightning LoRAs make everything brighter",
      "details": "Have annoying side effects that affect image brightness",
      "from": "Kijai"
    },
    {
      "limitation": "FlashVSR bad for distant faces",
      "details": "Produces poor results for faces that are not very close, faces at distance tend to fall apart",
      "from": "Tony(5090)"
    },
    {
      "limitation": "Most 2.2 LoRAs are crap while 2.1 LoRAs are bad at movement",
      "details": "General quality issues with different LoRA versions for different use cases",
      "from": "Dream Making"
    },
    {
      "limitation": "MOE LoRA doesn't work well in existing I2V workflows",
      "details": "Produces wobbly outputs when used in standard I2V setups",
      "from": "Draken"
    },
    {
      "limitation": "Wan 2.2 non-5B models not true 24fps",
      "details": "Only 5B model supports true 24fps, others have frame rate limitations",
      "from": "garbus"
    },
    {
      "limitation": "OVI poor consistency even at 720p",
      "details": "Terrible consistency issues despite good prompt following and expression",
      "from": "mdkb"
    },
    {
      "limitation": "rCM requires 4 steps minimum",
      "details": "Despite claims of 1-step generation, actual usage requires 4 steps, 1-step not demonstrated",
      "from": "Kijai"
    },
    {
      "limitation": "FP8 dramatic quality reduction",
      "details": "Especially noticeable in video extension tasks and longer generations",
      "from": "Zabo"
    },
    {
      "limitation": "HuMo doesn't do long videos well",
      "details": "Works well with loras but struggles with longer video generation",
      "from": "Ablejones"
    },
    {
      "limitation": "Ditto LoRAs only work well with people/humanoids",
      "details": "Training was mostly around humans, doesn't work well with other subjects like animals",
      "from": "Dever"
    },
    {
      "limitation": "Ditto tends to inject unwanted humans into scenes",
      "details": "Adds faces or people even when not prompted, especially with pixel art style",
      "from": "Dream Making"
    },
    {
      "limitation": "Style transfer can lose original movement/motion",
      "details": "Movement is often lost during style transfer process",
      "from": "Dream Making"
    },
    {
      "limitation": "Lipsync models struggle with eye direction control",
      "details": "Hard to get speakers to look in specific directions, crucial for dialogue scenes",
      "from": "mdkb"
    },
    {
      "limitation": "Ditto breaks other VACE functionalities",
      "details": "Style transfer LoRAs are incompatible with other VACE features",
      "from": "Draken"
    },
    {
      "limitation": "Pose estimation limited to basic movements",
      "details": "Current pose estimators can't handle complex movements well",
      "from": "Draken"
    },
    {
      "limitation": "Ditto works best with specific trained instructions",
      "details": "Works with exact instructions used to train the model, may not generalize well",
      "from": "Govind Singh"
    },
    {
      "limitation": "Krea model very noisy in normal workflows",
      "details": "Rough on normal workflows, supposed to be used 3 latents at a time like causvid",
      "from": "Kijai"
    },
    {
      "limitation": "Krea realtime quality is disappointing even at 14B",
      "details": "Model has to lose tons of quality to handle the strange context",
      "from": "Kijai"
    },
    {
      "limitation": "Krea realtime doesn't work well as LoRA",
      "details": "It's distilled so similar case to causvid as LoRA - only full model worked properly",
      "from": "Kijai"
    },
    {
      "limitation": "MoCha needs background removal for reference",
      "details": "Reference images need background removed for proper function",
      "from": "DawnII"
    },
    {
      "limitation": "Wan 2.2 since release has been more of the same",
      "details": "Basically Wan 2.1 extensions, no relevant release to some users' taste",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "limitation": "MoCha only works well with humans",
      "details": "Doesn't work on architecture or non-human subjects",
      "from": "Charlie"
    },
    {
      "limitation": "MoCha produces CG-like results",
      "details": "Often looks artificial, sometimes bad CG quality, inconsistent results",
      "from": "Screeb"
    },
    {
      "limitation": "MoCha context issues with multiple characters",
      "details": "For 300+ frames, applies reference to wrong characters and selected character disappears",
      "from": "Visionmaster2"
    },
    {
      "limitation": "MoCha doubles VRAM usage",
      "details": "Cannot be used like normal model due to frame dimension doubling",
      "from": "Kijai"
    },
    {
      "limitation": "Ditto processing speed varies with motion",
      "details": "Fast-paced sequences take longer to process than closeups/portraits",
      "from": "Govind Singh"
    },
    {
      "limitation": "MoCha mask doesn't work well",
      "details": "Model often ignores the mask and sometimes replaces multiple characters instead of just the masked one",
      "from": "Juampab12"
    },
    {
      "limitation": "QwenImage VAE only works with newer models",
      "details": "Can't get it to work with actual old models that weren't trained with the new VAE",
      "from": "Kijai"
    },
    {
      "limitation": "Rolling Force model not useful for normal workflows",
      "details": "Only for realtime stuff, it's just 1.3B parameters",
      "from": "Kijai"
    },
    {
      "limitation": "Context drift still occurs with video extensions",
      "details": "Information outside given frames still gets lost in long video generation",
      "from": "Juampab12"
    },
    {
      "limitation": "SVI shot workflow motion continuation",
      "details": "Single motion frame limitation means it can't properly continue motion, ends up with jerky transitions",
      "from": "Kijai"
    },
    {
      "limitation": "Character consistency in I2V chaining",
      "details": "If person closes eyes at end of shot, next generation can't guess what the person's eyes look like",
      "from": "Draken"
    },
    {
      "limitation": "Exposure degradation in long generations",
      "details": "Exposure always goes up over time, creating gradual brightness increases that are part of quality degradation",
      "from": "Ablejones"
    },
    {
      "limitation": "Distill LoRA contamination",
      "details": "Distill LoRAs can interfere with testing new techniques and smallest changes can break continuation workflows",
      "from": "Kijai"
    },
    {
      "limitation": "SVI quality degrades over time in long chains",
      "details": "Noticeable quality loss and detail blurring in third chain and beyond",
      "from": "voxJT"
    },
    {
      "limitation": "Mocha fails with two-character scenarios",
      "details": "Either applies reference to both characters or outputs wrong character, gradual disappearance over time",
      "from": "Visionmaster2"
    },
    {
      "limitation": "Ditto has effective 3-second optimal limit",
      "details": "Quality severely degrades beyond 72 frames despite technical capability for longer videos",
      "from": "Drommer-Kille"
    },
    {
      "limitation": "SVI clothing inconsistency with camera movements",
      "details": "Person's clothes change when camera zooms in/out from full body to half body",
      "from": "army"
    },
    {
      "limitation": "Video models perform poorly with dark starting frames",
      "details": "I2V models consistently struggle when given dark initial frames",
      "from": "Ablejones"
    },
    {
      "limitation": "No custom mask input in native ComfyUI",
      "details": "Native ComfyUI nodes don't allow manual adjustment of masks for video generation",
      "from": "Ablejones"
    },
    {
      "limitation": "Wan 2.2 ignores clip embeds in native",
      "details": "Only WanAnimate model uses clip embeds, other 2.2 models ignore them in WanImageToVideo node",
      "from": "Kijai"
    },
    {
      "limitation": "MTV Crafter limited to 49 frames",
      "details": "Model architecture limited to 49 frame generation, may degrade with longer sequences",
      "from": "DawnII"
    },
    {
      "limitation": "Light/Tiny VAEs require code changes for native support",
      "details": "Cannot load light or tiny VAE variants without modifying native implementation",
      "from": "Kijai"
    },
    {
      "limitation": "HoloCine not fully functional without proper implementation",
      "details": "Needs varlen attention and rope setup, currently only works as normal T2V without scene switching capabilities",
      "from": "Kijai"
    },
    {
      "limitation": "SVI degrades significantly after 25 seconds",
      "details": "Gets even worse with lots of movement, causing quality degradation",
      "from": "Zabo"
    },
    {
      "limitation": "Context windows prone to artifacts at transitions",
      "details": "Often need to batch 4 generations to get 1 good result without ghosting",
      "from": "blake37"
    },
    {
      "limitation": "HoloCine currently only 480p resolution",
      "details": "Though GitHub mentions 720p capability",
      "from": "aikitoria"
    },
    {
      "limitation": "5-frame SVI method causes image degradation",
      "details": "Pushing model with 5 images is like 5x the strength, contributes to quality loss",
      "from": "Draken"
    },
    {
      "limitation": "SVI distill LoRAs don't work perfectly",
      "details": "Never gonna work perfectly with distill loras like lightx2v, errors accumulate eventually",
      "from": "Kijai"
    },
    {
      "limitation": "Shot method doesn't continue motion",
      "details": "Can't continue motion from single frame, just I2V. Only stabilizes long continuous generations",
      "from": "Kijai"
    },
    {
      "limitation": "2.1 LoRAs on 2.2 high noise",
      "details": "No 2.1 LoRA works properly on 2.2 high noise, architecture too different",
      "from": "Kijai"
    },
    {
      "limitation": "VAE degradation in extensions",
      "details": "Last frames from output never as clean as original input unless original was super low res. VAE encode/decode degrades over time",
      "from": "Draken"
    },
    {
      "limitation": "ComfyUI loops not practical for iterative work",
      "details": "Can't stop and play with each segment, try multiple times. Set it and forget it doesn't work well for this type of work",
      "from": "Ablejones"
    },
    {
      "limitation": "Film lora degrades over time",
      "details": "Unlike shot lora which can theoretically do infinite length, film lora will degrade over time",
      "from": "Kijai"
    },
    {
      "limitation": "Film lora incompatible with reference padding",
      "details": "Film lora will freeze if you try to use reference frames/padding",
      "from": "Ablejones"
    },
    {
      "limitation": "Color degradation over time",
      "details": "Color still degrades over time even with SVI",
      "from": "VK (5080 128gb)"
    },
    {
      "limitation": "Wan removes extra limbs from niche characters",
      "details": "Accurate training to prevent body horror means characters that should have extra limbs get them removed quickly",
      "from": "Ablejones"
    },
    {
      "limitation": "SVI doesn't work great with Wan 2.2",
      "details": "Shot SVI setup with wan2.2 didn't work great, though film method with multiple frames not tested",
      "from": "Draken"
    },
    {
      "limitation": "SVI Shot LoRA incompatible with Wan 2.2",
      "details": "Shot LoRA completely breaks when used with 2.2, people have tried without success",
      "from": "Kijai"
    },
    {
      "limitation": "Latent manipulation always causes artifacts",
      "details": "Any editing of first frame in latents causes light flashing, color shifting, ghosting, or echo effects. Only trimming end frames works cleanly",
      "from": "lemuet"
    },
    {
      "limitation": "Can't combine video latents without VAE decode",
      "details": "Video latents after first frame contain 4 images each, can't extract single image representation without full VAE decode",
      "from": "Kijai"
    },
    {
      "limitation": "LongCat begins repeating after 6 seconds for single long pieces",
      "details": "Cannot generate truly long videos in one go, needs extension method",
      "from": "aikitoria"
    },
    {
      "limitation": "Holocine character consistency weaker with mixed models",
      "details": "Less strong character consistency when using Holocine high + WAN low",
      "from": "mamad8"
    },
    {
      "limitation": "LongCat distill LoRA poor prompt following",
      "details": "16 step distill version didn't follow prompt (dog didn't eat ice cream as prompted)",
      "from": "aikitoria"
    },
    {
      "limitation": "WAN VAE produces noise grids on detailed inputs",
      "details": "Same ugly outputs with noise patterns, no progress made on VAE quality",
      "from": "aikitoria"
    },
    {
      "limitation": "FP8 scaled models won't work with original Holocine repo",
      "details": "Compatibility issue between quantized models and original implementation",
      "from": "seitanism"
    },
    {
      "limitation": "HoloCine license restrictions",
      "details": "CC BY-NC-SA 4.0 license requires attribution and prevents commercial use",
      "from": "Tony(5090)"
    },
    {
      "limitation": "WAN VAE noise grids cannot be removed",
      "details": "Video upscaling methods like SeedVR2 and Topaz preserve the artifacts",
      "from": "aikitoria"
    },
    {
      "limitation": "LongCat consistency issues",
      "details": "Heavy consistency degradation when objects leave and return to frame",
      "from": "Shubhooooo"
    },
    {
      "limitation": "SVI-Film only supports 5 frames",
      "details": "Cannot use 9 or 11 frames, only trained for 5-frame input",
      "from": "Ablejones"
    },
    {
      "limitation": "LongCat can't maintain identity well",
      "details": "Seems to care about init image even less than Pusa",
      "from": "scf"
    },
    {
      "limitation": "LongCat no first/last frame setup",
      "details": "Don't have setup for first/last frame, attention split code doesn't account for that possibility",
      "from": "Kijai"
    },
    {
      "limitation": "LongCat lacks control options",
      "details": "No LoRAs or control available yet, more curiosity/novelty right now",
      "from": "Kijai"
    },
    {
      "limitation": "Holocine 15 second limit",
      "details": "Current holocine trained on 15sec, longer generations have quality issues",
      "from": "NebSH"
    },
    {
      "limitation": "LongCat prompt understanding",
      "details": "Doesn't understand prompts as well as Wan 2.2, may need better prompting techniques",
      "from": "avataraim"
    },
    {
      "limitation": "fp16 precision with Wan models",
      "details": "Consistently produces NaN results leading to black outputs, doesn't work with quantizations",
      "from": "Kijai"
    },
    {
      "limitation": "MoCha long generation capability",
      "details": "Worse at long generations as it lacks innate ability, somewhat works with context windows",
      "from": "Kijai"
    },
    {
      "limitation": "HoloCine only 15 second model released",
      "details": "1 minute model not working/not released yet",
      "from": "avataraim"
    },
    {
      "limitation": "LongCat refinement creates noisy texture at full strength",
      "details": "Refinement lora at 1.0 strength creates noisy texture, especially visible on focused parts like hair",
      "from": "Kijai"
    },
    {
      "limitation": "PUSA doesn't hold reference frames properly with Holocine",
      "details": "Not holding the first frame past the first couple frames, not really 'working' for reference consistency",
      "from": "JohnDopamine"
    },
    {
      "limitation": "WanAnimate doesn't use CFG in original implementation",
      "details": "Original doesn't use CFG at all for efficiency, may need CFG for finer facial expression control",
      "from": "Kijai"
    },
    {
      "limitation": "Wan2.2 LightVAE shows quality loss",
      "details": "Despite claims, shows considerable quality loss compared to Wan2.1 VAE",
      "from": "Fawks"
    },
    {
      "limitation": "Wan VAE has moving grid patterns on hair in i2v",
      "details": "Snaps to different gradient for each patch every frame, especially visible on character hair",
      "from": "aikitoria"
    },
    {
      "limitation": "Spacepxl's VAE upscaler fails on video decoding",
      "details": "Only trained on images, will fail hard if used for video decoding",
      "from": "spacepxl"
    },
    {
      "limitation": "LongCat limited to 6 seconds without extension",
      "details": "Extension is only way to get longer generations",
      "from": "Zabo"
    },
    {
      "limitation": "Wan VAE latents can't be cleanly cut due to temporal information",
      "details": "Can't extract single frame from latent that packs 4 frames due to causal convolutions",
      "from": "mamad8"
    },
    {
      "limitation": "OVI cannot do video-to-video with audio syncing like InfinityTalk",
      "details": "Kijai couldn't get it to work when tried",
      "from": "Kijai"
    },
    {
      "limitation": "OVI has 10 second maximum generation limit",
      "details": "15 second generations fail, recommend 8 seconds for better results",
      "from": "avataraim"
    },
    {
      "limitation": "Video extension using one frame at a time lacks temporal consistency",
      "details": "Processing individual frames will never give consistency, need temporal context",
      "from": "spacepxl"
    },
    {
      "limitation": "Current WAN models have soft latents causing artifacts",
      "details": "Pupil problems when not enough resolution, probably need more faces in training data",
      "from": "spacepxl"
    },
    {
      "limitation": "Latent space upscalers create ghosting",
      "details": "Almost all upscalers in latent space create ghosting or awful tiling, best found so far is DenRakEiw/WAN_NN_Latent_Upscale but still has ghost entries",
      "from": "mdkb"
    },
    {
      "limitation": "WAN 5B LoRA training difficulties",
      "details": "Impossible to train LoRAs for WAN 5B, VAE seems too tuned to their dataset",
      "from": "Kiwv"
    },
    {
      "limitation": "WAN can't handle faster paced content",
      "details": "For faster pace things WAN just can't do it, needs 24fps capability",
      "from": "Kiwv"
    },
    {
      "limitation": "LongCat has no extra control compared to Wan 2.2",
      "details": "No additional control features, just supposed to work better with I2V",
      "from": "Kijai"
    },
    {
      "limitation": "Video continuation loses reference when subjects turn around",
      "details": "Once a person turns around and video ends, continuation cannot recover original face without reference input",
      "from": "Draken"
    },
    {
      "limitation": "SVI-shot sticks too closely to reference",
      "details": "Tries to snap back to reference image even when scene changes, causing unrealistic effects",
      "from": "Ablejones"
    },
    {
      "limitation": "LongCat incompatible with VACE blocks",
      "details": "Too different architecture and dimensions to work with existing VACE",
      "from": "Kijai"
    },
    {
      "limitation": "Below 5 steps causes major quality issues",
      "details": "Massive lighting smoothness issues, depth inconsistency, and general blurriness",
      "from": "Pandaabear"
    },
    {
      "limitation": "Current AI video quality not suitable for production consumption",
      "details": "Purely prompt-to-video or image-to-video looks like 'AI slop' and not enjoyable for content consumption",
      "from": "Kijai"
    },
    {
      "limitation": "LTX 2 has unusable audio quality",
      "details": "Despite technical specs like 4K 50fps, audio quality makes it unusable compared to Wan",
      "from": "Kijai"
    },
    {
      "limitation": "EXR files incompatible with pose systems",
      "details": "Pose detection/processing systems cannot handle EXR format images",
      "from": "Guus"
    },
    {
      "limitation": "LongCat may not be sufficient improvement for ecosystem development",
      "details": "While clearly better than Wan, may not be enough improvement to justify training new ecosystems around it",
      "from": "Kijai"
    },
    {
      "limitation": "ChronoEdit doesn't work well with short prompts",
      "details": "Needs detailed prompts or LLM enhancement for good results",
      "from": "Kijai"
    },
    {
      "limitation": "LongCat terrible at 2D content",
      "details": "Performance issues with 2D/animated content",
      "from": "Ada"
    },
    {
      "limitation": "ChronoEdit is I2V only, no T2V capability",
      "details": "Purely image-to-video, cannot generate from text alone",
      "from": "CJ"
    },
    {
      "limitation": "Lightning LoRA changes exposure and has poor motion",
      "details": "Makes output brighter and motion quality is not the best",
      "from": "Kijai"
    },
    {
      "limitation": "ChronoEdit does more processing than necessary",
      "details": "Quality is solid but computationally inefficient compared to alternatives",
      "from": "Kiwv"
    },
    {
      "limitation": "ChronoEdit fails on same prompts where Qwen fails",
      "details": "No improvement over existing solutions in problem cases",
      "from": "aikitoria"
    },
    {
      "limitation": "LongCat has no reference images support",
      "details": "If character turns around by bad luck, becomes useless for continuation",
      "from": "aikitoria"
    },
    {
      "limitation": "LongCat is T2V only for some features",
      "details": "HoloCine is T2V only, limiting usefulness",
      "from": "aikitoria"
    },
    {
      "limitation": "Context windows reset subject changes",
      "details": "If subject has a tear running down cheek, tear will disappear in new context window",
      "from": "blake37"
    },
    {
      "limitation": "Context windows increase bad gen chances",
      "details": "Each context window has its own chance of doing something weird or creating hallucinations",
      "from": "blake37"
    },
    {
      "limitation": "New 1030 LoRA tends to zoom in camera",
      "details": "Even with 'camera still' or 'camera stationary' prompts, still does zooming",
      "from": "WorldX"
    },
    {
      "limitation": "VACE has autoregressive burn in and color drift",
      "details": "Gets worse over time with autoregressive extension, not good at staying coherent",
      "from": "Benjaminimal"
    },
    {
      "limitation": "LongCat is hit or miss on prompt adherence",
      "details": "Outputs appear as series of images rather than video in some cases",
      "from": "Ness"
    },
    {
      "limitation": "LongCat has different architecture from Wan",
      "details": "Not trainable with Wan trainer",
      "from": "Kiwv"
    },
    {
      "limitation": "ChronoEdit prompt adherence is random",
      "details": "Works better for animating existing images than redrawing from different perspectives",
      "from": "Ness"
    },
    {
      "limitation": "Long video generation causes progressive blur",
      "details": "Videos get more and more blurry during extended generation chains",
      "from": "aikitoria"
    },
    {
      "limitation": "LightX motion continuity",
      "details": "Motion is not totally continuous when using 4-step generation",
      "from": "aikitoria"
    },
    {
      "limitation": "Veo 3.1 fast motion quality",
      "details": "Motion is awful 99% of the time, overlit, over sharpened, fake AI looking",
      "from": "Ruairi Robinson"
    }
  ],
  "hardware": [
    {
      "requirement": "RAM for video processing",
      "details": "Even 64GB RAM can have issues, may need 80GB+ swap size. 100GB RAM user also reports memory issues",
      "from": "Ablejones"
    },
    {
      "requirement": "VRAM optimization flags",
      "details": "--highvram, --gpu-only, --normalvram, --cache-none options available for different memory constraints",
      "from": "Ablejones"
    },
    {
      "requirement": "DWPose processing speed",
      "details": "5090 takes ~17min for 30fps 10sec clip at 720x1280, reduced to 3min with .torchscript",
      "from": "rob"
    },
    {
      "requirement": "ONNX GPU runtime for pose detection",
      "details": "Without onnxruntime-gpu, pose detection runs on CPU and takes much longer",
      "from": "Kijai"
    },
    {
      "requirement": "Wan 2.2 upscaling performance",
      "details": "Takes 10 minutes to upscale single image from 1080 to 4K on RTX 5090",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "H100 performance for I2V",
      "details": "101 frames at 24fps plus upscale takes roughly 148s on H100 with 2-3 LoRAs + sage attention",
      "from": "topmass"
    },
    {
      "requirement": "Cloud CPU performance",
      "details": "Cloud setups often have subpar CPU performance, affecting mask processing speed dramatically",
      "from": "Kijai"
    },
    {
      "requirement": "GPU vs CPU for mask operations",
      "details": "GPU processing achieves 11409.67 it/s vs CPU 341.85 it/s for DrawMaskOnImage operations",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM for 12GB setup",
      "details": "Euler/beta scheduler works well for 12GB VRAM constraints",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "requirement": "3060 12GB limits",
      "details": "Max 65 frames at 1024x576, 25fps, 6 steps, ~7min generation time",
      "from": "mdkb"
    },
    {
      "requirement": "512 video training",
      "details": "Needs 48GB VRAM for 512 resolution video training",
      "from": "Ryzen"
    },
    {
      "requirement": "Ovi fp8 VRAM",
      "details": "16GB VRAM required even with fp8 quantization",
      "from": "Stad"
    },
    {
      "requirement": "OVI VRAM requirements",
      "details": "Needs high-end GPU, 5090 and up recommended, too heavy for most users currently",
      "from": "Charlie"
    },
    {
      "requirement": "Wan 2.2 5B VRAM",
      "details": "Cannot run on 12GB VRAM without quantization despite being 5B parameters",
      "from": "Stad"
    },
    {
      "requirement": "ComfyUI-Upscale-CUDAspeed acceleration",
      "details": "50% acceleration on single 4090, 70% with 4090+4070ti, supports 20 series and up",
      "from": "piscesbody"
    },
    {
      "requirement": "OVI runtime environment",
      "details": "Python 3.12.7, torch 2.8.0+cu128, flash-attn 2.7.4.post1 confirmed working",
      "from": "TK_999"
    },
    {
      "requirement": "OVI VRAM",
      "details": "24GB VRAM in fp8 mode, uses 16.4-19GB on 3090",
      "from": "yukass"
    },
    {
      "requirement": "Smooth mix RAM usage",
      "details": "Causes more frequent OOM crashes, only 2-3 gens vs regular wan",
      "from": "Rainsmellsnice"
    },
    {
      "requirement": "4K upscale processing",
      "details": "CPU gets work during 4K upscale at end of Wan 2.2 workflow",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "HuMo fp8 model",
      "details": "11.3GB model works on RTX 3090, original 23GB model needs 32GB VRAM",
      "from": "yukass"
    },
    {
      "requirement": "RTX 50-series GPU compatibility",
      "details": "ONNX Runtime 1.23.0 JIT compilation issues with compute capability 9.0+",
      "from": "D-EFFECTS"
    },
    {
      "requirement": "Mask expand operations",
      "details": "256+ radius expansions slow even with Kornia GPU update",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "requirement": "OVI VRAM usage",
      "details": "Requires more than 8GB VRAM, needs optimization for lower VRAM usage",
      "from": "Stad"
    },
    {
      "requirement": "OVI generation time",
      "details": "213 seconds for 720p generation",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "requirement": "Native vs Wrapper performance",
      "details": "Native 121 frames takes 29:54, Wrapper would do under 10 minutes",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "Memory configuration for Intel 13th/14th gen",
      "details": "Use 2x64GB sticks instead of 4x32GB to avoid straining memory controller and enable XMP",
      "from": "Ryzen"
    },
    {
      "requirement": "VRAM for FP16 vs FP8",
      "details": "FP16 uses double the memory compared to FP8, rarely worth the cost unless you have a monster GPU",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 5090 performance",
      "details": "Can achieve 720p video generation in 160 seconds with optimized Lightning LoRA settings",
      "from": "Lan8mark"
    },
    {
      "requirement": "5090 performance example",
      "details": "81 frames at 720p in 160 seconds using resolution reduction workflow",
      "from": "Lan8mark"
    },
    {
      "requirement": "RTX 3090 Wan 2.2 performance",
      "details": "fp8_e5m2: 170s gen time, k8 gguf: 360s gen time with block swap 20",
      "from": "Mattis"
    },
    {
      "requirement": "Low-end training feasibility",
      "details": "17s/it, 3000 steps (~14hrs) on 6GB laptop GPU using AI-Toolkit with RAM training",
      "from": "Ada"
    },
    {
      "requirement": "4090 performance benchmark",
      "details": "Sampling 81 frames at 992x512 with 40 steps: 4.29s/it",
      "from": "Kijai"
    },
    {
      "requirement": "5090 performance benchmark",
      "details": "Sampling 121 frames at 992x512 with 40 steps: 3.29s/it with flash_attn, 2.02s/it with sage and fp16 fast. Max allocated memory: 24.534 GB",
      "from": "Kijai"
    },
    {
      "requirement": "ComfyUI storage usage",
      "details": "my current comfy portable install is 1.2 tb, its 800gb in just the main models currently",
      "from": "hudson223"
    },
    {
      "requirement": "VACE 2.2 dual model VRAM usage",
      "details": "those two amount to 19GB of files and yet run without filling up 12GB VRam",
      "from": "mdkb"
    },
    {
      "requirement": "OVI works on 8GB VRAM",
      "details": "User confirmed OVI working on 8GB VRAM setup",
      "from": "Stad"
    },
    {
      "requirement": "160 seconds generation time mentioned",
      "details": "User complaining about slow upscale times of 160 seconds",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "requirement": "RAM upgrade impact",
      "details": "64GB vs 16GB RAM shows night and day difference - instant prompt changes vs seconds/minutes of waiting",
      "from": "Miku"
    },
    {
      "requirement": "OVI generation time on 5090",
      "details": "90s for 5s clip at base settings, 173s for 720p 50 steps",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "WAN upscaler performance",
      "details": "150s for 720p to 2K, 350s for 480p to 2K on 5090",
      "from": "Lan8mark"
    },
    {
      "requirement": "VRAM needs with 4090",
      "details": "User didn't even try WAN animate with 4090, waited for 5090 upgrade",
      "from": "Juampab12"
    },
    {
      "requirement": "RTX 5090 cooling",
      "details": "MSI Suprim Liquid keeps temperatures under 68-72 degrees even during full hour usage",
      "from": "GOD_IS_A_LIE"
    },
    {
      "requirement": "Dual GPU setup issues",
      "details": "Dual RTX setup can cause instability, single 5090 recommended for stability",
      "from": "Charlie"
    },
    {
      "requirement": "RAM limitations",
      "details": "64GB RAM may not be sufficient for dual GPU setups with heavy workloads",
      "from": "Charlie"
    },
    {
      "requirement": "5090 upscaling setup",
      "details": "5090 with 128GB RAM recommended for upscaling workflows on vast.ai",
      "from": "FL13"
    },
    {
      "requirement": "Cold start latency",
      "details": "Serverless cold start under 1 second, few seconds for massive models",
      "from": "mallardgazellegoosewildcat"
    },
    {
      "requirement": "Local generation electricity costs",
      "details": "20 kWh in 48h, equals 540\u20ac/year for continuous use",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "New Lightx2v full model size",
      "details": "24GB per file for high and low noise models",
      "from": "Ashtar"
    },
    {
      "requirement": "VRAM for 1920x1080 I2V",
      "details": "RTX 5090 can handle 1920x1080 I2V generation",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "StepVideo memory usage",
      "details": "768px \u00d7 768px \u00d7 102f = 76.42 GB, 1061s generation time",
      "from": "Colin"
    },
    {
      "requirement": "OVI 5B inference time",
      "details": "Takes about an hour on RTX 3060 according to YouTube channel tests",
      "from": "mdkb"
    },
    {
      "requirement": "FlashVSR VRAM",
      "details": "Peak VRAM usage is 11GB, much less than other diffusion upscalers, but speed benefits mainly on H100",
      "from": "yi"
    },
    {
      "requirement": "Video training on RTX 5090",
      "details": "Using 256x256 resolution to avoid OOM when training with 81 frames",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "FlashVSR VRAM usage",
      "details": "Reserves 16GB on RTX 3060 12GB (more than available), works with text encode cache",
      "from": "mdkb"
    },
    {
      "requirement": "SageAttention speed improvement",
      "details": "Can be more than twice as fast depending on resolution, also reduces VRAM usage",
      "from": "Kijai"
    },
    {
      "requirement": "MSI Suprim RTX performance",
      "details": "20-25% faster than FE at 104% power limit, lower temps, silent at 33% fan speed",
      "from": "seitanism"
    },
    {
      "requirement": "OVI model compatibility",
      "details": "Runs on RTX 3060 12GB with fp8 models and quantization disabled",
      "from": "U\u0337r\u0337a\u0337b\u0337e\u0337w\u0337e\u0337"
    },
    {
      "requirement": "fp8_fast mode compatibility",
      "details": "Works fine on 2.2 models with 5090, but works poorly quality-wise on 2.1 models",
      "from": "Kijai"
    },
    {
      "requirement": "FlashVSR processing speed",
      "details": "241 frames at 1248x1568 takes about 5 minutes, 150 seconds for upscaling vs 10-15min with Topaz",
      "from": "DawnII"
    },
    {
      "requirement": "SeedVR2 VRAM usage with block swapping",
      "details": "Can run on 32GB VRAM with block swapping, minimal speed impact (3:56 vs 3:49 without)",
      "from": "seitanism"
    },
    {
      "requirement": "VRAM usage with fp32 VAE",
      "details": "Doubles VRAM usage compared to bf16",
      "from": "Kijai"
    },
    {
      "requirement": "VACE frame scaling",
      "details": "Roughly 2-4 GB per 25 frames at 480p on 14B model, doubles for 720p",
      "from": "Dever"
    },
    {
      "requirement": "Generation speed improvement",
      "details": "New distill models generate video in 1 minute vs 25 minutes",
      "from": "aikitoria"
    },
    {
      "requirement": "FlashVSR VRAM usage",
      "details": "20GB VRAM for processing larger resolutions, 8.6GB VRAM for 100 frame batches at 1024x576, 10GB system RAM",
      "from": "Kijai"
    },
    {
      "requirement": "WAN Animate frame limits",
      "details": "720p 1000 frames easily achievable on 5090 with 128GB RAM, should work fine on 4090 with similar specs",
      "from": "Charlie"
    },
    {
      "requirement": "Block-Sparse Attention GPU requirements",
      "details": "Only achieves ideal acceleration on NVIDIA A100/A800 GPUs, H100/H800 may be slower than dense attention",
      "from": "JohnDopamine"
    },
    {
      "requirement": "VRAM usage increase after updates",
      "details": "Blockswap needed to increase from 25 to 35 for same 1120x704 97-frame generation, now uses 99% VRAM",
      "from": "phazei"
    },
    {
      "requirement": "5090 VRAM issues reported",
      "details": "Even 5090 users experiencing OOM on VAE decode, may need tiled VAE",
      "from": "Provydets"
    },
    {
      "requirement": "720p generation time",
      "details": "1 hour initially, reduced to 30 mins with easycache on RTX 3060",
      "from": "mdkb"
    },
    {
      "requirement": "5090 performance for OVI",
      "details": "Quick generation on RTX 5090 for 720p OVI",
      "from": "Charlie"
    },
    {
      "requirement": "VRAM for Wan 2.2 with Lightning LoRA",
      "details": "DGX Spark vs RTX 6000 Pro performance comparison provided",
      "from": "tarn59"
    },
    {
      "requirement": "RAM upgrade",
      "details": "Charlie got 128GB sticks (FURY Beast 128GB 5600MT/s DDR5 CL36) after having enough with 64GB",
      "from": "Charlie"
    },
    {
      "requirement": "PyTorch version compatibility",
      "details": "Charlie and others recommend staying on PyTorch 2.7.1 instead of 2.9.0+cu128 to avoid issues",
      "from": "Charlie"
    },
    {
      "requirement": "VRAM usage with high frame counts",
      "details": "177 frames can cause memory issues depending on GPU VRAM, images get converted to 32bit",
      "from": "Gateway"
    },
    {
      "requirement": "RAM allocation issues",
      "details": "29.3 GiB array allocation errors possible with large generations, use --cache-none flag",
      "from": "BestWind"
    },
    {
      "requirement": "RAM usage for Wan 2.2 14B training",
      "details": "Extremely high RAM usage shown in RunPod training",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "Krea realtime inference speed",
      "details": "11fps using 4 inference steps on single NVIDIA B200 GPU",
      "from": "Dever"
    },
    {
      "requirement": "MoCha is heavy model to run",
      "details": "Requires significant resources, need to limit input frames",
      "from": "Kijai"
    },
    {
      "requirement": "MoCha VRAM usage",
      "details": "About double normal usage due to frame concatenation, 11GB with fp8 at default resolution",
      "from": "Kijai"
    },
    {
      "requirement": "Krea performance on RTX 4090",
      "details": "140 seconds 720p video generated in ~140 seconds",
      "from": "aipmaster"
    },
    {
      "requirement": "Sage attention GPU compatibility",
      "details": "Requires SM89 compute capability, RTX 6000 PRO not compatible",
      "from": "HeadOfOliver"
    },
    {
      "requirement": "32GB RAM not enough for some workflows",
      "details": "32GB system RAM insufficient for Wan 2.2 workflows, especially with upscaling",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 5090 shouldn't need tiled VAE",
      "details": "Unless doing huge resolutions, tiled VAE shouldn't be necessary",
      "from": "Kijai"
    },
    {
      "requirement": "VAE uses ~10GB VRAM normally",
      "details": "Normal VRAM usage for VAE decoding, but not used simultaneously with model",
      "from": "Kijai"
    },
    {
      "requirement": "CUDA version compatibility",
      "details": "Issues arise with CUDA versions less than 12, though 11.8 can work with proper path configuration",
      "from": "Lodis"
    },
    {
      "requirement": "VAE encoding overhead",
      "details": "Multiple frame processing requires VAE decode/encode cycles which increases memory usage",
      "from": "CJ"
    },
    {
      "requirement": "SVI memory usage",
      "details": "No additional VRAM/memory requirements, just LoRA weights",
      "from": "Kijai"
    },
    {
      "requirement": "Long Ditto generations",
      "details": "24GB VRAM and 128GB system RAM recommended for extended video generation",
      "from": "JohnDopamine"
    },
    {
      "requirement": "Wan 2.2 VAE speed",
      "details": "2.2 VAE is not fast, lightweight alternatives provide speed at quality cost",
      "from": "Kijai"
    },
    {
      "requirement": "WSL memory configuration",
      "details": "Need to manually configure memory limits in .wslconfig, default allocation insufficient",
      "from": "seitanism"
    },
    {
      "requirement": "WSL memory overhead",
      "details": "WSL uses significant RAM overhead compared to dual boot Linux, 3x slower performance possible with VRAM saturation",
      "from": "seitanism"
    },
    {
      "requirement": "Q8 model + VACE memory usage",
      "details": "Q8 quantized model combined with VACE module requires substantial RAM",
      "from": "seitanism"
    },
    {
      "requirement": "Blackwell GPU driver needs",
      "details": "Latest drivers needed for new Blackwell GPUs at launch, Ubuntu prebuilt drivers usually outdated",
      "from": "Kijai"
    },
    {
      "requirement": "HoloCine model size",
      "details": "57GB each for high and low noise models in fp32",
      "from": "Tachyon"
    },
    {
      "requirement": "96GB VRAM recommendation",
      "details": "VK suggests 96GB VRAM needs to become norm for these large models",
      "from": "VK (5080 128gb)"
    },
    {
      "requirement": "3090 insufficient for newer Wan models",
      "details": "Can't really push newer Wan models through on 3090",
      "from": "TimHannan"
    },
    {
      "requirement": "SSD performance issues",
      "details": "M.2 SSDs reading at 300-500MB/s instead of advertised 7000MB/s affecting model loading times",
      "from": "seitanism"
    },
    {
      "requirement": "165 frames at 1280x720",
      "details": "Took 36 minutes with 5090",
      "from": "Drommer-Kille"
    },
    {
      "requirement": "245 frames at 960x540",
      "details": "Requires full blockswap for VRAM handling",
      "from": "Hashu"
    },
    {
      "requirement": "HoloCine models",
      "details": "57GB for both low and high models",
      "from": "Koba"
    },
    {
      "requirement": "fp8_scaled performance",
      "details": "Works with unmerged loras, no significant degradation observed",
      "from": "Kijai"
    },
    {
      "requirement": "HoloCine sparse model VRAM",
      "details": "30.5GB VRAM with 20 blockswap for 281 frames at 832x480",
      "from": "mamad8"
    },
    {
      "requirement": "Normal 2.2 generation",
      "details": "Max 18.625 GB allocated memory for 241 frames at 832x480 with 6 total steps",
      "from": "Kijai"
    },
    {
      "requirement": "SVI training hardware",
      "details": "Requires medium GPU cluster - authors used 8 to 64 GPUs for training",
      "from": "Ablejones"
    },
    {
      "requirement": "RTX 4090 for 30 second generation",
      "details": "With swap block 40, takes about 500 seconds generation time",
      "from": "avataraim"
    },
    {
      "requirement": "20 minutes for 350 frame generation",
      "details": "Using context overlap method",
      "from": "VK"
    },
    {
      "requirement": "7 minutes for 255 frames",
      "details": "Without context overlap",
      "from": "VK"
    },
    {
      "requirement": "HoloCine VRAM usage",
      "details": "Takes about 70GB VRAM for 480p, 720p crashes on Pro 6000",
      "from": "BNP4535353"
    },
    {
      "requirement": "HoloCine generation speed",
      "details": "1 hour for 16 seconds on official GitHub implementation, 25% faster with Flash Attention 2.8.3",
      "from": "BNP4535353"
    },
    {
      "requirement": "Wan 2.2 720p benchmark",
      "details": "189 frames in 142s on H800",
      "from": "Ada"
    },
    {
      "requirement": "LongCat VRAM with offloading",
      "details": "4090 needs about half offloaded for bf16, full offload under 10GB. 93 frames 640x608 16 steps: max 19.014 GB",
      "from": "Kijai"
    },
    {
      "requirement": "Holocine memory requirements",
      "details": "241 frames at higher res OOMs at 24GB VRAM + 64GB RAM, requires all frames in VRAM at once",
      "from": "Cseti"
    },
    {
      "requirement": "Holocine checkpoint size",
      "details": "Checkpoints more than 50GB each, FP8 versions available to reduce size",
      "from": "Fawks"
    },
    {
      "requirement": "HoloCine VRAM usage",
      "details": "Around 18GB VRAM usage with swap block 20",
      "from": "avataraim"
    },
    {
      "requirement": "LongCat generation times at 640x384",
      "details": "5 steps: 45 sec, 10 steps: 75 sec, 15 steps: 110 sec",
      "from": "avataraim"
    },
    {
      "requirement": "SageAttention 2.2.0 Windows requirement",
      "details": "Windows only, requires Python 3.10",
      "from": "avataraim"
    },
    {
      "requirement": "LongCat VRAM usage",
      "details": "Around 17GB VRAM with 30 blocks swapped for 832x480 at 97-149 frames on 24GB card",
      "from": "JohnDopamine"
    },
    {
      "requirement": "LongCat generation speed",
      "details": "166 seconds for i2v on RTX 4090, about 2-3 minutes for 15 second videos",
      "from": "JohnDopamine"
    },
    {
      "requirement": "LongCat speed issues on RTX 5090",
      "details": "Initially 64s/it, then dropped to 25s/it, performance seems inconsistent",
      "from": "Zabo"
    },
    {
      "requirement": "Native vs ComfyUI speed difference",
      "details": "Native takes 50 minutes per gen, ComfyUI takes only 2-3 minutes for same content",
      "from": "avataraim"
    },
    {
      "requirement": "50+ GB model causing OOM at 1280p",
      "details": "1280p resolution causes out of memory issues with full precision model, 832x480 works fine",
      "from": "BNP4535353"
    },
    {
      "requirement": "Model precision affects speed significantly",
      "details": "Loading at full precision makes generation much slower with most data offloaded",
      "from": "Ada"
    },
    {
      "requirement": "High-end GPU for large resolutions",
      "details": "User with 96GB VRAM can handle 720p, 5090 limited to 832x480 for 253 frames or 1280x720 for 161 frames",
      "from": "BNP4535353"
    },
    {
      "requirement": "RAM consumption significant",
      "details": "Biggest consumption is actually RAM rather than GPU memory for high resolution generation",
      "from": "BNP4535353"
    },
    {
      "requirement": "RAM usage for WAN 2.2",
      "details": "WSL2 peaks at 92GB RAM with wan high/low (about 16+16+10+1GB models plus overhead)",
      "from": "pagan"
    },
    {
      "requirement": "3090ti for LoRA training",
      "details": "24GB VRAM, but needs to rent GPUs for large datasets (550 videos, 121 frames each)",
      "from": "Kiwv"
    },
    {
      "requirement": "Memory management recommendation",
      "details": "256GB RAM ordered for $720 to solve memory issues",
      "from": "seitanism"
    },
    {
      "requirement": "LongCat VRAM usage",
      "details": "11GB VRAM with 30 blocks swapped, 20GB without swapping on 5090",
      "from": "Kijai"
    },
    {
      "requirement": "LongCat generation time",
      "details": "Around 20 minutes for 16 steps on 5090, 13:54 for 10 steps",
      "from": "Kijai"
    },
    {
      "requirement": "4090 with 128GB system RAM performance",
      "details": "Max memory 11.084GB, generation in 503.10 seconds",
      "from": "JohnDopamine"
    },
    {
      "requirement": "TensorRT interpolation performance",
      "details": "3 second interpolations when properly set up",
      "from": "voxJT"
    },
    {
      "requirement": "Torch compile VRAM impact",
      "details": "Can increase VRAM usage by 50% while reducing speed, user saw 15.5GB+2 shared vs 11.3GB plain",
      "from": "Gleb Tretyak"
    },
    {
      "requirement": "High resolution Wan2.2 generation",
      "details": "1200x960x5sec used 12.5GB peak VRAM on RTX 4070 Ti Super (16GB VRAM)",
      "from": "Gleb Tretyak"
    },
    {
      "requirement": "Tested resolutions",
      "details": "Users successfully generated up to 1920x640 (wide angle) and 1600x900, 720p recommended for quality",
      "from": "hudson223"
    },
    {
      "requirement": "Block swap VRAM tradeoff",
      "details": "600MB LoRA with 20 block swap now uses 300MB more VRAM but moves with block swap for better performance",
      "from": "Kijai"
    },
    {
      "requirement": "ChronoEdit VRAM",
      "details": "At least 5 frames at 1024x1024 with Wan I2V requirements",
      "from": "Kijai"
    },
    {
      "requirement": "Chinese GPU alternatives",
      "details": "Huawei Atlas 300I DUO: 96GB VRAM for $1.5k but much lower TOPS performance vs NVIDIA RTX 6000 Pro at $10k",
      "from": "Aaron_PhD"
    },
    {
      "requirement": "BF16 support",
      "details": "Requires at least 30xx series for BF16, older cards need FP16 conversion",
      "from": "Lodis"
    },
    {
      "requirement": "ChronoEdit model size",
      "details": "32GB for full model",
      "from": "xwsswww"
    },
    {
      "requirement": "LongCat VRAM usage",
      "details": "Barely uses 60% VRAM with higher resolution and more frames vs WAN bringing close to OOM",
      "from": "Pandaabear"
    },
    {
      "requirement": "LongCat base precision",
      "details": "Needs bf16, doesn't work on fp16",
      "from": "Kijai"
    },
    {
      "requirement": "GPU pooling with Raylight",
      "details": "Get 60-70% performance of second card added to first card inference speed, but downgrades faster card to match slower card",
      "from": "Kinasato"
    },
    {
      "requirement": "4x 5090 setup considerations",
      "details": "600W x 4 is an issue with 5090s, RTX 6000 Pro might be better alternative",
      "from": "Ada"
    },
    {
      "requirement": "Multi-GPU VRAM pooling",
      "details": "No way to pool VRAM of GPUs, but nice for training. One 5090 is plenty for inference unless doing LLMs",
      "from": "Lumifel"
    }
  ],
  "community_creations": [
    {
      "creation": "After Effects plugin for VACE inpainting",
      "type": "tool",
      "description": "Custom plugin for easy VACE inpainting workflows",
      "from": "SonidosEnArmon\u00eda"
    },
    {
      "creation": "Batch resize with configurable batch sizes",
      "type": "node",
      "description": "Modified resize node that processes giant batches in smaller chunks",
      "from": "stenandrimpy"
    },
    {
      "creation": "Updated wrapper preprocessing example",
      "type": "workflow",
      "description": "Updated example workflow using new preprocessing nodes",
      "from": "Kijai"
    },
    {
      "creation": "Modified QWen VL Object Detection",
      "type": "tool",
      "description": "Allows model unloading, creates bboxes for face detection workflows",
      "from": "piscesbody"
    },
    {
      "creation": "Face bbox output for preprocessing",
      "type": "node",
      "description": "Added face bbox output to existing preprocessing workflow for easier face swapping",
      "from": "Kijai"
    },
    {
      "creation": "Wanx Troopers documentation site",
      "type": "documentation",
      "description": "GitHub organization for condensed documentation and tidbits, accepts PRs",
      "from": "42hub"
    },
    {
      "creation": "Blockify GPU optimization",
      "type": "node improvement",
      "description": "Rewrote blockify code in pytorch with GPU device option for massive speed improvements",
      "from": "Kijai"
    },
    {
      "creation": "AI Toolkit blockswapping implementation",
      "type": "feature",
      "description": "Built-in blockswapping that offloads to CPU during training",
      "from": "Ryzen"
    },
    {
      "creation": "HuMO I2V fix",
      "type": "patch",
      "description": "PR submitted to enable proper I2V functionality in HuMO",
      "from": "Ablejones"
    },
    {
      "creation": "New checkpoint with Lightning LoRAs",
      "type": "model",
      "description": "Best 2.2 results with great motion, prompt following, includes light LoRAs without motion loss",
      "from": "Ada"
    },
    {
      "creation": "ComfyUI-Upscale-CUDAspeed",
      "type": "tool",
      "description": "High-performance CUDA-accelerated upscaling plugin with multi-GPU support and Tensor Core optimization",
      "from": "piscesbody"
    },
    {
      "creation": "Wan 2.2 character LoRAs",
      "type": "lora",
      "description": "Character training using diffusion-pipe repo, 950 images same cosplayer, rank 32, 5e-5 learning rate",
      "from": "Kytra"
    },
    {
      "creation": "Music analysis suite",
      "type": "nodes",
      "description": "Custom nodes with 3 algorithms for music analysis, accounting for frame drift",
      "from": "Fill"
    },
    {
      "creation": "Matrix bullet time effect",
      "type": "workflow",
      "description": "1:1 recreation using Wan 2.1 and VACE for interpolation between 10 camera frames",
      "from": "Neex"
    },
    {
      "creation": "WanAnimate V2 workflow",
      "type": "workflow",
      "description": "Updated example workflow using preprocessing nodes",
      "from": "Kijai"
    },
    {
      "creation": "Smooth Mix Wan 2.2 model",
      "type": "model",
      "description": "Merged model optimized for frame interpolation",
      "from": "Community"
    },
    {
      "creation": "Radial Length Helper node",
      "type": "tool",
      "description": "Makes snapped values based on input and gives list of valid lengths for resolution calculation",
      "from": "hudson223"
    },
    {
      "creation": "ComfyUI_RH_Ovi",
      "type": "node",
      "description": "ComfyUI implementation for OVI model by RunningHub",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "creation": "Midas VFX relight system",
      "type": "tool",
      "description": "Face embedding integrated relight system with self-shadowing capabilities, work-related so cannot be shared",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "creation": "Curly hair LoRA",
      "type": "lora",
      "description": "Special LoRA trained for curly hair since base models don't handle it well",
      "from": "Thom293"
    },
    {
      "creation": "USDU workflow for video extension fixing",
      "type": "workflow",
      "description": "Complex workflow using USDU upscale in latent space to fix seams and quality issues in extended videos",
      "from": "mdkb"
    },
    {
      "creation": "WanVideo Scheduler visualization",
      "type": "node",
      "description": "Helps visualize sigma values and understand shift parameter effects",
      "from": "Kijai"
    },
    {
      "creation": "LoRA extraction and pruning tools",
      "type": "tool",
      "description": "Extract similar models as LoRAs and prune existing LoRAs to reduce storage",
      "from": "woctordho"
    },
    {
      "creation": "OVI native implementation",
      "type": "node",
      "description": "Native ComfyUI implementation of OVI with proper model loading/patching",
      "from": "Kytra"
    },
    {
      "creation": "Flux bokeh LoRA",
      "type": "lora",
      "description": "Swirl bokeh only - trained on specific lens effects",
      "from": "slmonker(5090D 32GB)"
    },
    {
      "creation": "Easy WAN 2.2 latent upscale article",
      "type": "workflow",
      "description": "PDF article about latent upscale and boosting video generation speed by 3\u20134\u00d7",
      "from": "Lan8mark"
    },
    {
      "creation": "2K video upscaler with WAN 2.2",
      "type": "workflow",
      "description": "160 seconds to upscale from 720p to 1440p",
      "from": "Lan8mark"
    },
    {
      "creation": "WanMoeKSampler with CFG guidance",
      "type": "node",
      "description": "Fixed sigma_shift/step split with added CFG guidance and tooltips",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "creation": "rCM LoRA extraction",
      "type": "lora",
      "description": "Extracted LoRA version of rCM consistency model for Wan",
      "from": "Kijai"
    },
    {
      "creation": "WAN upscaler workflow",
      "type": "workflow",
      "description": "Handles high-quality upscaling from 720p/360p to 2K with improved stability",
      "from": "Lan8mark"
    },
    {
      "creation": "ComfyUI Essentials fork",
      "type": "tool",
      "description": "Modified version with small changes to nodes for upscaler workflow",
      "from": "Lan8mark"
    },
    {
      "creation": "MAGREF WAN 2.2 integration",
      "type": "workflow",
      "description": "Multi-character consistency solution for I2V generation",
      "from": "Elvaxorn"
    },
    {
      "creation": "SEC ComfyUI nodes",
      "type": "node",
      "description": "Enhanced SAM2 guidance nodes for better video segmentation",
      "from": "ArtOfficial"
    },
    {
      "creation": "Tiny VAE loader node",
      "type": "node",
      "description": "Loader for TAE VAE in WanVideoWrapper for faster preview",
      "from": "Kijai"
    },
    {
      "creation": "RCM LoRA",
      "type": "lora",
      "description": "Distillation LoRA by Nvidia devs for speed without motion loss",
      "from": "JohnDopamine"
    },
    {
      "creation": "Kijai's extracted Lightx2v LoRA",
      "type": "lora",
      "description": "Properly extracted version of the official LoRA that actually works",
      "from": "Kijai"
    },
    {
      "creation": "WanMoE KSampler with skimmed CFG",
      "type": "node",
      "description": "Fork of MoE sampler with added CFG adjusting capabilities",
      "from": "GalaxyTimeMachine"
    },
    {
      "creation": "Kijai's LightX2V High LoRA extraction",
      "type": "lora",
      "description": "64 rank LoRA extracted from full model with additional patches missing from original release",
      "from": "Kijai"
    },
    {
      "creation": "AI Toolkit video training",
      "type": "tool",
      "description": "Used locally to train Wan 2.2 for specific tricks like skateboard Tre Flip",
      "from": "Drommer-Kille"
    },
    {
      "creation": "Pusa LoRA",
      "type": "lora",
      "description": "Allows Wan T2V model to use input images for I2V or frame extension",
      "from": "Kijai"
    },
    {
      "creation": "CFG Helper Node",
      "type": "node",
      "description": "Node for creating complex CFG schedules in workflows",
      "from": "Kijai"
    },
    {
      "creation": "Text Encode Cache Node",
      "type": "node",
      "description": "Caches text encodings to file to prevent OOM issues",
      "from": "mdkb"
    },
    {
      "creation": "PyTorch 2.9.0 VAE workaround",
      "type": "code fix",
      "description": "Workaround that does conv3d in fp32 to bypass PyTorch bug while minimizing VRAM usage",
      "from": "Kijai"
    },
    {
      "creation": "Video stitching workflow with VACE",
      "type": "workflow",
      "description": "Uses VACE frame inpainting to connect video segments with temporal consistency",
      "from": "Koba"
    },
    {
      "creation": "Batch video processing setup",
      "type": "workflow",
      "description": "Uses load video node and meta batch manager for long video upscaling",
      "from": "happy.j"
    },
    {
      "creation": "Two-video pose concatenation system",
      "type": "tool",
      "description": "Concatenates 3D pose estimations from two videos into 2D space with controllable mixing strength",
      "from": "Mads Hagbarth Damsbo"
    },
    {
      "creation": "WAN 2.2 Mega Rapid AIO GGUF conversion",
      "type": "model",
      "description": "Converted fp8 model to q5km, q4km, q3km formats - q4 looks similar to fp8 but uses less storage (10.8GB vs 16.1GB)",
      "from": "patientx"
    },
    {
      "creation": "Car LoRA training",
      "type": "lora",
      "description": "Trained WAN LoRA on 10 images of same green car, learned impressive details but limited color flexibility",
      "from": "Dever"
    },
    {
      "creation": "Cartoon style LoRA",
      "type": "lora",
      "description": "Custom trained cartoon style LoRA for Wan",
      "from": "The Shadow (NYC)"
    },
    {
      "creation": "WanVideoWrapper memory dump tool",
      "type": "tool",
      "description": "Tool for generating memory dumps, works on Linux with cuda malloc disabled",
      "from": "Kijai"
    },
    {
      "creation": "Clownshark temporal mask workflow",
      "type": "workflow",
      "description": "Complex workflow using clownshark sampler with temporal masks for regional conditioning",
      "from": "TK_999"
    },
    {
      "creation": "Ditto LoRAs resized to lower rank",
      "type": "lora",
      "description": "Kijai resized original rank 128 LoRAs to average rank 29 for easier sharing and usage",
      "from": "Kijai"
    },
    {
      "creation": "Ditto LoRA extraction",
      "type": "technique",
      "description": "Method for extracting LoRAs from full modules using LoraExtractKJ node",
      "from": "Kijai"
    },
    {
      "creation": "Krea realtime LoRA",
      "type": "lora",
      "description": "Extracted LoRA from Krea realtime model for use with Wan 2.2",
      "from": "aipmaster"
    },
    {
      "creation": "Image concatenation workflow",
      "type": "workflow",
      "description": "Workflow for comparing results side by side with prompts overlay",
      "from": "Kijai"
    },
    {
      "creation": "Extracted Krea realtime LoRA",
      "type": "lora",
      "description": "2.5GB LoRA extracted from Krea realtime model for use with Wan 2.2",
      "from": "aipmaster"
    },
    {
      "creation": "Colab tunneling code",
      "type": "script",
      "description": "Code to use built-in colab tunneling for ComfyUI instead of external services",
      "from": "Draken"
    },
    {
      "creation": "WanVideoWrapper MoCha support",
      "type": "node",
      "description": "Kijai implemented MoCha in ComfyUI WanVideoWrapper",
      "from": "Kijai"
    },
    {
      "creation": "Aipmaster's Krea workflow",
      "type": "workflow",
      "description": "Complete workflow using Krea LoRA with dual sampling for realistic results",
      "from": "aipmaster"
    },
    {
      "creation": "VAE conversion script",
      "type": "tool",
      "description": "Claude-generated script to reverse convert diffusers VAE format to ComfyUI format",
      "from": "Kijai"
    },
    {
      "creation": "Ditto global style LoRA",
      "type": "lora",
      "description": "Powerful style transfer for video, can change entire anime episodes to different styles",
      "from": "JohnDopamine"
    },
    {
      "creation": "Loop nodes for context sliding",
      "type": "workflow",
      "description": "Automated I2V chaining using ComfyUI's general loop nodes for continuous video generation",
      "from": "Kijai"
    },
    {
      "creation": "WanVideoWrapper",
      "type": "node",
      "description": "Kijai's ComfyUI wrapper for Wan video models",
      "from": "seitanism"
    },
    {
      "creation": "Wanx Troopers knowledge base",
      "type": "tool",
      "description": "Community-maintained update tracker for Wan ecosystem",
      "from": "42hub"
    },
    {
      "creation": "SVI workflow adaptations",
      "type": "workflow",
      "description": "Community adaptations of SVI for different Wan model versions",
      "from": "Dever"
    },
    {
      "creation": "Custom mask input node",
      "type": "node",
      "description": "Allows manual mask specification for native ComfyUI video generation",
      "from": "Ablejones"
    },
    {
      "creation": "SVI LoRA conversions",
      "type": "model conversion",
      "description": "Converted SVI LoRAs from fp32 diffsynth format to fp16 safetensors for native ComfyUI",
      "from": "Kijai"
    },
    {
      "creation": "VK's SVI extension workflow",
      "type": "workflow",
      "description": "Improved method using reference image as first frame for better consistency in video extension",
      "from": "VK (5080 128gb)"
    },
    {
      "creation": "Suplex LoRA",
      "type": "lora",
      "description": "Wrestling suplex move LoRA uploaded to Civitai",
      "from": "Lodis"
    },
    {
      "creation": "HoloCine sparse model implementation",
      "type": "tool",
      "description": "Experimental implementation of sparse model for long video generation",
      "from": "mamad8"
    },
    {
      "creation": "Seamless video workflow with auto frame removal",
      "type": "workflow",
      "description": "Automatically removes stray frames for seamless output",
      "from": "VK (5080 128gb)"
    },
    {
      "creation": "Holocine experimental workflow",
      "type": "workflow",
      "description": "ComfyUI workflow for Holocine using video attention split method",
      "from": "avataraim"
    },
    {
      "creation": "Mamad8's Holocine implementation",
      "type": "custom implementation",
      "description": "Implementation of Holocine split prompting + sparse inter-shot self attention in wrapper",
      "from": "mamad8"
    },
    {
      "creation": "LightX quantile 0.15 LoRA",
      "type": "lora",
      "description": "Distillation LoRA by Kijai for faster inference",
      "from": "Kijai"
    },
    {
      "creation": "LongCat distill LoRA merge",
      "type": "lora",
      "description": "Kijai merged the distill LoRA by renaming layers and splitting fused qkv components",
      "from": "Kijai"
    },
    {
      "creation": "Ovi 512x512 hack",
      "type": "workflow",
      "description": "Modified ovi_fusion_engine.py area calculations for faster 512x512 generation",
      "from": "reallybigname"
    },
    {
      "creation": "Holocine ComfyUI nodes",
      "type": "node",
      "description": "WanVideoHolocineShotBuilder, WanVideoHolocinePromptEncode, WanVideoHolocineSetShotAttention nodes",
      "from": "shaggss"
    },
    {
      "creation": "LongCat ComfyUI wrapper",
      "type": "node",
      "description": "Implementation in WanVideoWrapper for LongCat model support",
      "from": "Kijai"
    },
    {
      "creation": "SageAttention 2.2.0 Windows wheel",
      "type": "tool",
      "description": "Pre-built wheel for Windows Python 3.10 users",
      "from": "avataraim"
    },
    {
      "creation": "HoloCine ComfyUI integration",
      "type": "node",
      "description": "Pull request adding HoloCine support to WanVideoWrapper",
      "from": "Kijai"
    },
    {
      "creation": "MediaSyncer improvements",
      "type": "tool",
      "description": "Side by side video comparison, zooming fills letterboxing, A-B loop option, handles 12+ videos simultaneously",
      "from": "phazei"
    },
    {
      "creation": "SageAttention Windows wheel",
      "type": "tool",
      "description": "Windows wheel for SageAttention 2.2.0 with torch compile support",
      "from": "avataraim"
    },
    {
      "creation": "FP32 to FP8 conversion script",
      "type": "script",
      "description": "Claude Opus scripted converter for fp32 to fp8 checkpoints",
      "from": "JohnDopamine"
    },
    {
      "creation": "Wan 2.1 VAE 2x upscaler",
      "type": "model",
      "description": "Trained VAE decoder that upscales 2x and reduces noise patterns",
      "from": "spacepxl"
    },
    {
      "creation": "ComfyUI-VAE-Utils",
      "type": "node",
      "description": "Custom VAE loader and decode nodes for upscaling workflows",
      "from": "spacepxl"
    },
    {
      "creation": "HuMo i2v mode modifications",
      "type": "patch",
      "description": "Custom changes to allow HuMo to work in i2v mode",
      "from": "Ablejones"
    },
    {
      "creation": "WanImageToVideo batch masks node",
      "type": "node",
      "description": "Allows WanImageToVideo node to work with batch of masks for svi-film",
      "from": "Ablejones"
    },
    {
      "creation": "LongCat refinement LoRA",
      "type": "lora",
      "description": "Improves video generation quality when used with specific sampling parameters",
      "from": "Kijai"
    },
    {
      "creation": "WAN 2x VAE",
      "type": "model",
      "description": "Provides significant quality improvements with better detail and sharpness",
      "from": "spacepxl"
    },
    {
      "creation": "Video extension helper node",
      "type": "node",
      "description": "Being developed to simplify video extension workflows",
      "from": "Kijai"
    },
    {
      "creation": "24fps-ifier WAN LoRA",
      "type": "lora",
      "description": "Planned LoRA to make WAN work at 24fps for faster paced content, using 550 videos dataset with 121 frames each",
      "from": "Kiwv"
    },
    {
      "creation": "Uncensoring LoRA for WAN",
      "type": "lora",
      "description": "Large dataset LoRA project using 550 videos from r34, manually captioned, to completely uncensor WAN",
      "from": "Kiwv"
    },
    {
      "creation": "WanVideoWrapper LongCat support",
      "type": "node",
      "description": "ComfyUI wrapper supporting LongCat model with extended video workflows",
      "from": "Kijai"
    },
    {
      "creation": "LLM-assisted segment prompting workflow",
      "type": "workflow",
      "description": "Uses LLM to generate prompts for next video segments based on last frame",
      "from": "JohnDopamine"
    },
    {
      "creation": "LightX2V 1030 LoRA extraction",
      "type": "lora",
      "description": "Community member extracted LoRA from full model since official ComfyUI-compatible version wasn't initially available",
      "from": "DawnII"
    },
    {
      "creation": "WanVideo ComfyUI integration update",
      "type": "node",
      "description": "Major update including LongCat support, improved LoRA handling, and fp32 norm layers",
      "from": "Kijai"
    },
    {
      "creation": "ChronoEdit ComfyUI conversion",
      "type": "model",
      "description": "Converted NVIDIA ChronoEdit to ComfyUI format with distill LoRA",
      "from": "Kijai"
    },
    {
      "creation": "LightX2V 1030 LoRA",
      "type": "lora",
      "description": "Third iteration 2.2 high noise distill LoRA with better prompt adherence and camera control",
      "from": "Kijai"
    },
    {
      "creation": "ChronoEdit distill LoRA",
      "type": "lora",
      "description": "Distill LoRA for ChronoEdit functionality with Wan models",
      "from": "Kijai"
    },
    {
      "creation": "wan2.2_lightx2v_1030",
      "type": "lora",
      "description": "New high noise speed LoRA for 4 steps, rank 64, bf16",
      "from": "avataraim"
    },
    {
      "creation": "LongCat face consistency workflow",
      "type": "workflow",
      "description": "Uses face inpaint with Phantom to maintain character identity in video extensions",
      "from": "Ablejones"
    },
    {
      "creation": "LongCat refinement LoRA",
      "type": "lora",
      "description": "LongCat_refinement_lora_rank128_bf16.safetensors for assisting upscaling",
      "from": "Kijai"
    }
  ]
}