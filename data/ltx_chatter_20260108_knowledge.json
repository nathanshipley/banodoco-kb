{
  "channel": "ltx_chatter",
  "date_range": "2026-01-08 to 2026-01-16",
  "messages_processed": 10000,
  "chunks_processed": 25,
  "api_usage": {
    "input_tokens": 314390,
    "output_tokens": 65809,
    "estimated_cost": 1.930305
  },
  "extracted_at": "2026-02-01T23:39:23.777850Z",
  "discoveries": [
    {
      "finding": "SDE samplers require 1000+ steps for proper convergence",
      "details": "Over 1,000 steps with SDE sampler is how diffusion/flow models are meant to be used. Around 1000 steps is what it takes to converge a noisy SDE with flow models, tested on Flux",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "Temporal upscale lora doubles frames through interpolation",
      "details": "The temporal upscale lora actually doubles the frames (interpolates), so frame count in video combine node needs to be 2x when using it",
      "from": "Grimm1111"
    },
    {
      "finding": "Frame count rule is 8n+1 for total frames",
      "details": "Frames need to be divisible by 8, plus 1 frame. This applies to frame_count (total number of frames), not FPS",
      "from": "buggz"
    },
    {
      "finding": "Seed hunting can be more effective than high step counts",
      "details": "Getting more seeds in can help more than doing many steps",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "finding": "Native 1080p uses 4x more RAM than upscaled 1080p",
      "details": "Raw 1080 generation uses about 4x the RAM compared to generating at lower resolution and upscaling",
      "from": "Kiwv"
    },
    {
      "finding": "VRAM debug nodes can solve slowdown issues",
      "details": "Placing VRAM debug nodes between text encoder and ksampler, and after video generation clears memory and maintains consistent 2s/it speeds",
      "from": "Phr00t"
    },
    {
      "finding": "Abliterated Gemma model works with LTX",
      "details": "User successfully created and tested an abliterated version that adheres better to certain prompts",
      "from": "Kiwv"
    },
    {
      "finding": "FP4 projections model can work in CLIP encoder",
      "details": "Using ltx-2-19b-dev-fp4_projections_only.safetensors in DualClipLoader can help with VRAM issues",
      "from": "JUSTSWEATERS"
    },
    {
      "finding": "Spatial upscaler may not need distilled LoRA",
      "details": "Testing showed disabling distilled lora in upscaler route with 20 steps/cfg 4 produced results",
      "from": "TK_999"
    },
    {
      "finding": "Separating VAE from model provides better control and avoids duplication",
      "details": "VAE separated from checkpoint gives more control and doesn't need duplication for every model, plus can be used for VAE pre-caching for trainers",
      "from": "yi"
    },
    {
      "finding": "CRF/LTXVPreprocess adds compression artifacts to bridge train-test gap",
      "details": "Adds h.264-like compression artifacts to pristine images to prevent static/frozen I2V results by bridging gap between training on real video frames vs pristine T2I images",
      "from": "976496720370348032"
    },
    {
      "finding": "Sigmas are animation curves",
      "details": "Realization that sigmas function as animation curves for video generation",
      "from": "JUSTSWEATERS"
    },
    {
      "finding": "Variable latent compression rates being developed",
      "details": "Future versions will have variable latent compression rates or variable latent spaces, still WIP. Latent temporal upscaler works against temporal compression by spreading over double the latents in time axis",
      "from": "976496720370348032"
    },
    {
      "finding": "Full prompting of audio content improves audio generation quality",
      "details": "When using audio continuation, prompt for the complete text that will be spoken in the final video, including what was said in the audio prefix that's being kept",
      "from": "976496720370348032"
    },
    {
      "finding": "Non-distilled models perform significantly better than distilled versions",
      "details": "30 steps cfg 4.0 on undistilled fp8 shows much bigger quality difference than expected compared to distilled versions",
      "from": "Kijai"
    },
    {
      "finding": "LTX Video 2 can be used effectively for upscaling other model outputs",
      "details": "Using LTX to upscale Wananimate generations to 1080p creates a 'killer combo'",
      "from": "David Snow"
    },
    {
      "finding": "Audio continuation works by model attending to masked audio and video latents",
      "details": "The model attends to the audio latents that are left masked and to the video latents too for voice matching",
      "from": "976496720370348032"
    },
    {
      "finding": "48 FPS significantly improves I2V quality by fixing temporal compression issues",
      "details": "Fixes smudginess and glitchy artifacts in fast moving frames for I2V. Better than temporal upscaler.",
      "from": "Ada"
    },
    {
      "finding": "LTX-2 has impressive prompt adherence for complex sequences",
      "details": "Can handle complex prompts with dialogue and multiple actions: 'character x does that, then says:\"blablabla\", then turns around and goes somewhere and picks up a book, then says:\"etcetc\" then the character jumps out a window' and executes everything correctly in a 20s video",
      "from": "seitanism"
    },
    {
      "finding": "Spatial inpainting is possible with LTX-2",
      "details": "Can use spatial_mask input to generate parts of frames/all frames. Different from temporal inpainting which regenerates sections of video/audio.",
      "from": "976496720370348032"
    },
    {
      "finding": "Model supports frame counts of 8k+1 format",
      "details": "Frame count must be multiple of 8, plus 1: 1, 9, 17, 25... (8k+1 for k in N)",
      "from": "976496720370348032"
    },
    {
      "finding": "Full attention across all video latents enables true 20-second context",
      "details": "Unlike AnimDiff's 16 frame context, LTX-2 provides full attention between all video latents up to ~484 frames (20 seconds)",
      "from": "976496720370348032"
    },
    {
      "finding": "LTX-2 struggles with non-upright character poses",
      "details": "Model is completely overwhelmed with characters doing handstands or similar non-standard poses",
      "from": "seitanism"
    },
    {
      "finding": "Disabling sage attention patch and torch compile can help with OOM issues",
      "details": "Juan Gea found that after disabling these features, sampling worked better and reduced memory pressure",
      "from": "Juan Gea"
    },
    {
      "finding": "Text encoding can use up to 50GB RAM with fp8 model",
      "details": "This seems excessive for a 12B model but is currently happening",
      "from": "CDS"
    },
    {
      "finding": "Canceling and restarting workflow during ksampler step makes inference 12x faster",
      "details": "100% repeatable and reliable method - faster to cancel then restart than let it complete",
      "from": "Phr00t"
    },
    {
      "finding": "LTX2 has automatic padding capability for video continuation",
      "details": "Kijai added option to pad input frames automatically - if you input 81 frames and set end time to 10s, it will create new frames to reach that duration and mask them out for extension",
      "from": "Kijai"
    },
    {
      "finding": "Same seed produces extremely similar results even at different resolutions",
      "details": "When using same seed and settings but just increasing resolution, the model generates the same video content just sharper",
      "from": "seitanism"
    },
    {
      "finding": "CFG causes significant VRAM increase",
      "details": "CFG mode causes batched processing which increases VRAM usage by 4GB compared to CFG 1.0",
      "from": "Kijai"
    },
    {
      "finding": "Model can handle negative frame positioning for reference",
      "details": "Negative values in frame positioning place reference latents 'in the past' using positional encodings, not Python-style indexing",
      "from": "976496720370348032"
    },
    {
      "finding": "fp8 distilled model has better audio quality",
      "details": "fp8 distilled doesn't have the tinny sound that the regular fp8 model has",
      "from": "David Snow"
    },
    {
      "finding": "Sigma values can be used as animation curves",
      "details": "Sigmas could be used as like an animation curve for controlling animation timing",
      "from": "JUSTSWEATERS"
    },
    {
      "finding": "Custom text encoders can significantly change output",
      "details": "Using different text encoders like antislop models produces notably different results for the same prompt",
      "from": "Kiwv"
    },
    {
      "finding": "LTX Video 2 inpainting works well",
      "details": "Inpainting functionality produces good results that are hard to distinguish from original content",
      "from": "Hashu"
    },
    {
      "finding": "Samplers/Schedulers affect lipsync in I2V",
      "details": "Using DDIM sampler in first KSampler improves lipsync, default settings were producing no lipsync",
      "from": "Elvaxorn"
    },
    {
      "finding": "LTX-2 performs better with manual sigmas than basic schedulers",
      "details": "Manual sigmas provide better results compared to BasicScheduler or LTXVScheduler",
      "from": "yi"
    },
    {
      "finding": "Distill LoRA works with 20 steps, full model needs 40 steps",
      "details": "20 steps only works if you include the distill lora for stage 2, otherwise 40 steps is needed for full model",
      "from": "psylent_gamer"
    },
    {
      "finding": "VAE encoding uses significant VRAM",
      "details": "Encoding 121 frames at 1024x1024 uses like 15GB VRAM",
      "from": "Kijai"
    },
    {
      "finding": "FP8 distill vs full model quality difference",
      "details": "Full model provides significantly better quality than distilled version, especially for consistency",
      "from": "psylent_gamer"
    },
    {
      "finding": "Image compression factor can fix motionless video issues",
      "details": "Increasing image compression factor helps add motion to videos while maintaining quality, works even without camera LoRA",
      "from": "1100803024298975263"
    },
    {
      "finding": "48 fps generation reduces artifacts",
      "details": "Generating at 48fps instead of 24fps can help with sharpness and reduce blurry artifacts",
      "from": "Ada"
    },
    {
      "finding": "Perfect input images cause motionless videos",
      "details": "When input image is too perfect with no blur, the generated video becomes motionless, especially in portraiture",
      "from": "1100803024298975263"
    },
    {
      "finding": "Two-pass procedure with distill LoRA",
      "details": "Advantage of distill LoRA is doing 20 steps undistilled, then 3 steps with latent upscalers + LoRA",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "finding": "Q8 GGUF is better than fp8",
      "details": "Q8 provides better quality than fp8, though it's slower on 40xx series cards and above",
      "from": "Kijai"
    },
    {
      "finding": "Distilled model produces blurry faces",
      "details": "LTX2 distilled model tends to generate blurry or out-of-focus faces, feels like focus issues rather than motion blur",
      "from": "Kijai"
    },
    {
      "finding": "FP8 scaled is closest to BF16 quality",
      "details": "General motion and structure wise, FP8 scaled performs most similarly to BF16",
      "from": "Kijai"
    },
    {
      "finding": "NAG (Negative Attention Guidance) works with LTX Video 2",
      "details": "Without NAG prompt, jeep was green, with NAG it changed color as expected",
      "from": "Kijai"
    },
    {
      "finding": "Distill lora acts as lightx2v equivalent",
      "details": "Consider the distill lora as the lightx2v for LTX",
      "from": "Kijai"
    },
    {
      "finding": "Audio upscaler AudioSR shows no notable difference",
      "details": "Testing showed no improvements when using AudioSR for audio enhancement",
      "from": "toxicvenom117"
    },
    {
      "finding": "Higher FPS reduces blurriness",
      "details": "You can avoid blurriness to some extent by increasing the FPS value",
      "from": "KingGore2023"
    },
    {
      "finding": "NAG (Negative Audio/visual Guidance) now works with LTX Video 2",
      "details": "Can add negative prompts like 'cartoon' to avoid unwanted styles, or 'music' to remove background music. Works similarly to how it did with Wan",
      "from": "Kijai"
    },
    {
      "finding": "Can adjust distill strength even when using the distilled model",
      "details": "Previously thought this wasn't possible, but distill strength can be modified for quality tuning",
      "from": "Kijai"
    },
    {
      "finding": "FP8 scaled Gemma 3 text encoder available",
      "details": "New fp8_scaled version is closer to bf16 quality than normal fp8, requires latest ComfyUI commit",
      "from": "Kijai"
    },
    {
      "finding": "Enhance-A-Video works with LTX Video 2",
      "details": "Universal method originally for CogVideoX, applied only to video attention layers, weaker effect than other models",
      "from": "Kijai"
    },
    {
      "finding": "Using dev model with distilled LoRA at negative weight (-0.3 to -0.6) fixes plastic skin issue",
      "details": "Loading distill fp8 model with distill LoRA at -0.60 weight removes shiny plastic skin appearance from humans while maintaining speed benefits",
      "from": "David Snow"
    },
    {
      "finding": "Embedding connectors are different between dev and distill models",
      "details": "The embeddings_connector weights are completely different between dev and distill models, giving almost completely different results when swapped",
      "from": "Kijai"
    },
    {
      "finding": "SLG works with LTX-Video 2",
      "details": "SLG guidance works but has huge effect on block 1, block 2 seems good with subtle effects, stronger near start of blocks",
      "from": "Kijai"
    },
    {
      "finding": "Removing negative prompts can improve results",
      "details": "Removing all negative conditioning completely fixed a t2v generation that wasn't working properly",
      "from": "BobbyD4AI"
    },
    {
      "finding": "Distil LoRA at negative weight fixes shiny skin and AI contrast issues",
      "details": "Using distil LoRA on upscale pass only at -0.5 weight reduces shiny plastic look and overly contrasted AI appearance. Don't use on first pass as it can affect motion.",
      "from": "David Snow"
    },
    {
      "finding": "LTXVAddGuideMulti node ignores frame_idx parameter",
      "details": "When adding guide for image at index 16, it adds at the end and ignores the frame_idx number",
      "from": "Phr00t"
    },
    {
      "finding": "Audio-only LoRA training works extremely well and fast",
      "details": "Training runs at 2 steps per second on RTX5090",
      "from": "mamad8"
    },
    {
      "finding": "New Comfy-Org Gemma models have better accuracy than fp8_scaled",
      "details": "fpmixed and fp4_mixed versions target 95% and 90% fp4 layers respectively with minimal loss",
      "from": "comfy"
    },
    {
      "finding": "Distilled model produces much better results than non-distilled",
      "details": "User reported getting much better results switching from non-distilled to distilled model",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "Audio quality improves after upscaler pass",
      "details": "Sound becomes cleaner after upscaling, especially music clarity",
      "from": "N0NSens"
    },
    {
      "finding": "Dev model produces better audio than distilled model",
      "details": "Dev model after upscaler produces much better sound quality than distilled model after upscaler",
      "from": "N0NSens"
    },
    {
      "finding": "VAE tiled decoding causes flashing/flickering",
      "details": "Flash/flickering always at same point in videos caused by VAE decode (Tile), doesn't happen with regular VAE decode",
      "from": "mamad8"
    },
    {
      "finding": "I2V mask level affects movement",
      "details": "If it doesn't move under 0.3 mask level then something else is broken",
      "from": "Kijai"
    },
    {
      "finding": "CFG works fine with distilled model for few steps",
      "details": "CFG for few steps while using distill seems to work fine, especially with SLG",
      "from": "Kijai"
    },
    {
      "finding": "LTX2 uses modality-aware classifier-free guidance with separate control parameters",
      "details": "st controls text guidance strength, sm controls cross-modal (audio\u2194video) guidance strength. Default values: Video st=3, sm=3; Audio st=7, sm=3",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "finding": "Keyframe conditioning uses reference latent technique with positional embeddings",
      "details": "Model attends to dedicated new latent frame matching the set index, then frame is discarded using LTXCropGuides. Doesn't put keyframe directly into tensor due to latent space architecture",
      "from": "976496720370348032"
    },
    {
      "finding": "Audio latents are fixed at 25 per second, video latents are 8 pixel frames per latent frame",
      "details": "Video pixel frames per second is variable, so audio latent initialization needs to know pixel frame count and fps to match properly",
      "from": "976496720370348032"
    },
    {
      "finding": "Detail daemon produces floating orbs and particles as byproduct",
      "details": "Particles are intentional motion-generating effect but can be excessive",
      "from": "David Snow"
    },
    {
      "finding": "Using -1 frame index creates looping videos",
      "details": "When last frame is similar/identical to first frame, -1 index produces looping generations",
      "from": "Gleb Tretyak"
    },
    {
      "finding": "GGUF models work with multiple loaders",
      "details": "Unsloth GGUF models work but require their specific loader",
      "from": "cocktailprawn1212"
    },
    {
      "finding": "Distilled LoRA can be used with full model at positive values",
      "details": "Examples show dev model + distilled lora at positive values working well",
      "from": "Hashu"
    },
    {
      "finding": "Distilled model can use negative LoRA values",
      "details": "Distilled model + distill lora at negative values produces good results",
      "from": "Hashu"
    },
    {
      "finding": "res_2s with 8 steps performs better than 4 steps",
      "details": "res_2s with 8 steps is 100% better than 4 steps but twice slower",
      "from": "N0NSens"
    },
    {
      "finding": "cfg > 1 with 'music' in negative prompt removes unwanted music",
      "details": "Using cfg higher than 1 with 'music' in negative prompt successfully prevents unwanted music generation",
      "from": "foxydits"
    },
    {
      "finding": "LTX2 has VRAM limits even with offloading",
      "details": "VRAM offload can only offload model weights, but if activations don't fit in 24GB then generation size is still too large (1080p 480 frames failed)",
      "from": "Kijai"
    },
    {
      "finding": "Close-up portrait prompts trigger image zoom behavior",
      "details": "Using 'A close-up portrait' in T2V prompts causes the model to generate static images with zoom effects instead of proper video",
      "from": "hicho"
    },
    {
      "finding": "LTX2 can generate high-quality still images",
      "details": "The close-up portrait 'bug' actually generates very high quality images, can be used as an image generator workflow",
      "from": "hicho"
    },
    {
      "finding": "Quotation marks in prompts improve lip sync",
      "details": "Using quotation marks around spoken dialogue in prompts helps generate better lip movement",
      "from": "hicho"
    },
    {
      "finding": "Video extension works like inpainting",
      "details": "Video extension is analogous to image inpainting - describing the whole video including unchanged parts helps the model find better fitting solutions",
      "from": "Kijai"
    },
    {
      "finding": "Film grain helps with I2V motion - the more grain, the more likely the image will move",
      "details": "Adding film grain to images before I2V processing significantly improves motion generation",
      "from": "Elvaxorn"
    },
    {
      "finding": "Audio start time affects video motion in I2V",
      "details": "Having audio start time at 0.0 screws up motion. Audio start time should be equal to or higher than video start time (e.g., 0.2) to prevent static video at beginning",
      "from": "Elvaxorn"
    },
    {
      "finding": "JPEG compression adds macroblock noise similar to MPEG video compression",
      "details": "Model likely learned during training that this macroblock noise is associated with cinematic sequences",
      "from": "138332118890708992"
    },
    {
      "finding": "Spatial audio masking works",
      "details": "Can mask out specific characters in audio while keeping others",
      "from": "Kijai"
    },
    {
      "finding": "PyTorch 2.9.0+cu130 vs 2.8.0+cu129 performance difference",
      "details": "Full HD 81 frames render time: 201 sec vs 89 sec on 2.9.0+cu130",
      "from": "protector131090"
    },
    {
      "finding": "LTX2 has severe I2V VRAM issues due to per-token timestep handling",
      "details": "I2V uses gigabytes more VRAM than T2V, similar to WAN 5B issue with huge intermediate timestep tensors",
      "from": "Kijai"
    },
    {
      "finding": "Different embedding connectors exist for dev vs distilled models",
      "details": "Dev and distilled models have different embedding_connector files, mixing them produces different outputs",
      "from": "Kijai"
    },
    {
      "finding": "Q8 distilled model performs better than Q8 dev + distilled LoRA",
      "details": "Initial testing shows Q8 distilled gives better results than using Q8 dev with distilled LoRA for I2V use cases",
      "from": "Choowkee"
    },
    {
      "finding": "LTX2 can accept video input for video-to-video generation",
      "details": "Can feed video directly into LTXVImgToVideoInplace node, works with 721 frames for continuous video generation",
      "from": "ZombieMatrix"
    },
    {
      "finding": "NSFW content generation varies with text encoders",
      "details": "Stock gemma prevents touching underwear, while abliterated gemma allows touching underwear and certain provocative motions",
      "from": "Choowkee"
    },
    {
      "finding": "Video compression helps with motion",
      "details": "LTX uses video compression artifacts to derive motion, so adding compression to a scene can drive up the amount of motion it interprets",
      "from": "ZombieMatrix"
    },
    {
      "finding": "Memory optimization breakthrough for I2V",
      "details": "Can now run 1920x1088x193 I2V with no extra reserve VRAM on 4090, previously needed 7GB reserve",
      "from": "Kijai"
    },
    {
      "finding": "Upscale sampler improves audio quality",
      "details": "Both upscale sampler and step count can definitely clean up the audio",
      "from": "garbus"
    },
    {
      "finding": "Distilled LoRA provides massive speedup",
      "details": "Can be used with both Q4 GGUF and regular models, provides significant quality improvement especially at 8 steps",
      "from": "Kiwv"
    },
    {
      "finding": "Audio quality is linked to resolution",
      "details": "Higher resolutions like 1920x1088 give better audio quality than lower resolutions like 960x544",
      "from": "568465354158768129"
    },
    {
      "finding": "Frame rate affects video quality significantly",
      "details": "50fps produces noticeably better quality than 24fps, smoother animations at higher resolutions",
      "from": "Choowkee"
    },
    {
      "finding": "Resolution affects motion quality",
      "details": "960x544 produces almost static motion, while higher resolutions have smoother animations",
      "from": "568465354158768129"
    },
    {
      "finding": "1440x800 is minimum resolution for lip sync",
      "details": "Below this resolution, lip sync quality degrades significantly",
      "from": "568465354158768129"
    },
    {
      "finding": "50fps produces much faster action execution and more fluid motion with micro expressions not present in 24fps versions",
      "details": "50fps shows lots of little micro expressions that aren't there on the 24fps version, definitely more fluid",
      "from": "Choowkee"
    },
    {
      "finding": "Resolution affects I2V results significantly - higher resolution doesn't always mean better quality",
      "details": "1920x1080 resolution minimum recommended, but changing resolutions is like changing the seed. Higher resolution does not mean the scene is actually better",
      "from": "568465354158768129"
    },
    {
      "finding": "Frame count changes can dramatically affect output quality",
      "details": "Changing total frames from 151 to 153 made video much worse - miscalculating frames (151 instead of 153) produced better results",
      "from": "568465354158768129"
    },
    {
      "finding": "Random noise seed value has barely any noticeable effect in I2V",
      "details": "Changing random noise_seed values had minimal impact, might have more effect on T2V but not tested yet",
      "from": "568465354158768129"
    },
    {
      "finding": "Distilled LoRA strength of 0.8 instead of 1.0 makes skin look less plastic",
      "details": "Reducing distill lora value from 1.0 to 0.8 makes skin less plastic looking",
      "from": "568465354158768129"
    },
    {
      "finding": "Training dataset bias affects generation based on fps and resolution",
      "details": "Different fps and resolution settings trigger different training data - 50fps suggests model was probably trained on 50-60 fps content",
      "from": "568465354158768129"
    },
    {
      "finding": "VRAM usage memory optimization for I2V",
      "details": "Kijai's av_model.py modification drops I2V VRAM usage to near T2V levels, cuts VRAM use by ~1GB at higher resolutions",
      "from": "Kijai"
    },
    {
      "finding": "Higher FPS can reduce motion artifacts",
      "details": "48-50 FPS generation shows less texture smearing and breaking, helps with motion jank",
      "from": "David Snow"
    },
    {
      "finding": "First pass vs second pass quality differences",
      "details": "First pass maintains better character consistency and skin quality, second pass often introduces latex-like skin appearance",
      "from": "\u30e9D."
    },
    {
      "finding": "Audio loudness affects motion generation",
      "details": "Louder audio files produce more movement in lip sync generations, quiet speech can produce static results",
      "from": "protector131090"
    },
    {
      "finding": "av_model.py update provides significant speed improvements",
      "details": "Latest av_model.py shows 38.6s vs 36.9s performance improvement, also faster generation at 299.80s vs 313.99s from 15 hours prior",
      "from": "Choowkee"
    },
    {
      "finding": "Sage attention patch node works better than command line flag",
      "details": "Using Kijai's patch node instead of --use-sage-attention flag resolves dtype errors and works properly",
      "from": "Kijai"
    },
    {
      "finding": "Mixed precision text encoder gives better results",
      "details": "Using the mixed precision TE from Comfy-Org/ltx-2 split_files/text_encoders should give better results than regular fp8",
      "from": "TK_999"
    },
    {
      "finding": "Higher FPS improves quality for fast action",
      "details": "Model gives higher quality at >40fps, especially need 40-50fps for fast action scenes to avoid morphing degradation",
      "from": "nikolatesla20"
    }
  ],
  "troubleshooting": [
    {
      "problem": "Audio stopping halfway through video",
      "solution": "When using temporal upscale lora, set frame count in video combine node to 2x because the lora interpolates and doubles frames",
      "from": "Grimm1111"
    },
    {
      "problem": "Image to video looks like slideshow",
      "solution": "Adjust strength parameter - lower strength gives more motion but less adherence to original image",
      "from": "gordo"
    },
    {
      "problem": "Getting 'baked' look with too much CFG at 20 steps",
      "solution": "Try disabling sage attention and fp16 accumulation",
      "from": "NC17z"
    },
    {
      "problem": "Kernel error on 2060",
      "solution": "Fixed with latest kitchen version",
      "from": "hicho"
    },
    {
      "problem": "OOM issues with high resolution",
      "solution": "Use reserve RAM setting (--reserve-vram) and ensure sufficient system RAM for offloading",
      "from": "avataraim"
    },
    {
      "problem": "OOM errors during generation",
      "solution": "Use --reserve-vram 5 or higher, and try FP4 projections model in clip encoder",
      "from": "TK_999"
    },
    {
      "problem": "Model slowdown after first generation",
      "solution": "Place VRAM debug nodes after text encoder and after video generation to clear memory",
      "from": "Phr00t"
    },
    {
      "problem": "Missing tokenizer in abliterated model",
      "solution": "Include the model tokenizer in the file during conversion",
      "from": "Kiwv"
    },
    {
      "problem": "ComfyUI crashes during VAE decode",
      "solution": "Try regular VAE decode instead of tiled, or reduce overlap from 4096",
      "from": "Tachyon"
    },
    {
      "problem": "Unpin errors in ComfyUI",
      "solution": "Usually caused by custom nodes messing with model weights",
      "from": "comfy"
    },
    {
      "problem": "RuntimeError: Padding size should be less than corresponding input dimension",
      "solution": "Set trim audio duration to not 0 - you're starting after the index start in the second node",
      "from": "Q-"
    },
    {
      "problem": "OOM errors on 4090 with 64GB RAM",
      "solution": "Use --reserve-vram 2 (or higher) to offload more, or create 32GB swapfile, or use --lowvram --cache-none --reserve-vram 8",
      "from": "NebSH, taoofai, tamilboy"
    },
    {
      "problem": "CUDA locks every 4th run on 4090",
      "solution": "Restart ComfyUI - no easy way to remove models yet, open PRs exist to fix issues",
      "from": "Q-"
    },
    {
      "problem": "Static image in I2V with no movement",
      "solution": "Lower I2V strength to under 0.4, increase LTXVPreprocess strength value from 33 to higher, use more detailed prompts",
      "from": "Kijai, 976496720370348032"
    },
    {
      "problem": "Garbled faces in fast motion",
      "solution": "Turn fps up, but user wanted 24fps specifically",
      "from": "Benjimon, Ruairi Robinson"
    },
    {
      "problem": "Audio VAE encode needs stereo audio",
      "solution": "Use Audio Change Channels node or AudioBatch custom node to convert mono to stereo",
      "from": "976496720370348032, 421114995925843968"
    },
    {
      "problem": "Portrait videos (832/1216) causing severe artifacts",
      "solution": "Try smaller final resolution, use full model instead of distilled, or make shorter video. Portrait breaks more easily with high token count",
      "from": "976496720370348032"
    },
    {
      "problem": "Videos freeze when using character mutations or out-of-distribution content",
      "solution": "Use preprocessing (CRF), lower strength, and detailed prompting to help with frozen videos",
      "from": "976496720370348032"
    },
    {
      "problem": "Character won't animate in I2V, tends toward realism",
      "solution": "Try using pose control or other control signals, as the model may not identify animated characters well",
      "from": "Juan Gea"
    },
    {
      "problem": "Volume jumps significantly in audio continuation",
      "solution": "Adjust audio levels, original audio becomes quiet compared to generated portion",
      "from": "FUNZO"
    },
    {
      "problem": "RuntimeError about tensors on different devices with prompt enhancer",
      "solution": "Update ComfyUI to latest version, issue was fixed in recent update",
      "from": "yi"
    },
    {
      "problem": "OOM errors with Gemma3 in LTX custom node workflows",
      "solution": "Use the native ComfyUI workflow instead, or enable VRAM reserve settings",
      "from": "Juan Gea"
    },
    {
      "problem": "FigureCanvasAgg tostring_rgb error in VisualizeSigmasKJ node",
      "solution": "Just mute/disable this node as it's only for visualization",
      "from": "NebSH"
    },
    {
      "problem": "OOM errors on RTX 4090 with 24GB VRAM",
      "solution": "Use --reserve-vram 4 to 6. Higher values significantly slow performance (10x slower at reserve-vram 10)",
      "from": "protector131090"
    },
    {
      "problem": "I2V crashes on 30xx series",
      "solution": "Use FP8 models with specific combinations - shared working combo available",
      "from": "The Shadow (NYC)"
    },
    {
      "problem": "Motion freezing in V2V",
      "solution": "Issue occurs because V2V compresses 4 frames to 1 latent, causing problems with fast moving scenes where 4 frames are very different",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "problem": "Prompt enhancer causing errors in I2V",
      "solution": "Disable/bypass the prompt enhancer in official workflows",
      "from": "Juan Gea"
    },
    {
      "problem": "Quality degradation with higher frame counts",
      "solution": "Combined resolution+frames shouldn't exceed certain values. Either use many frames at low resolution or few frames at high resolution",
      "from": "seitanism"
    },
    {
      "problem": "Different checkpoints selected causing errors",
      "solution": "Ensure same checkpoint is used across all model loaders - distilled vs dev fp8 mismatch causes issues",
      "from": "Kagi"
    },
    {
      "problem": "4090 getting stuck on first custom ksampler with green bar",
      "solution": "Use --reserve-vram 4 or higher, Juan Gea suggests at least 4-5, up to 10 with controlnet",
      "from": "391020191338987522, Juan Gea"
    },
    {
      "problem": "OOM errors on second sampler",
      "solution": "Use higher --reserve-vram values incrementally, disable sage attention patch and torch compile",
      "from": "Juan Gea, seitanism"
    },
    {
      "problem": "Sage attention error about tensor dtypes",
      "solution": "Try without the startup argument - startup argument now overrides the patch node",
      "from": "Kijai"
    },
    {
      "problem": "Frozen/static results in I2V",
      "solution": "Use LTXVPreprocess with high strength (33+), weaker conditioning in InPlace node, long detailed prompts, pose control if needed",
      "from": "976496720370348032"
    },
    {
      "problem": "Mat1/mat2 error with text encoder",
      "solution": "Update KJ nodes to nightly using 'switch ver', ensure correct connector type loaded (gemma must be first)",
      "from": "Juan Gea, Kijai"
    },
    {
      "problem": "Prompt enhancer causing censorship and no movement",
      "solution": "Use llama.cpp server with prefill support, or disable prompt enhancer entirely",
      "from": "Ada, Juan Gea"
    },
    {
      "problem": "High VRAM usage with distilled LoRA",
      "solution": "Issue was CFG causing batched mode, not the LoRA itself. Use CFG 1.0 to reduce VRAM by 4GB",
      "from": "Kijai"
    },
    {
      "problem": "Device mismatch error (cuda:0 and cpu)",
      "solution": "Common when objects aren't on same device - need to move tensors to same location, often by encoding images to latents",
      "from": "Scruffy"
    },
    {
      "problem": "res_2s sampler error on Mac",
      "solution": "Download RES4LYF pack or change sampler to euler_ancestral in both stages",
      "from": "seitanism"
    },
    {
      "problem": "Upscaling destroys lip sync",
      "solution": "Use lower denoise (0.2) and lower shift (0.2-0.4), though results may still vary depending on face size",
      "from": "dj47"
    },
    {
      "problem": "LTXVGemmaEnhancePrompt node error: 'LTXAVTEModel_' object has no attribute 'processor'",
      "solution": "Bypass the enhancer node",
      "from": "Q-"
    },
    {
      "problem": "Black fade appearing in generations",
      "solution": "Try different prompts first, shouldn't be happening in the first place",
      "from": "Kiwv"
    },
    {
      "problem": "ComfyUI disconnecting with no errors during generation",
      "solution": "Issue identified but no specific solution provided yet",
      "from": "140897652324827137"
    },
    {
      "problem": "VAE encode/decode causing ghosting and duplicate images in V2V",
      "solution": "VAE from LTX is confirmed buggy, issue persists even with tiled VAE decode",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "problem": "Crash during clip loading",
      "solution": "Use KJ's workflow version or load gemma with dual clip loader",
      "from": "Michael Carychao"
    },
    {
      "problem": "Static image outputs instead of video",
      "solution": "Try lower resolution - gradually decrease so longest edge is 1024",
      "from": "Gleb Tretyak"
    },
    {
      "problem": "AttributeError: 'FigureCanvasAgg' object has no attribute 'tostring_rgb'",
      "solution": "Matplotlib version issue, Kijai will add fallback for different versions",
      "from": "psylent_gamer"
    },
    {
      "problem": "RAM usage issues with LTX-2",
      "solution": "Use --cache-ram with threshold or --cache-none for nuclear option, also try --disable-pinned-memory",
      "from": "Kijai"
    },
    {
      "problem": "Model crashes during tiled decode",
      "solution": "Increase page size to 65GB made workflows bearable, doesn't crash during tiled decode anymore",
      "from": "Tachyon"
    },
    {
      "problem": "I2V results super saturated and plastic style",
      "solution": "Add more noise to the image - if image is too perfect or synthetic, model behaves differently than with noisy or movie-like images",
      "from": "Lodis"
    },
    {
      "problem": "Slow I2V performance",
      "solution": "Use --reserve-vram with values starting from 1 or 2",
      "from": "Kijai"
    },
    {
      "problem": "--reserve-vram parameter is in gigabytes",
      "solution": "Use --reserve-vram 2 instead of 2048 - the parameter is in GB, not MB",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI crashes with 24GB VRAM",
      "solution": "Use --cache-none flag, gemma 4-bit on CPU, and fp8 checkpoint. Try --reserve-vram flag",
      "from": "onama"
    },
    {
      "problem": "Audio artifacts in continue workflow",
      "solution": "Set CFG to 4 with full fp8 model and 20-30 steps, or use distilled model with proper CFG settings",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "problem": "Same video output with time node",
      "solution": "Time node starts at 5 seconds but input is shorter - use padding node instead",
      "from": "Kijai"
    },
    {
      "problem": "GGUF models generate noise",
      "solution": "Need ComfyUI-GGUF PR #399 to load metadata properly, or use vantage fork",
      "from": "Kijai"
    },
    {
      "problem": "VAE tiling artifacts visible as squares",
      "solution": "Tiled VAE generates visible square artifacts with LTX, especially at higher resolutions like 2K",
      "from": "David Snow"
    },
    {
      "problem": "Slow second stage inference",
      "solution": "Second stage at twice resolution should be slower but 160s/it at 99% VRAM suggests memory issues",
      "from": "411637464806195202"
    },
    {
      "problem": "Line artifacts at high resolution",
      "solution": "Width & height must be divisible by 32+1, frame count by 8+1. Stay below 1537x1537, use 1280x720 or 1080x1080",
      "from": "Gill Bastar"
    },
    {
      "problem": "AttributeError: 'VAE' object has no attribute 'latent_frequency_bins'",
      "solution": "Update KJnodes or change version to 'nightly'",
      "from": "Juan Gea"
    },
    {
      "problem": "GGUF models need tokenizer support",
      "solution": "Currently needs a PR https://github.com/city96/ComfyUI-GGUF/pull/399",
      "from": "Kijai"
    },
    {
      "problem": "VAE ghosting artifacts during encode/decode",
      "solution": "Use temporal tiles during VAE encode and decode with overlap - don't use temporal_overlap=0",
      "from": "976496720370348032"
    },
    {
      "problem": "Sage attention flag doesn't work with model",
      "solution": "Don't use --use-sage-attention flag, use Kijai's sage patch node instead",
      "from": "Kijai"
    },
    {
      "problem": "OOM issues on RTX 5090",
      "solution": "Use --reserve-vram flag, don't use --lowvram or --highvram",
      "from": "Kijai"
    },
    {
      "problem": "Memory management issues in ComfyUI",
      "solution": "Use --reserve-vram instead of other memory flags",
      "from": "Kijai"
    },
    {
      "problem": "MelBandRoFormerSampler error on Mac",
      "solution": "Use Mac-compatible fork: https://github.com/Brainkeys/ComfyUI-MelBandRoFormer-Mac/tree/main",
      "from": "buggz"
    },
    {
      "problem": "OOM errors with limited VRAM",
      "solution": "Turn up VRAM reserve settings to prevent out of memory issues",
      "from": "Gill Bastar"
    },
    {
      "problem": "GGUFLoaderKJ 'dict' object has no attribute 'startswith'",
      "solution": "Try updating KJNodes",
      "from": "162053387582439425"
    },
    {
      "problem": "VAE loading crashes ComfyUI",
      "solution": "Launch ComfyUI with --cache-none argument to confirm if it's RAM related issue",
      "from": "Kijai"
    },
    {
      "problem": "Text encoder connector missing error",
      "solution": "Error occurs when connector is missing in text encoder setup",
      "from": "Kijai"
    },
    {
      "problem": "Mat1 and mat2 shapes multiplication error",
      "solution": "Try --preview-method none in ComfyUI startup arguments",
      "from": "yi"
    },
    {
      "problem": "Audio becomes desynced when using temporal upscaling",
      "solution": "Set source fps correctly - if source was 20fps, set upscale to 40fps, not 48fps",
      "from": "Xor"
    },
    {
      "problem": "Portrait/vertical video has jittery motion issues",
      "solution": "General known issue with vertical video generation, developers mentioned portrait improvements planned for version 2.1",
      "from": "TK_999"
    },
    {
      "problem": "Plastic shiny skin on humans with distill model",
      "solution": "Add distill LoRA to distill model at negative weight (-0.3 to -0.6) to reduce plastic appearance",
      "from": "David Snow"
    },
    {
      "problem": "FP4 not working",
      "solution": "Update PyTorch to CUDA 13 version",
      "from": "D'Squarius Green, Jr."
    },
    {
      "problem": "Model unloading when only changing prompts",
      "solution": "Text encoder needs to run when prompts change, this is normal behavior",
      "from": "Kijai"
    },
    {
      "problem": "Tensor size mismatch error in LTX",
      "solution": "Remove smZnodes extension",
      "from": "Aergo"
    },
    {
      "problem": "OOM issues on audio-image-to-video workflow",
      "solution": "Reduce resolution and duration, avoid upscale nodes if VRAM limited",
      "from": "V\u00e9role"
    },
    {
      "problem": "Audio not working in workflow with detailers",
      "solution": "Detailers may be causing audio issues in workflow",
      "from": "NC17z"
    },
    {
      "problem": "ComfyUI failing to load after system crash",
      "solution": "Delete all CUDA related software and Python installations, then reinstall fresh",
      "from": "Charlie"
    },
    {
      "problem": "Power Puter node 'IsNot not supported' error",
      "solution": "Change 'is not' to '!=' or update rgthree version",
      "from": "411637464806195202"
    },
    {
      "problem": "Static output in I2V",
      "solution": "Lower I2V mask level - if it doesn't move under 0.3 then something else is broken",
      "from": "Kijai"
    },
    {
      "problem": "Enhance-a-Video not working properly with LTX2",
      "solution": "Changes output too much, doesn't seem to burn like with other models, maybe because of audio attention - try video only",
      "from": "Kijai"
    },
    {
      "problem": "Black outputs when changing frame count",
      "solution": "Too many frames can cause black outputs",
      "from": "seitanism"
    },
    {
      "problem": "input tensor must fit into 32-bit index math when decoding",
      "solution": "Something becomes too large (like 1024x1024x297), exact cause unknown yet",
      "from": "mamad8"
    },
    {
      "problem": "Weight error with new nodes",
      "solution": "Needs ComfyUI updated to specific commit bd0e6825e84606e0706bbb5645e9ea1f4ad8154d",
      "from": "Kijai"
    },
    {
      "problem": "Stylistic drift in I2V second pass",
      "solution": "Happening in upscaler pass, related to distill lora usage",
      "from": "David Snow"
    },
    {
      "problem": "Expected mat1 and mat2 to have the same dtype, but got: c10::Half != c10::BFloat16",
      "solution": "Text encoder should run at bf16, use fixed node to convert fp16 to bf16",
      "from": "KingGore2023"
    },
    {
      "problem": "FileNotFoundError: Gemma3 tokenizer not found for GGUF",
      "solution": "Put tokenizer files from huggingface.co/smhf72/gemma-3-12b-it-extras-comfy into text encoders folder",
      "from": "yi"
    },
    {
      "problem": "Enhanced Prompt node error with GGUF: 'LTXAVTEModel_' object has no attribute 'processor'",
      "solution": "Disable Enhanced Prompt node when using GGUF models",
      "from": "Tachyon"
    },
    {
      "problem": "RuntimeError: The size of tensor a (1920) must match the size of tensor b (261248)",
      "solution": "Remove ComfyUI_smZNodes - it overwrites comfy samplers and breaks LTX2 due to nested video + audio tensors",
      "from": "Kijai"
    },
    {
      "problem": "Garbled mess output with GGUF distilled model",
      "solution": "If using full fp8 model without distill LoRA, bump first pass steps to 40. Otherwise use distill LoRA at 20 steps",
      "from": "Tachyon"
    },
    {
      "problem": "Blurry GGUF output",
      "solution": "Use distill LoRA in upscale phase, or increase steps if not using LoRA",
      "from": "Tachyon"
    },
    {
      "problem": "ComfyUI GGUF nodes don't support LTX2",
      "solution": "Use Vantage-GGUF fork: https://github.com/vantagewithai/Vantage-GGUF",
      "from": "Kijai"
    },
    {
      "problem": "I2V gives slideshow-like results unless using exact image as prompt",
      "solution": "Try compression value of 90 for stylized images, or lower conditioning strength below 1.0",
      "from": "N0NSens"
    },
    {
      "problem": "Second frame cuts in I2V",
      "solution": "Lower mask level (conditioning strength) to let model modify the latent and bring it into distribution",
      "from": "976496720370348032"
    },
    {
      "problem": "GGUF Gemma models not working with LTX-2",
      "solution": "Use correct GGUF source - unsloth GGUF requires their specific loader",
      "from": "cocktailprawn1212"
    },
    {
      "problem": "OOM errors on image2video with 2x 3090",
      "solution": "Use Reddit fix for 24GB or less VRAM systems",
      "from": "noodlz"
    },
    {
      "problem": "Model merges producing garbage output",
      "solution": "Need to save metadata from original model including license and model configs",
      "from": "Kijai"
    },
    {
      "problem": "GGUF models 4x slower than FP8",
      "solution": "Something appears wrong with current GGUF implementation - shouldn't be 4x slower",
      "from": "garbus"
    },
    {
      "problem": "OOM at high resolutions with reserve-vram",
      "solution": "Increase --reserve-vram value (try 6-7 for high res), estimation has issues at huge resolutions",
      "from": "Kijai"
    },
    {
      "problem": "1080p 480 frame generation fails with OOM",
      "solution": "Split latents and merge using overlap nodes for smooth 20 second footage from two 10-second segments",
      "from": "dj47"
    },
    {
      "problem": "Lip sync not working consistently",
      "solution": "Adjust compression noise applied to image or lower image_cond_strength",
      "from": "TK_999"
    },
    {
      "problem": "Getting slideshow/zoom instead of animation in I2V",
      "solution": "Increase img compression from default 30-33 to 40-ish range, or lower image_strength to 0.8",
      "from": "foxydits"
    },
    {
      "problem": "Static/frozen videos in I2V",
      "solution": "Use LTXVPreprocess node on input image and set proper image preprocessing",
      "from": "976496720370348032"
    },
    {
      "problem": "ComfyUI memory leaks after multiple generations",
      "solution": "Use --disable-cuda-malloc flag and restart ComfyUI regularly, or use vacuum button",
      "from": "pagan"
    },
    {
      "problem": "System freezing during high resolution generation",
      "solution": "Use --reserve-vram flag (tested with values 4-8GB)",
      "from": "217076219072479232"
    },
    {
      "problem": "Metadata is required for audio VAE error",
      "solution": "Use VAELoader KJ and the LTX2 audio vae bf16",
      "from": "zelgo_"
    },
    {
      "problem": "ComfyUI crashes with LTX2",
      "solution": "Disable video previews in ComfyUI and use --cache-none flag",
      "from": "Tachyon"
    },
    {
      "problem": "OOM errors",
      "solution": "Use --reserve-vram 4 or higher, increase page file to 65GB on Windows",
      "from": "Tachyon"
    },
    {
      "problem": "Static images in I2V",
      "solution": "Add film grain to images and ensure audio start time is at least 0.2",
      "from": "Elvaxorn"
    },
    {
      "problem": "Converting mono audio to stereo",
      "solution": "Use Join Audio Channels node and plug same audio into both left and right channels",
      "from": "Elvaxorn"
    },
    {
      "problem": "Python/ComfyUI dying during generation",
      "solution": "Usually due to running out of RAM + pagefile. Use --cache-none or increase pagefile",
      "from": "Kijai"
    },
    {
      "problem": "I2V VRAM issues and freezing at higher resolutions",
      "solution": "Change memory_usage_factor in supported_models.py from 0.061 to 0.2 for safer operation, or 0.16 for 5090",
      "from": "Elvaxorn"
    },
    {
      "problem": "Video snapping back to original image at end in I2V",
      "solution": "Use LTXVRemoveGuides node to remove guides before decode when using guided generation",
      "from": "Kijai"
    },
    {
      "problem": "OOM errors despite large RAM/VRAM",
      "solution": "Use --reserve-vram flag or modify memory_usage_factor, both achieve more offloading",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI updates reset memory_usage_factor changes",
      "solution": "Re-edit the value back to custom setting after updates",
      "from": "Elvaxorn"
    },
    {
      "problem": "Metadata loading issues causing audio problems",
      "solution": "Ensure using GGUF node with metadata code applied, city96 merged PR for ComfyUI-GGUF metadata support",
      "from": "Kijai"
    },
    {
      "problem": "GGUF models running slow despite memory optimizations",
      "solution": "Update memory_usage_factor setting - was seeing 15.53s/it on dev-Q8 vs 5.52s/it on dev-fp8",
      "from": "garbus"
    },
    {
      "problem": "T2V workflows breaking with memory optimization",
      "solution": "Updated av_model file fixes T2V case while maintaining I2V improvements",
      "from": "Kijai"
    },
    {
      "problem": "Massive RAM usage and crashes on lower VRAM systems",
      "solution": "Memory optimization in av_model.py reduces VRAM usage by ~5GB at higher resolutions",
      "from": "Kijai"
    },
    {
      "problem": "Dark scene artifacts with certain samplers",
      "solution": "er_sde on first sampler stage was causing artifacts in dark scenes, switching samplers helped",
      "from": "garbus"
    },
    {
      "problem": "VAE decode causing PC crashes",
      "solution": "Try normal VAE decode instead of LTX VAE decode, check frame count limits",
      "from": "David Snow"
    },
    {
      "problem": "'post_quant_conv.weight' error with VAE",
      "solution": "Use Kijai's VAE loader node instead of standard ones when using GGUF quantized models",
      "from": "Tachyon"
    },
    {
      "problem": "Metadata required for audio VAE error",
      "solution": "Use audio VAE from Kijai's repo: LTX2_audio_vae_bf16.safetensors",
      "from": "Tachyon"
    },
    {
      "problem": "Tensor size mismatch error",
      "solution": "Disable video sampling previews in ComfyUI settings",
      "from": "Tachyon"
    },
    {
      "problem": "Custom node causing tensor errors",
      "solution": "Disable problematic custom nodes that interfere with LTX2",
      "from": "201830562200027137"
    },
    {
      "problem": "AttributeError with GGUF loader",
      "solution": "Use separate GGUF UNet loader node instead of regular loader",
      "from": "JUSTSWEATERS"
    },
    {
      "problem": "KeyError: 'num_crops' error with Enhance Prompt Node",
      "solution": "Most users have disabled or completely removed the Enhance Prompt Node from workflows",
      "from": "NC17z"
    },
    {
      "problem": "'VAE' object has no attribute 'latent_frequency_bins' error",
      "solution": "KJ nodes update solved the issue",
      "from": "xwsswww"
    },
    {
      "problem": "CLIPLoaderGGUF error: 'Unexpected text model architecture type in GGUF file: 'gemma3''",
      "solution": "Update City96 GGUF nodes to latest version, or use merged pull requests for comfy gguf node. Also need tokenizer.model in clip folder",
      "from": "568465354158768129"
    },
    {
      "problem": "Long pause before sampler starts first step",
      "solution": "Normal behavior - first step should take the longest, but 10+ minutes indicates hardware issues or offloading problems",
      "from": "568465354158768129"
    },
    {
      "problem": "I2V not following reference image when same size as latent",
      "solution": "ComfyUI I2V workflow resizes image so longest side is 1536, then feeds to LTXVImgToVideoInplace - image isn't supposed to be same size as latent",
      "from": "411637464806195202"
    },
    {
      "problem": "GGUF model producing noise output",
      "solution": "Update ComfyUI-GGUF nodes to nightly version, LTX2 support was only merged recently",
      "from": "Kijai"
    },
    {
      "problem": "ComfyUI disconnecting with VAE tiled at low settings",
      "solution": "Use --cache-none flag, issue is RAM limitation not VRAM (16GB RAM is tough)",
      "from": "Kijai"
    },
    {
      "problem": "Second stage upscaling very slow (5 minutes per iteration)",
      "solution": "Check if using --normal-vram or --reserve-vram flags, may be causing slowdowns",
      "from": "NC17z"
    },
    {
      "problem": "Vertical video artifacts at 1080p+",
      "solution": "Consensus is vertical video doesn't work well with current model",
      "from": "ucren"
    },
    {
      "problem": "Error running sage attention: Input tensors must be in dtype of torch.float16 or torch.bfloat16",
      "solution": "Use Kijai's patch node instead of --use-sage-attention flag",
      "from": "Kijai"
    },
    {
      "problem": "Blurry/artifacted I2V results with grid patterns",
      "solution": "Switch to euler_ancestral sampler, increase distill lora strength to 1.0, don't connect distill lora to first stage if using 40 steps",
      "from": "seitanism"
    },
    {
      "problem": "Frame limit crash on VAE: input tensor must fit into 32-bit index math",
      "solution": "Known limitation at high frame counts (above 281 frames), VAE decode bottleneck",
      "from": "NebSH"
    },
    {
      "problem": "Gemma won't load after latent2rgb PR",
      "solution": "Need to rebase PR to be up to date with main",
      "from": "Kijai"
    }
  ],
  "comparisons": [
    {
      "comparison": "SDE vs ODE samplers",
      "verdict": "SDE adds noise every step providing quality, exploration and error-correction but requires more steps. ODE samplers (euler, dpm, unipc, deis) are faster. Most people prefer speed so should use ODE and distilled models",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "comparison": "Temporal upscaler steps",
      "verdict": "4 steps res_2s sounds much better than 20 steps for temporal upscaling, reduces visual artifacts",
      "from": "TK_999"
    },
    {
      "comparison": "LTX2 vs WAN 2.2",
      "verdict": "LTX2 does way more than WAN 2.2 - has audio/dialog built in, variable frame rate, much faster in 1 model",
      "from": "Phr00t"
    },
    {
      "comparison": "FP8 vs FP4 models",
      "verdict": "FP4 didn't provide much speed improvement, sometimes slower than FP8",
      "from": "D'Squarius Green, Jr."
    },
    {
      "comparison": "Abliterated vs non-abliterated Gemma",
      "verdict": "Abliterated version adheres better to prompts, especially for soft core content",
      "from": "Kiwv"
    },
    {
      "comparison": "RTX Pro 5000 48GB vs RTX 5090",
      "verdict": "Pro 5000 probably better for this model right now due to VRAM, but worse in future. 4090 mod with 48GB is best deal",
      "from": "Benjimon"
    },
    {
      "comparison": "FP4 models vs others",
      "verdict": "FP4 models are fast but really suck in quality",
      "from": "Grimm1111"
    },
    {
      "comparison": "High res only vs two stage pipeline",
      "verdict": "High res only really hurts motion compared to two stage pipeline",
      "from": "Benjimon"
    },
    {
      "comparison": "HuMo vs LTX character consistency",
      "verdict": "HuMo's character consistency is way better so far",
      "from": "AJO"
    },
    {
      "comparison": "Distilled vs non-distilled models",
      "verdict": "Non-distilled models produce significantly better quality, distilled models change too much at cfg=1",
      "from": "Kijai"
    },
    {
      "comparison": "fp8 vs bf16 models",
      "verdict": "Full bf16 model recommended over fp8 for best quality, any speedup shortcut can be disastrous",
      "from": "421114995925843968"
    },
    {
      "comparison": "T2V vs I2V speed",
      "verdict": "T2V and I2V are basically the same speed with very minor differences",
      "from": "976496720370348032"
    },
    {
      "comparison": "LTX temporal tiling vs other methods",
      "verdict": "LTX temporal tiling is better, use larger overlap especially spatial",
      "from": "976496720370348032"
    },
    {
      "comparison": "Distilled vs Full model quality",
      "verdict": "No big difference noticed between full model and fp8 model. Distilled model may do better upscaling but has apparent differences",
      "from": "seitanism"
    },
    {
      "comparison": "res_2s vs euler_ancestral scheduler",
      "verdict": "res_2s 40 steps gave overbaked outputs. euler_ancestral 40 steps looked similar to res_2s 20 steps. Stick with euler_ancestral",
      "from": "seitanism"
    },
    {
      "comparison": "Custom node workflow vs ComfyUI template",
      "verdict": "ComfyUI-LTXVideo example workflows way better than ComfyUI template - template i2v basically has no motion",
      "from": "protector131090"
    },
    {
      "comparison": "LTX-2 vs Sora",
      "verdict": "Having local sora 1.5 moment - most exciting release since wan2.2",
      "from": "VK (5080 128gb), seitanism"
    },
    {
      "comparison": "Separated components vs default workflow",
      "verdict": "Separated components take more VRAM sadly, OOM in situations where default works",
      "from": "Gleb Tretyak"
    },
    {
      "comparison": "FP8 vs BF16 quality",
      "verdict": "BF16 has significantly better quality than FP8, especially for character LoRAs",
      "from": "NebSH"
    },
    {
      "comparison": "Distilled vs Full model",
      "verdict": "Distilled model has 'crushed' AI look, full model preferred for natural motion. FP8 gives better motion than FP4 despite FP4 being sharper",
      "from": "Grimm1111"
    },
    {
      "comparison": "Different NVIDIA drivers",
      "verdict": "No speed difference found between driver versions 570.86.16, 580.119.02, and 590.48.01 - exact same inference times",
      "from": "Kijai"
    },
    {
      "comparison": "Manual sigmas vs BasicScheduler vs LTXVScheduler",
      "verdict": "Manual sigmas work better, though basic scheduler differences were minimal",
      "from": "psylent_gamer"
    },
    {
      "comparison": "FP8 distill vs FP8 full model",
      "verdict": "Full model significantly better quality and consistency, distill version has issues with consistency",
      "from": "psylent_gamer"
    },
    {
      "comparison": "LTX-2 vs WAN 2.5",
      "verdict": "This is what WAN 2.5 was supposed to be",
      "from": "VK (5080 128gb)"
    },
    {
      "comparison": "LTX2 vs WAN image quality",
      "verdict": "WAN has better raw output quality, LTX2 requires higher resolution (1080p) to match WAN 720p sharpness",
      "from": "seitanism"
    },
    {
      "comparison": "Q8 vs fp8 quality",
      "verdict": "Q8 is better quality than fp8, closer to full bf16 model",
      "from": "Kijai"
    },
    {
      "comparison": "Q6 vs fp8",
      "verdict": "fp8 is usually worse than Q6",
      "from": "Ada"
    },
    {
      "comparison": "Distilled vs full model",
      "verdict": "At 20-30 steps distilled was better, but full model might be better at 50+ steps",
      "from": "Ada"
    },
    {
      "comparison": "fp8 scaled vs Q8",
      "verdict": "With WanVideo 2.1, fp8 scaled was nearly as close as Q8",
      "from": "Kijai"
    },
    {
      "comparison": "Quantization quality ranking",
      "verdict": "fp16 > Q8 > fp8scaled >> Q6",
      "from": "Kijai"
    },
    {
      "comparison": "First stage vs second stage audio quality",
      "verdict": "First stage voice is better and less robotic than second stage",
      "from": "Gleb Tretyak"
    },
    {
      "comparison": "Distill vs full model",
      "verdict": "Distill is fast but dumb - get desired result on 10th try with fast iterations, or 3rd try with long iterations using full model",
      "from": "N0NSens"
    },
    {
      "comparison": "LTX distilled fp8 vs q8 gguf",
      "verdict": "Hard to tell which is better in quality, but fp8 should be 30-40% faster on ada+ GPUs",
      "from": "David Snow"
    },
    {
      "comparison": "FP8 scaled Gemma vs normal FP8",
      "verdict": "FP8 scaled is closer to bf16 quality than normal fp8",
      "from": "Kijai"
    },
    {
      "comparison": "40 steps no distill vs distilled versions",
      "verdict": "40 steps without distill has very real, subtle movement quality",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "comparison": "Dev model vs Distill model audio quality",
      "verdict": "Dev model has awful audio with high pitched drone, distill model has better audio",
      "from": "David Snow"
    },
    {
      "comparison": "Full dev model with distill LoRA vs pure distill model",
      "verdict": "Dev + distill LoRA at 0.6 makes skin far more natural than just distill model, but uses more RAM",
      "from": "David Snow"
    },
    {
      "comparison": "Q8 vs FP8 quality",
      "verdict": "Small quality improvement with Q8 over FP8",
      "from": "D'Squarius Green, Jr."
    },
    {
      "comparison": "With vs without spatial upscaler",
      "verdict": "Huge difference in quality, upscaler significantly improves results",
      "from": "TK_999"
    },
    {
      "comparison": "LTX vs WAN for debris/action scenes",
      "verdict": "LTX handles debris fields with better clarity than WAN in FP8, WAN gets extreme dithering",
      "from": "ZombieMatrix"
    },
    {
      "comparison": "WAN vs LTX prompt following",
      "verdict": "WAN is more responsive to prompting, LTX fights prompts more but is good for basic trained scenarios",
      "from": "Grimm1111"
    },
    {
      "comparison": "Comfy Gemma fpmixed vs fp8_scaled",
      "verdict": "fpmixed should be better than fp8_scaled with extra weight_scale_2 on every layer",
      "from": "comfy"
    },
    {
      "comparison": "fp8 vs mixed Gemma models",
      "verdict": "Mixed is slightly bigger (12.7gb vs 12.3gb) but offers better accuracy",
      "from": "TK_999"
    },
    {
      "comparison": "Distilled vs non-distilled model",
      "verdict": "Distilled gives much much better results",
      "from": "Gleb Tretyak"
    },
    {
      "comparison": "Dev model vs distilled model audio quality",
      "verdict": "Dev model produces much better sound after upscaler",
      "from": "N0NSens"
    },
    {
      "comparison": "LTX2 vs Wan/Kandinsky for 2D animation",
      "verdict": "LTX2 is noticeably worse than Wan or Kandinsky for 2D animation",
      "from": "Ada"
    },
    {
      "comparison": "FP8_scaled vs other quantizations",
      "verdict": "FP8_scaled looks closest to bf16, especially for main subjects",
      "from": "Kijai"
    },
    {
      "comparison": "Native LTX workflows vs ComfyUI defaults",
      "verdict": "ComfyUI native defaults aren't the best",
      "from": "Kijai"
    },
    {
      "comparison": "Distilled vs full model audio quality",
      "verdict": "Distilled model has somewhat worse audio quality as it's a compromise model trained for few steps",
      "from": "976496720370348032"
    },
    {
      "comparison": "Quality order for model types",
      "verdict": "Full model > Q8 > fp8 in terms of quality",
      "from": "Tachyon"
    },
    {
      "comparison": "fp16 vs Q8 speed and quality",
      "verdict": "fp16 is faster than Q8 despite higher VRAM usage, similar quality between fp16 and Q8",
      "from": "Underdog"
    },
    {
      "comparison": "GGUF speed on Apple Silicon",
      "verdict": "GGUF cuts generation time by more than half compared to full models on M4 Mac",
      "from": "buggz"
    },
    {
      "comparison": "GGUF Q6 vs Q8",
      "verdict": "No difference between Q6 and Q8 - no reason to use Q8 over Q6",
      "from": "Kijai"
    },
    {
      "comparison": "Full model vs Distilled model quality",
      "verdict": "Many users report distilled model produces better results than full model",
      "from": "Gleb Tretyak"
    },
    {
      "comparison": "GGUF vs FP8 speed",
      "verdict": "GGUF always slower but uses less VRAM - price of lower VRAM usage",
      "from": "Xor"
    },
    {
      "comparison": "Different GGUF repositories",
      "verdict": "Q4 can be slightly better in some repos (unsloth, others) due to mixed layers with some Q5",
      "from": "Kijai"
    },
    {
      "comparison": "LTX2 vs LatentSync 1.6/InfiniteTalk v2v",
      "verdict": "LTX2 lip-sync works far better than both alternatives",
      "from": "KingGore2023"
    },
    {
      "comparison": "V2V vs Audio I2V for lip sync",
      "verdict": "V2V lip-sync works better than Audio I2V, never produces still images",
      "from": "KingGore2023"
    },
    {
      "comparison": "Dev model vs Distill model audio quality",
      "verdict": "Dev model is slow but produces great sound, especially during upscaling. Distill model is 4x faster but final sound is really bad",
      "from": "N0NSens"
    },
    {
      "comparison": "I2V vs T2V memory usage",
      "verdict": "I2V uses significantly more VRAM - instantly 5GB+ higher memory usage when I2V node is added",
      "from": "Kijai"
    },
    {
      "comparison": "Direct high-res vs upscale approach",
      "verdict": "Generating 30 steps 1280x1024@161 frames direct takes 8 minutes vs same with upscale pass completes under 3 minutes",
      "from": "138332118890708992"
    },
    {
      "comparison": "Q8 distilled vs Q8 dev + distilled LoRA",
      "verdict": "Q8 distilled performs better for I2V use cases",
      "from": "Choowkee"
    },
    {
      "comparison": "LTX2 vs LTX1 quality",
      "verdict": "LTX2 has similar hard quality ceiling as LTX1, fast and versatile but limited quality",
      "from": "Christian Sandor"
    },
    {
      "comparison": "LTX2 realistic vs anime",
      "verdict": "I2V works much better for realistic images than anime, realistic images animate way easier",
      "from": "Kijai"
    },
    {
      "comparison": "GGUF vs FP8 speed",
      "verdict": "GGUF should be only couple percent slower than FP8 on 3090, not massively different",
      "from": "Kijai"
    },
    {
      "comparison": "Stock vs abliterated text encoders",
      "verdict": "Abliterated allows more provocative motions but doesn't uncensor the underlying model, which wasn't trained on NSFW",
      "from": "Kiwv"
    },
    {
      "comparison": "FP8 vs full precision text encoders",
      "verdict": "FP8 version shows no degrading effects that the eye can see for I2V",
      "from": "Choowkee"
    },
    {
      "comparison": "Distilled vs non-distilled with LoRA",
      "verdict": "Non-distilled with lora in 2nd sampler looked better in limited testing",
      "from": "MechanimaL"
    },
    {
      "comparison": "Q8 GGUF vs FP8 performance",
      "verdict": "Q8-DEV GGUF: 15.53s/it vs dev-fp8: 5.52s/it on same workflow",
      "from": "garbus"
    },
    {
      "comparison": "Q4 GGUF vs FP8",
      "verdict": "GGUF takes about 3 minutes for 33 frames vs FP8 taking 1 minute, but uses less VRAM",
      "from": "JUSTSWEATERS"
    },
    {
      "comparison": "With vs without distilled LoRA",
      "verdict": "Without distilled LoRA looks much worse, especially noticeable difference in quality",
      "from": "568465354158768129"
    },
    {
      "comparison": "Different resolutions",
      "verdict": "1920x1080 gives best overall results, 1280x720 is good compromise",
      "from": "multiple users"
    },
    {
      "comparison": "FP8 vs GGUF performance",
      "verdict": "FP8 is faster than GGUF as long as you're not offloading, but GGUF gives 10GB free VRAM where FP8 gives only 3GB",
      "from": "JUSTSWEATERS"
    },
    {
      "comparison": "LTX-2 vs Wan2.2 for video generation",
      "verdict": "LTX2 is relatively weak in stage 1, can use Wan2.2 to generate videos in stage 1 then LTX2 for stage 2, but reaches limits in stage 2 most of the time",
      "from": "KingGore2023"
    },
    {
      "comparison": "I2V vs T2V strength",
      "verdict": "I2V seems a lot stronger than T2V, though T2V can sometimes produce really nice results. I2V is mixed - can give good results but also bad ones",
      "from": "particle9"
    },
    {
      "comparison": "Q4 vs Q8 GGUF quality",
      "verdict": "No noticeable difference in quality between Q4 and Q8, prompting has bigger impact than quantization level",
      "from": "Hevi"
    },
    {
      "comparison": "Distilled vs Dev model skin quality",
      "verdict": "Even Dev model struggles with realistic skin, no clear benefit over distilled for skin realism",
      "from": "\u30e9D."
    },
    {
      "comparison": "LTX2 vs Infinite for lip sync",
      "verdict": "LTX2 is far better than Infinite for video-to-video lip sync applications",
      "from": "Stef"
    },
    {
      "comparison": "LTX2 vs WAN I2V",
      "verdict": "LTX2 main features are automatic voiceover and speed, but video quality and prompt understanding are worse than WAN",
      "from": "N0NSens"
    },
    {
      "comparison": "Official WF vs KJ WF",
      "verdict": "KJ WF using Dynamic Lora at 0.8 and CFG schedule shows more animation than Official WF using Dev+Distilled at 1.0",
      "from": "The Shadow (NYC)"
    },
    {
      "comparison": "LTX2 vs Sora2/VEO3",
      "verdict": "Biggest influence on quality is prompt and using Gemma fpmixed for prompt adherence",
      "from": "nacho.money"
    }
  ],
  "tips": [
    {
      "tip": "Use lower frame rates for longer videos",
      "context": "Lower frame rates mean fewer frames to generate, making longer videos easier to create, though lip syncing isn't as good",
      "from": "Phr00t"
    },
    {
      "tip": "Use 24 fps as standard",
      "context": "24p is used in the states, 25p elsewhere. Official LTX workflow uses 24 fps",
      "from": "Daflon"
    },
    {
      "tip": "Generate at 20fps and upscale to 40fps via temporal upscaler",
      "context": "For better efficiency while maintaining quality",
      "from": "Xor"
    },
    {
      "tip": "Use audio-to-audio models to improve LTX-2 audio quality",
      "context": "Audio quality from LTX-2 is poor but can be enhanced with voice cloning tools like Elevenlabs or audio upscaling models",
      "from": "Kiwv"
    },
    {
      "tip": "Disable previews for LTX-2",
      "context": "To avoid certain errors",
      "from": "TK_999"
    },
    {
      "tip": "Use larger resolution to fix overblown/plastic looking results",
      "context": "When videos look blown out",
      "from": "Kiwv"
    },
    {
      "tip": "20 seconds appears to be the sweet spot before distortion starts",
      "context": "For maintaining coherence in longer videos",
      "from": "Tachyon"
    },
    {
      "tip": "Use CoPilot on Windows for troubleshooting errors",
      "context": "Right-click errors in Edge browser for instant help",
      "from": "NC17z"
    },
    {
      "tip": "Update ComfyUI for latest nodes and fixes",
      "context": "When experiencing issues with workflows",
      "from": "Tachyon"
    },
    {
      "tip": "Be very verbose with LTX2 prompts",
      "context": "For getting proper lip movement and speech",
      "from": "Q-"
    },
    {
      "tip": "Specify person with country of origin to avoid Bollywood influence",
      "context": "When prompt doesn't specify nationality",
      "from": "yi"
    },
    {
      "tip": "Use strong clear prompts from Gemini with interleaved ethnicities",
      "context": "To avoid Bollywood influence in generations",
      "from": "ZombieMatrix"
    },
    {
      "tip": "Resolution should be divisible by 32+1 and frames 8+1",
      "context": "Following model requirements, though it picks closest anyway",
      "from": "Q-"
    },
    {
      "tip": "Use images where face is larger for better identity adherence",
      "context": "Small faces are harder for model to maintain identity throughout longer videos",
      "from": "976496720370348032"
    },
    {
      "tip": "Horizontal images work better than portrait for I2V",
      "context": "Portrait images tend to just move frame up and down",
      "from": "JUSTSWEATERS"
    },
    {
      "tip": "Reduce LTXVPreprocess strength from 33 to lower for better results",
      "context": "When having identity preservation issues",
      "from": "976496720370348032"
    },
    {
      "tip": "Use temporal upscaler for very dynamic videos with texture smear",
      "context": "Good remedy for VAE struggles with dynamic content, denoise more after applying",
      "from": "976496720370348032"
    },
    {
      "tip": "Use LTXVSetAudioVideoMaskByTime node for audio/video editing",
      "context": "For continuation and editing workflows, this node defines noise mask on av_latents from start to end editing time",
      "from": "976496720370348032"
    },
    {
      "tip": "Wipe negative prompts when using non-distilled models",
      "context": "Negative prompts do nothing with distilled models at cfg=1, but should be removed for proper non-distilled usage",
      "from": "976496720370348032"
    },
    {
      "tip": "Use non-quantized Gemma3 model for better voice quality",
      "context": "Quantized models can affect audio quality, full precision works like a charm",
      "from": "976496720370348032"
    },
    {
      "tip": "Prompt for animation details when character won't move",
      "context": "Increase preprocessing and lower weight, add detailed prompts for movement when characters appear frozen",
      "from": "Juan Gea"
    },
    {
      "tip": "Use higher resolution to improve generation quality",
      "context": "Higher resolution can help with various quality issues including character animation",
      "from": "976496720370348032"
    },
    {
      "tip": "Use 48 FPS for I2V generations",
      "context": "Significantly improves quality by counteracting temporal compression artifacts",
      "from": "Ada"
    },
    {
      "tip": "Add IC detailer LoRA to second stage",
      "context": "Will be night and day difference in upscaling quality",
      "from": "scf"
    },
    {
      "tip": "Lower resolution allows longer videos",
      "context": "At 540p can do 1000 frames easily. At 960x960, 1000 frames outputs garbage. 16fps with 640x640 could do 1 minute videos",
      "from": "seitanism"
    },
    {
      "tip": "Use separate start/end times for audio and video",
      "context": "Can control audio and video regeneration independently in continuation workflows",
      "from": "976496720370348032"
    },
    {
      "tip": "Test long context with specific prompts",
      "context": "Ask for something that shows consistency like 'person going out a door and coming back at the end'",
      "from": "976496720370348032"
    },
    {
      "tip": "Tone down CFG for full model",
      "context": "High CFG can cause skin quality issues",
      "from": "976496720370348032"
    },
    {
      "tip": "For 2D animation, add '2d Japanese traditional anime sequence' to start of prompt",
      "context": "When trying to generate 2D style content",
      "from": "VK (5080 128gb)"
    },
    {
      "tip": "Don't describe the image in I2V prompts, focus on actions",
      "context": "Model already sees the image, describing it leads to static results",
      "from": "seitanism"
    },
    {
      "tip": "Voices are better and less robotic if you describe voice details",
      "context": "Include accent, tone etc in audio prompts",
      "from": "seitanism"
    },
    {
      "tip": "Use 'soft spoken' keyword to control audio volume/intensity",
      "context": "When audio is too loud or intense",
      "from": "VK (5080 128gb)"
    },
    {
      "tip": "Set dual clip loader device to CPU to help with memory",
      "context": "Reduces VRAM usage during text encoding",
      "from": "yi"
    },
    {
      "tip": "Use ffmpeg to extend videos with padding",
      "context": "For video continuation - simpler command: ffmpeg -i input.mp4 -vf 'tpad=stop_duration=5:color=gray' -af 'apad=pad_dur=5' output.mp4",
      "from": "976496720370348032"
    },
    {
      "tip": "Remove audio latent inputs to save VRAM",
      "context": "When you don't need audio generation",
      "from": "Kijai"
    },
    {
      "tip": "Use VAE tiled decode for VRAM safety",
      "context": "Safer option when concerned about VRAM usage",
      "from": "JUSTSWEATERS"
    },
    {
      "tip": "Less tiling gives better results",
      "context": "Reduces tiling artifacts and helps with lip syncing, but increases OOM risk",
      "from": "Grimm1111"
    },
    {
      "tip": "Use detailed negative prompts for better results",
      "context": "LTX likes wordy prompts, use comprehensive negative prompts",
      "from": "Q-"
    },
    {
      "tip": "Do side by side tests when comparing models",
      "context": "Keep same everything except clip model, same seed etc.",
      "from": "Scruffy"
    },
    {
      "tip": "Use 2D animation prompts for consistent frame timing",
      "context": "Use prompts like '2d animation, The animation plays on twos with stepped/limited timing and held poses every two frames, slight frame-to-frame registration wiggle, minimal motion blur'",
      "from": "JUSTSWEATERS"
    },
    {
      "tip": "Make video longer than needed for interpolation",
      "context": "When using interpolation, last frame usually gets corrupted, so make video a few frames longer and cut it",
      "from": "Ruairi Robinson"
    },
    {
      "tip": "Use higher FPS for better results",
      "context": "You will get better results with higher FPS like 48",
      "from": "Benjimon"
    },
    {
      "tip": "Use res2s with 15 steps for better results",
      "context": "General usage",
      "from": "yi"
    },
    {
      "tip": "Write long prompts once as templates then modify",
      "context": "Long prompts seem to help with results",
      "from": "Lodis"
    },
    {
      "tip": "Use visualize node for sigmas to understand them better",
      "context": "Res4lyf has one and KJNodes has another option",
      "from": "Kijai"
    },
    {
      "tip": "For vid2vid, model works better with continuous shots rather than fast changing scenes",
      "context": "Video-to-video generation",
      "from": "VK (5080 128gb)"
    },
    {
      "tip": "Inject noise into initial picture to add motion",
      "context": "When input image is too perfect and causes motionless video",
      "from": "Lodis"
    },
    {
      "tip": "Use camera LoRA to fix motionless issues",
      "context": "When generating videos with static input images",
      "from": "1100803024298975263"
    },
    {
      "tip": "Generate at double frame rate",
      "context": "Use 48fps instead of 24fps to solve smeared faces and fast movement issues",
      "from": "Juan Gea"
    },
    {
      "tip": "Use VAE tile size of 1536 on 4090",
      "context": "Default is 512, can increase to 1536 for better performance on 4090",
      "from": "David Snow"
    },
    {
      "tip": "Run text encoding separately for memory",
      "context": "Save text encoding to file and load it to work around memory limitations",
      "from": "Kijai"
    },
    {
      "tip": "Set Gemma to CPU device",
      "context": "Massive speed improvement when changing prompts, avoids swapping",
      "from": "onama"
    },
    {
      "tip": "Don't use negative prompts with distilled model",
      "context": "CFG 1 doesn't support negative prompts effectively",
      "from": "KingGore2023"
    },
    {
      "tip": "Use detailer lora with strength 0.5-0.75 on both stages",
      "context": "For better quality results",
      "from": "David Snow"
    },
    {
      "tip": "Skip audio in 2nd stage while keeping it for video",
      "context": "For efficiency when first stage audio is better",
      "from": "hicho"
    },
    {
      "tip": "Add 'with tears' to prompts for crying emotion",
      "context": "Model tends to generate laughing instead of crying without explicit direction",
      "from": "KingGore2023"
    },
    {
      "tip": "Keep NAG negative prompts short and specific",
      "context": "Don't use huge negative prompt word lists, just define things you don't want to see",
      "from": "Kijai"
    },
    {
      "tip": "Use Res_2s first stage, euler_a second stage for high quality",
      "context": "Combination produces very clean outputs with crisp motion",
      "from": "David Snow"
    },
    {
      "tip": "Lower distill LoRA strength to 0.9 for second stage",
      "context": "Appears to improve quality in multi-stage workflows",
      "from": "IceAero"
    },
    {
      "tip": "Reduce video resolution for audio-only generation",
      "context": "Can make audio generation much faster since video part is still needed but can be low res",
      "from": "Kijai"
    },
    {
      "tip": "Use split models to avoid loading video VAE for audio-only",
      "context": "When using Kijai's split models, don't need to load video VAE at all for audio generation",
      "from": "Kijai"
    },
    {
      "tip": "Use proper sentence negatives instead of tag-style negatives",
      "context": "When writing negative prompts, use sentence fragments like 'A person is standing perfectly still, staring at the camera'",
      "from": "TK_999"
    },
    {
      "tip": "SLG, PAG, SEG are better than negative prompts",
      "context": "These guidance methods are more effective than traditional negative prompting",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "Use keyframing for complex effects",
      "context": "For effects like characters jumping out of screens, generate before and after frames and fill in the blanks",
      "from": "421114995925843968"
    },
    {
      "tip": "Set last keyframe 10 frames before actual end",
      "context": "When using guidance, allows trimming last 10 frames for cleaner transitions",
      "from": "JUSTSWEATERS"
    },
    {
      "tip": "Describe smooth transitions between frames",
      "context": "For first-to-last frame workflows, describe first frame, transition, then last frame to avoid slideshow effect",
      "from": "Benjimon"
    },
    {
      "tip": "Use dev model to reduce plastic look",
      "context": "dev model helps with plastic appearance compared to other variants",
      "from": "Benjimon"
    },
    {
      "tip": "Pick best stage 1 video before stage 2",
      "context": "Not efficient but fast - select best output from stage 1 before continuing to stage 2",
      "from": "KingGore2023"
    },
    {
      "tip": "Add small value to mask for better results",
      "context": "When using masking workflows, adding small value works better",
      "from": "224611423869730818"
    },
    {
      "tip": "Start with weight 2.0 for Enhance-a-Video",
      "context": "Initial recommendation for Enhance-a-Video weight",
      "from": "Kijai"
    },
    {
      "tip": "Prompt for specific explicit movements for better results",
      "context": "When prompting for character actions",
      "from": "Kijai"
    },
    {
      "tip": "The prompt matters a lot for quality",
      "context": "Getting good results from the model",
      "from": "KingGore2023"
    },
    {
      "tip": "Find the right seed - annoying but necessary",
      "context": "Since model won't follow prompt perfectly",
      "from": "KingGore2023"
    },
    {
      "tip": "Use subgraphs for repeated functions, not to obfuscate major workflow parts",
      "context": "Proper subgraph usage",
      "from": "Kijai"
    },
    {
      "tip": "Lower I2V mask level will always move but you lose character at too low levels",
      "context": "Balancing movement vs character preservation",
      "from": "Kijai"
    },
    {
      "tip": "Try other seeds when troubleshooting",
      "context": "When getting static or poor results",
      "from": "Kijai"
    },
    {
      "tip": "Use LTXVImg2VideoInPlace node for frame 0 instead of AddGuide",
      "context": "For classic i2v conditioning which is stronger and better than keyframe method",
      "from": "976496720370348032"
    },
    {
      "tip": "Set keyframe index to k-1 for k pixel frames (zero-based)",
      "context": "When using keyframe workflows, but adherence won't be perfect",
      "from": "976496720370348032"
    },
    {
      "tip": "Try longer videos with keyframes further apart or fewer keyframes",
      "context": "For challenging interpolation styles",
      "from": "976496720370348032"
    },
    {
      "tip": "LTX2 really relies on prompt quality",
      "context": "Prompt variations and descriptive motion change results significantly even with same seed",
      "from": "KingGore2023"
    },
    {
      "tip": "Can introduce partial CFG even with distill model",
      "context": "For additional control over generation",
      "from": "Kijai"
    },
    {
      "tip": "Try compression value of 40 first, 90 is very high",
      "context": "When dealing with stylized images that produce still frames",
      "from": "976496720370348032"
    },
    {
      "tip": "Don't use more than 3 LoRAs with LTX2",
      "context": "Performance consideration, prefer distilled original model",
      "from": "KingGore2023"
    },
    {
      "tip": "Can use euler_A on upscale to tone down res_2 effects",
      "context": "When using resolution upscaling",
      "from": "David Snow"
    },
    {
      "tip": "Seed hunting is key for video generation",
      "context": "LTX has always been unpredictable, need to try different seeds",
      "from": "N0NSens"
    },
    {
      "tip": "Tweak settings with fixed seed first",
      "context": "Before seed hunting, optimize settings on one seed",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "Split workflow into lowres and upscale parts",
      "context": "Deal with lowres until you find needed seed, then upscale - faster with distilled",
      "from": "N0NSens"
    },
    {
      "tip": "Use regen fixed seed for multi-sampler workflows",
      "context": "Regen sampler 1 with fixed seed for baseline, then sampler 2 - cuts gen time in half",
      "from": "foxydits"
    },
    {
      "tip": "Add 'no backing music' to prompts",
      "context": "Helps prevent unwanted audio generation even with small audio prompts",
      "from": "TK_999"
    },
    {
      "tip": "Lower second frame strength in 3-keyframe guider",
      "context": "Can force movement when using guider for 3 keyframes",
      "from": "JUSTSWEATERS"
    },
    {
      "tip": "Make second keyframe noisier",
      "context": "Alternative method to force movement in keyframe guidance",
      "from": "KingGore2023"
    },
    {
      "tip": "Prompt the entire video including masked parts",
      "context": "When doing video extension, describe the whole video including unchanged parts for better results",
      "from": "976496720370348032"
    },
    {
      "tip": "Use LTXVPreprocess on input images",
      "context": "Helps avoid frozen, semi-frozen and cut-on-2nd-frame videos in I2V",
      "from": "976496720370348032"
    },
    {
      "tip": "Set video start time to at least 0.1",
      "context": "In video extension workflows to avoid generating T2V instead of I2V",
      "from": "Elvaxorn"
    },
    {
      "tip": "Use detailed prompts with LTX2",
      "context": "Model responds much better to detailed prompting vs simple prompts",
      "from": "nacho.money"
    },
    {
      "tip": "Use elaborate prompting for multiple speakers",
      "context": "Format as 'Person A says: XXXXX, Person B says: YYYYYYY' etc.",
      "from": "976496720370348032"
    },
    {
      "tip": "Use grainy realistic pics over smooth AI pics for I2V",
      "context": "Smoother AI-generated images don't work as well as grainy, realistic photos",
      "from": "Elvaxorn"
    },
    {
      "tip": "Use Detail Daemon for free quality improvement",
      "context": "Particularly useful for image models, less so on video models",
      "from": "David Snow"
    },
    {
      "tip": "Use --cache-none over pagefile when possible",
      "context": "Both are slower but cache-none just does more reads, not writes to disk",
      "from": "Kijai"
    },
    {
      "tip": "Use image compression value of 0 if still getting motion",
      "context": "If motion is achieved with 0 compression, there are no other downsides",
      "from": "Kijai"
    },
    {
      "tip": "Higher image compression = more motion, less adherence to init image",
      "context": "Default is around 32-33, can go lower to maintain character better",
      "from": "Kijai"
    },
    {
      "tip": "Underclock GPU by 30-40% to dramatically lower temps and extend lifespan",
      "context": "Only lose 10-15% speed but GPU runs at 58C vs much higher, extends life significantly",
      "from": "Underdog"
    },
    {
      "tip": "Use detailer LoRA at low value (0.2) during second pass",
      "context": "Helps with details but can add artifacts if used too aggressively",
      "from": "David Snow"
    },
    {
      "tip": "Realistic images animate much easier than anime/stylized content",
      "context": "Model works better with photorealistic input images for I2V",
      "from": "Kijai"
    },
    {
      "tip": "Use I2V for NSFW content generation",
      "context": "I2V is really the current ticket for NSFW, using video input frames for motion samples is very effective",
      "from": "foxydits"
    },
    {
      "tip": "Disable compression for specific use cases",
      "context": "You can disable compression entirely if you start with a few frames of video, but that's a pretty specific use-case",
      "from": "foxydits"
    },
    {
      "tip": "Use 25 frames for crossover in infinite workflows",
      "context": "For SVI style infinite workflow, it seems to work best with a full 25 frames for a crossover",
      "from": "ZombieMatrix"
    },
    {
      "tip": "Lower LoRA strength from default",
      "context": "Found lora strength 0.6 to be better than the default 1.0 in ComfyUI workflow",
      "from": "MechanimaL"
    },
    {
      "tip": "Use quantized version of Gemma 12b",
      "context": "When working with limited VRAM, helps reduce memory usage",
      "from": "Kiwv"
    },
    {
      "tip": "Try Q4 quantization for better VRAM efficiency",
      "context": "Especially useful when generating at high resolutions like 1088x1920",
      "from": "Kiwv"
    },
    {
      "tip": "1280x720 is a good resolution spot",
      "context": "Balance between quality and generation time",
      "from": "Tachyon"
    },
    {
      "tip": "Disable Florence2 and other LoRAs when testing",
      "context": "Florence2 is 'sdpa attention seeker' and can cause memory issues",
      "from": "568465354158768129"
    },
    {
      "tip": "Use tiled VAE decode for large resolutions",
      "context": "Helps prevent OOM when working with high resolution outputs",
      "from": "568465354158768129"
    },
    {
      "tip": "Use Pro tip: Stick a VAE decode and video combine on first stage to see if generation is jank",
      "context": "Will save hours in the long run by catching bad generations early",
      "from": "David Snow"
    },
    {
      "tip": "Find the seed, tweak prompt and settings, then use upscaling stage for final iteration",
      "context": "Better workflow approach than always running full pipeline",
      "from": "NC17z"
    },
    {
      "tip": "Use tiled VAE decoding to avoid OOM errors",
      "context": "Especially important with limited VRAM - helps manage memory usage during upscaling",
      "from": "568465354158768129"
    },
    {
      "tip": "Film grain helps everything with video models",
      "context": "Noise seems to really help video models think there is motion",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "tip": "Use euler_A for first pass and dpmpp_sde for second pass",
      "context": "Good sampler combination - euler_A is fast, dpmpp_sde adds detail",
      "from": "David Snow"
    },
    {
      "tip": "Generate at 48fps then reduce to 24fps",
      "context": "Theory to reduce motion artifacts then get normal playback speed",
      "from": "David Snow"
    },
    {
      "tip": "Do seed hunting before the sampler",
      "context": "For better results with IC detailer and maintaining likeness",
      "from": "dg1860"
    },
    {
      "tip": "Lower or disable IC detailer strength",
      "context": "High IC detailer values can kill likeness to source image",
      "from": "dg1860"
    },
    {
      "tip": "Use LoRAs for better likeness retention",
      "context": "Trained LoRAs help maintain character consistency during lip sync",
      "from": "NebSH"
    },
    {
      "tip": "Use distilled lora on both first and second stage for significantly better results",
      "context": "When using distilled model workflow",
      "from": "Choowkee"
    },
    {
      "tip": "Use default image rescale vs matching to frame size for better I2V results",
      "context": "When doing image-to-video generation",
      "from": "The Shadow (NYC)"
    },
    {
      "tip": "Connect explode lora to both stages for better results",
      "context": "When using loras in multi-stage workflows",
      "from": "seitanism"
    },
    {
      "tip": "Use denoised output when passing from first to second pass",
      "context": "Multi-pass upscaling workflows",
      "from": "TK_999"
    }
  ],
  "news": [
    {
      "update": "Universal Music Group partnered with NVIDIA",
      "details": "Partnership announced for AI music generation",
      "from": "NC17z"
    },
    {
      "update": "Wan2gp released support",
      "details": "Support was released for LTX-2",
      "from": "Benjimon"
    },
    {
      "update": "New Nvidia Studio Driver includes LTX optimizations",
      "details": "January NVIDIA Studio Driver provides RTX optimizations for LTX-2, along with PyTorch NVFP4 and NVFP8 support in ComfyUI",
      "from": "zelgo_"
    },
    {
      "update": "Abliterated Gemma model released",
      "details": "Gemma-3-12b-Abliterated-LTX2 available as drop-in replacement, removes censoring bias but doesn't unlock much extra coherent content",
      "from": "Kiwv"
    },
    {
      "update": "Official NVIDIA collaboration on quantizations",
      "details": "LTX worked with NVIDIA to release high quality fp8 and nvfp4 quantizations",
      "from": "ltx-2"
    },
    {
      "update": "GGUF version of LTX-2 transformer released",
      "details": "16GB transformer-only q6 GGUF uploaded, though ComfyUI compatibility uncertain",
      "from": "zelgo_"
    },
    {
      "update": "Reddit AMA with Lightricks CEO is live",
      "details": "CEO answering questions about LTX-2 on r/StableDiffusion",
      "from": "ltx-2"
    },
    {
      "update": "ComfyUI optimizations released recently",
      "details": "Recent commits in past 1-2 days with optimizations, more coming",
      "from": "Lodis"
    },
    {
      "update": "Company pronunciation clarified",
      "details": "Lightricks is pronounced 'Light-tricks' meaning 'Tricks of light'",
      "from": "976496720370348032"
    },
    {
      "update": "Previews are coming back to LTX-2",
      "details": "Resolution is tiny for LTX latent space so preview isn't great, but better than no preview",
      "from": "Kijai, yi"
    },
    {
      "update": "Prompt enhancer will be fixed to work with LTXV Audio Text Encoder Loader",
      "details": "Lightricks team working on compatibility fix",
      "from": "mkupchik_lightricks"
    },
    {
      "update": "Work being done on advanced memory management",
      "details": "ComfyUI team working on memory improvements so --reserve-vram won't be needed",
      "from": "Kijai"
    },
    {
      "update": "Memory management worked on in ComfyUI past months",
      "details": "ComfyUI v0.8.2 optimized memory for LTX-2, still being worked on",
      "from": "Kijai"
    },
    {
      "update": "New audio-video model research",
      "details": "26B size model that's closed source but runnable on consumer hardware",
      "from": "yi"
    },
    {
      "update": "Hunyuan Video 1.5 rumored to be 40B with MoE architecture",
      "details": "Someone with inside knowledge mentioned 40B size",
      "from": "yi"
    },
    {
      "update": "LTX 2.1 releasing in February",
      "details": "Will fix various issues mentioned in current version",
      "from": "Lodis"
    },
    {
      "update": "LTX team working on quality improvements",
      "details": "Team said they would try to improve quality issues in version 2.5",
      "from": "Ada"
    },
    {
      "update": "Memory management improvements in ComfyUI",
      "details": "PR https://github.com/Comfy-Org/ComfyUI/pull/11748 apparently improved memory management significantly",
      "from": "yi"
    },
    {
      "update": "GGUF quantized models available",
      "details": "Kijai has released GGUF quantized versions of LTX Video 2",
      "from": "D'Squarius Green, Jr."
    },
    {
      "update": "Portrait, sound, and I2V improvements planned for LTX 2.1",
      "details": "Developers mentioned these as goals for the next version",
      "from": "Gleb Tretyak"
    },
    {
      "update": "Gemma 3 GGUF support available via PR",
      "details": "Pull request #402 adds Gemma 3 GGUF support to ComfyUI-GGUF",
      "from": "Kijai"
    },
    {
      "update": "New Comfy-Org Gemma models released",
      "details": "fpmixed and fp4_mixed versions available, targeting different fp4 layer percentages",
      "from": "comfy"
    },
    {
      "update": "City96 GGUF support for Gemma added",
      "details": "Pull requests 399 and 402 add LTX 2.0 GGUF + Gemma3 GGUF support",
      "from": "japar"
    },
    {
      "update": "bitsandbytes required for Gemma quant",
      "details": "Need to install bitsandbytes through Comfy Manager for fp4 Gemma quantization",
      "from": "zelgo_"
    },
    {
      "update": "Dynamic rank reduced distill lora released",
      "details": "First working dynamic rank reduced distill lora available in bf16 and fp8 (fp8 not recommended)",
      "from": "Kijai"
    },
    {
      "update": "Developers mentioned I2V, vertical, sound are goals for version 2.1",
      "details": "These are areas targeted for improvement in next version",
      "from": "Gleb Tretyak"
    },
    {
      "update": "LTX distilled LoRA released by Kijai",
      "details": "Community-created distilled LoRA available",
      "from": "David Snow"
    },
    {
      "update": "Version 2.1 expected in February",
      "details": "Should solve current workflow and compatibility problems",
      "from": "Lodis"
    },
    {
      "update": "LTX 2.1 announced for February",
      "details": "Fixed version 2.1 coming in February with better I2V and variable latent space compression",
      "from": "Lodis"
    },
    {
      "update": "ComfyUI uploaded FP4 quant of Gemma",
      "details": "New FP4 quantized Gemma model available, around 9GB",
      "from": "zelgo_"
    },
    {
      "update": "VACE-LTX-Video-0.9 available for 2B model",
      "details": "Available on HuggingFace but only for 2B model, not 13B",
      "from": "NebSH"
    },
    {
      "update": "New embeddings connector for Distill model",
      "details": "New embeddings connector specifically for distilled model available in Kijai's repo",
      "from": "Elvaxorn"
    },
    {
      "update": "Lower rank LoRAs for distill model released",
      "details": "Kijai released lower rank versions of distill LoRAs (original was 7GB)",
      "from": "Tachyon"
    },
    {
      "update": "Join Audio Channels node added to ComfyUI core",
      "details": "Added 2 days ago in nightly build for converting mono to stereo audio",
      "from": "Kijai"
    },
    {
      "update": "Improved cache management PR in development",
      "details": "PR #10779 for better RAM management, hopefully default behavior in future",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI-GGUF metadata support merged",
      "details": "city96 merged PR for LTX model support in official ComfyUI-GGUF nodes",
      "from": "Kijai"
    },
    {
      "update": "LTX 2.1 version coming with better I2V",
      "details": "Upcoming 2.1 version mentioned to have improved I2V capabilities",
      "from": "Kijai"
    },
    {
      "update": "ComfyUI previews PR available but pending",
      "details": "Preview functionality PR exists but ComfyUI dev unsure about final implementation affecting everything",
      "from": "Kijai"
    },
    {
      "update": "Memory optimization breakthrough",
      "details": "Major VRAM reduction for I2V workflows - saves ~5GB at higher resolutions by optimizing timestep embeddings",
      "from": "Kijai"
    },
    {
      "update": "GGUF compatibility improvements",
      "details": "GGUF models now working with memory optimizations and various text encoders",
      "from": "various"
    },
    {
      "update": "Kijai updated av_model.py file",
      "details": "New file provides improvements for image to video generation, dramatically reduces VRAM usage",
      "from": "Gleb Tretyak"
    },
    {
      "update": "City96 updated GGUF nodes",
      "details": "Now supports Gemma3 GGUF and other improvements",
      "from": "Tachyon"
    },
    {
      "update": "ComfyUI memory management improvements being worked on",
      "details": "Someone is working on fixing ComfyUI's memory management issues",
      "from": "Ada"
    },
    {
      "update": "Torch 2.9.0+cu130 delivers 2.2x speedup",
      "details": "Full HD 81 frame renders dropped from 201 seconds to 89 seconds compared to torch 2.8.0 (unconfirmed)",
      "from": "NebSH"
    },
    {
      "update": "Kijai's optimizations reduce VRAM usage",
      "details": "No more OOMs when generating at 1536x1536 for T2V, maximum was previously 1089x1089",
      "from": "Gill Bastar"
    },
    {
      "update": "ComfyUI GGUF officially supports LTX2",
      "details": "LTX2 GGUF support merged to main branch",
      "from": "yi"
    },
    {
      "update": "Memory optimization fix completed",
      "details": "Kijai's updated av_model.py maintains identical output to original while reducing VRAM usage and improving speed",
      "from": "Kijai"
    },
    {
      "update": "Kijai released latent2rgb preview support for LTX2",
      "details": "PR allows preview during generation, works with upscale model but is slow",
      "from": "Kijai"
    },
    {
      "update": "Someone reuploading old loras from community to Civitai",
      "details": "Earth zoom out LTX 2 lora being redistributed",
      "from": "boop"
    }
  ],
  "workflows": [
    {
      "workflow": "Voice to video with audio replacement",
      "use_case": "Created video using Kijai's voice to video workflow, generated audio in SUNO, then replaced voice using RePlay with RVC model",
      "from": "NC17z"
    },
    {
      "workflow": "First Frame to Last Frame",
      "use_case": "For controlled video generation",
      "from": "Tachyon"
    },
    {
      "workflow": "Using WAS LM Studio nodes for prompt generation from image analysis",
      "use_case": "Getting prompts from image analysis",
      "from": "buggz"
    },
    {
      "workflow": "Using VRAM debug nodes for memory management",
      "use_case": "Preventing OOM and maintaining consistent generation speeds",
      "from": "Phr00t"
    },
    {
      "workflow": "Video-to-video restyling without controlnet",
      "use_case": "Restyling existing videos",
      "from": "hicho"
    },
    {
      "workflow": "Audio injection workflow for long generations",
      "use_case": "Creating videos with synchronized audio up to 4 minutes",
      "from": "ZombieMatrix"
    },
    {
      "workflow": "Audio to video with empty latent instead of image",
      "use_case": "Creating video from audio alone, similar to KJ's i+a2v workflow",
      "from": "TK_999"
    },
    {
      "workflow": "Split audio into segments for long generation",
      "use_case": "Take 4-minute song, split up, generate each with different related prompts, stitch back together",
      "from": "976496720370348032"
    },
    {
      "workflow": "Three stage workflow with temporal and spatial upscaling",
      "use_case": "Better quality results using both upscalers chained together",
      "from": "various users"
    },
    {
      "workflow": "Video temporal inpainting using masking",
      "use_case": "Edit part of existing video by masking audio and video latents for specific time segments",
      "from": "976496720370348032"
    },
    {
      "workflow": "Audio/video continuation using masking",
      "use_case": "Replace parts of existing video/audio while maintaining continuity",
      "from": "976496720370348032"
    },
    {
      "workflow": "I2V with audio continuation",
      "use_case": "Generate video from single frame while continuing existing audio track",
      "from": "Kijai"
    },
    {
      "workflow": "V2V latent upscaling",
      "use_case": "Upscale existing videos using latent space processing",
      "from": "hicho"
    },
    {
      "workflow": "LTX for upscaling other model outputs",
      "use_case": "Use LTX Video 2 to upscale Wananimate generations to higher resolution",
      "from": "David Snow"
    },
    {
      "workflow": "Split lowres/upscale workflow for slower hardware",
      "use_case": "When upscale pass is 5x slower than lowres generation (1min gen -> 5min upsc). Run fast lowres, enable upscaler only if result is good",
      "from": "N0NSens"
    },
    {
      "workflow": "Spatial tiling for high resolution",
      "use_case": "Split latent from low res run, process individually with highres, stitch together with overlap blending using LTXVLoopingSampler",
      "from": "976496720370348032"
    },
    {
      "workflow": "Audio-only generation workaround",
      "use_case": "Generate 'redundant' video and throw out video stream, keeping audio stream when audio-only model not available",
      "from": "976496720370348032"
    },
    {
      "workflow": "LTX + WAN upscale pipeline",
      "use_case": "Less mushy detail, use low denoising just enough to fix lost details from LTX, stick to 720p or 1600x960 then upscale to 1080p",
      "from": "dj47"
    },
    {
      "workflow": "Video upscale using WAN + SeedVR2 + grading",
      "use_case": "Cleaning up LTX output while maintaining lip sync, keeps color shifts manageable",
      "from": "dj47"
    },
    {
      "workflow": "Video continuation with audio masking",
      "use_case": "Extending videos while preserving original segments and adding new content",
      "from": "Kijai"
    },
    {
      "workflow": "Two-stage generation with distilled LoRA for upscaling",
      "use_case": "First stage generates base video, second stage with distilled LoRA adds texture/detail",
      "from": "976496720370348032"
    },
    {
      "workflow": "Auto-regressive video generation",
      "use_case": "Creating arbitrarily long videos using temporal tiles with LoopingSampler",
      "from": "976496720370348032"
    },
    {
      "workflow": "Distilled fp8 model with 2K upscaling workflow",
      "use_case": "Generating and upscaling videos to 2K resolution, takes 100sec total",
      "from": "avataraim"
    },
    {
      "workflow": "Audio + image to video workflow",
      "use_case": "Combining custom audio with image input for video generation",
      "from": "multiple users discussing"
    },
    {
      "workflow": "2-stage generation with distill LoRA",
      "use_case": "20 steps first pass with bf16, then 8 steps distill second stage",
      "from": "NebSH"
    },
    {
      "workflow": "Spatial vs temporal model upscaling",
      "use_case": "Spatial is for resolution, temporal is only for FPS",
      "from": "Lodis"
    },
    {
      "workflow": "Audio continuation workflow",
      "use_case": "Can copy original audio and continue it or feed custom audio, works like MaskGCT but for video",
      "from": "421114995925843968"
    },
    {
      "workflow": "Two-stage generation with upscaler",
      "use_case": "Getting sharp results - first stage low res, second stage with spatial upscaler and detailer LoRA",
      "from": "David Snow"
    },
    {
      "workflow": "Keyframe test workflow",
      "use_case": "Combining different images to make a video with movement",
      "from": "Owlie"
    },
    {
      "workflow": "Audio continuation workflow",
      "use_case": "Extending video with consistent audio, requires proper CFG settings",
      "from": "Kijai"
    },
    {
      "workflow": "Two-stage generation with detailer lora",
      "use_case": "High quality video generation using detailer lora on both first and second stages",
      "from": "avataraim"
    },
    {
      "workflow": "Audio voice refinement via TTS iterations",
      "use_case": "Improving generated audio quality through multiple TTS passes",
      "from": "Gleb Tretyak"
    },
    {
      "workflow": "TTS Audio Suite workflow",
      "use_case": "Fixing voices in generated videos, alternative to training voice LoRAs",
      "from": "722193816089788427"
    },
    {
      "workflow": "Multi-stage sampling with different CFG",
      "use_case": "First step with CFG4 and no distill lora, last 7 steps with CFG1 and distill lora",
      "from": "N0NSens"
    },
    {
      "workflow": "Spatial inpaint + first frame + depth control",
      "use_case": "Basically have face control similar to VACE but without reference",
      "from": "Hashu"
    },
    {
      "workflow": "Two-stage generation with different models",
      "use_case": "Use dev model for first pass, dev + distill LoRA for second pass upscaling to get better quality",
      "from": "BobbyD4AI"
    },
    {
      "workflow": "Video extension with keyframes",
      "use_case": "Generate separate 121-frame sections and join them, splitting audio cleanly",
      "from": "A.I.Warper"
    },
    {
      "workflow": "Multiple keyframe workflow with custom audio",
      "use_case": "Creating videos with multiple guided keyframes and custom audio, works without upscale",
      "from": "FUNZO"
    },
    {
      "workflow": "Video extension/temporal outpainting",
      "use_case": "Take 5s clip and generate next 5s using LTXV Set Audio Video Mask By Time node",
      "from": "74637656272666624"
    },
    {
      "workflow": "Audio + image generation workflow",
      "use_case": "Adding sound to existing videos, available in specified channel from Kijai",
      "from": "Moonbow"
    },
    {
      "workflow": "Dev model + distill lora for upscaler",
      "use_case": "Better audio quality - use dev model at basic sampler and dev model + distill lora at upscaler",
      "from": "N0NSens"
    },
    {
      "workflow": "Hybrid workflow using LTX with Kijai ComfyUI nodes",
      "use_case": "Allows using GGUF and 4bit gemma",
      "from": "zelgo_"
    },
    {
      "workflow": "Combined audio + image with keyframes",
      "use_case": "Prerecorded audio with images reacting to audio and singing",
      "from": "Owlie"
    },
    {
      "workflow": "Using compression settings to drive motion",
      "use_case": "When not getting motion in generations, increase compression to help drive more motion",
      "from": "Tachyon"
    },
    {
      "workflow": "Multi-keyframe setup with AddGuide",
      "use_case": "Creating videos with multiple keyframe conditioning points",
      "from": "The Shadow (NYC)"
    },
    {
      "workflow": "Split model workflow",
      "use_case": "Loading model components separately to manage VRAM usage",
      "from": "Kijai"
    },
    {
      "workflow": "Keyframe workflow with LTXCropGuides",
      "use_case": "Setting keyframes at specific indices then removing reference latents after generation",
      "from": "976496720370348032"
    },
    {
      "workflow": "Multi-keyframe workflow with STG node",
      "use_case": "Creating videos with multiple keyframes at specific positions",
      "from": "The Shadow (NYC)"
    },
    {
      "workflow": "Audio + first frame to video",
      "use_case": "Start frame + audio input for video generation",
      "from": "TK_999"
    },
    {
      "workflow": "GGUF workflow with model switches",
      "use_case": "Switching between different GGUF models in single workflow",
      "from": "buggz"
    },
    {
      "workflow": "Video and audio extension using masking",
      "use_case": "Extending existing videos with new content while maintaining continuity",
      "from": "976496720370348032"
    },
    {
      "workflow": "Multi-stage upscaling 256->512->1024->2048",
      "use_case": "Progressive upscaling for higher resolution outputs, though color shifts occur at 2K",
      "from": "KingGore2023"
    },
    {
      "workflow": "Audio-video lip sync using V2V",
      "use_case": "Creating lip-synced videos with better results than I2V approach",
      "from": "KingGore2023"
    },
    {
      "workflow": "I2V with audio masking and film grain preprocessing",
      "use_case": "Creating lip-sync videos from images with proper motion",
      "from": "Elvaxorn"
    },
    {
      "workflow": "Video extension with turn-by-turn dialog",
      "use_case": "Creating longer stories by extending video segments with different speakers",
      "from": "138332118890708992"
    },
    {
      "workflow": "Continuous video generation using SVI method",
      "use_case": "Generate longer videos by feeding last 9 frames from one generation into next for continuation",
      "from": "ZombieMatrix"
    },
    {
      "workflow": "Two-stage generation with spatial upscale",
      "use_case": "Official workflow renders in 2 stages to achieve 1080p quality",
      "from": "Christian Sandor"
    },
    {
      "workflow": "Video-to-video using LTXVImgToVideoInplace",
      "use_case": "Feed video directly into I2V node for video-to-video processing, can handle 721+ frames",
      "from": "ZombieMatrix"
    },
    {
      "workflow": "Three-pass generation with upscaler",
      "use_case": "Using upscaler as third pass increases quality even more than two-pass",
      "from": "David Snow"
    },
    {
      "workflow": "SVI style infinite generation",
      "use_case": "Can extend videos indefinitely using 3x5s segments with 1s crossover, though audio blending needs work",
      "from": "ZombieMatrix"
    },
    {
      "workflow": "Direct long generation without context windows",
      "use_case": "60s direct generation with single KSampler, maintains subject fidelity",
      "from": "ZombieMatrix"
    },
    {
      "workflow": "Q4 GGUF with distilled LoRA setup",
      "use_case": "Memory-efficient generation with good quality using 8 steps",
      "from": "568465354158768129"
    },
    {
      "workflow": "Two-stage generation with distilled LoRA",
      "use_case": "T2V workflow using distilled LoRA in stage 2 for speed improvement",
      "from": "568465354158768129"
    },
    {
      "workflow": "Using Wan2.2 for stage 1, then LTX2 for stage 2",
      "use_case": "Compensating for LTX2's weakness in stage 1 generation",
      "from": "KingGore2023"
    },
    {
      "workflow": "Infinite length video generation workflow",
      "use_case": "Creating extended video sequences beyond normal limits",
      "from": "ZombieMatrix"
    },
    {
      "workflow": "Adding audio to existing videos with very low denoise",
      "use_case": "Audio generation for input video while keeping original video untouched",
      "from": "627140525916422145"
    },
    {
      "workflow": "Infinite length video generation",
      "use_case": "Creating long-form videos using anchor image method with cross points",
      "from": "ZombieMatrix"
    },
    {
      "workflow": "Two-stage I2V with different samplers",
      "use_case": "First pass with euler_A, second pass with dpmpp_sde for speed/quality balance",
      "from": "David Snow"
    },
    {
      "workflow": "Audio-driven lip sync I2V",
      "use_case": "Using audio input to drive facial animation with LoRAs for consistency",
      "from": "dg1860"
    },
    {
      "workflow": "Multi-keyframe I2V with audio",
      "use_case": "361 frames using first and 341st frame as keyframes with distilled lora and ic detailer lora",
      "from": "Daflon"
    },
    {
      "workflow": "Two-stage upscaling with distilled lora",
      "use_case": "First stage sampling followed by upscaling pass using distilled lora",
      "from": "multiple users"
    },
    {
      "workflow": "Image-guided I2V with masked video",
      "use_case": "First frame > middle guide frame > masked video for controlled generation",
      "from": "TK_999"
    }
  ],
  "settings": [
    {
      "setting": "Steps for high quality",
      "value": "1201 steps",
      "reason": "For maximum quality with SDE sampler, though very slow",
      "from": "Benjimon"
    },
    {
      "setting": "Reserve VRAM",
      "value": "--reserve-vram 8 or 5",
      "reason": "For handling VRAM limitations and enabling system RAM offloading",
      "from": "avataraim"
    },
    {
      "setting": "Temporal upscaler steps",
      "value": "4 steps",
      "reason": "Using res_2s scheduler, better than 20 steps which causes artifacts",
      "from": "TK_999"
    },
    {
      "setting": "Frame rate",
      "value": "24 fps",
      "reason": "Standard used in official workflows, 24p in US, 25p elsewhere",
      "from": "Daflon"
    },
    {
      "setting": "--reserve-vram",
      "value": "5",
      "reason": "Helps with OOM issues by offloading to system RAM",
      "from": "TK_999"
    },
    {
      "setting": "VAE decode overlap",
      "value": "4096",
      "reason": "Set to avoid temporal tiling artifacts, but may cause high RAM usage",
      "from": "Kiwv"
    },
    {
      "setting": "Steps for res2s sampler",
      "value": "25",
      "reason": "Default used, but may test if less is sufficient",
      "from": "Mandark"
    },
    {
      "setting": "Upscaler steps",
      "value": "20 steps, CFG 4",
      "reason": "Testing configuration for spatial upscaler",
      "from": "TK_999"
    },
    {
      "setting": "reserve-vram",
      "value": "4GB for 4090",
      "reason": "Prevents OOM errors, can drop to 2-3GB for lower res/length",
      "from": "Gill Bastar, Tachyon"
    },
    {
      "setting": "I2V strength",
      "value": "Under 0.4 for movement",
      "reason": "Higher values can cause frozen/static results",
      "from": "Kijai"
    },
    {
      "setting": "I2V strength",
      "value": "0.8 for character preservation",
      "reason": "Maintains character but loses identity faster",
      "from": "Kijai"
    },
    {
      "setting": "I2V strength",
      "value": "1.0 for best character retention",
      "reason": "Keeps character better but still loses it over time",
      "from": "Kijai"
    },
    {
      "setting": "LTXVPreprocess strength",
      "value": "33 standard, increase for frozen videos",
      "reason": "Higher values help unfreeze static results",
      "from": "976496720370348032"
    },
    {
      "setting": "Final resolution height",
      "value": "Not over ~1000 pixels for base tile",
      "reason": "Prevents artifacts and duplications",
      "from": "976496720370348032"
    },
    {
      "setting": "FPS encoding after temporal upscaler",
      "value": "Double the original fps",
      "reason": "Temporal upscaler doubles fps, need to encode at double rate or video looks slowmo",
      "from": "976496720370348032"
    },
    {
      "setting": "CFG scale for non-distilled",
      "value": "4.0",
      "reason": "Default recommended setting for non-distilled models",
      "from": "Kijai"
    },
    {
      "setting": "Steps for non-distilled",
      "value": "30 steps first sampler, 3 steps second sampler",
      "reason": "Proper step count needed for full model quality",
      "from": "Tachyon"
    },
    {
      "setting": "Steps for distilled",
      "value": "8 steps",
      "reason": "Sufficient for distilled fp8 model",
      "from": "Kijai"
    },
    {
      "setting": "Mask parameters",
      "value": "mask_audio=True, mask_video=True",
      "reason": "Will regenerate audio/video between mask_start_time and mask_end_time",
      "from": "976496720370348032"
    },
    {
      "setting": "--reserve-vram",
      "value": "4-6 for RTX 4090",
      "reason": "Prevents OOM errors, higher values significantly slow performance",
      "from": "protector131090"
    },
    {
      "setting": "Frame count for quality balance",
      "value": "960x960x571 at 30fps",
      "reason": "Close to limit but works fine for quality/length balance",
      "from": "seitanism"
    },
    {
      "setting": "Custom sigmas usage",
      "value": "Use custom sigmas from distilled model",
      "reason": "These are the ones used in distilling the distilled model",
      "from": "976496720370348032"
    },
    {
      "setting": "Spatial inpainting setup",
      "value": "start_time=0, end_time=big number, mask_video=True, mask_audio=False",
      "reason": "Generates just video latents and keeps audio",
      "from": "976496720370348032"
    },
    {
      "setting": "--reserve-vram",
      "value": "4-12 depending on use case",
      "reason": "4090 needs 4-5 minimum, up to 10+ with controlnet. 3090 users suggest 11-12. Value depends on resolution, frame count, models used",
      "from": "Juan Gea, Q!, seitanism"
    },
    {
      "setting": "LTXVPreprocess strength",
      "value": "33+",
      "reason": "High values help prevent frozen/static results in I2V",
      "from": "976496720370348032"
    },
    {
      "setting": "Temporal overlap in tiled decoder",
      "value": "Minimum 0 (modify code)",
      "reason": "Can improve results, default minimum is 16 but reducing helps",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09, tavi.halperin"
    },
    {
      "setting": "Page file size",
      "value": "60-100GB",
      "reason": "Fixes random crashes, 100GB works well but 60GB may be enough",
      "from": "Owlie"
    },
    {
      "setting": "CFG",
      "value": "1.0",
      "reason": "Reduces VRAM usage by 4GB compared to higher CFG values",
      "from": "Kijai"
    },
    {
      "setting": "Steps with distilled LoRA",
      "value": "8 or lower",
      "reason": "Default workflow has 20 steps but distilled LoRA should use fewer steps",
      "from": "MechanimaL"
    },
    {
      "setting": "Upscaling denoise",
      "value": "0.2",
      "reason": "Lower denoise with shift 0.2-0.4 helps preserve details when upscaling",
      "from": "dj47"
    },
    {
      "setting": "NVFP4 inference time",
      "value": "7.35s/it for 8 steps",
      "reason": "1920x1088x193 frames on 5090 with sage attention",
      "from": "Kijai"
    },
    {
      "setting": "FPS",
      "value": "25",
      "reason": "Official model training framerate, shown on website",
      "from": "Kiwv"
    },
    {
      "setting": "Reserve VRAM",
      "value": "4-9 depending on setup",
      "reason": "Prevents OOM during generation, 9 seems optimal for 5090 8-second I2V",
      "from": "toxicvenom117"
    },
    {
      "setting": "Resolution for working setup",
      "value": "1280x704",
      "reason": "Works for all setups, can multiply by 2 for higher res",
      "from": "avataraim"
    },
    {
      "setting": "Frame count",
      "value": "121 frames standard, up to 334+ possible",
      "reason": "Model can handle longer sequences, 20 seconds official, 30+ seconds seen working",
      "from": "multiple users"
    },
    {
      "setting": "Steps for full model",
      "value": "40 steps",
      "reason": "Reference code default, 20 steps only works with distill LoRA",
      "from": "Kijai"
    },
    {
      "setting": "CFG for full model",
      "value": "3",
      "reason": "Original code uses CFG 3, not 4 like some examples",
      "from": "Kijai"
    },
    {
      "setting": "Page file size",
      "value": "64-65GB",
      "reason": "Needed if under 100GB RAM to prevent crashes",
      "from": "Tachyon"
    },
    {
      "setting": "Reserve VRAM",
      "value": "Start with 1 or 2",
      "reason": "For slow I2V performance issues",
      "from": "Kijai"
    },
    {
      "setting": "CFG",
      "value": "4",
      "reason": "For full model with 20-30 steps to avoid audio degradation",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "setting": "CFG",
      "value": "1",
      "reason": "For distilled model, disable distilled LoRA when using this",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "setting": "Steps",
      "value": "20-30",
      "reason": "Minimum steps for undistilled model, low steps produce bad results",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "setting": "--reserve-vram",
      "value": "2",
      "reason": "Reserves 2GB VRAM, improved speed from 06:12<21:04 to 01:01<01:19",
      "from": "NebSH"
    },
    {
      "setting": "VAE tile size",
      "value": "1536",
      "reason": "Better performance on 4090, default is 512",
      "from": "David Snow"
    },
    {
      "setting": "Audio fps",
      "value": "48",
      "reason": "Must be changed along with video fps when generating at double rate",
      "from": "Juan Gea"
    },
    {
      "setting": "--reserve-vram",
      "value": "6GB for 4K on RTX 4090",
      "reason": "Enables 4K generation without OOM",
      "from": "Kijai"
    },
    {
      "setting": "Detailer lora strength",
      "value": "0.5-0.75",
      "reason": "Optimal quality improvement without artifacts",
      "from": "David Snow"
    },
    {
      "setting": "Full model CFG and steps",
      "value": "CFG 3.5, 40 steps",
      "reason": "Default settings for full model",
      "from": "NebSH"
    },
    {
      "setting": "Distilled model settings",
      "value": "CFG 1 with distill lora",
      "reason": "Standard configuration for distilled workflow",
      "from": "gopnik"
    },
    {
      "setting": "1920x1088 generation",
      "value": "CFG 3.5, 40 steps, FPS 24, euler_ancestral, distilled lora str 1, detailer lora 0.75 str on both stages, --reserve-vram 6GB",
      "reason": "Successfully tested on RTX 4090 without OOM",
      "from": "gopnik"
    },
    {
      "setting": "NAG impact on generation time",
      "value": "~10% slower",
      "reason": "Performance impact is relatively minimal for the quality improvement",
      "from": "Kijai"
    },
    {
      "setting": "Distill LoRA strength for second stage",
      "value": "0.9",
      "reason": "Appears to improve quality compared to full strength",
      "from": "IceAero"
    },
    {
      "setting": "Resolution for testing",
      "value": "1280x704 for normal, 2560x1408 possible on RTX 3090",
      "reason": "Workflow was originally set for 2K, normal resolution is lower",
      "from": "avataraim"
    },
    {
      "setting": "Enhance-A-Video strength",
      "value": "8.0 tested without breaking",
      "reason": "Surprisingly high values can work without issues",
      "from": "David Snow"
    },
    {
      "setting": "Dev model recommended settings",
      "value": "40 steps, CFG 4, Euler sampler",
      "reason": "Standard workflow parameters mentioned by experienced users",
      "from": "Hashu"
    },
    {
      "setting": "I2V without distill LoRA",
      "value": "1st stage: Euler 40 steps CFG 3, 2nd stage: Euler 8 steps CFG 3",
      "reason": "Recommended for experiencing I2V at its fullest",
      "from": "toxicvenom117"
    },
    {
      "setting": "Distill LoRA negative weight",
      "value": "-0.3 to -0.6",
      "reason": "Fixes plastic skin while maintaining speed benefits",
      "from": "David Snow"
    },
    {
      "setting": "SLG block selection",
      "value": "Block 2",
      "reason": "Good balance, not too strong like block 1",
      "from": "Kijai"
    },
    {
      "setting": "Distil LoRA weight on upscale",
      "value": "-0.5",
      "reason": "Reduces shiny skin and AI contrast issues",
      "from": "David Snow"
    },
    {
      "setting": "VRAM reserve",
      "value": "12GB",
      "reason": "For heavy LoRA usage, though 6GB might be sufficient",
      "from": "David Snow"
    },
    {
      "setting": "CFG for distil model",
      "value": "1.0",
      "reason": "Recommended CFG setting for distilled model",
      "from": "David Snow"
    },
    {
      "setting": "Frame intervals",
      "value": "8",
      "reason": "LTX model uses 8-frame intervals (not 4n+1)",
      "from": "TK_999"
    },
    {
      "setting": "Enhance-a-Video weight",
      "value": "2.0",
      "reason": "Starting recommendation",
      "from": "Kijai"
    },
    {
      "setting": "I2V mask level threshold",
      "value": "under 0.3",
      "reason": "If no movement below this, something is broken",
      "from": "Kijai"
    },
    {
      "setting": "I2V mask level",
      "value": "0.5 on first and last",
      "reason": "User example for troubleshooting static output",
      "from": "Gleb Tretyak"
    },
    {
      "setting": "Compression level",
      "value": "40ish max tested",
      "reason": "Maximum compression tested by developer",
      "from": "Kijai"
    },
    {
      "setting": "Native LTX sampler",
      "value": "res_2s",
      "reason": "Default sampler used in native workflows",
      "from": "Tachyon"
    },
    {
      "setting": "ComfyUI sampler",
      "value": "Euler",
      "reason": "Default sampler in ComfyUI",
      "from": "Tachyon"
    },
    {
      "setting": "Preferred samplers",
      "value": "lcm and er_sde",
      "reason": "Developer preferences - lcm for standard, er_sde for stronger option",
      "from": "Kijai"
    },
    {
      "setting": "CFG value for distilled model",
      "value": "1.0",
      "reason": "Distilled model designed to work at CFG=1",
      "from": "976496720370348032"
    },
    {
      "setting": "Steps for full model without distill LoRA",
      "value": "40",
      "reason": "Need higher steps for quality without distillation",
      "from": "Tachyon"
    },
    {
      "setting": "Steps for distilled model",
      "value": "8 + 3",
      "reason": "Default working configuration",
      "from": "zelgo_"
    },
    {
      "setting": "Upscale steps",
      "value": "2-3 steps",
      "reason": "Second stage upscaling requires fewer steps, specified by sigmas to control creativity level",
      "from": "976496720370348032"
    },
    {
      "setting": "Image compression for stylized images",
      "value": "40-90",
      "reason": "Higher compression helps with motion in stylized content",
      "from": "N0NSens"
    },
    {
      "setting": "Conditioning strength for I2V",
      "value": "Less than 1.0",
      "reason": "Helps avoid second frame cuts by allowing model to modify latent",
      "from": "976496720370348032"
    },
    {
      "setting": "Detail daemon settings from David Snow",
      "value": "Referenced workflow settings",
      "reason": "Proven settings for good results",
      "from": "Gleb Tretyak"
    },
    {
      "setting": "--reserve-vram",
      "value": "6-7 for high resolution",
      "reason": "Prevents OOM at huge resolutions due to estimation issues",
      "from": "Kijai"
    },
    {
      "setting": "Distilled model steps",
      "value": "8 steps with 1 CFG",
      "reason": "Standard configuration for distilled model",
      "from": "buggz"
    },
    {
      "setting": "Dev model steps",
      "value": "20 steps with CFG 4",
      "reason": "Standard configuration for dev model",
      "from": "buggz"
    },
    {
      "setting": "VRAM usage target",
      "value": "90-95% max",
      "reason": "Prevents Windows shared memory slowdown",
      "from": "Kijai"
    },
    {
      "setting": "temporal_tile_length",
      "value": "16-24",
      "reason": "Default 16 is safe, 24 almost causes OOM, 50 causes cooking/performance issues",
      "from": "garbus"
    },
    {
      "setting": "image_compression",
      "value": "40",
      "reason": "Increases from default 30-33 to fix slideshow behavior",
      "from": "foxydits"
    },
    {
      "setting": "image_strength",
      "value": "0.8",
      "reason": "Lower values help avoid slideshow/zoom behavior",
      "from": "garbus"
    },
    {
      "setting": "CFG",
      "value": "4",
      "reason": "Default setting for fp8 full model with proper step counts",
      "from": "Tachyon"
    },
    {
      "setting": "--reserve-vram",
      "value": "8",
      "reason": "Improved second sampler speed to 11s/it on 1088p 193 frames",
      "from": "217076219072479232"
    },
    {
      "setting": "Audio start time",
      "value": "0.2 or higher",
      "reason": "Prevents static video at beginning when audio start time equals video start time",
      "from": "Elvaxorn"
    },
    {
      "setting": "Reserve VRAM",
      "value": "7",
      "reason": "Needed for I2V with 384 LoRA on high VRAM cards",
      "from": "Kijai"
    },
    {
      "setting": "Detail Daemon strength",
      "value": "0.4 for normal, up to 5.0 for overkill",
      "reason": "Higher values provide more detail enhancement",
      "from": "David Snow"
    },
    {
      "setting": "Page file size",
      "value": "65GB",
      "reason": "Prevents ComfyUI crashes when system runs out of RAM",
      "from": "Tachyon"
    },
    {
      "setting": "Distill LoRA strength",
      "value": "0.8 to 1.0",
      "reason": "For use with fp8-dev model in I2V",
      "from": "Tachyon"
    },
    {
      "setting": "memory_usage_factor",
      "value": "0.2 for safety, 0.16 for 5090",
      "reason": "Prevents I2V VRAM issues and freezing at higher resolutions",
      "from": "Elvaxorn"
    },
    {
      "setting": "reserve-vram",
      "value": "4-20 depending on card",
      "reason": "Alternative solution for VRAM issues, achieves same offloading as memory_usage_factor",
      "from": "Kijai"
    },
    {
      "setting": "image compression",
      "value": "0 for character consistency, 32-33 default",
      "reason": "Lower values maintain character better, higher values allow more motion",
      "from": "Kijai"
    },
    {
      "setting": "detailer LoRA strength",
      "value": "0.2",
      "reason": "Low value prevents artifacts while improving details",
      "from": "David Snow"
    },
    {
      "setting": "Compression strength",
      "value": "0.50",
      "reason": "Helps with motion generation by providing compression artifacts for LTX to interpret",
      "from": "hicho"
    },
    {
      "setting": "LoRA strength",
      "value": "0.6",
      "reason": "Better results than default 1.0 setting in ComfyUI workflow",
      "from": "MechanimaL"
    },
    {
      "setting": "Steps for audio quality",
      "value": "40 steps",
      "reason": "40 steps compared to 20 gives much cleaner and consistent audio",
      "from": "IceAero"
    },
    {
      "setting": "Max shift",
      "value": "1.88",
      "reason": "Lower than default steep settings for better results",
      "from": "TK_999"
    },
    {
      "setting": "Frame calculation",
      "value": "Always divisible by 8, then add 1 (e.g., 152/8 + 1 = 153 for 25fps 6-second video)",
      "reason": "Required for proper model function",
      "from": "Choowkee"
    },
    {
      "setting": "Reserve VRAM",
      "value": "4GB for high resolution i2v",
      "reason": "Prevents OOM on higher resolutions",
      "from": "Tachyon"
    },
    {
      "setting": "Spatial upscaler tiles",
      "value": "Use more tiles for memory management",
      "reason": "Prevents OOM during spatial upscaling",
      "from": "568465354158768129"
    },
    {
      "setting": "ComfyUI launch flags",
      "value": "--normalvram --preview-method none",
      "reason": "Works well for RTX 4060 TI 16GB setup",
      "from": "568465354158768129"
    },
    {
      "setting": "FPS",
      "value": "25 fps instead of 24 fps",
      "reason": "Much better results, more like Pixar/Disney movie quality",
      "from": "568465354158768129"
    },
    {
      "setting": "Resolution",
      "value": "1440x800 or 1920x1080 minimum",
      "reason": "Better results than lower resolutions",
      "from": "568465354158768129"
    },
    {
      "setting": "Distilled LoRA strength",
      "value": "0.8 instead of 1.0",
      "reason": "Makes skin look less plastic/waxy",
      "from": "568465354158768129"
    },
    {
      "setting": "Sampler",
      "value": "euler_sde instead of euler",
      "reason": "Better results in certain cases, made things better",
      "from": "568465354158768129"
    },
    {
      "setting": "Steps for res2 sampler",
      "value": "Half the steps compared to euler",
      "reason": "res2 sampler requires half the steps of standard samplers",
      "from": "568465354158768129"
    },
    {
      "setting": "VAE decode tiling",
      "value": "4 4 4 instead of 512",
      "reason": "Doesn't matter much for video quality but more efficient",
      "from": "KingGore2023"
    },
    {
      "setting": "Image strength",
      "value": "0.6 default",
      "reason": "Controls how much generation can change the first frame, 1.0 keeps it as-is, 0.0 almost ignores input",
      "from": "zelgo_"
    },
    {
      "setting": "Detail daemon",
      "value": "0.25 on second pass",
      "reason": "Adds detail without being too aggressive",
      "from": "David Snow"
    },
    {
      "setting": "Enhance a video",
      "value": "4 on both passes",
      "reason": "Consistent enhancement across pipeline",
      "from": "David Snow"
    },
    {
      "setting": "FPS",
      "value": "48-50 fps",
      "reason": "Reduces motion artifacts and texture smearing",
      "from": "David Snow"
    },
    {
      "setting": "Distilled lora strength",
      "value": "1.0",
      "reason": "Better results than lower values like 0.2",
      "from": "seitanism"
    },
    {
      "setting": "CFG schedule",
      "value": "Start at 3, end at 0.2",
      "reason": "Works well with dynamic lora at 0.8 strength",
      "from": "The Shadow (NYC)"
    },
    {
      "setting": "Chunking",
      "value": "4 chunks",
      "reason": "Ensures VRAM usage below other functions, 2 chunks cuts peak but 4 is safer",
      "from": "Kijai"
    },
    {
      "setting": "Steps for distilled model",
      "value": "40 steps first stage, 3 steps second stage",
      "reason": "Standard configuration for two-stage workflow",
      "from": "nacho.money"
    }
  ],
  "concepts": [
    {
      "term": "SDE vs ODE samplers",
      "explanation": "SDE samplers add noise every step for quality/exploration/error-correction but need more steps. ODE samplers (euler, dpm, unipc, deis) just solve equations to sample, faster but potentially lower quality",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "term": "Abliteration",
      "explanation": "Removes internal censoring biases in models, makes them uncensored but potentially less intelligent",
      "from": "Kiwv"
    },
    {
      "term": "NVFP4",
      "explanation": "FP4 quantization format proprietary to NVIDIA 5000 series (Blackwell) cards",
      "from": "Kiwv"
    },
    {
      "term": "Abliteration",
      "explanation": "Process to uncensor models, new method causes less IQ loss compared to old style",
      "from": "Mandark"
    },
    {
      "term": "Projections only model",
      "explanation": "Separated projection components from main checkpoint to reduce disk usage and fix low VRAM issues",
      "from": "yi"
    },
    {
      "term": "Temporal tiling artifacts",
      "explanation": "Nasty artifacts that occur with many video models during VAE decode",
      "from": "TK_999"
    },
    {
      "term": "Abliterated model",
      "explanation": "Model finetuned to remove censoring bias, won't say 'sorry I can't do that', gives more accurate prompts in iffy territory",
      "from": "Kiwv"
    },
    {
      "term": "LTXVPreprocess/CRF",
      "explanation": "Adds h.264-like compression artifacts to images to bridge train-test gap between real video frames and pristine T2I images",
      "from": "976496720370348032"
    },
    {
      "term": "Latent guides",
      "explanation": "Used for controlling generation with reference frames, strength <1.0 gives more freedom, 1.0 for most important keyframes",
      "from": "976496720370348032"
    },
    {
      "term": "Audio/Video masking",
      "explanation": "mask_audio=True means regenerate audio between mask times, mask_video=True means regenerate video - think of mask as the verb to edit",
      "from": "976496720370348032"
    },
    {
      "term": "Slope_len parameter",
      "explanation": "Method for building phase-in-phase-out with original video pixels, a kind of crossfade that determines blend coefficients",
      "from": "976496720370348032"
    },
    {
      "term": "V2V latent processing",
      "explanation": "Video-to-video processing in latent space allows for better quality upscaling and modifications",
      "from": "hicho"
    },
    {
      "term": "res_2s scheduler",
      "explanation": "Does 2 steps within 1 step, so 20 steps = effectively 80 steps",
      "from": "yi"
    },
    {
      "term": "Latent volume constraint",
      "explanation": "WxHxF (width x height x frames) of video sections affects quality. Not about original video length but the latent volume used",
      "from": "976496720370348032"
    },
    {
      "term": "Spatial vs temporal inpainting",
      "explanation": "Spatial = generating parts of frames/all frames with masks. Temporal = regenerating certain sections of video/audio timeline",
      "from": "976496720370348032"
    },
    {
      "term": "projections_only",
      "explanation": "Component used with gemma text encoder to reduce size, part of FP8 optimization",
      "from": "BobbyD4AI"
    },
    {
      "term": "Execution Order Controller",
      "explanation": "Ensures gemma text encoder loads and runs first, then unloads to free memory for next model - reduces VRAM consumption",
      "from": "mkupchik_lightricks"
    },
    {
      "term": "LTXVPreprocess img_compression parameter",
      "explanation": "Using larger values can help with memory issues and model performance",
      "from": "mkupchik_lightricks"
    },
    {
      "term": "Comfy-kitchen",
      "explanation": "PyTorch extension layer that adds NVIDIA-specific low-bit (FP8/FP4) tensor layouts and kernels for LTX 2's NVFP4 checkpoints to run fast and fit in VRAM",
      "from": "Grimm1111"
    },
    {
      "term": "Latent masking for continuation",
      "explanation": "Model can attend to masked latents whether they're before or after the regenerated region, enabling flexible video editing",
      "from": "976496720370348032"
    },
    {
      "term": "Reference latent mechanism",
      "explanation": "LoopingSampler conditions on last part of previous video using reference latents rather than direct masking",
      "from": "976496720370348032"
    },
    {
      "term": "Reserve VRAM",
      "explanation": "Blocks a certain amount of VRAM from being used by ComfyUI for inference, helps prevent OOM",
      "from": "224611423869730818"
    },
    {
      "term": "Abliterated models",
      "explanation": "Models with safety blocks removed, but don't suddenly unlock much - real changes are in retrained models that alter token usage",
      "from": "Scruffy"
    },
    {
      "term": "Batch size in training",
      "explanation": "Amount of videos evaluated per step in training - more videos gives more accurate evaluation but doesn't speed up training",
      "from": "Kiwv"
    },
    {
      "term": "Manual sigmas",
      "explanation": "Steps/denoise rate values that don't have names unlike scheduler-generated sigmas",
      "from": "yi"
    },
    {
      "term": "Pinned memory",
      "explanation": "Feature that makes offloading faster by a lot sometimes, enabled by default",
      "from": "Kijai"
    },
    {
      "term": "Cache strategies",
      "explanation": "cache-none executes every node each run with reduced RAM/VRAM usage, cache-ram uses RAM pressure caching with threshold",
      "from": "Kijai"
    },
    {
      "term": "Noise injection in latent space",
      "explanation": "Adding noise to latent representation rather than image level to create motion in static inputs",
      "from": "Lodis"
    },
    {
      "term": "Distilled LoRA vs distilled model",
      "explanation": "Distilled LoRA is for two-pass procedure, distilled model is direct replacement - don't use both together",
      "from": "Lodis"
    },
    {
      "term": "fp8 scaled",
      "explanation": "A specific type of fp8 quantization that's closer to full precision than regular fp8",
      "from": "Kijai"
    },
    {
      "term": "Temporal overlap in VAE",
      "explanation": "LTX's VAE has temporal convolutions that create boundary effects on temporal tiles - overlap prevents artifacts",
      "from": "976496720370348032"
    },
    {
      "term": "LL and LN notation",
      "explanation": "L=lora, N=none. So LN means lora on first pass, none on second pass",
      "from": "David Snow"
    },
    {
      "term": "NAG for LTX",
      "explanation": "Negative Attention Guidance works with LTX - just one extra crossattn call without significant slowdown",
      "from": "Kijai"
    },
    {
      "term": "NAG (Negative Audio/visual Guidance)",
      "explanation": "System for adding negative prompts to avoid unwanted elements in both video and audio generation",
      "from": "Kijai"
    },
    {
      "term": "Joint model architecture",
      "explanation": "LTX Video 2 requires video part even for audio-only generation since it's a unified model",
      "from": "Kijai"
    },
    {
      "term": "Distill strength adjustment",
      "explanation": "Can modify the strength of distillation even when using pre-distilled models",
      "from": "Kijai"
    },
    {
      "term": "Embedding connectors",
      "explanation": "Additional layers for the text encoder stored in checkpoints, separate files available when not using full checkpoint",
      "from": "Kijai"
    },
    {
      "term": "CFG batching",
      "explanation": "When ComfyUI has enough memory, it batches positive and negative inputs into single model run instead of separate runs - uses more memory but can be faster",
      "from": "Kijai"
    },
    {
      "term": "Distil LoRA negative weight",
      "explanation": "Using LoRA at negative weight to counteract effects - works because LoRA represents difference between distill and dev models",
      "from": "Kijai"
    },
    {
      "term": "Embedding connector weights difference",
      "explanation": "Dev and distill models have different embedding connector weights, and LoRA doesn't include this difference",
      "from": "Kijai"
    },
    {
      "term": "Embedding connector weights",
      "explanation": "Normally in checkpoint file, separate files needed when using GGUF or separate diffusion models instead of checkpoints",
      "from": "Kijai"
    },
    {
      "term": "Dynamic rank reduction",
      "explanation": "Method to reduce LoRA size while retaining specified percentage of fro value per layer",
      "from": "Kijai"
    },
    {
      "term": "SLG",
      "explanation": "Works well with CFG for few steps with distilled model",
      "from": "Kijai"
    },
    {
      "term": "Embedder_connector weights",
      "explanation": "Section that connects between Gemma text encoder and the main model, has different weights between dev and distill models",
      "from": "Kijai"
    },
    {
      "term": "Reference latent technique",
      "explanation": "Method used for keyframe conditioning where model attends to dedicated latent frame rather than direct tensor insertion",
      "from": "976496720370348032"
    },
    {
      "term": "Mask level conditioning",
      "explanation": "Value (1 - strength) determines noise level applied to latent, with 1.0 keeping unmodified and lower values allowing model modification",
      "from": "976496720370348032"
    },
    {
      "term": "Cross-modal guidance (sm)",
      "explanation": "Parameter controlling how tightly video generation follows audio conditioning, critical for lip sync and timing",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "term": "Strength parameter in keyframes",
      "explanation": "Controls influence towards reference image - high strength pushes towards ref image",
      "from": "The Shadow (NYC)"
    },
    {
      "term": "STG sigmas",
      "explanation": "Sigmas in STG node are not used directly, closest one from real sigmas is chosen instead",
      "from": "Gleb Tretyak"
    },
    {
      "term": "Model activations vs weights",
      "explanation": "Model size is static, VRAM usage varies on input size due to temporary tensors and activations",
      "from": "Kijai"
    },
    {
      "term": "Activations vs model weights",
      "explanation": "VRAM offload only affects model weights, activations still need to fit in VRAM for generation",
      "from": "Kijai"
    },
    {
      "term": "Peak VRAM consumption",
      "explanation": "Usually occurs at feed forward activation stage, most critical for memory management",
      "from": "Kijai"
    },
    {
      "term": "slope_len parameter",
      "explanation": "Creates fading intensity latent mask affecting masked frames to different degrees, works in 8-frame increments, values below 8 have no effect",
      "from": "138332118890708992"
    },
    {
      "term": "Dev connector",
      "explanation": "Additional layers between the text encoder and the model, different in distill vs dev model",
      "from": "Kijai"
    },
    {
      "term": "Macroblock noise",
      "explanation": "Type of noise from JPEG compression similar to MPEG video compression that model associates with cinematic sequences",
      "from": "138332118890708992"
    },
    {
      "term": "embedding_connector",
      "explanation": "Component that changes based on model type (dev vs distilled), affects output when mixed between model types",
      "from": "Kijai"
    },
    {
      "term": "per-token timestep handling",
      "explanation": "Memory inefficient process causing I2V to use gigabytes more VRAM than T2V, similar to WAN 5B issue",
      "from": "Kijai"
    },
    {
      "term": "IC LoRA",
      "explanation": "Image Conditioning LoRA that requires guides/reference images, used for detailing and upscaling",
      "from": "Kijai"
    },
    {
      "term": "Timestep embedding optimization",
      "explanation": "Creating timestep embed per token when using masks creates huge tensors with mostly identical values. Optimization only does embeddings for unique timesteps then expands later",
      "from": "Kijai"
    },
    {
      "term": "Abliterated text encoders",
      "explanation": "Modified text encoders that remove safety restrictions, but don't uncensor the underlying diffusion model which wasn't trained on NSFW content",
      "from": "Kiwv"
    },
    {
      "term": "Distilled LoRA",
      "explanation": "A compressed version that provides speedup for inference, works with both GGUF and regular models",
      "from": "multiple users"
    },
    {
      "term": "Mixed layer approach",
      "explanation": "Quantization method used in Q4 and below, Q5 and above use different approach",
      "from": "Kijai"
    },
    {
      "term": "Manual sigmas",
      "explanation": "Setting used in stage 2 when reducing acceleration weight - typically increase steps manually when using manual sigmas",
      "from": "\u30e9D."
    },
    {
      "term": "Noise compression",
      "explanation": "Process to introduce H.264-like compression artifacts to simulate video frames and encourage motion in the model",
      "from": "976496720370348032"
    },
    {
      "term": "Reference image resizing in I2V",
      "explanation": "ComfyUI I2V workflow resizes image so longest side is 1536 for proper noise compression, image isn't supposed to be same size as latent",
      "from": "Kijai"
    },
    {
      "term": "Image strength in I2V",
      "explanation": "Controls how much the generation can deviate from the input image - higher values keep closer to original, lower values allow more change",
      "from": "138332118890708992"
    },
    {
      "term": "Character drift",
      "explanation": "Issue where character appearance changes during video generation, especially noticeable in second pass upscaling",
      "from": "David Snow"
    },
    {
      "term": "8+1 frame rule",
      "explanation": "Frame count must be 1 modulus 8 (leave remainder of 1 when divided by 8) due to how LTXV video latents work - 8 latent frames per pixel frame plus one for initial image",
      "from": "976496720370348032"
    },
    {
      "term": "FPS vs frame count confusion",
      "explanation": "FPS for conditioning the model's motion speed is different from encoding FPS and different from total frame count requirements",
      "from": "976496720370348032"
    },
    {
      "term": "Audio latent granularity",
      "explanation": "Audio latents are 25 latents per second and have different time granularity than video, making cutting/splicing complex",
      "from": "976496720370348032"
    }
  ],
  "resources": [
    {
      "resource": "LTX-2 FP4 model",
      "url": "https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp4.safetensors",
      "type": "model",
      "from": "Mandark"
    },
    {
      "resource": "Gemma abliterated model",
      "url": "https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2",
      "type": "model",
      "from": "Kiwv"
    },
    {
      "resource": "Audio super resolution",
      "url": "https://github.com/haoheliu/versatile_audio_super_resolution",
      "type": "tool",
      "from": "Kiwv"
    },
    {
      "resource": "First Frame to Last Frame workflow",
      "url": "https://gist.github.com/kabachuha/dafd6952bdc00050b4d6b594d11bec6c",
      "type": "workflow",
      "from": "Tachyon"
    },
    {
      "resource": "Lightweight Gemma FP4",
      "url": "https://huggingface.co/GitMylo/LTX-2-comfy_gemma_fp8_e4m3fn/resolve/main/ltx-2-19b-dev-fp4_projections_only.safetensors",
      "type": "model",
      "from": "avataraim"
    },
    {
      "resource": "ComfyUI BF16 support PR",
      "url": "https://github.com/Comfy-Org/ComfyUI/pull/11713",
      "type": "repo",
      "from": "patientx"
    },
    {
      "resource": "Example workflows",
      "url": "https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows",
      "type": "workflow",
      "from": "Xor"
    },
    {
      "resource": "Better abliteration technique",
      "url": "https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration",
      "type": "repo",
      "from": "Mandark"
    },
    {
      "resource": "LTX-2-comfy_gemma_fp8_e4m3fn",
      "url": "https://huggingface.co/GitMylo/LTX-2-comfy_gemma_fp8_e4m3fn",
      "type": "model",
      "from": "Phr00t"
    },
    {
      "resource": "LTX Detailer LoRA",
      "url": "https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer",
      "type": "lora",
      "from": "N0NSens"
    },
    {
      "resource": "ComfyUI LTX2 workflows",
      "url": "https://blog.comfy.org/p/ltx-2-open-source-audio-video-ai",
      "type": "workflow",
      "from": "Tachyon"
    },
    {
      "resource": "SageAttention for Windows",
      "url": "https://github.com/sdbds/SageAttention-for-windows",
      "type": "repo",
      "from": "Tachyon"
    },
    {
      "resource": "Triton for Windows",
      "url": "https://github.com/woct0rdho/triton-windows",
      "type": "repo",
      "from": "Tachyon"
    },
    {
      "resource": "PainterLTXV2",
      "url": "https://github.com/princepainter/ComfyUI-PainterLTXV2",
      "type": "repo",
      "from": "avataraim"
    },
    {
      "resource": "Gemma-3-12b-Abliterated-LTX2",
      "url": "https://huggingface.co/FusionCow/Gemma-3-12b-Abliterated-LTX2/tree/main",
      "type": "model",
      "from": "Kiwv"
    },
    {
      "resource": "ComfyUI_MusicTools for voice naturalization",
      "url": "https://github.com/jeankassio/ComfyUI_MusicTools",
      "type": "tool",
      "from": "Xor"
    },
    {
      "resource": "AudioBatch for audio channel conversion",
      "url": "https://github.com/set-soft/ComfyUI-AudioBatch",
      "type": "tool",
      "from": "421114995925843968"
    },
    {
      "resource": "Video upscale using WAN FusionX",
      "url": "https://civitai.com/models/1714513/video-upscale-or-enhancer-using-wan-fusionx-ingredients",
      "type": "workflow",
      "from": "Samuca"
    },
    {
      "resource": "ComfyUI-AudioSR wrapper",
      "url": "https://github.com/Saganaki22/ComfyUI-AudioSR",
      "type": "tool",
      "from": "drbaph"
    },
    {
      "resource": "AudioSR model",
      "url": "https://huggingface.co/drbaph/AudioSR",
      "type": "model",
      "from": "drbaph"
    },
    {
      "resource": "LTX-2 ICLoRA workflow",
      "url": "https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/LTX-2_ICLoRA_All_Distilled.json",
      "type": "workflow",
      "from": "976496720370348032"
    },
    {
      "resource": "Transformer-only model variants",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/blob/main/diffusion_models/ltx-2-19b-distilled-fp8_transformer_only.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "LTX Video official workflows",
      "url": "https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows",
      "type": "workflow",
      "from": "MechanimaL"
    },
    {
      "resource": "Smaller Gemma3 quantized model",
      "url": "https://huggingface.co/unsloth/gemma-3-12b-it-bnb-4bit/tree/main",
      "type": "model",
      "from": "MechanimaL"
    },
    {
      "resource": "LTX-2 GGUF transformer",
      "url": "https://huggingface.co/smthem/LTX-2-Test-gguf/tree/main",
      "type": "model",
      "from": "zelgo_"
    },
    {
      "resource": "Alternative Gemma3 models for testing",
      "url": "https://huggingface.co/reedmayhew/Suno-Song-Generator-gemma3-12B-HF",
      "type": "model",
      "from": "Scruffy"
    },
    {
      "resource": "Anti-slop Gemma3 model",
      "url": "https://huggingface.co/sam-paech/gemma-3-12b-it-antislop",
      "type": "model",
      "from": "Scruffy"
    },
    {
      "resource": "ComfyUI LTXVideo custom nodes",
      "url": "https://github.com/Lightricks/ComfyUI-LTXVideo",
      "type": "repo",
      "from": "NebSH"
    },
    {
      "resource": "IC Detailer LoRA",
      "url": "https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/blob/main/ltx-2-19b-ic-lora-detailer.safetensors",
      "type": "lora",
      "from": "scf"
    },
    {
      "resource": "AudioSR for audio upscaling",
      "url": "https://github.com/Saganaki22/ComfyUI-AudioSR",
      "type": "tool",
      "from": "976496720370348032"
    },
    {
      "resource": "FP8 Gemma text encoder",
      "url": "https://huggingface.co/GitMylo/LTX-2-comfy_gemma_fp8_e4m3fn",
      "type": "model",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "resource": "FlashSR for audio processing",
      "url": "https://github.com/ysharma3501/FlashSR",
      "type": "tool",
      "from": "224611423869730818"
    },
    {
      "resource": "Versatile Audio Super Resolution",
      "url": "https://github.com/haoheliu/versatile_audio_super_resolution",
      "type": "tool",
      "from": "MOV"
    },
    {
      "resource": "LTX-2 example workflows",
      "url": "https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows",
      "type": "workflow",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "Gemma 3 12B model for LTXV Audio Text Encoder",
      "url": "https://huggingface.co/Comfy-Org/ltx-2/blob/main/split_files/text_encoders/gemma_3_12B_it.safetensors",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "FP8 Gemma model",
      "url": "https://huggingface.co/GitMylo/LTX-2-comfy_gemma_fp8_e4m3fn/blob/main/gemma_3_12B_it_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "ComfyUI memory management PR",
      "url": "https://github.com/Comfy-Org/ComfyUI/pull/11741",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Video upscale workflow using WAN",
      "url": "https://civitai.com/models/1714513/video-upscale-or-enhancer-using-wan-fusionx-ingredients",
      "type": "workflow",
      "from": "dj47"
    },
    {
      "resource": "ComfyUI AudioMass node",
      "url": "https://github.com/jtydhr88/ComfyUI-AudioMass",
      "type": "node",
      "from": "Kagi"
    },
    {
      "resource": "ComfyUI nodes registry for audio",
      "url": "https://registry.comfy.org/?nodes_index%5Bquery%5D=audio",
      "type": "tool",
      "from": "Kagi"
    },
    {
      "resource": "LTX-2 prompting guide",
      "url": "https://ltx.io/model/model-blog/prompting-guide-for-ltx-2",
      "type": "guide",
      "from": "avataraim"
    },
    {
      "resource": "NVFP4 model",
      "url": "https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp4.safetensors",
      "type": "model",
      "from": "seitanism"
    },
    {
      "resource": "TEAR LoRA",
      "url": "https://huggingface.co/oumoumad/LTX-2-19b-LoRA-TEAR/tree/main",
      "type": "lora",
      "from": "hicho"
    },
    {
      "resource": "Gemma 3 12b model",
      "url": "https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized",
      "type": "model",
      "from": "theUnlikely"
    },
    {
      "resource": "Gemma-3-12b-Abliterated-LTX2",
      "url": "https://huggingface.co/FusionCow/Gemma-3-12b-Abliterated-LTX2/tree/main",
      "type": "model",
      "from": "TK_999"
    },
    {
      "resource": "Featherless AI model search",
      "url": "https://featherless.ai/model-families/gemma3/12b",
      "type": "resource",
      "from": "Scruffy"
    },
    {
      "resource": "HuggingFace search tool",
      "url": "https://hf.tst.eu",
      "type": "tool",
      "from": "Scruffy"
    },
    {
      "resource": "CrystalDiskInfo",
      "url": "https://sourceforge.net/projects/crystaldiskinfo/",
      "type": "tool",
      "from": "garbus"
    },
    {
      "resource": "LTXV2_comfy weights",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/tree/main",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "LTX-2-SDNQ-4bit-dynamic",
      "url": "https://huggingface.co/Disty0/LTX-2-SDNQ-4bit-dynamic",
      "type": "model",
      "from": "cocktailprawn1212"
    },
    {
      "resource": "Audio-video model research paper",
      "url": "https://arxiv.org/abs/2601.04151",
      "type": "research",
      "from": "yi"
    },
    {
      "resource": "ComfyUI Noise nodes",
      "url": "https://github.com/BlenderNeko/ComfyUI_Noise",
      "type": "repo",
      "from": "Lodis"
    },
    {
      "resource": "LTX ComfyUI official",
      "url": "https://github.com/Lightricks/ComfyUI-LTXVideo",
      "type": "repo",
      "from": "NebSH"
    },
    {
      "resource": "Kijai GGUF models",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/tree/main/diffusion_models",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Vantage GGUF fork",
      "url": "https://github.com/vantagewithai/Vantage-GGUF",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI-GGUF PR #399",
      "url": "https://github.com/city96/ComfyUI-GGUF/pull/399",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "LTX Detailer LoRA",
      "url": "https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/tree/main",
      "type": "model",
      "from": "zelgo_"
    },
    {
      "resource": "Gemma 4bit GGUF",
      "url": "https://huggingface.co/unsloth/gemma-3-12b-it-bnb-4bit/tree/main",
      "type": "model",
      "from": "zelgo_"
    },
    {
      "resource": "Vantage LTX GGUF variants",
      "url": "https://huggingface.co/vantagewithai/LTX-2-GGUF/tree/main",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "Calcuis GGUF nodes",
      "url": "https://github.com/calcuis/gguf",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "City96 GGUF loader PR",
      "url": "https://github.com/city96/ComfyUI-GGUF/pull/399",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "ComfyUI memory management improvement",
      "url": "https://github.com/Comfy-Org/ComfyUI/pull/11748",
      "type": "repo",
      "from": "yi"
    },
    {
      "resource": "ComfyUI preview latent support",
      "url": "https://github.com/Comfy-Org/ComfyUI/pull/11741",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Adaptive Guidance for ComfyUI",
      "url": "https://github.com/asagi4/ComfyUI-Adaptive-Guidance",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Mac-compatible MelBandRoFormer",
      "url": "https://github.com/Brainkeys/ComfyUI-MelBandRoFormer-Mac/tree/main",
      "type": "repo",
      "from": "buggz"
    },
    {
      "resource": "LTX training example",
      "url": "https://huggingface.co/kabachuha/ltx2-hydraulic-press",
      "type": "model",
      "from": "224611423869730818"
    },
    {
      "resource": "LTX VAE location",
      "url": "Kijai's HuggingFace",
      "type": "model",
      "from": "Juan Gea"
    },
    {
      "resource": "FP8 scaled Gemma 3 text encoder",
      "url": "https://huggingface.co/Comfy-Org/ltx-2/blob/main/split_files/text_encoders/gemma_3_12B_it_fp8_scaled.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Enhance-A-Video",
      "url": "https://github.com/NUS-HPC-AI-Lab/Enhance-A-Video",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "GGUF loader options",
      "url": "https://github.com/city96/ComfyUI-GGUF/pull/399",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "Gemma-3-12b-Abliterated single file",
      "url": "https://huggingface.co/FusionCow/Gemma-3-12b-Abliterated-LTX2/tree/main",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "LTX-2 dev embeddings connector",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/blob/main/text_encoders/ltx-2-19b-embeddings_connector_dev_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "LTX-2 distill embeddings connector",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/blob/main/text_encoders/ltx-2-19b-embeddings_connector_distill_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Gemma 3 GGUF support PR",
      "url": "https://github.com/city96/ComfyUI-GGUF/pull/402",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "QuantStack LTX-2 GGUF models",
      "url": "https://huggingface.co/QuantStack/LTX-2-GGUF/tree/main/LTX-2-dev",
      "type": "model",
      "from": "zelgo_"
    },
    {
      "resource": "Alternative LTX-2 GGUF models",
      "url": "https://huggingface.co/vantagewithai/LTX-2-GGUF/tree/main/dev",
      "type": "model",
      "from": "Miku"
    },
    {
      "resource": "Comfy-Org LTX-2 Gemma fpmixed",
      "url": "https://huggingface.co/Comfy-Org/ltx-2/blob/main/split_files/text_encoders/gemma_3_12B_it_fpmixed.safetensors",
      "type": "model",
      "from": "comfy"
    },
    {
      "resource": "Comfy-Org LTX-2 Gemma fp4_mixed",
      "url": "https://huggingface.co/Comfy-Org/ltx-2/blob/main/split_files/text_encoders/gemma_3_12B_it_fp4_mixed.safetensors",
      "type": "model",
      "from": "comfy"
    },
    {
      "resource": "Abliterated Gemma model",
      "url": "https://huggingface.co/FusionCow/Gemma-3-12b-Abliterated-LTX2/tree/main",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "LTX Video example workflows",
      "url": "https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows",
      "type": "workflow",
      "from": "David Snow"
    },
    {
      "resource": "City96 GGUF LTX support pull requests",
      "url": "https://github.com/city96/ComfyUI-GGUF/pull/399 and https://github.com/city96/ComfyUI-GGUF/pull/402",
      "type": "repo",
      "from": "japar"
    },
    {
      "resource": "Kijai LTXV2 ComfyUI repo",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy",
      "type": "repo",
      "from": "Gleb Tretyak"
    },
    {
      "resource": "Dynamic rank reduced distill lora",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/tree/main/loras",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "Required ComfyUI commit",
      "url": "https://github.com/Comfy-Org/ComfyUI/commit/bd0e6825e84606e0706bbb5645e9ea1f4ad8154d",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Shutter Encoder",
      "url": "https://www.shutterencoder.com/",
      "type": "tool",
      "from": "Kijai"
    },
    {
      "resource": "Owlie's prompt example",
      "url": "https://pastebin.com/7WhL0HWj",
      "type": "workflow",
      "from": "Owlie"
    },
    {
      "resource": "ComfyUI image examples",
      "url": "https://files.catbox.moe/5uy9w5.png and https://files.catbox.moe/ara1ck.png",
      "type": "workflow",
      "from": "Owlie"
    },
    {
      "resource": "Gemma 3 12b Abliterated",
      "url": "https://huggingface.co/FusionCow/Gemma-3-12b-Abliterated-LTX2/tree/main",
      "type": "model",
      "from": "David Snow"
    },
    {
      "resource": "Vantage-GGUF fork",
      "url": "https://github.com/vantagewithai/Vantage-GGUF",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Gemma tokenizer files",
      "url": "https://huggingface.co/smhf72/gemma-3-12b-it-extras-comfy",
      "type": "model",
      "from": "yi"
    },
    {
      "resource": "FP4 text encoders",
      "url": "https://huggingface.co/Comfy-Org/ltx-2/tree/main/split_files/text_encoders",
      "type": "model",
      "from": "zelgo_"
    },
    {
      "resource": "GGUF text encoders",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/tree/main/text_encoders",
      "type": "model",
      "from": "buggz"
    },
    {
      "resource": "Official LTX workflows",
      "url": "https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows",
      "type": "workflow",
      "from": "976496720370348032"
    },
    {
      "resource": "LTX-2 paper",
      "url": "https://arxiv.org/abs/2601.03233",
      "type": "research",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "resource": "24GB VRAM fix",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1q5k6al/fix_to_make_ltxv2_work_with_24gb_or_less_of_vram/",
      "type": "guide",
      "from": "Underdog"
    },
    {
      "resource": "GGUF workflow example",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1q8gffc/using_gguf_models_for_ltx2_in_t2v/",
      "type": "workflow",
      "from": "buggz"
    },
    {
      "resource": "Reddit OOM fix for 24GB VRAM",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1q5k6al/fix_to_make_ltxv2_work_with_24gb_or_less_of_vram/",
      "type": "guide",
      "from": "noodlz"
    },
    {
      "resource": "GGUF models from QuantStack",
      "url": "https://huggingface.co/QuantStack/LTX-2-GGUF/tree/main/LTX-2-dev",
      "type": "model",
      "from": "zelgo_"
    },
    {
      "resource": "Kijai LTXV2 ComfyUI models",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/tree/main",
      "type": "model",
      "from": "various"
    },
    {
      "resource": "Audio separation tools",
      "url": "https://github.com/jeankassio/ComfyUI_MusicTools",
      "type": "tool",
      "from": "267974636690472960"
    },
    {
      "resource": "Bartowski Gemma GGUF",
      "url": "https://huggingface.co/bartowski/google_gemma-3-12b-it-GGUF",
      "type": "model",
      "from": "cocktailprawn1212"
    },
    {
      "resource": "Kijai's LTXV2 models",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/tree/main",
      "type": "model",
      "from": "Elvaxorn"
    },
    {
      "resource": "VACE-LTX-Video-0.9",
      "url": "https://huggingface.co/ali-vilab/VACE-LTX-Video-0.9",
      "type": "model",
      "from": "NebSH"
    },
    {
      "resource": "LTX-2 video extend workflow",
      "url": "https://github.com/Rolandjg/LTX-2-video-extend-ComfyUI/blob/main/ltx-video-and-audio-extend.json",
      "type": "workflow",
      "from": "Miku"
    },
    {
      "resource": "Earth Zoom Out LoRA",
      "url": "https://discord.com/channels/1076117621407223829/1459859512025546792",
      "type": "lora",
      "from": "627140525916422145"
    },
    {
      "resource": "SageAttention 2 precompiled for cu130 torch 2.9.0",
      "url": "https://github.com/woct0rdho/SageAttention/releases/download/v2.2.0-windows.post3/sageattention-2.2.0+cu130torch2.9.0.post3-cp39-abi3-win_amd64.whl",
      "type": "tool",
      "from": "Tachyon"
    },
    {
      "resource": "ComfyUI cache management PR",
      "url": "https://github.com/Comfy-Org/ComfyUI/pull/10779",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Join Audio Channels commit",
      "url": "https://github.com/Comfy-Org/ComfyUI/commit/027042db6811c875562296f0a6b797c89d59e426",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Kijai's LoRA versions",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/tree/main/loras",
      "type": "model",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "LTX2 LoRAs by NebSH",
      "url": "https://huggingface.co/Nebsh/RollTransition https://huggingface.co/Nebsh/POVObject/ https://huggingface.co/Nebsh/LTX2_Pushtoglass https://huggingface.co/Nebsh/LTX2_Herocam_Lora/tree/main https://huggingface.co/Nebsh/LTX2_Animatediff_style/",
      "type": "lora",
      "from": "NebSH"
    },
    {
      "resource": "VRAM fix Reddit post",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1qa1or3/ltx2_1080p_lipsync_if_you_liked_the_previous_one/",
      "type": "guide",
      "from": "Elvaxorn"
    },
    {
      "resource": "Vantage GGUF loader",
      "url": "https://github.com/vantagewithai/Vantage-GGUF",
      "type": "node",
      "from": "David Snow"
    },
    {
      "resource": "Dev embedding connector",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/blob/main/text_encoders/ltx-2-19b-embeddings_connector_dev_bf16.safetensors",
      "type": "model",
      "from": "Kijai"
    },
    {
      "resource": "LTX2 Rapid Merges",
      "url": "https://huggingface.co/Phr00t/LTX2-Rapid-Merges",
      "type": "model",
      "from": "Phr00t"
    },
    {
      "resource": "Memory optimized ComfyUI branch",
      "url": "https://github.com/kijai/ComfyUI/tree/ltx2_memory",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Heretic Gemma FP8 text encoder",
      "url": "https://huggingface.co/DreamFast/gemma-3-12b-it-heretic/blob/main/comfyui/gemma_3_12B_it_heretic_fp8_e4m3fn.safetensors",
      "type": "model",
      "from": "Choowkee"
    },
    {
      "resource": "AudioSR for audio enhancement",
      "url": "https://audioldm.github.io/audiosr/",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "ComfyUI AudioSR nodes",
      "url": "https://github.com/Saganaki22/ComfyUI-AudioSR",
      "type": "nodes",
      "from": "David Snow"
    },
    {
      "resource": "Unsloth LTX-2 GGUF models",
      "url": "https://huggingface.co/unsloth/LTX-2-GGUF/tree/main",
      "type": "model",
      "from": "568465354158768129"
    },
    {
      "resource": "QuantStack LTX-2 GGUF models",
      "url": "https://huggingface.co/QuantStack/LTX-2-GGUF/tree/main/LTX-2-dev",
      "type": "model",
      "from": "568465354158768129"
    },
    {
      "resource": "LTX-2 Distilled LoRA",
      "url": "https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors",
      "type": "model",
      "from": "568465354158768129"
    },
    {
      "resource": "Kijai's Audio VAE",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/blob/main/VAE/LTX2_audio_vae_bf16.safetensors",
      "type": "model",
      "from": "568465354158768129"
    },
    {
      "resource": "Official LTX example workflows",
      "url": "https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows",
      "type": "workflow",
      "from": "David Snow"
    },
    {
      "resource": "Vantage GGUF Nodes",
      "url": "https://github.com/vantagewithai/Vantage-Nodes",
      "type": "repo",
      "from": "568465354158768129"
    },
    {
      "resource": "Kijai's updated av_model.py",
      "url": "https://github.com/kijai/ComfyUI/blob/ac4daffd80cecbc56ee0e31f2b521114fa0f8e08/comfy/ldm/lightricks/av_model.py",
      "type": "repo",
      "from": "568465354158768129"
    },
    {
      "resource": "Lightricks workflows customized for GGUF",
      "url": "",
      "type": "workflow",
      "from": "568465354158768129"
    },
    {
      "resource": "Gemma 3 FP8 scaled model",
      "url": "https://huggingface.co/Comfy-Org/ltx-2/blob/main/split_files/text_encoders/gemma_3_12B_it_fp8_scaled.safetensors",
      "type": "model",
      "from": "Xor"
    },
    {
      "resource": "LTXV2 ComfyUI models",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy",
      "type": "model",
      "from": "Xor"
    },
    {
      "resource": "ComfyUI-GGUF (City96)",
      "url": "https://github.com/city96/ComfyUI-GGUF",
      "type": "repo",
      "from": "568465354158768129"
    },
    {
      "resource": "ComfyUI-GGUF (CRCODE22 fork)",
      "url": "https://github.com/CRCODE22/ComfyUI-GGUF",
      "type": "repo",
      "from": "568465354158768129"
    },
    {
      "resource": "LTX2-Infinity workflow",
      "url": "https://github.com/Z-L-D/LTX2-Infinity",
      "type": "workflow",
      "from": "ZombieMatrix"
    },
    {
      "resource": "Unsloth Gemma 3 GGUF",
      "url": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF",
      "type": "model",
      "from": "*Dan"
    },
    {
      "resource": "Gemma tokenizer.model",
      "url": "https://huggingface.co/google/gemma-3-12b-it/resolve/main/tokenizer.model",
      "type": "model",
      "from": "*Dan"
    },
    {
      "resource": "Kijai's memory-optimized av_model.py",
      "url": "https://github.com/kijai/ComfyUI/blob/ltx2_memory/comfy/ldm/lightricks/av_model.py",
      "type": "code",
      "from": "Kijai"
    },
    {
      "resource": "Gemma 3 12B IT GGUF",
      "url": "https://huggingface.co/unsloth/gemma-3-12b-it-GGUF/tree/main",
      "type": "model",
      "from": "cocktailprawn1212"
    },
    {
      "resource": "ComfyUI-GGUF commit for LTX2",
      "url": "https://github.com/city96/ComfyUI-GGUF/commit/58625e1cb63a8b8dd1bf4e0221de032b5135d0d2",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "Volume control browser extension",
      "url": "mentioned but not linked",
      "type": "tool",
      "from": "David Snow"
    },
    {
      "resource": "LTX2 lip sync workflow",
      "url": "https://www.reddit.com/r/StableDiffusion/comments/1qa1or3/ltx2_1080p_lipsync_if_you_liked_the_previous_one/",
      "type": "workflow",
      "from": "dg1860"
    },
    {
      "resource": "Kijai Dynamic Lora bf16 r175",
      "url": "https://huggingface.co/Kijai/LTXV2_comfy/tree/main/loras",
      "type": "lora",
      "from": "The Shadow (NYC)"
    },
    {
      "resource": "Mixed precision text encoder",
      "url": "https://huggingface.co/Comfy-Org/ltx-2/tree/main/split_files/text_encoders",
      "type": "model",
      "from": "TK_999"
    },
    {
      "resource": "Latent2rgb PR",
      "url": "https://github.com/Comfy-Org/ComfyUI/pull/11741",
      "type": "repo",
      "from": "Kijai"
    },
    {
      "resource": "LTX-2 IC-LoRA-Detailer",
      "url": "https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/tree/main",
      "type": "lora",
      "from": "The Shadow (NYC)"
    }
  ],
  "limitations": [
    {
      "limitation": "Model cannot do violence",
      "details": "LTX-2 seems unable to generate violent content, ignores prompts asking for violence like stabbing",
      "from": "Kiwv"
    },
    {
      "limitation": "Audio quality is poor",
      "details": "Audio sounds like 64kbps mp3 from p2p sharing era, needs post-processing with audio-to-audio models",
      "from": "Mandark"
    },
    {
      "limitation": "High VRAM usage for native high resolution",
      "details": "Native 1080p uses 4x more VRAM than upscaled approach, causes OOM issues",
      "from": "Kiwv"
    },
    {
      "limitation": "Gemma model censorship",
      "details": "Q4 version outputs 'this prompt contains unsafe content' for boxing prompts, returning original prompt",
      "from": "leodev"
    },
    {
      "limitation": "Model degrades after 20 seconds",
      "details": "Quality falls apart over 20s, coherence issues in longer videos",
      "from": "Tachyon"
    },
    {
      "limitation": "Violence prompts don't work well",
      "details": "Model doesn't seem to have knowledge of violence, failed on violent prompts",
      "from": "Kiwv"
    },
    {
      "limitation": "No VACE equivalent",
      "details": "Unlike WAN, LTX2 doesn't have VACE functionality and never will",
      "from": "Grimm1111"
    },
    {
      "limitation": "Distilled model with distill setup",
      "details": "Makes LTX turn into dumb first version - generates anything but what's in prompt",
      "from": "N0NSens"
    },
    {
      "limitation": "Cannot reliably prevent subtitles in output",
      "details": "No reliable way to stop model from putting subtitles on screen",
      "from": "Q-"
    },
    {
      "limitation": "Compressed latent space causes quality loss",
      "details": "LTX is fast because of very compressed latent space, but this causes garbled faces and details in wide shots",
      "from": "976496720370348032"
    },
    {
      "limitation": "Portrait videos more prone to artifacts",
      "details": "Portrait can break more easily than landscape with high token count or long duration",
      "from": "976496720370348032"
    },
    {
      "limitation": "Character identity loss over time",
      "details": "Model loses character identity as video progresses, especially with higher I2V strength",
      "from": "Kijai, Nokai"
    },
    {
      "limitation": "No support for pure audio-only continuation",
      "details": "Cannot continue from audio only - need to mask both audio and video segments",
      "from": "976496720370348032"
    },
    {
      "limitation": "Model struggles with animated character identity",
      "details": "LTX tends toward realism and may not identify animated characters well, often results in frozen animation",
      "from": "Juan Gea"
    },
    {
      "limitation": "Distilled models lose more character identity",
      "details": "While faster, distilled versions sacrifice identity preservation compared to full models",
      "from": "Kijai"
    },
    {
      "limitation": "Some images refuse to animate",
      "details": "Certain images only generate slide shows instead of proper animation, even with prompt enhancer",
      "from": "Zueuk"
    },
    {
      "limitation": "NVFP4 doesn't work with control LoRAs",
      "details": "FP4 quantization is incompatible with control LoRA functionality",
      "from": "cyncratic"
    },
    {
      "limitation": "Memory management issues with custom node workflows",
      "details": "LTX custom node workflows can cause OOM errors that don't occur with native workflows",
      "from": "Juan Gea"
    },
    {
      "limitation": "I2V has temporal compression issues",
      "details": "Smudginess and glitchy artifacts in fast moving frames, though 48 FPS helps counteract this",
      "from": "Ada"
    },
    {
      "limitation": "Cannot generate audio-only without redundant video",
      "details": "No audio-only derived model available yet, need to generate video and discard it",
      "from": "976496720370348032"
    },
    {
      "limitation": "Static camera LoRA affects 2D anime style",
      "details": "Turns 2D retro anime into 3D style",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "Quality degrades with high frame count at high resolution",
      "details": "960x960 at 1000 frames outputs garbage due to tensor size limits",
      "from": "seitanism"
    },
    {
      "limitation": "Struggles with non-standard character poses",
      "details": "Completely overwhelmed with characters doing handstands or similar poses",
      "from": "seitanism"
    },
    {
      "limitation": "1024 token limit on text encoder",
      "details": "ComfyUI reports prompt length exceeding limit",
      "from": "PeroBueno"
    },
    {
      "limitation": "I2V struggles with frozen/static results without control signals",
      "details": "Especially with 2D/3D animation, often needs pose control to get movement",
      "from": "Juan Gea"
    },
    {
      "limitation": "Character likeness not fully preserved in I2V",
      "details": "Characters tend to morph toward realistic look and lose original appearance",
      "from": "Juan Gea"
    },
    {
      "limitation": "WAN upscale destroys lip sync",
      "details": "If you have videos with people talking, upscaling breaks synchronization",
      "from": "seitanism"
    },
    {
      "limitation": "Memory management issues cause model reloading",
      "details": "Before decoding latents, ComfyUI empties VRAM causing models to reload repeatedly",
      "from": "yi"
    },
    {
      "limitation": "Preview quality is poor",
      "details": "Resolution is tiny for LTX latent space so preview isn't great",
      "from": "Kijai"
    },
    {
      "limitation": "Model doesn't understand tabs or musical notation",
      "details": "Never saw captions with tabs during training, would need LoRA training for musical chord notation",
      "from": "976496720370348032"
    },
    {
      "limitation": "Upscaling destroys lip sync",
      "details": "Even at low denoise strength (0.09), upscaling significantly degrades lip synchronization",
      "from": "seitanism"
    },
    {
      "limitation": "Non-linear memory usage",
      "details": "Memory usage isn't linear and can spike unexpectedly during generation",
      "from": "Kijai"
    },
    {
      "limitation": "Full autoregressive audio+video is tricky",
      "details": "Time granularities of audio and video latent spaces don't match, making full autoregressive generation challenging",
      "from": "976496720370348032"
    },
    {
      "limitation": "fp4 quantization performance",
      "details": "fp4 is twice as slow and quality is too low, even on Blackwell cards where it's supposed to be accelerated",
      "from": "Shawneau \ud83c\udf41 [CA]"
    },
    {
      "limitation": "Long video generation quality degradation",
      "details": "200+ frame generations work but quality falls apart over very long sequences",
      "from": "Shawneau \ud83c\udf41 [CA]"
    },
    {
      "limitation": "Custom audio integration",
      "details": "Still can't get custom audio to work properly",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "limitation": "Focus issues with camera movement",
      "details": "LTX loves moving cameras and focal shifts but subject is never in focus",
      "from": "dj47"
    },
    {
      "limitation": "Looping sampler cannot handle audio",
      "details": "Audio functionality not available with looping sampler",
      "from": "Chandler \u2728 \ud83c\udf88"
    },
    {
      "limitation": "Advanced noise node incompatibility",
      "details": "Advanced noise node from res4lyf doesn't work with LTX",
      "from": "l\u0488u\u0488c\u0488i\u0488f\u0488e\u0488r\u0488"
    },
    {
      "limitation": "FP4 quantization quality degradation",
      "details": "FP4 quants will degrade video quality even with Blackwell series",
      "from": "yi"
    },
    {
      "limitation": "Audio quality issues",
      "details": "Audio has robotic noise, needs improvement for next LTX version",
      "from": "Lodis"
    },
    {
      "limitation": "Fast changing scenes problems",
      "details": "Model doesn't like to switch scenes, works better with continuous shots",
      "from": "VK (5080 128gb)"
    },
    {
      "limitation": "Model size estimation issues",
      "details": "ComfyUI thinks model is 35GB during inference, memory estimation for offloading not quite accurate",
      "from": "yi"
    },
    {
      "limitation": "Blurry output at same resolution",
      "details": "LTX2 outputs less sharp videos than WAN at same resolution, need 1080p to match WAN 720p quality",
      "from": "seitanism"
    },
    {
      "limitation": "Tiled VAE artifacts",
      "details": "Visible square artifacts when using tiled VAE, especially at high resolutions like 2K",
      "from": "David Snow"
    },
    {
      "limitation": "Distilled model face blur",
      "details": "Distilled model produces blurry/out-of-focus faces, not just motion blur",
      "from": "Juan Gea"
    },
    {
      "limitation": "Perfect images cause no motion",
      "details": "Too perfect input images with no blur result in motionless video output",
      "from": "1100803024298975263"
    },
    {
      "limitation": "LoRAs degrade audio quality",
      "details": "LoRAs can considerably degrade audio depending on training method",
      "from": "ZeusZeus (RTX 4090)"
    },
    {
      "limitation": "Q4 quality is poor",
      "details": "Q4 quantization produces poor quality, not recommended unless absolutely necessary",
      "from": "Kijai"
    },
    {
      "limitation": "--cache-none breaks advanced workflows",
      "details": "Using --cache-none for memory makes video extension and advanced workflows impossible",
      "from": "Zueuk"
    },
    {
      "limitation": "Model doesn't follow complex prompts well",
      "details": "Neither dev nor distilled models follow detailed prompts for complex actions like character switching positions",
      "from": "N0NSens"
    },
    {
      "limitation": "Negative prompts don't work with distilled model",
      "details": "CFG 1 prevents negative prompt functionality",
      "from": "KingGore2023"
    },
    {
      "limitation": "Lower quantization reduces speed",
      "details": "Lower the quant, slower it becomes",
      "from": "Kijai"
    },
    {
      "limitation": "VAE produces ghosting artifacts",
      "details": "Encoding and decoding video produces ghosting artifacts that affect v2v pipelines",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "limitation": "NAG doesn't go through MLPs",
      "details": "Major limitation as MLPs are a significant part of the model architecture",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "limitation": "Audio prompt doesn't seem to do much",
      "details": "Based on limited testing, audio prompting may have minimal effect",
      "from": "Kijai"
    },
    {
      "limitation": "Torch compile doesn't work well with offloading",
      "details": "Too many graph breaks, device copies and recompiles make it very slow (30s/it to 200s/it)",
      "from": "Kijai"
    },
    {
      "limitation": "Characters sometimes don't talk in I2V",
      "details": "Audio is present but no mouth movement, different seed can solve it",
      "from": "PeroBueno"
    },
    {
      "limitation": "Low resolution generations are blurry",
      "details": "720p results in blurred output, need higher resolution but causes OOM issues",
      "from": "N0NSens"
    },
    {
      "limitation": "Portrait/vertical video has motion issues",
      "details": "Jittery motion in vertical videos, likely due to limited portrait video training data from platforms like TikTok, Instagram, Snapchat that don't release data",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "limitation": "Audio quality issues with dev model",
      "details": "High pitched drone present in dev model audio that isn't in distill model",
      "from": "David Snow"
    },
    {
      "limitation": "Plastic skin appearance with distill model",
      "details": "Humans rendered with distill model have shiny plastic skin texture",
      "from": "David Snow"
    },
    {
      "limitation": "Low quality output without distill LoRA on dev model",
      "details": "Dev model at 40 steps CFG 4 produces much worse quality than with distill LoRA, feels like needs extra 40 steps",
      "from": "Wildminder"
    },
    {
      "limitation": "Video extension uses significantly more VRAM",
      "details": "VRAM usage at least double compared to basic txt2video or img2video, 24GB sufficient for 10s ~1080p txt2video but 480p barely works for extension",
      "from": "74637656272666624"
    },
    {
      "limitation": "Poor text/logo generation",
      "details": "Model struggles with text generation, example showed 'Retro Synthwave' rendered as unreadable text",
      "from": "David Snow"
    },
    {
      "limitation": "Character consistency issues",
      "details": "Model changes character face after 3 seconds, requires tedious manual correction",
      "from": "KingGore2023"
    },
    {
      "limitation": "Poor prompt following compared to alternatives",
      "details": "Model doesn't listen to prompting well, generates what it wants rather than following prompts closely",
      "from": "Grimm1111"
    },
    {
      "limitation": "2D animation quality",
      "details": "Model is great at realism and 3D animation but bad at 2D - noticeably worse than Wan or Kandinsky for 2D",
      "from": "Ada"
    },
    {
      "limitation": "Enhance-a-Video compatibility",
      "details": "Not working properly with LTX2, changes output too much and doesn't seem to burn like other models",
      "from": "Kijai"
    },
    {
      "limitation": "Ethnic bias in outputs",
      "details": "Model tends to skew characters towards being African American or Indian, even when not prompted",
      "from": "David Snow"
    },
    {
      "limitation": "VAE compression issues",
      "details": "VAE has more compression and is somewhat bugged - door frames warp and motion becomes transparent, not good for v2v",
      "from": "\u4f01\u9d5d\uff0850% CASH 50%GOLD\uff09"
    },
    {
      "limitation": "Rank reduction limits",
      "details": "Even rank32/64 loses too much quality - to keep 90% fro the average rank is still 175",
      "from": "Kijai"
    },
    {
      "limitation": "Keyframe adherence not perfect",
      "details": "Due to reference latent technique, adherence to keyframes can sometimes be relatively low",
      "from": "976496720370348032"
    },
    {
      "limitation": "Model resists drawn/stylized content",
      "details": "Tries to render photoreal instead of maintaining artistic styles",
      "from": "N0NSens"
    },
    {
      "limitation": "Challenging interpolation with certain styles",
      "details": "Some artistic styles are difficult for the model to interpolate smoothly",
      "from": "976496720370348032"
    },
    {
      "limitation": "Enhanced Prompt node incompatible with GGUF",
      "details": "Node doesn't work with GGUF models, must be disabled",
      "from": "Tachyon"
    },
    {
      "limitation": "ComfyUI sampler modifications break LTX2",
      "details": "Any sampler overrides break due to nested video + audio tensor structure",
      "from": "Kijai"
    },
    {
      "limitation": "Distilled model audio quality compromise",
      "details": "Audio quality somewhat worse due to being optimized for fewer steps",
      "from": "976496720370348032"
    },
    {
      "limitation": "Full model produces poor results for many users",
      "details": "Many report distilled model works better than full model",
      "from": "Gleb Tretyak"
    },
    {
      "limitation": "Context windows don't work with audio",
      "details": "Nested video/audio latents not supported in context windows currently",
      "from": "Kijai"
    },
    {
      "limitation": "Poor text rendering",
      "details": "LTX struggles with text generation due to compressive VAEs",
      "from": "hicho"
    },
    {
      "limitation": "Model mistakes skin for clothing",
      "details": "Known issue where model confuses skin and clothing",
      "from": "Zabo"
    },
    {
      "limitation": "Face consistency issues in I2V",
      "details": "No luck with face consistency in image-to-video despite trying various settings",
      "from": "tamilboy"
    },
    {
      "limitation": "High resolution generation causes system freezing",
      "details": "Even 720p generation can freeze the entire PC during processing",
      "from": "Elvaxorn"
    },
    {
      "limitation": "Color shifts at 2K upscaling",
      "details": "Progressive upscaling works well up to 1024, but 2048 causes bad color shifting",
      "from": "KingGore2023"
    },
    {
      "limitation": "Audio I2V lip sync inconsistency",
      "details": "Lip sync strength cannot be adjusted and doesn't work reliably for talking, works better for music",
      "from": "AJO"
    },
    {
      "limitation": "Model doesn't like some input images",
      "details": "Certain images consistently fail to animate properly regardless of settings",
      "from": "TK_999"
    },
    {
      "limitation": "First-to-last frame guidance unreliable",
      "details": "Too inconsistent in using guides, requires experimentation with every image/prompt pair",
      "from": "Phr00t"
    },
    {
      "limitation": "Prompt not masked in video extension",
      "details": "The prompt applies to the whole video, not just the new part being generated",
      "from": "Kijai"
    },
    {
      "limitation": "Distill LoRA destroys audio quality",
      "details": "Using distill LoRA with dev model results in poor audio quality during upscaling",
      "from": "Wildminder"
    },
    {
      "limitation": "Multiple person dialog issues",
      "details": "Describing multiple people in same prompt usually makes main character carry out dialog of both",
      "from": "138332118890708992"
    },
    {
      "limitation": "Subtitle data contamination",
      "details": "Model has subtitle contamination, future versions will caption subtitles better for neg-prompting",
      "from": "976496720370348032"
    },
    {
      "limitation": "Some images don't work with specific names",
      "details": "Certain image/name combinations cause static results due to apparent content filtering",
      "from": "Elvaxorn"
    },
    {
      "limitation": "I2V extremely random, some seeds work others don't",
      "details": "No matter how much noise added, some seeds simply won't animate properly",
      "from": "Kijai"
    },
    {
      "limitation": "Fast motion causes blurred/shredded anatomy",
      "details": "During camera movement or subject motion, faces and limbs become blurry smudges even at 48fps",
      "from": "foxydits"
    },
    {
      "limitation": "Hard quality ceiling similar to LTX1",
      "details": "Model is fast and versatile but has fundamental quality limitations",
      "from": "Christian Sandor"
    },
    {
      "limitation": "3090 with 64GB RAM limited to ~321 frames at 1920x1088",
      "details": "Maximum resolution/length capacity even with all optimizations applied",
      "from": "Underdog"
    },
    {
      "limitation": "VRAM is hard cap even with full model offloading",
      "details": "Cannot exceed VRAM limits even when offloading entire model to RAM",
      "from": "Kijai"
    },
    {
      "limitation": "Model inconsistency",
      "details": "This model is pretty random with everything",
      "from": "Kijai"
    },
    {
      "limitation": "Audio quality issues",
      "details": "Audio in general is a weakness with this model, produces sibilant/droning sounds",
      "from": "David Snow"
    },
    {
      "limitation": "I2V subject consistency",
      "details": "I2V after 1-2 seconds will lose most of the original subject and change it to someone else (though others report good consistency)",
      "from": "568465354158768129"
    },
    {
      "limitation": "4K generation issues",
      "details": "LTX does not like 3840x2160 direct generation, took 53 minutes and had problems",
      "from": "ZombieMatrix"
    },
    {
      "limitation": "NSFW generation limited",
      "details": "Underlying diffusion model wasn't trained on NSFW content, need LoRAs to achieve that",
      "from": "Kiwv"
    },
    {
      "limitation": "Complex prompts become dumpster fire",
      "details": "Model struggles with very complex, multi-part prompts with many scene changes",
      "from": "Tachyon"
    },
    {
      "limitation": "Cannot handle cuts or complex interactions well",
      "details": "Model doesn't follow complex prompt instructions that involve multiple scene transitions",
      "from": "Ablejones"
    },
    {
      "limitation": "Prompts only ~75% accurate",
      "details": "Even official example prompts don't execute perfectly, model typically follows first 40% of complex prompts",
      "from": "Ablejones"
    },
    {
      "limitation": "Lower resolutions produce static motion",
      "details": "Resolutions like 960x544 result in very static, low-quality movement",
      "from": "568465354158768129"
    },
    {
      "limitation": "50fps requires more than 16GB VRAM",
      "details": "Unless you want only 3 second videos",
      "from": "568465354158768129"
    },
    {
      "limitation": "I2V is hit or miss depending on the image",
      "details": "Sometimes doesn't animate at all, very dependent on input image quality",
      "from": "Tachyon"
    },
    {
      "limitation": "Hard to keep character consistency in stage 2",
      "details": "Character consistency is difficult to maintain during the upscaling stage",
      "from": "KingGore2023"
    },
    {
      "limitation": "Wan2.2 has obvious limitations",
      "details": "Limited to 81 frames, 16fps constraints",
      "from": "KingGore2023"
    },
    {
      "limitation": "LTX2 is relatively weak in stage 1",
      "details": "First stage generation is not as strong as other models",
      "from": "KingGore2023"
    },
    {
      "limitation": "Fast camera movements cause major problems",
      "details": "Model struggles with rapid camera motion, though higher FPS helps reduce artifacts",
      "from": "1099257978277867542"
    },
    {
      "limitation": "Multiple person scenes degrade quickly",
      "details": "Quality drops significantly when adding more people, works best with single person scenes",
      "from": "Hevi"
    },
    {
      "limitation": "Character consistency in second pass",
      "details": "Upscaling pass often changes character appearance and introduces unrealistic skin textures",
      "from": "\u30e9D."
    },
    {
      "limitation": "Vertical video quality issues",
      "details": "Crazy artifacts when doing 1080p or higher vertical video generation",
      "from": "ucren"
    },
    {
      "limitation": "T2I performance",
      "details": "LTX2 as text-to-image is very bad, not optimized for still image generation",
      "from": "protector131090"
    },
    {
      "limitation": "Cannot handle complex actions",
      "details": "Model refuses to generate woman kicking car to slide it across street, can't do pants pulling down actions",
      "from": "nikolatesla20"
    },
    {
      "limitation": "Poor understanding of audio noise sources",
      "details": "Anything with noise sounds like sampling window isn't long enough and just repeats",
      "from": "381513363517341698"
    },
    {
      "limitation": "8-frame boundary thinking",
      "details": "Hard to describe short transient actions like 1-frame flash due to model thinking in 8-frame boundaries",
      "from": "138332118890708992"
    },
    {
      "limitation": "Frame count limits",
      "details": "Cannot generate more than 281 frames due to VAE limitations and 32-bit index math constraints",
      "from": "NebSH"
    },
    {
      "limitation": "Audio/video latent synchronization",
      "details": "Cannot cut video and audio latents along same timeline due to different time granularities",
      "from": "976496720370348032"
    }
  ],
  "hardware": [
    {
      "requirement": "VRAM for different approaches",
      "details": "Can work with 4GB VRAM minimum using system RAM offloading. Native high-res needs much more VRAM than upscaled approach",
      "from": "Moonbow"
    },
    {
      "requirement": "System RAM importance",
      "details": "System RAM more important than VRAM for offloading. Works on 2060 with proper settings",
      "from": "hicho"
    },
    {
      "requirement": "NVFP4 requirements",
      "details": "Needs Blackwell GPU (RTX 5090) and Torch 13 for NVFP4 model format",
      "from": "Kiwv"
    },
    {
      "requirement": "Performance benchmark",
      "details": "40 iterations took 08:58 (13.46s per iteration) for 1201 steps",
      "from": "Benjimon"
    },
    {
      "requirement": "RTX 5090 performance",
      "details": "Can handle 720p 20 second videos without VRAM offload, but system RAM maxes during VAE decode",
      "from": "Tachyon"
    },
    {
      "requirement": "16GB VRAM + 32GB RAM",
      "details": "Can work with FP8 model and reserve VRAM settings, struggles without optimizations",
      "from": "JUSTSWEATERS"
    },
    {
      "requirement": "Long video generation",
      "details": "5000+ frames requires significant time, 1080 frames took 200s to generate",
      "from": "ZombieMatrix"
    },
    {
      "requirement": "System RAM usage",
      "details": "64GB RAM can max out during tiled decode for 1920x960, 242 frames",
      "from": "Tachyon"
    },
    {
      "requirement": "4090 with 64GB RAM needs swapfile",
      "details": "OOMs during upscaler stage, system RAM spikes to 60GB+, 32GB swapfile fixes it",
      "from": "taoofai"
    },
    {
      "requirement": "24GB VRAM needs reserve-vram setting",
      "details": "Use --reserve-vram 4 for 4090, can drop to 2-3GB for lower res/length",
      "from": "Gill Bastar"
    },
    {
      "requirement": "Modified 4090 with 48GB best value",
      "details": "4090 mod is best deal for 48GB, D variant available for ~3k and only 10% slower but super loud",
      "from": "Benjimon"
    },
    {
      "requirement": "VRAM usage difference between T2V and I2V",
      "details": "I2V uses more VRAM than T2V due to conditioning image encoding, affects maximum frame counts",
      "from": "protector131090"
    },
    {
      "requirement": "Performance timing for 1080p I2V",
      "details": "1080p 121 frames I2V on RTX 4090 takes 1939 seconds, 720p takes 180 seconds",
      "from": "protector131090"
    },
    {
      "requirement": "T2V performance on RTX 4090",
      "details": "15 second T2V video takes 190 seconds to generate on RTX 4090",
      "from": "GalaxyTimeMachine"
    },
    {
      "requirement": "CUDA version for NVFP4",
      "details": "NVFP4 requires CUDA 13, users on CUDA 12.9 see no performance difference",
      "from": "protector131090"
    },
    {
      "requirement": "FP8 compatibility",
      "details": "FP8 works on RTX 4000 series, NVFP4 requires RTX 5000 series and up",
      "from": "Scruffy"
    },
    {
      "requirement": "Memory requirements for full model",
      "details": "Full bf16 model requires significant VRAM, may need RAM offload on cards below RTX 6000 Pro",
      "from": "421114995925843968"
    },
    {
      "requirement": "VRAM management for RTX 4090",
      "details": "24GB VRAM + 128GB RAM needs --reserve-vram 4-6 to prevent OOM. Higher values (10+) cause 10x slowdown",
      "from": "Juan Gea"
    },
    {
      "requirement": "RTX 3090 optimization",
      "details": "24GB VRAM + 64GB RAM, recommended startup: --lowvram --cache-none --reserve-vram 4",
      "from": "1100803024298975263"
    },
    {
      "requirement": "Performance comparison",
      "details": "720p 21 frames i2v takes 5 seconds on fp8 non-distilled. 3 minutes for full generation",
      "from": "GalaxyTimeMachine (RTX4090)"
    },
    {
      "requirement": "Upscale performance hit",
      "details": "Upscale pass 5x slower than lowres generation (1min gen -> 5min upsc) on lower-end hardware",
      "from": "N0NSens"
    },
    {
      "requirement": "PyTorch CUDA requirement",
      "details": "Need pytorch with cu130 or higher for optimized CUDA operations",
      "from": "NebSH"
    },
    {
      "requirement": "RTX 4090 VRAM usage",
      "details": "Needs --reserve-vram 4+ minimum, up to 10+ with controlnet. Uses 96-98% VRAM regularly",
      "from": "391020191338987522, Juan Gea"
    },
    {
      "requirement": "RTX 3090 recommended settings",
      "details": "--reserve-vram 11-12, works but less VRAM than newer cards",
      "from": "Juan Gea, Q!"
    },
    {
      "requirement": "Page file requirements",
      "details": "60-100GB system page file needed to prevent random crashes",
      "from": "Owlie"
    },
    {
      "requirement": "Speed comparison after memory fix",
      "details": "16.47s/it on first sampler, flash attention gives ~15 second improvement (3m24 to 3m09)",
      "from": "Juan Gea, 1100803024298975263"
    },
    {
      "requirement": "VRAM for 1920x1088x193 frames",
      "details": "Requires reserve-vram 6-7 GB on Linux, more with CFG enabled",
      "from": "Kijai"
    },
    {
      "requirement": "5090 performance",
      "details": "1920x1088 193 frames (8 seconds) takes 97s total, 60s inference time with 8+3 steps",
      "from": "toxicvenom117"
    },
    {
      "requirement": "Reserve VRAM scaling",
      "details": "Higher reserved VRAM leads to longer inference times due to unnecessary offloading",
      "from": "Kijai"
    },
    {
      "requirement": "RAM usage with full model",
      "details": "128GB RAM works fine with full model, 64GB may cause crashes",
      "from": "David Snow"
    },
    {
      "requirement": "VRAM reserve settings",
      "details": "RTX 5090: reserve VRAM 9 optimal for 8-second I2V. Users with 12GB+64GB RAM need reserve VRAM 2-4. 128GB RAM users still need reserve VRAM 4-5",
      "from": "multiple users"
    },
    {
      "requirement": "RAM usage and paging",
      "details": "Model uses 50-60GB pagefile even with 64GB RAM and only 35GB RAM usage. Major disk writing issue affecting SSD health",
      "from": "Fred"
    },
    {
      "requirement": "Performance benchmarks",
      "details": "5090: 1920x1088p 8-second I2V takes varying times based on reserve VRAM settings. 1280x704 takes 40sec, 2K upscale total 100sec",
      "from": "toxicvenom117"
    },
    {
      "requirement": "Training hardware needs",
      "details": "RTX Pro 6000 sufficient for training with batch size 4, would take about a week straight. H100s more expensive with no speed benefit for this use case",
      "from": "Kiwv"
    },
    {
      "requirement": "RAM for high resolution",
      "details": "Need around 64GB pagefile if under 100GB RAM, 128GB RAM recommended",
      "from": "Lodis"
    },
    {
      "requirement": "VRAM for VAE",
      "details": "15GB VRAM for encoding 121 frames at 1024x1024",
      "from": "Kijai"
    },
    {
      "requirement": "Memory management",
      "details": "RAM usage reports wildly varying, 64GB Windows has issues, 64GB Linux works better",
      "from": "Kijai"
    },
    {
      "requirement": "GGUF for quantization",
      "details": "GGUF will be better for quants, especially gemma quant will save massive RAM",
      "from": "yi"
    },
    {
      "requirement": "32GB RAM with --cache-none",
      "details": "Can run LTX2 on 32GB RAM using --cache-none but breaks advanced workflows",
      "from": "Zueuk"
    },
    {
      "requirement": "24GB VRAM struggles",
      "details": "24GB VRAM has issues, needs gemma 4-bit on CPU and careful memory management",
      "from": "Zueuk"
    },
    {
      "requirement": "GGUF slower on 40xx series",
      "details": "GGUF doesn't have fp8 speedups for 40 series cards, but good for 30 series",
      "from": "FUNZO"
    },
    {
      "requirement": "128GB RAM for full model",
      "details": "Probably need 96GB+ for larger models, user uses 128GB",
      "from": "David Snow"
    },
    {
      "requirement": "Q8 GGUF is 20.4GB",
      "details": "Q8 distilled model size is 20.4GB",
      "from": "Tachyon"
    },
    {
      "requirement": "2060 6GB with 64GB RAM works",
      "details": "Generated 121 frames at 384x672 in 380 seconds using gemma bnb",
      "from": "hicho"
    },
    {
      "requirement": "RTX 4090 4K generation",
      "details": "Can generate 4K video with --reserve-vram 6GB and 128GB RAM",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 5090 high resolution",
      "details": "Used around 70GB VRAM for 4K generation, 10 minutes per 5s video",
      "from": "D'Squarius Green, Jr."
    },
    {
      "requirement": "RTX 3090 high resolution",
      "details": "Successfully generated 2560x1408 resolution",
      "from": "NC17z"
    },
    {
      "requirement": "Q6 quantization memory usage",
      "details": "Still causes OOM issues even on high-end systems",
      "from": "boop"
    },
    {
      "requirement": "VRAM for 1080p 10 seconds",
      "details": "24GB VRAM/32GB system RAM can handle 1080p 10 second generation in 455 seconds",
      "from": "boop"
    },
    {
      "requirement": "VRAM for 2560x1408",
      "details": "RTX 3090 can handle 2560x1408 resolution",
      "from": "NC17z"
    },
    {
      "requirement": "Performance difference",
      "details": "FP8 should be 30-40% faster than Q8 GGUF on ada+ GPUs",
      "from": "mallardgazellegoosewildcat2"
    },
    {
      "requirement": "VRAM usage for different configurations",
      "details": "4090 can handle distill model workflows but dev + distill LoRA OOMs on second upscale pass. FP4 can load on RTX 2060.",
      "from": "theUnlikely"
    },
    {
      "requirement": "Generation speed benchmarks",
      "details": "4090: 90 seconds for 10s 720p video. RTX 3090: 15 minutes for 1080p 10 seconds (was actually 20 seconds). 720p renders in 2 minutes on unspecified GPU.",
      "from": "BobbyD4AI"
    },
    {
      "requirement": "Distill LoRA memory usage",
      "details": "Distill LoRA is 7GB, very heavy for a LoRA and uses significant RAM",
      "from": "Kijai"
    },
    {
      "requirement": "VRAM and RAM for longer videos",
      "details": "Need sufficient VRAM + RAM for longer than 4 second videos, works 'for the most part' if you have enough",
      "from": "Moonbow"
    },
    {
      "requirement": "fp4 model requirements",
      "details": "Need 50xx series GPU, new driver, and torch+cu130 for fp4 quantization",
      "from": "TK_999"
    },
    {
      "requirement": "RTX 3090 limitations",
      "details": "RTX 3090/64GB RAM struggles with 1080p 20 second upscale, requires VRAM offloading",
      "from": "dj47"
    },
    {
      "requirement": "4060 Ti performance",
      "details": "I2V 1280x704 361 dev gguf takes 10:15min on 3060 12GB",
      "from": "Daflon"
    },
    {
      "requirement": "VRAM for rank reduction",
      "details": "SVD requests 136GB VRAM for large weights, script needs adjustment to skip problematic weights",
      "from": "Kijai"
    },
    {
      "requirement": "VAE tiled decoding VRAM",
      "details": "Takes max 4GB for decoding 5s 720p videos, but ComfyUI removes whole model instead of offloading blocks",
      "from": "yi"
    },
    {
      "requirement": "4090 limitations",
      "details": "Hard to experiment with only 4090 - user wishes for more powerful GPU",
      "from": "happy.j"
    },
    {
      "requirement": "Memory optimization",
      "details": "Can lower peak VRAM usage by 1GB at larger inputs by chunking ffn",
      "from": "Kijai"
    },
    {
      "requirement": "4090 + 64GB RAM setup",
      "details": "Can run fp8 text encoder and choose between distilled or full model",
      "from": "berserk4501"
    },
    {
      "requirement": "3090 + 64GB RAM with modifications",
      "details": "Can run full fp16 model with cache clearing and page file usage (10-15GB)",
      "from": "Underdog"
    },
    {
      "requirement": "M4 Mac Mini with 64GB",
      "details": "GGUF provides 2x+ speed improvement, full dev takes ~28 minutes for 640x352x73",
      "from": "buggz"
    },
    {
      "requirement": "ComfyUI launch parameters for 24GB VRAM",
      "details": "--cache-none --disable-auto-launch --fast fp16_accumulation --reserve-vram 4",
      "from": "Underdog"
    },
    {
      "requirement": "VRAM management",
      "details": "4090 can do up to 4K resolution for 121 frames with --reserve-vram and fp8",
      "from": "Kijai"
    },
    {
      "requirement": "Memory for GGUF",
      "details": "GGUF Q4 LTX2 + Q4 Gemma still won't fit in 24GB VRAM together",
      "from": "138332118890708992"
    },
    {
      "requirement": "Performance impact",
      "details": "GGUF models always slower but use less VRAM - trade-off for memory usage",
      "from": "Xor"
    },
    {
      "requirement": "Sage attention benefit",
      "details": "5-10% speed improvement with sage attention for LTX2",
      "from": "belair3"
    },
    {
      "requirement": "Memory usage with large models",
      "details": "User with 128GB RAM hits 80-85GB usage, 96GB RAM + 120GB swap still crashes after multiple generations",
      "from": "ZombieMatrix"
    },
    {
      "requirement": "24GB VRAM limitations",
      "details": "Cannot handle 1080p 480 frames even with VRAM offloading due to activation memory limits",
      "from": "Kijai"
    },
    {
      "requirement": "5090 performance",
      "details": "Even 5090 users experience OOMs and hiccups with high resolution generation",
      "from": "Elvaxorn"
    },
    {
      "requirement": "RTX 4060 TI 16GB compatibility",
      "details": "Can run Q4 LTX-2 + fp8 Gemma3 but may hit VRAM limits and crash",
      "from": "568465354158768129"
    },
    {
      "requirement": "I2V memory usage",
      "details": "I2V uses significantly more VRAM than T2V - instantly 5GB+ higher when I2V node is added",
      "from": "Kijai"
    },
    {
      "requirement": "High resolution I2V",
      "details": "1280x1024@161 frames requires careful memory management, often causes OOM during upscale",
      "from": "138332118890708992"
    },
    {
      "requirement": "5090 with reserve-vram 4",
      "details": "Can OOM with 384 LoRA in I2V, need reserve-vram 7 for stable operation",
      "from": "Tachyon"
    },
    {
      "requirement": "System RAM for cache-none",
      "details": "Using --cache-none helps with VRAM but requires sufficient system RAM to avoid constant model reloading",
      "from": "Kijai"
    },
    {
      "requirement": "I2V VRAM usage",
      "details": "I2V uses gigabytes more VRAM than T2V due to per-token timestep handling inefficiency",
      "from": "Kijai"
    },
    {
      "requirement": "1024x1024x121 I2V",
      "details": "Runs on 4090 without reserve-vram or tweaks after optimizations",
      "from": "Kijai"
    },
    {
      "requirement": "Q4 1080p 10sec I2V",
      "details": "Possible on 3090 with Q4 quantization",
      "from": "Hevi"
    },
    {
      "requirement": "3090 limits",
      "details": "Can handle up to 321 frames at 1920x1088 with distilled models and 64GB RAM",
      "from": "Underdog"
    },
    {
      "requirement": "VRAM for high resolution I2V",
      "details": "Can now do 1920x1088x193 I2V with no reserve VRAM on 4090, saves ~5GB with optimization",
      "from": "Kijai"
    },
    {
      "requirement": "High resolution generation capabilities",
      "details": "1440x1440x361 possible on 16GB VRAM, 128GB RAM with optimizations",
      "from": "Gleb Tretyak"
    },
    {
      "requirement": "2560x1440 I2V requirements",
      "details": "2560x1440x241 I2V uses 60% VRAM on 4090 with 128GB RAM",
      "from": "David Snow"
    },
    {
      "requirement": "4K generation performance",
      "details": "3840x2160x49 frames takes 38+ minutes on single 3090",
      "from": "ZombieMatrix"
    },
    {
      "requirement": "Q4 GGUF on 16GB",
      "details": "Q4 GGUF works well on 16GB VRAM, smaller than FP8 so should work with quantized text encoder",
      "from": "Kiwv"
    },
    {
      "requirement": "VRAM usage scales with resolution",
      "details": "1088x1920 @145 frames uses almost all 16GB VRAM, leaves little room for LoRAs",
      "from": "568465354158768129"
    },
    {
      "requirement": "System memory important for offloading",
      "details": "Uses 128GB system memory for offloading when VRAM is insufficient, paging file setup crucial",
      "from": "568465354158768129"
    },
    {
      "requirement": "4GB VRAM likely insufficient",
      "details": "Would require extensive offloading making generation extremely slow",
      "from": "568465354158768129"
    },
    {
      "requirement": "1440x1440x481 works on 16GB VRAM",
      "details": "Takes 26 minutes on 16GB VRAM + 128GB RAM setup",
      "from": "Gleb Tretyak"
    },
    {
      "requirement": "VRAM for 50fps",
      "details": "Requires more than 16GB VRAM for reasonable video lengths",
      "from": "568465354158768129"
    },
    {
      "requirement": "RTX 3090 performance",
      "details": "Takes about 5 minutes to generate 5 seconds of 720p with upscaling stage",
      "from": "NC17z"
    },
    {
      "requirement": "RTX 3080 12GB performance",
      "details": "Can generate 5 seconds of 720p in under a minute with distilled model",
      "from": "garbus"
    },
    {
      "requirement": "RTX 3090 with GGUF",
      "details": "GGUF model Q4 + gguf gemma Q4 can go up to 35 seconds with I2V 720p",
      "from": "Hevi"
    },
    {
      "requirement": "Generation speeds",
      "details": "121 frames at 1920x1088 in about 315 seconds, 1m49s video took 16m21s to generate",
      "from": "particle9"
    },
    {
      "requirement": "16GB RAM limitation",
      "details": "16GB RAM is tough for LTX2, may cause disconnections even with VAE tiling",
      "from": "Kijai"
    },
    {
      "requirement": "Q4 GGUF performance",
      "details": "Model Q4 GGUF + Gemma Q4 GGUF = ~35 seconds for 720p on RTX 3090",
      "from": "Hevi"
    },
    {
      "requirement": "4090 laptop capability",
      "details": "RTX 4090 laptop with 16GB VRAM and 46GB RAM is capable for LTX2 generation",
      "from": "870722151316082689"
    },
    {
      "requirement": "Memory usage with new optimization",
      "details": "New av_model.py drops I2V VRAM usage to near T2V levels, tested faster on both RTX 5080 and PRO 6000",
      "from": "BitPoet (Chris)"
    },
    {
      "requirement": "VRAM reduction with chunking",
      "details": "Chunking can reduce VRAM by up to 5GB at 1080p, 2GB at 1216x1216 with 121 frames",
      "from": "Kijai"
    },
    {
      "requirement": "RTX 4060 TI 16GB performance",
      "details": "Can handle 1920x1088 @ 25 FPS, 201 frames with 6GB VRAM unused using sage attention",
      "from": "568465354158768129"
    },
    {
      "requirement": "RTX 3060 12GB limitations",
      "details": "OOM on tiled decode with 481 frames at 1024x544, needed to switch to spatial decode",
      "from": "Daflon"
    },
    {
      "requirement": "Speed improvements",
      "details": "8 steps in 14 seconds (1.81s/step) for I2V 281 frames 1280x720 with KJ py + torch/cu upgrade",
      "from": "NebSH"
    }
  ],
  "community_creations": [
    {
      "creation": "Custom LTX fork with GUI",
      "type": "tool",
      "description": "Fork of LTX with graphical interface for running outside ComfyUI",
      "from": "Benjimon"
    },
    {
      "creation": "Low VRAM workflow",
      "type": "workflow",
      "description": "Optimized workflow for very low VRAM usage with model recommendations",
      "from": "avataraim"
    },
    {
      "creation": "Abliterated Gemma model",
      "type": "model",
      "description": "BF16 version created, FP8 version planned for upload",
      "from": "Kiwv"
    },
    {
      "creation": "VRAM Debug workflow optimization",
      "type": "workflow",
      "description": "Method using VRAM debug nodes to maintain consistent generation speeds",
      "from": "Phr00t"
    },
    {
      "creation": "Audio channel conversion code snippet",
      "type": "utility",
      "description": "Custom node code for converting mono to stereo audio",
      "from": "976496720370348032"
    },
    {
      "creation": "Transformer-only model extraction script",
      "type": "tool",
      "description": "Removes VAEs and connector from checkpoint, saves ~5GB per model",
      "from": "Kijai"
    },
    {
      "creation": "Join channels node",
      "type": "node",
      "description": "ComfyUI PR for joining audio channels to complement split channels node",
      "from": "Kijai"
    },
    {
      "creation": "LTXVSetAudioVideoMaskByTime node",
      "type": "node",
      "description": "Custom node for defining audio/video masks for continuation and editing workflows",
      "from": "976496720370348032"
    },
    {
      "creation": "V2V detailer workflow",
      "type": "workflow",
      "description": "Video-to-video detailing workflow from LTX nodes for upscaling and enhancement",
      "from": "David Snow"
    },
    {
      "creation": "FP8 quantized Gemma3 text encoder",
      "type": "model",
      "description": "Community member quantized Gemma3 to FP8 at ~11GB, smaller than existing versions",
      "from": "cyncratic"
    },
    {
      "creation": "Split workflow for slower hardware",
      "type": "workflow",
      "description": "Separates lowres generation and upscaling to save time on slower systems",
      "from": "N0NSens"
    },
    {
      "creation": "LTXVLoopingSampler",
      "type": "node",
      "description": "Allows spatial tiling on video latents with overlap blending",
      "from": "976496720370348032"
    },
    {
      "creation": "LTX2 prompt enhancer fix",
      "type": "tool",
      "description": "Uses llama.cpp server with prefill support to bypass censorship issues",
      "from": "Ada"
    },
    {
      "creation": "LTXVAudioVideoMask node",
      "type": "node",
      "description": "Node for masking audio and video portions during generation",
      "from": "Kijai"
    },
    {
      "creation": "LTXVAddLatents node",
      "type": "node",
      "description": "Node to add empty latents of same spatial dimension for extending videos",
      "from": "976496720370348032"
    },
    {
      "creation": "Video padding automation",
      "type": "node enhancement",
      "description": "Added automatic padding option to extend input videos when end time exceeds input length",
      "from": "Kijai"
    },
    {
      "creation": "Distilled fp8 + 2K upscale workflow",
      "type": "workflow",
      "description": "Complete workflow for generating with distilled fp8 model and upscaling to 2K resolution",
      "from": "avataraim"
    },
    {
      "creation": "30k video training dataset",
      "type": "dataset",
      "description": "Dataset of ~30k videos prepared for LTX 2 uncensoring, includes realism, 3D, anime content",
      "from": "Kiwv"
    },
    {
      "creation": "LTX-2 LoRA training",
      "type": "lora",
      "description": "Successfully trained person LoRA using video interview dataset on fal",
      "from": "NebSH"
    },
    {
      "creation": "Sigma visualization nodes",
      "type": "node",
      "description": "Visualize nodes for sigmas in Res4lyf and KJNodes",
      "from": "Kijai"
    },
    {
      "creation": "Dynamic rank reduced distill lora",
      "type": "lora",
      "description": "Reduced size version of distill lora from rank 384 to ~242 while retaining 95% fro value",
      "from": "Kijai"
    },
    {
      "creation": "Video extend workflow",
      "type": "workflow",
      "description": "Workflow for extending video length using padding",
      "from": "Kijai"
    },
    {
      "creation": "I2V looping workflow",
      "type": "workflow",
      "description": "Image to video workflow with looping capability",
      "from": "Gleb Tretyak"
    },
    {
      "creation": "First community LoRA",
      "type": "lora",
      "description": "Community member's first LoRA creation for LTX2",
      "from": "627140525916422145"
    },
    {
      "creation": "Kijai's distilled LoRA",
      "type": "lora",
      "description": "Community-created distilled LoRA as alternative to original",
      "from": "David Snow"
    },
    {
      "creation": "fp16 to bf16 conversion node",
      "type": "node",
      "description": "Fixed node to handle dtype conversion issues",
      "from": "KingGore2023"
    },
    {
      "creation": "Modified multi-keyframe workflow",
      "type": "workflow",
      "description": "Modified version of KJ's original test workflow for multi-keyframe generation",
      "from": "The Shadow (NYC)"
    },
    {
      "creation": "GGUF workflow with switches",
      "type": "workflow",
      "description": "Workflow that switches between different GGUF models",
      "from": "buggz"
    },
    {
      "creation": "Low VRAM adjusted workflow",
      "type": "workflow",
      "description": "Workflow adjusted to run on 16GB VRAM systems",
      "from": "611243496753594371"
    },
    {
      "creation": "KJNodes memory monitoring",
      "type": "tool",
      "description": "Nodes for recording and visualizing VRAM usage with HTML reports, requires --disable-cuda-malloc flag",
      "from": "Kijai"
    },
    {
      "creation": "Earth Zoom Out LoRA",
      "type": "lora",
      "description": "LoRA for creating earth zoom out effects, trained and shared by community member",
      "from": "627140525916422145"
    },
    {
      "creation": "Video extension workflow",
      "type": "workflow",
      "description": "Complete workflow for extending videos with audio using masking techniques",
      "from": "Elvaxorn"
    },
    {
      "creation": "Fast Film Grain node",
      "type": "node",
      "description": "Adds film grain to images to improve I2V motion generation",
      "from": "VRGameDevGirl84(RTX 5090)"
    },
    {
      "creation": "Detail Daemon",
      "type": "node",
      "description": "Provides free quality improvement, particularly useful for image models",
      "from": "David Snow"
    },
    {
      "creation": "LTXVSetAudioVideoMaskByTime",
      "type": "node",
      "description": "Node for masking audio and video by time ranges, available in KJ nodes",
      "from": "Elvaxorn"
    },
    {
      "creation": "Multiple LTX2 LoRAs",
      "type": "lora",
      "description": "RollTransition, POVObject, PushToGlass, HeroCam, AnimateDiff style LoRAs",
      "from": "NebSH"
    },
    {
      "creation": "LTX2 Rapid Merges",
      "type": "model",
      "description": "Model merges including less distilled + detail versions",
      "from": "Phr00t"
    },
    {
      "creation": "Beavis and Butthead LoRA",
      "type": "lora",
      "description": "LoRA training at step 8000, should reach 20000 steps, audio sounds pretty good and can associate correct voices by name",
      "from": "627140525916422145"
    },
    {
      "creation": "LTX2-Infinity workflow",
      "type": "workflow",
      "description": "Infinite length video workflow for creating extended video sequences",
      "from": "ZombieMatrix"
    },
    {
      "creation": "Hero Shot LoRA",
      "type": "lora",
      "description": "LoRA for cinematic hero shot style generations",
      "from": "NebSH"
    },
    {
      "creation": "LTX2 memory optimization code",
      "type": "code",
      "description": "Modified av_model.py that reduces I2V VRAM usage while maintaining identical output",
      "from": "Kijai"
    },
    {
      "creation": "Infinite length workflow for LTX2",
      "type": "workflow",
      "description": "Work-in-progress workflow for generating very long videos with anchor image method",
      "from": "ZombieMatrix"
    },
    {
      "creation": "Sage attention patch node",
      "type": "node",
      "description": "Patches forward function in FeedForward class as model patch, active only when node is active",
      "from": "Kijai"
    },
    {
      "creation": "VRAM chunking node",
      "type": "node",
      "description": "Reduces VRAM usage up to 1GB at higher resolutions but may change output with fp8 fast",
      "from": "Kijai"
    }
  ]
}